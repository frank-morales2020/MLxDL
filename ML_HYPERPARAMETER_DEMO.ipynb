{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJbJOROs8wDq+GSpGkb+TT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/ML_HYPERPARAMETER_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm1gR31LorHP",
        "outputId": "cf9dc3f5-4d18-4cc3-a2d9-bc9ceded9eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Machine Learning Hyperparameter Demo (Scikit-learn) ---\n",
            "\n",
            "1. Linear Regression (Ridge):\n",
            "   Hyperparams: alpha=10.0, solver='cholesky'\n",
            "   R^2 score: 0.96\n",
            "\n",
            "2. Logistic Regression:\n",
            "   Hyperparams: penalty='l2', C=0.1, solver='liblinear', class_weight='balanced'\n",
            "   Accuracy: 0.82\n",
            "\n",
            "3. Naive Bayes (Gaussian):\n",
            "   Hyperparams: var_smoothing=1e-08\n",
            "   Accuracy: 0.98\n",
            "\n",
            "4. Decision Tree:\n",
            "   Hyperparams: criterion='gini', max_depth=4, min_samples_split=5\n",
            "   Accuracy: 1.00\n",
            "\n",
            "5. Random Forest:\n",
            "   Hyperparams: n_estimators=150, max_depth=8, max_features='sqrt', criterion='entropy'\n",
            "   Accuracy: 1.00\n",
            "\n",
            "6. Gradient Boosted Trees:\n",
            "   Hyperparams: n_estimators=100, learning_rate=0.05, max_depth=3, min_samples_split=10\n",
            "   Accuracy: 1.00\n",
            "\n",
            "7. K-Nearest Neighbor:\n",
            "   Hyperparams: n_neighbors=7, algorithm='kd_tree'\n",
            "   Accuracy: 1.00\n",
            "\n",
            "8. K-Means:\n",
            "   Hyperparams: n_clusters=4, init='k-means++', max_iter=500\n",
            "   Silhouette Score (higher is better): 0.67\n",
            "\n",
            "9. Principal Component Analysis:\n",
            "   Hyperparams: n_components=2, svd_solver='randomized'\n",
            "   Explained Variance Ratio: 0.96 (Variance captured by 2 components)\n",
            "\n",
            "10. Dense Neural Networks (MLP):\n",
            "    Hyperparams: hidden_layer_sizes=(10, 5), activation='relu', solver='adam', alpha=0.001, learning_rate_init=0.01\n",
            "    Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, make_regression, make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import silhouette_score # Used to evaluate clustering\n",
        "\n",
        "# --- Data Preparation ---\n",
        "# 1. Classification Data (Iris)\n",
        "iris = load_iris()\n",
        "X_clf, y_clf = iris.data, iris.target\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Regression Data\n",
        "X_reg, y_reg = make_regression(n_samples=100, n_features=4, noise=10, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Clustering/Unsupervised Data\n",
        "X_cluster, y_cluster = make_blobs(n_samples=150, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# 4. Scaled Data (Required for KNN, PCA, MLP)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_clf)\n",
        "X_test_scaled = scaler.transform(X_test_clf)\n",
        "X_pca_scaled = scaler.fit_transform(X_clf)\n",
        "\n",
        "\n",
        "print(\"--- Machine Learning Hyperparameter Demo (Scikit-learn) ---\")\n",
        "\n",
        "# =================================================================\n",
        "# 1. Linear Regression (using Ridge for L1/L2 Penalty demo)\n",
        "# =================================================================\n",
        "model_lr = Ridge(alpha=10.0, solver='cholesky', fit_intercept=True)\n",
        "model_lr.fit(X_train_reg, y_train_reg)\n",
        "print(f\"\\n1. Linear Regression (Ridge):\")\n",
        "print(f\"   Hyperparams: alpha={model_lr.alpha}, solver='{model_lr.solver}'\")\n",
        "print(f\"   R^2 score: {model_lr.score(X_test_reg, y_test_reg):.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 2. Logistic Regression\n",
        "# =================================================================\n",
        "model_logreg = LogisticRegression(penalty='l2', C=0.1, solver='liblinear', class_weight='balanced', random_state=42)\n",
        "model_logreg.fit(X_train_clf, y_train_clf)\n",
        "print(f\"\\n2. Logistic Regression:\")\n",
        "print(f\"   Hyperparams: penalty='{model_logreg.penalty}', C={model_logreg.C}, solver='{model_logreg.solver}', class_weight='{model_logreg.class_weight}'\")\n",
        "print(f\"   Accuracy: {model_logreg.score(X_test_clf, y_test_clf):.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 3. Naive Bayes (Gaussian)\n",
        "# =================================================================\n",
        "# var_smoothing in GaussianNB acts as the Alpha (Laplace/Lidstone smoothing) prior\n",
        "model_nb = GaussianNB(var_smoothing=1e-08)\n",
        "model_nb.fit(X_train_clf, y_train_clf)\n",
        "print(f\"\\n3. Naive Bayes (Gaussian):\")\n",
        "print(f\"   Hyperparams: var_smoothing={model_nb.var_smoothing}\")\n",
        "print(f\"   Accuracy: {model_nb.score(X_test_clf, y_test_clf):.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 4. Decision Tree\n",
        "# =================================================================\n",
        "model_dt = DecisionTreeClassifier(criterion='gini', max_depth=4, min_samples_split=5, random_state=42)\n",
        "model_dt.fit(X_train_clf, y_train_clf)\n",
        "print(f\"\\n4. Decision Tree:\")\n",
        "print(f\"   Hyperparams: criterion='{model_dt.criterion}', max_depth={model_dt.max_depth}, min_samples_split={model_dt.min_samples_split}\")\n",
        "print(f\"   Accuracy: {model_dt.score(X_test_clf, y_test_clf):.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 5. Random Forest\n",
        "# =================================================================\n",
        "model_rf = RandomForestClassifier(n_estimators=150, max_depth=8, max_features='sqrt', criterion='entropy', random_state=42)\n",
        "model_rf.fit(X_train_clf, y_train_clf)\n",
        "print(f\"\\n5. Random Forest:\")\n",
        "print(f\"   Hyperparams: n_estimators={model_rf.n_estimators}, max_depth={model_rf.max_depth}, max_features='{model_rf.max_features}', criterion='{model_rf.criterion}'\")\n",
        "print(f\"   Accuracy: {model_rf.score(X_test_clf, y_test_clf):.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 6. Gradient Boosted Trees\n",
        "# =================================================================\n",
        "model_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, min_samples_split=10, random_state=42)\n",
        "model_gb.fit(X_train_clf, y_train_clf)\n",
        "print(f\"\\n6. Gradient Boosted Trees:\")\n",
        "print(f\"   Hyperparams: n_estimators={model_gb.n_estimators}, learning_rate={model_gb.learning_rate}, max_depth={model_gb.max_depth}, min_samples_split={model_gb.min_samples_split}\")\n",
        "print(f\"   Accuracy: {model_gb.score(X_test_clf, y_test_clf):.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 7. K-Nearest Neighbor (KNN)\n",
        "# =================================================================\n",
        "model_knn = KNeighborsClassifier(n_neighbors=7, algorithm='kd_tree')\n",
        "model_knn.fit(X_train_scaled, y_train_clf)\n",
        "print(f\"\\n7. K-Nearest Neighbor:\")\n",
        "print(f\"   Hyperparams: n_neighbors={model_knn.n_neighbors}, algorithm='{model_knn.algorithm}'\")\n",
        "print(f\"   Accuracy: {model_knn.score(X_test_scaled, y_test_clf):.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 8. K-Means (Unsupervised)\n",
        "# =================================================================\n",
        "model_kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=500, n_init='auto', random_state=42)\n",
        "model_kmeans.fit(X_cluster)\n",
        "score = silhouette_score(X_cluster, model_kmeans.labels_) # Evaluation metric for clustering\n",
        "print(f\"\\n8. K-Means:\")\n",
        "print(f\"   Hyperparams: n_clusters={model_kmeans.n_clusters}, init='{model_kmeans.init}', max_iter={model_kmeans.max_iter}\")\n",
        "print(f\"   Silhouette Score (higher is better): {score:.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# 9. Principal Component Analysis (PCA)\n",
        "# =================================================================\n",
        "# PCA is a transformation; the 'explained_variance_ratio_' shows its effectiveness.\n",
        "model_pca = PCA(n_components=2, svd_solver='randomized')\n",
        "model_pca.fit(X_pca_scaled)\n",
        "print(f\"\\n9. Principal Component Analysis:\")\n",
        "print(f\"   Hyperparams: n_components={model_pca.n_components}, svd_solver='{model_pca.svd_solver}'\")\n",
        "print(f\"   Explained Variance Ratio: {model_pca.explained_variance_ratio_.sum():.2f} (Variance captured by 2 components)\")\n",
        "\n",
        "# =================================================================\n",
        "# 10. Dense Neural Networks (MLPClassifier)\n",
        "# =================================================================\n",
        "model_nn = MLPClassifier(\n",
        "    hidden_layer_sizes=(10, 5), # Hidden Layer Sizes\n",
        "    activation='relu',          # Activation\n",
        "    solver='adam',              # Solver\n",
        "    alpha=0.001,                # Alpha (L2 regularization)\n",
        "    learning_rate_init=0.01,    # Learning Rate\n",
        "    max_iter=500,               # Max Iterations\n",
        "    random_state=42\n",
        ")\n",
        "model_nn.fit(X_train_scaled, y_train_clf)\n",
        "print(f\"\\n10. Dense Neural Networks (MLP):\")\n",
        "print(f\"    Hyperparams: hidden_layer_sizes={model_nn.hidden_layer_sizes}, activation='{model_nn.activation}', solver='{model_nn.solver}', alpha={model_nn.alpha}, learning_rate_init={model_nn.learning_rate_init}\")\n",
        "print(f\"    Accuracy: {model_nn.score(X_test_scaled, y_test_clf):.2f}\")"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2bjjl1GWJtSYEctHzcEun",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/gemini_ft_vertexai_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install  -q gcsfs==2024.3.1\n",
        "!pip install  -q accelerate==0.31.0\n",
        "!pip install  -q transformers==4.45.2\n",
        "!pip install  -q  datasets==2.19.2\n",
        "!pip install google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai  -q\n",
        "!pip install tensorflow_datasets -q"
      ],
      "metadata": {
        "id": "aLWo2TiXxtP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.cloud import storage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "TRAIN_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_sequences.jsonl\"\n",
        "VALIDATION_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_sequences.jsonl\""
      ],
      "metadata": {
        "id": "zsz3voHfx8fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently Supported Models for Supervised Fine-Tuning (SFT) via the vertexai.preview.tuning.sft module and Generative AI Studio:\n",
        "\n",
        "* Gemini 2.0 Flash-Lite (gemini-2.0-flash-lite-001)\n",
        "* Gemini 2.0 Flash (gemini-2.0-flash-001)\n",
        "* Gemini 1.5 Flash (gemini-1.5-flash-002)\n",
        "* Gemini 1.5 Pro (gemini-1.5-pro-002)\n",
        "* Gemini 1.0 Pro (gemini-1.0-pro-002)"
      ],
      "metadata": {
        "id": "T8p_GB7i0ddU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# --- Data Loading from Google Drive ---\n",
        "zip_path = '/content/gdrive/MyDrive/datasets/CMAPSSData.zip'\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(f\"Error: CMAPSSData.zip not found at {zip_path}. Please ensure the file is correctly located in your Google Drive.\")\n",
        "    raise FileNotFoundError(f\"CMAPSSData.zip not found at {zip_path}\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        if zip_ref.testzip() is None:  # Check for ZIP file integrity\n",
        "            zip_ref.extractall(extract_dir)\n",
        "            print(f\"Extracted dataset files to: {extract_dir}\")\n",
        "        else:\n",
        "            print(\"Error: ZIP file integrity check failed. The file may not be a valid ZIP file.\")\n",
        "            raise zipfile.BadZipFile(\"ZIP file integrity check failed.\")\n",
        "\n",
        "except zipfile.BadZipFile as e:\n",
        "    print(f\"Error extracting ZIP file: {e}\")\n",
        "    print(\n",
        "        \"The uploaded file may not be a valid or complete ZIP file. \"\n",
        "        \"Please ensure you have uploaded the correct file, that it is not corrupted, \"\n",
        "        \"and that it is a standard ZIP archive.\"\n",
        "    )\n",
        "    raise  # Stop execution if extraction fails\n",
        "\n",
        "# --- Prepare NASA CMAPSS Data and Save to JSONL in GCS ---\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Process all four subsets\n",
        "data_subsets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "\n",
        "for data_subset in data_subsets:\n",
        "    train_file = os.path.join(extract_dir, f'train_{data_subset}.txt')\n",
        "    test_file = os.path.join(extract_dir, f'test_{data_subset}.txt')\n",
        "    rul_file = os.path.join(extract_dir, f'RUL_{data_subset}.txt')\n",
        "\n",
        "    SENSOR_COLUMNS = ['sensor' + str(i).zfill(2) for i in range(1, 22)]\n",
        "    OP_SETTING_COLUMNS = ['op_setting_' + str(i) for i in range(1, 4)]\n",
        "    DATA_COLUMNS = ['unit_nr', 'time_cycles'] + OP_SETTING_COLUMNS + SENSOR_COLUMNS\n",
        "\n",
        "    # Load training data\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        test_df = pd.read_csv(test_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        rul_df = pd.read_csv(rul_file, names=['RUL'], delim_whitespace=True, header=None)\n",
        "\n",
        "        train_df.columns = DATA_COLUMNS\n",
        "        test_df.columns = DATA_COLUMNS\n",
        "\n",
        "        print(f\"\\nProcessing data subset: {data_subset}\")\n",
        "        print(\"Shape of train_df after loading:\", train_df.shape)\n",
        "        print(\"train_df head after loading:\\n\", train_df.head())\n",
        "        print(\"Shape of test_df:\", test_df.shape)\n",
        "        print(\"test_df head after loading:\\n\", test_df.head())\n",
        "        print(\"Shape of RUL data:\", rul_df.shape)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data files for subset {data_subset}: {e}\")\n",
        "        raise  # Stop execution if a file is missing\n",
        "\n",
        "    def create_jsonl(df, rul_df, output_path, sequence_length=30, is_test=False):\n",
        "        grouped_data = df.groupby('unit_nr')\n",
        "        rul_values = rul_df.values.tolist()  # Convert RUL DataFrame to list\n",
        "        engine_count = 0  # To track which RUL value to use\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            for unit_nr, unit_data in grouped_data:\n",
        "                num_cycles = len(unit_data)\n",
        "                data_values = unit_data.drop(['unit_nr'], axis=1).values.tolist()\n",
        "                json_data = []  # Initialize an empty list to hold JSON objects\n",
        "\n",
        "                for i in range(max(0, num_cycles - sequence_length + 1)):\n",
        "                    sequence = data_values[i:i + sequence_length]\n",
        "                    rul = num_cycles - (i + sequence_length)\n",
        "\n",
        "                    # Ensure RUL is not out of bounds\n",
        "                    if engine_count < len(rul_values):\n",
        "                        current_rul = rul_values[engine_count][0]  # Get the RUL value\n",
        "                    else:\n",
        "                        current_rul = 0  # Or some default value if RUL data is exhausted\n",
        "\n",
        "                    if len(sequence) == sequence_length:\n",
        "                        json_record = {\"sequence\": sequence, \"sequence_length\": len(sequence), \"rul\": current_rul}  # Include sequence length\n",
        "                        json_data.append(json_record)\n",
        "\n",
        "                # Write all JSON objects to the file at once\n",
        "                with open(output_path, 'w') as f:\n",
        "                    for json_record in json_data:\n",
        "                        f.write(json.dumps(json_record) + '\\n')\n",
        "\n",
        "                engine_count += 1  # Increment engine counter\n",
        "\n",
        "    local_train_jsonl_path = f\"cmapss_{data_subset}_train_sequences.jsonl\"\n",
        "    local_test_jsonl_path = f\"cmapss_{data_subset}_test_sequences.jsonl\"\n",
        "\n",
        "    # Create JSONL for training\n",
        "    create_jsonl(train_df, rul_df, local_train_jsonl_path, is_test=False)\n",
        "    print(f\"Created {local_train_jsonl_path}\")\n",
        "\n",
        "    # Create JSONL for testing\n",
        "    create_jsonl(test_df, rul_df, local_test_jsonl_path, is_test=True)\n",
        "    print(f\"Created {local_test_jsonl_path}\")\n",
        "\n",
        "    # --- Upload JSONL files to GCS ---\n",
        "    client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "    blob_train = bucket.blob(f\"cmapss_{data_subset}_train_sequences.jsonl\")  # Adapt to your naming scheme\n",
        "    blob_test = bucket.blob(f\"cmapss_{data_subset}_test_sequences.jsonl\")   # Adapt to your naming scheme\n",
        "\n",
        "    blob_train.upload_from_filename(local_train_jsonl_path)\n",
        "    print(f\"Uploaded training data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_train_sequences.jsonl\")\n",
        "\n",
        "    blob_test.upload_from_filename(local_test_jsonl_path)\n",
        "    print(f\"Uploaded evaluation data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_test_sequences.jsonl\")\n",
        "\n",
        "print(\"JSONL files created and uploaded.\")"
      ],
      "metadata": {
        "id": "ef-tr2sA3UBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def create_textual_dataset(input_file, output_file):\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                sequence = data.get(\"sequence\")\n",
        "                rul = data.get(\"rul\") # Assuming your data has an RUL\n",
        "\n",
        "                if sequence:\n",
        "                    # Create a simple textual description (you can make this more sophisticated)\n",
        "                    description = f\"Engine sensor readings over time: {np.array(sequence).flatten().tolist()}\"\n",
        "                    if rul is not None:\n",
        "                        output_data = {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": description}]}, {\"role\": \"model\", \"parts\": [{\"text\": f\"Remaining Useful Life: {rul}\"}]}]}\n",
        "                        outfile.write(json.dumps(output_data) + '\\n')\n",
        "                    else:\n",
        "                        output_data = {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": description}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"RUL prediction needed.\"}]}]}\n",
        "                        outfile.write(json.dumps(output_data) + '\\n')\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Define your input and output file paths\n",
        "input_train_file = \"cmapss_FD004_train_sequences.jsonl\"\n",
        "output_train_file_text = \"cmapss_FD004_train_text.jsonl\"\n",
        "\n",
        "input_test_file = \"cmapss_FD004_test_sequences.jsonl\"\n",
        "output_test_file_text = \"cmapss_FD004_test_text.jsonl\"\n",
        "\n",
        "# Create the textual datasets\n",
        "create_textual_dataset(input_train_file, output_train_file_text)\n",
        "create_textual_dataset(input_test_file, output_test_file_text)\n",
        "\n",
        "print(f\"Textual training data created: {output_train_file_text}\")\n",
        "print(f\"Textual testing data created: {output_test_file_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_DR3gIV19Vb",
        "outputId": "1d50a047-1f8c-4483-f209-34cd180c92ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual training data created: cmapss_FD004_train_text.jsonl\n",
            "Textual testing data created: cmapss_FD004_test_text.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp cmapss_FD004_train_text.jsonl gs://{BUCKET_NAME}/\n",
        "!gsutil cp cmapss_FD004_test_text.jsonl gs://{BUCKET_NAME}/"
      ],
      "metadata": {
        "id": "SbN2_-le4aUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.tuning import sft\n",
        "import vertexai\n",
        "import os\n",
        "from google.colab import auth\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Define your tuning parameters\n",
        "BASE_MODEL = \"gemini-1.5-pro-002\"  # Using the specific stable version\n",
        "TRAIN_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_text.jsonl\" # Point to the textual data\n",
        "VALIDATION_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\" # Point to the textual data\n",
        "TUNED_MODEL_DISPLAY_NAME = \"cmapss-text-tuned-gemini-1.5-pro\"\n",
        "EPOCHS = 3  # Adjust as needed\n",
        "LEARNING_RATE_MULTIPLIER = 1.0  # Adjust as needed\n",
        "\n",
        "# Start the fine-tuning job\n",
        "try:\n",
        "    sft_tuning_job = sft.train(\n",
        "        source_model=BASE_MODEL,\n",
        "        train_dataset=TRAIN_DATASET_URI,\n",
        "        validation_dataset=VALIDATION_DATASET_URI,\n",
        "        tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\n",
        "        epochs=EPOCHS,\n",
        "        learning_rate_multiplier=LEARNING_RATE_MULTIPLIER,\n",
        "    )\n",
        "\n",
        "    print(f\"Tuning job started: {sft_tuning_job.resource_name}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please double-check the base model name and your Vertex AI setup.\")"
      ],
      "metadata": {
        "id": "BWuf4aCx3tsf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
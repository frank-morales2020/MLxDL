{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "authorship_tag": "ABX9TyPoHMvZyUkZc06u8ffWvgqV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Phi_2_TPU_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/microsoft/phi-2"
      ],
      "metadata": {
        "id": "wJxxCLGCgT-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.47.0 datasets optimum-tpu==0.2.3 torch-xla==2.5.1 -q"
      ],
      "metadata": {
        "id": "5OssPbIFxC_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OxYEVEcquR4D"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "token=userdata.get('HF_TOKEN')\n",
        "login(token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax-ai-stack==2025.4.9\n",
        "!pip install -Uq \"jax[tpu]==0.5.3\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info orbax-checkpoint==0.11.12\n",
        "!pip install -Uq datasets"
      ],
      "metadata": {
        "id": "ffW2QzdD1Hod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "# Ignore the specific JAX warning about skipped cross-host ArrayMetadata validation\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\".*Skipped cross-host ArrayMetadata validation because only one process is found.*\",\n",
        "    category=UserWarning,  # Or Warning if the category is different\n",
        ")\n",
        "\n",
        "# All necessary imports from the original notebook\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import PartitionSpec as P, NamedSharding\n",
        "from jax import random\n",
        "import jax.nn.initializers as init\n",
        "import jax.nn as nn\n",
        "from jax.lib import xla_bridge\n",
        "from jax.experimental.mesh_utils import create_device_mesh\n",
        "import optax\n",
        "import time\n",
        "import orbax.checkpoint as orbax\n",
        "import numpy as np\n",
        "import shutil\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "import tiktoken\n",
        "import flax.nnx as nnx"
      ],
      "metadata": {
        "id": "ZW_q8m5u2Qx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "mesh = jax.make_mesh((1,), ('batch',))"
      ],
      "metadata": {
        "id": "Xq5lXwip01LO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an array of random values:\n",
        "x = jax.random.normal(jax.random.key(0), (8192, 8192))\n",
        "# and use jax.device_put to distribute it across devices:\n",
        "# Changed PartitionSpec to use the 'batch' axis to match the mesh\n",
        "y = jax.device_put(x, NamedSharding(mesh, P('batch', None)))\n",
        "jax.debug.visualize_array_sharding(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "dfUMuhve0897",
        "outputId": "c599dc49-0d8a-45f2-c25b-5abd19b984ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">          TPU 0          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla\n",
        "device=torch_xla.device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m13fqVAz0SMa",
        "outputId": "b63558b2-f61b-4dc4-ae6f-a47c9ddb4a04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
            "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq datasets bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZV7QL4D4rf-",
        "outputId": "6d370b49-ae9f-4b8b-c865-50880a4369f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bLO289Z9Nen",
        "outputId": "104f28bf-338d-43ff-a3b0-92c2e52755a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/504.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/504.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('WANDB_KEY')\n",
        "import wandb\n",
        "wandb.login(key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tcTnjoC_nVg",
        "outputId": "350b1bf8-682d-40d4-cab7-d070766e5dba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mf2023morales\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import torch_xla.core.xla_model as xm\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Authenticate with Hugging Face Hub using your token.\n",
        "try:\n",
        "    token = userdata.get('HF_TOKEN')\n",
        "    login(token=token)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to login to Hugging Face Hub: {e}\")\n",
        "\n",
        "# Load the Phi-2 model and tokenizer without quantization.\n",
        "# BitsAndBytes is not compatible with TPUs.\n",
        "\n",
        "device = xm.xla_device()\n",
        "print(f\"--- Loading non-quantized Phi-2 on {device} ---\")\n",
        "model_name = \"microsoft/Phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "# Load the model with BFloat16 precision, which is a good balance of memory and performance\n",
        "# for TPUs, and then move it to the XLA device.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare the model for PEFT training\n",
        "model.config.use_cache = False  # Recommended for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"--- Model loaded successfully! ---\")\n",
        "\n",
        "# Load and format the dataset.\n",
        "print(\"--- Loading dataset... ---\")\n",
        "# Split the dataset into train and test sets to have a proper evaluation\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "split_dataset = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "def format_data(examples):\n",
        "    formatted_texts = []\n",
        "    for instruction, context, response in zip(examples['instruction'], examples['context'], examples['response']):\n",
        "        formatted_text = f\"Instruction: {instruction}\\n\"\n",
        "        if context:\n",
        "            formatted_text += f\"Context: {context}\\n\"\n",
        "        formatted_text += f\"Response: {response}\"\n",
        "        formatted_texts.append(formatted_text)\n",
        "    return {\"text\": formatted_texts}\n",
        "\n",
        "formatted_train_dataset = train_dataset.map(format_data, batched=True)\n",
        "formatted_eval_dataset = eval_dataset.map(format_data, batched=True)\n",
        "\n",
        "# Tokenize the dataset and add labels for the trainer.\n",
        "def tokenize_function(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_train_dataset = formatted_train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval_dataset = formatted_eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define TrainingArguments for TPU and initialize the Trainer.\n",
        "output_dir = \"./phi-2_finetuned\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_torch_xla\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=500,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "iq-1_jG5A-5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting fine-tuning process... ---\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"--- Fine-tuning completed! ---\")\n",
        "\n",
        "# Evaluate and calculate perplexity after training\n",
        "print(\"--- Evaluating model and calculating perplexity... ---\")\n",
        "eval_results = trainer.evaluate()\n",
        "perplexity = math.exp(eval_results['eval_loss'])\n",
        "print(f\"Final Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Final Perplexity: {perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "xdK-Dzg3fThI",
        "outputId": "f4688349-d3f1-4e00-e6e2-2882319f3d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting fine-tuning process... ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250821_164457-p43gqnta</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/f2023morales/huggingface/runs/p43gqnta' target=\"_blank\">easy-cosmos-7</a></strong> to <a href='https://wandb.ai/f2023morales/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/f2023morales/huggingface' target=\"_blank\">https://wandb.ai/f2023morales/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/f2023morales/huggingface/runs/p43gqnta' target=\"_blank\">https://wandb.ai/f2023morales/huggingface/runs/p43gqnta</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='13509' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    3/13509 00:02 < 11:12:43, 0.33 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
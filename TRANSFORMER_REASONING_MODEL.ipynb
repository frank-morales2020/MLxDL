{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP7tcyxqGWPRzIEAvv1ytNk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/TRANSFORMER_REASONING_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "!pip install transformers -q\n",
        "!pip install torch -q"
      ],
      "metadata": {
        "id": "ANmkAB3Q6Fnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports and Setup (Ensure libraries are installed)\n",
        "# !pip install datasets -q\n",
        "# !pip install transformers -q # Still needed for scheduler\n",
        "# !pip install torch -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup # Using scheduler from transformers\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence # Keep pad_sequence for custom batching\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "# 2. Hyperparameters (Adjust as needed, comments added)\n",
        "# Consider increasing model capacity for reasoning tasks\n",
        "D_MODEL = 256  # SUGGESTION: Increase (e.g., 512, 768)\n",
        "NUM_LAYERS = 3 # SUGGESTION: Increase (e.g., 6, 8, 12)\n",
        "D_FF = 512     # SUGGESTION: Increase (e.g., 4 * D_MODEL)\n",
        "\n",
        "# Other hyperparameters from original code\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5 # SUGGESTION: Experiment with lower LR (e.g., 1e-5) and different schedulers for scratch training\n",
        "NUM_EPOCHS = 150    # Reduced default, rely on early stopping\n",
        "NUM_HEADS = 8\n",
        "DROPOUT = 0.1\n",
        "MAX_LEN = 128  # SUGGESTION: Analyze GSM8k lengths, consider increasing (e.g., 256, 512)\n",
        "WARMUP_STEPS = 1000\n",
        "GRADIENT_CLIPPING = 1.0\n",
        "PATIENCE = 10 # For Early Stopping\n",
        "\n",
        "print(\"Hyperparameters set.\")\n",
        "\n",
        "# 3. Load Dataset (Same as original)\n",
        "print(\"Loading GSM8k dataset...\")\n",
        "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "train_dataset = gsm8k_dataset['train']\n",
        "test_dataset = gsm8k_dataset['test']\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "# 4. Vocabulary Creation (Same as original - !!! MAJOR AREA FOR IMPROVEMENT !!!)\n",
        "# SUGGESTION: This custom word-level vocabulary is likely insufficient.\n",
        "# Strongly consider replacing this with a subword tokenizer (e.g., SentencePiece trained on GSM8k, or from Hugging Face).\n",
        "print(\"Building custom vocabulary (NOTE: This is a potential bottleneck)...\")\n",
        "def build_vocabulary(examples):\n",
        "    tokenizer = set()\n",
        "    for example in examples:\n",
        "        text = example['question'] + \" \" + example['answer']\n",
        "        # Simple split - may not handle numbers/symbols well\n",
        "        tokenizer.update(text.lower().split())\n",
        "    return sorted(list(tokenizer))\n",
        "\n",
        "vocabulary = build_vocabulary(train_dataset)\n",
        "vocab_size = len(vocabulary)\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "\n",
        "# Add special tokens (Same as original)\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "START_TOKEN = \"<start>\"\n",
        "END_TOKEN = \"<end>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "PAD_INDEX = 0\n",
        "START_INDEX = vocab_size\n",
        "END_INDEX = vocab_size + 1\n",
        "UNK_INDEX = vocab_size + 2\n",
        "\n",
        "word_to_index[PAD_TOKEN] = PAD_INDEX\n",
        "word_to_index[START_TOKEN] = START_INDEX\n",
        "word_to_index[END_TOKEN] = END_INDEX\n",
        "word_to_index[UNK_TOKEN] = UNK_INDEX\n",
        "\n",
        "index_to_word[PAD_INDEX] = PAD_TOKEN\n",
        "index_to_word[START_INDEX] = START_TOKEN\n",
        "index_to_word[END_INDEX] = END_TOKEN\n",
        "index_to_word[UNK_INDEX] = UNK_TOKEN\n",
        "\n",
        "updated_vocab_size = len(word_to_index)\n",
        "print(f\"Custom vocabulary built. Size: {updated_vocab_size}\")\n",
        "\n",
        "\n",
        "# 5. Data Processing Function (Same as original - tied to custom vocabulary)\n",
        "# SUGGESTION: Adapt this significantly if using a better tokenizer.\n",
        "def process_example(example, max_len, word_to_index):\n",
        "    question = example['question'].lower().split()\n",
        "    answer = example['answer'].lower().split() # Needs careful handling of GSM8k answer format\n",
        "\n",
        "    question_tokens = [word_to_index.get(word, UNK_INDEX) for word in question]\n",
        "    answer_tokens = [word_to_index.get(word, UNK_INDEX) for word in answer]\n",
        "\n",
        "    # Create source and target sequences with special tokens\n",
        "    src_tokens = [START_INDEX] + question_tokens + [END_INDEX]\n",
        "    # Target input for decoder starts with START, target output ends with END\n",
        "    tgt_input_tokens = [START_INDEX] + answer_tokens\n",
        "    tgt_output_tokens = answer_tokens + [END_INDEX]\n",
        "\n",
        "    # Truncate sequences if they exceed max_len\n",
        "    src_tokens = src_tokens[:max_len]\n",
        "    tgt_input_tokens = tgt_input_tokens[:max_len]\n",
        "    # Ensure target output aligns with target input length after potential truncation\n",
        "    tgt_output_tokens = tgt_output_tokens[:len(tgt_input_tokens)-1] + [END_INDEX] # Match length for loss\n",
        "\n",
        "    # Padding (Source)\n",
        "    src_padding_len = max_len - len(src_tokens)\n",
        "    src_tensor = torch.tensor(src_tokens + [PAD_INDEX] * src_padding_len)\n",
        "\n",
        "    # Padding (Target Input)\n",
        "    tgt_input_padding_len = max_len - len(tgt_input_tokens)\n",
        "    tgt_input_tensor = torch.tensor(tgt_input_tokens + [PAD_INDEX] * tgt_input_padding_len)\n",
        "\n",
        "    # Padding (Target Output - for loss calculation)\n",
        "    tgt_output_padding_len = max_len - len(tgt_output_tokens)\n",
        "    tgt_output_tensor = torch.tensor(tgt_output_tokens + [PAD_INDEX] * tgt_output_padding_len)\n",
        "\n",
        "    return src_tensor, tgt_input_tensor, tgt_output_tensor\n",
        "\n",
        "# 6. Custom Dataset Class (Same as original)\n",
        "class MathDataset(Dataset):\n",
        "    def __init__(self, dataset, max_len, word_to_index):\n",
        "        self.dataset = dataset\n",
        "        self.max_len = max_len\n",
        "        self.word_to_index = word_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.dataset[idx]\n",
        "        return process_example(example, self.max_len, self.word_to_index)\n",
        "\n",
        "# 7. Collate Function (Same as original)\n",
        "def collate_fn(batch):\n",
        "    src_tensors, tgt_in_tensors, tgt_out_tensors = zip(*batch)\n",
        "    src_tensors = pad_sequence(src_tensors, batch_first=True, padding_value=PAD_INDEX)\n",
        "    tgt_in_tensors = pad_sequence(tgt_in_tensors, batch_first=True, padding_value=PAD_INDEX)\n",
        "    tgt_out_tensors = pad_sequence(tgt_out_tensors, batch_first=True, padding_value=PAD_INDEX)\n",
        "    return src_tensors, tgt_in_tensors, tgt_out_tensors\n",
        "\n",
        "# 8. Create DataLoaders (Same as original)\n",
        "print(\"Creating DataLoaders...\")\n",
        "train_math_dataset = MathDataset(train_dataset, MAX_LEN, word_to_index)\n",
        "test_math_dataset = MathDataset(test_dataset, MAX_LEN, word_to_index)\n",
        "\n",
        "train_dataloader = DataLoader(train_math_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_math_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# 9. Transformer Model Definition (Same as original - Consider Pre-LN if increasing layers)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output, attn_probs\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, num_heads, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q_ = self.split_heads(self.W_q(Q))\n",
        "        K_ = self.split_heads(self.W_k(K))\n",
        "        V_ = self.split_heads(self.W_v(V))\n",
        "        output, attn_probs = self.scaled_dot_product_attention(Q_, K_, V_, mask)\n",
        "        output = self.combine_heads(output)\n",
        "        output = self.W_o(output)\n",
        "        return output, attn_probs\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # Post-LN\n",
        "        self.ffn = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model) # Post-LN\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        norm1_output = self.norm1(x + self.dropout(attn_output)) # Residual then Norm\n",
        "        ffn_output = self.ffn(norm1_output)\n",
        "        output = self.norm2(norm1_output + self.dropout(ffn_output)) # Residual then Norm\n",
        "        return output\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.masked_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # Post-LN\n",
        "        self.enc_dec_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model) # Post-LN\n",
        "        self.ffn = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model) # Post-LN\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        masked_attn_output, _ = self.masked_mha(x, x, x, tgt_mask)\n",
        "        norm1_output = self.norm1(x + self.dropout(masked_attn_output))\n",
        "        enc_dec_attn_output, _ = self.enc_dec_mha(norm1_output, enc_output, enc_output, src_mask) # Q=norm1_output, K=enc_output, V=enc_output\n",
        "        norm2_output = self.norm2(norm1_output + self.dropout(enc_dec_attn_output))\n",
        "        ffn_output = self.ffn(norm2_output)\n",
        "        output = self.norm3(norm2_output + self.dropout(ffn_output))\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000): # Default max_len, ensure >= MAX_LEN hyperparameter\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # pe = pe.unsqueeze(0).transpose(0, 1) # Removed this line\n",
        "        self.register_buffer('pe', pe) # pe is now (max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is (batch_size, seq_len, d_model)\n",
        "        # self.pe is (max_len, d_model)\n",
        "        # We want to add the positional encoding for each position in the sequence\n",
        "        # So we need to select the encodings up to seq_len\n",
        "        seq_len = x.size(1)\n",
        "        # Now we can add the positional encodings to the input embeddings\n",
        "        return x + self.pe[:seq_len, :].unsqueeze(0) # Added unsqueeze(0) for batch dimension\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        embedded = self.dropout(self.pos_encoding(self.embedding(src)))\n",
        "        output = embedded\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, mask)\n",
        "        return output\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, enc_output, src_mask, tgt_mask):\n",
        "        embedded = self.dropout(self.pos_encoding(self.embedding(tgt)))\n",
        "        output = embedded\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_output, src_mask, tgt_mask)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # Mask positions that are PAD_INDEX\n",
        "        # Shape: [batch_size, 1, 1, src_len]\n",
        "        src_mask = (src != PAD_INDEX).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        # Mask positions that are PAD_INDEX and subsequent positions\n",
        "        # Padding mask shape: [batch_size, 1, 1, tgt_len]\n",
        "        padding_mask = (tgt != PAD_INDEX).unsqueeze(1).unsqueeze(2)\n",
        "        # Subsequent mask shape: [1, 1, tgt_len, tgt_len]\n",
        "        tgt_len = tgt.size(1)\n",
        "        subsequent_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool().unsqueeze(0).unsqueeze(0)\n",
        "        # Combined mask shape: [batch_size, 1, tgt_len, tgt_len]\n",
        "        tgt_mask = padding_mask & subsequent_mask\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        enc_output = self.encoder(src, src_mask)\n",
        "        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        return output\n",
        "\n",
        "print(\"Transformer model definition complete.\")\n",
        "\n",
        "# 10. Initialize Model, Optimizer, Scheduler, Loss\n",
        "print(\"Initializing model, optimizer, scheduler...\")\n",
        "model = Transformer(updated_vocab_size, updated_vocab_size, D_MODEL, NUM_LAYERS, NUM_HEADS, D_FF, MAX_LEN, DROPOUT)\n",
        "\n",
        "# SUGGESTION: Consider weight initialization schemes if needed (e.g., Xavier)\n",
        "# for p in model.parameters():\n",
        "#     if p.dim() > 1:\n",
        "#         nn.init.xavier_uniform_(p)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01) # Added weight decay explicitly\n",
        "total_steps = len(train_dataloader) * NUM_EPOCHS # Estimate total steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_INDEX) # SUGGESTION: Consider label_smoothing=0.1\n",
        "print(\"Initialization complete.\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 11. Training Loop with Early Stopping\n",
        "print(\"Starting training loop...\")\n",
        "best_eval_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1} Train\")\n",
        "\n",
        "    for batch_idx, (src, tgt_in, tgt_out) in train_progress_bar:\n",
        "        src = src.to(device)\n",
        "        tgt_in = tgt_in.to(device)\n",
        "        tgt_out = tgt_out.to(device) # Shape: [batch_size, max_len]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model\n",
        "        # Input to decoder is tgt_in\n",
        "        output = model(src, tgt_in) # Output shape: [batch_size, max_len, vocab_size]\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        # Loss expects [N, C] and [N]\n",
        "        output_flat = output.view(-1, output.size(-1)) # Shape: [batch_size * max_len, vocab_size]\n",
        "        tgt_out_flat = tgt_out.view(-1)              # Shape: [batch_size * max_len]\n",
        "\n",
        "        loss = criterion(output_flat, tgt_out_flat)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING)\n",
        "        optimizer.step()\n",
        "        scheduler.step() # Step scheduler each step\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        train_progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # --- Evaluation Phase ---\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    eval_progress_bar = tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=f\"Epoch {epoch+1} Eval\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, tgt_in, tgt_out) in eval_progress_bar:\n",
        "            src = src.to(device)\n",
        "            tgt_in = tgt_in.to(device)\n",
        "            tgt_out = tgt_out.to(device)\n",
        "\n",
        "            output = model(src, tgt_in)\n",
        "\n",
        "            output_flat = output.view(-1, output.size(-1))\n",
        "            tgt_out_flat = tgt_out.view(-1)\n",
        "\n",
        "            loss = criterion(output_flat, tgt_out_flat)\n",
        "            total_eval_loss += loss.item()\n",
        "            eval_progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
        "    print(f\"Epoch {epoch+1} Average Evaluation Loss: {avg_eval_loss:.4f}\")\n",
        "\n",
        "    # --- Early Stopping Logic ---\n",
        "    if avg_eval_loss < best_eval_loss:\n",
        "        best_eval_loss = avg_eval_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Optional: Save the best model checkpoint\n",
        "        # torch.save(model.state_dict(), \"best_scratch_model.pth\")\n",
        "        print(f\"Evaluation loss improved to {best_eval_loss:.4f}. Resetting patience.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Evaluation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "        break # Exit the main training loop\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "# Optional: Load the best model if saved\n",
        "# if os.path.exists(\"best_scratch_model.pth\"):\n",
        "#     model.load_state_dict(torch.load(\"best_scratch_model.pth\"))\n",
        "#     print(\"Loaded best model checkpoint for inference.\")\n",
        "\n",
        "\n",
        "# 12. Inference Function (Adapted from original - greedy decoding)\n",
        "def translate_sentence(model_inf, sentence, word_to_index_inf, index_to_word_inf, max_len_inf, device_inf):\n",
        "    model_inf.eval()\n",
        "\n",
        "    # Tokenize input sentence\n",
        "    tokens = [word_to_index_inf.get(word.lower(), UNK_INDEX) for word in sentence.split()]\n",
        "    src_tokens = [START_INDEX] + tokens + [END_INDEX]\n",
        "    src_tokens = src_tokens[:max_len_inf] # Truncate\n",
        "\n",
        "    # Pad source sequence\n",
        "    src_padding_len = max_len_inf - len(src_tokens)\n",
        "    src_tensor = torch.tensor(src_tokens + [PAD_INDEX] * src_padding_len).unsqueeze(0).to(device_inf) # Add batch dim\n",
        "\n",
        "    # Generate target sequence step-by-step (greedy)\n",
        "    tgt_tokens = [START_INDEX]\n",
        "    for i in range(max_len_inf - 1): # Max output length\n",
        "        tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0).to(device_inf) # Add batch dim\n",
        "\n",
        "        # Create masks for current input\n",
        "        # src_mask and tgt_mask shapes need to be correct for model\n",
        "        # Assuming batch_first=True was handled consistently:\n",
        "        src_mask = model_inf.make_src_mask(src_tensor)\n",
        "        tgt_mask = model_inf.make_tgt_mask(tgt_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Encoder output only needs to be computed once\n",
        "            if i == 0:\n",
        "                enc_output = model_inf.encoder(src_tensor, src_mask)\n",
        "\n",
        "            # Decoder output for the current target sequence\n",
        "            output = model_inf.decoder(tgt_tensor, enc_output, src_mask, tgt_mask)\n",
        "            # output shape: [1, current_tgt_len, vocab_size]\n",
        "\n",
        "        # Get the prediction for the last token\n",
        "        pred_token = output.argmax(2)[:, -1].item()\n",
        "        tgt_tokens.append(pred_token)\n",
        "\n",
        "        # Stop if END token is predicted\n",
        "        if pred_token == END_INDEX:\n",
        "            break\n",
        "\n",
        "    # Convert token IDs back to words\n",
        "    translated_words = [index_to_word_inf.get(token, UNK_TOKEN) for token in tgt_tokens if token != START_INDEX and token != END_INDEX]\n",
        "    return \" \".join(translated_words)\n",
        "\n",
        "\n",
        "# 13. Example Inference (Using the trained scratch model)\n",
        "print(\"\\nRunning example inference with the trained scratch model...\")\n",
        "\n",
        "# Example question from the dataset (index 10)\n",
        "sample_question = test_dataset[10]['question']\n",
        "actual_answer = test_dataset[10]['answer']\n",
        "\n",
        "# Generate prediction using the inference function\n",
        "predicted_answer = translate_sentence(model, sample_question, word_to_index, index_to_word, MAX_LEN, device)\n",
        "\n",
        "print(\"\\n--- Example Inference (From-Scratch Model) ---\")\n",
        "print(f\"Question: {sample_question}\")\n",
        "print(f\"Actual Answer:\\n{actual_answer}\")\n",
        "print(\"-\" * 20)\n",
        "print(f\"Predicted Answer:\\n{predicted_answer}\") # Likely still poor without major changes\n",
        "print(\"-\" * 20)\n",
        "\n",
        "print(\"Code execution structure complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahRJ7IUue9mZ",
        "outputId": "3d815351-cc26-4880-d8c4-184484dd602a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n",
            "Hyperparameters set.\n",
            "Loading GSM8k dataset...\n",
            "Dataset loaded.\n",
            "Building custom vocabulary (NOTE: This is a potential bottleneck)...\n",
            "Custom vocabulary built. Size: 49241\n",
            "Creating DataLoaders...\n",
            "DataLoaders created.\n",
            "Transformer model definition complete.\n",
            "Initializing model, optimizer, scheduler...\n",
            "Initialization complete.\n",
            "Using device: cuda\n",
            "Starting training loop...\n",
            "\n",
            "--- Epoch 1/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Train: 100%|██████████| 234/234 [00:24<00:00,  9.63it/s, loss=10.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Training Loss: 10.8780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.88it/s, loss=10.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Evaluation Loss: 10.6785\n",
            "Evaluation loss improved to 10.6785. Resetting patience.\n",
            "\n",
            "--- Epoch 2/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Train: 100%|██████████| 234/234 [00:23<00:00,  9.77it/s, loss=9.97]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Average Training Loss: 10.3369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.74it/s, loss=10.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Average Evaluation Loss: 10.0151\n",
            "Evaluation loss improved to 10.0151. Resetting patience.\n",
            "\n",
            "--- Epoch 3/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Train: 100%|██████████| 234/234 [00:24<00:00,  9.74it/s, loss=9.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Average Training Loss: 9.7518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.56it/s, loss=9.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Average Evaluation Loss: 9.5507\n",
            "Evaluation loss improved to 9.5507. Resetting patience.\n",
            "\n",
            "--- Epoch 4/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Train: 100%|██████████| 234/234 [00:24<00:00,  9.70it/s, loss=9.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Average Training Loss: 9.2849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.25it/s, loss=9.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Average Evaluation Loss: 9.0808\n",
            "Evaluation loss improved to 9.0808. Resetting patience.\n",
            "\n",
            "--- Epoch 5/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Train: 100%|██████████| 234/234 [00:24<00:00,  9.69it/s, loss=8.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Average Training Loss: 8.7642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.52it/s, loss=8.76]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Average Evaluation Loss: 8.5656\n",
            "Evaluation loss improved to 8.5656. Resetting patience.\n",
            "\n",
            "--- Epoch 6/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Train: 100%|██████████| 234/234 [00:24<00:00,  9.66it/s, loss=7.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Average Training Loss: 8.2469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.42it/s, loss=8.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Average Evaluation Loss: 8.1065\n",
            "Evaluation loss improved to 8.1065. Resetting patience.\n",
            "\n",
            "--- Epoch 7/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Train: 100%|██████████| 234/234 [00:24<00:00,  9.64it/s, loss=7.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Average Training Loss: 7.8077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.40it/s, loss=7.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Average Evaluation Loss: 7.7385\n",
            "Evaluation loss improved to 7.7385. Resetting patience.\n",
            "\n",
            "--- Epoch 8/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Train: 100%|██████████| 234/234 [00:24<00:00,  9.63it/s, loss=7.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Average Training Loss: 7.4784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.30it/s, loss=7.63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Average Evaluation Loss: 7.5010\n",
            "Evaluation loss improved to 7.5010. Resetting patience.\n",
            "\n",
            "--- Epoch 9/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Train: 100%|██████████| 234/234 [00:24<00:00,  9.63it/s, loss=7.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Average Training Loss: 7.2709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.28it/s, loss=7.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Average Evaluation Loss: 7.3523\n",
            "Evaluation loss improved to 7.3523. Resetting patience.\n",
            "\n",
            "--- Epoch 10/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Train: 100%|██████████| 234/234 [00:24<00:00,  9.56it/s, loss=7.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Average Training Loss: 7.1280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.01it/s, loss=7.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Average Evaluation Loss: 7.2376\n",
            "Evaluation loss improved to 7.2376. Resetting patience.\n",
            "\n",
            "--- Epoch 11/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Train: 100%|██████████| 234/234 [00:24<00:00,  9.57it/s, loss=7.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Average Training Loss: 7.0091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.15it/s, loss=7.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Average Evaluation Loss: 7.1345\n",
            "Evaluation loss improved to 7.1345. Resetting patience.\n",
            "\n",
            "--- Epoch 12/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Train: 100%|██████████| 234/234 [00:24<00:00,  9.58it/s, loss=6.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Average Training Loss: 6.9038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.75it/s, loss=7.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Average Evaluation Loss: 7.0406\n",
            "Evaluation loss improved to 7.0406. Resetting patience.\n",
            "\n",
            "--- Epoch 13/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Train: 100%|██████████| 234/234 [00:24<00:00,  9.57it/s, loss=6.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Average Training Loss: 6.8156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.22it/s, loss=7.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Average Evaluation Loss: 6.9625\n",
            "Evaluation loss improved to 6.9625. Resetting patience.\n",
            "\n",
            "--- Epoch 14/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Train: 100%|██████████| 234/234 [00:24<00:00,  9.57it/s, loss=6.87]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Average Training Loss: 6.7219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.18it/s, loss=6.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Average Evaluation Loss: 6.8643\n",
            "Evaluation loss improved to 6.8643. Resetting patience.\n",
            "\n",
            "--- Epoch 15/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Train: 100%|██████████| 234/234 [00:24<00:00,  9.55it/s, loss=6.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Average Training Loss: 6.6354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.04it/s, loss=6.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Average Evaluation Loss: 6.8078\n",
            "Evaluation loss improved to 6.8078. Resetting patience.\n",
            "\n",
            "--- Epoch 16/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Train: 100%|██████████| 234/234 [00:24<00:00,  9.55it/s, loss=6.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Average Training Loss: 6.5766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.84it/s, loss=6.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Average Evaluation Loss: 6.7585\n",
            "Evaluation loss improved to 6.7585. Resetting patience.\n",
            "\n",
            "--- Epoch 17/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Train: 100%|██████████| 234/234 [00:24<00:00,  9.55it/s, loss=6.42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Average Training Loss: 6.5203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.07it/s, loss=6.75]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Average Evaluation Loss: 6.7139\n",
            "Evaluation loss improved to 6.7139. Resetting patience.\n",
            "\n",
            "--- Epoch 18/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Train: 100%|██████████| 234/234 [00:24<00:00,  9.54it/s, loss=6.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Average Training Loss: 6.4712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.87it/s, loss=6.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Average Evaluation Loss: 6.6752\n",
            "Evaluation loss improved to 6.6752. Resetting patience.\n",
            "\n",
            "--- Epoch 19/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Train: 100%|██████████| 234/234 [00:24<00:00,  9.54it/s, loss=6.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Average Training Loss: 6.4249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.97it/s, loss=6.66]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Average Evaluation Loss: 6.6323\n",
            "Evaluation loss improved to 6.6323. Resetting patience.\n",
            "\n",
            "--- Epoch 20/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Train: 100%|██████████| 234/234 [00:24<00:00,  9.55it/s, loss=6.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Average Training Loss: 6.3778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.18it/s, loss=6.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Average Evaluation Loss: 6.5985\n",
            "Evaluation loss improved to 6.5985. Resetting patience.\n",
            "\n",
            "--- Epoch 21/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Train: 100%|██████████| 234/234 [00:24<00:00,  9.54it/s, loss=6.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Average Training Loss: 6.3337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.10it/s, loss=6.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Average Evaluation Loss: 6.5666\n",
            "Evaluation loss improved to 6.5666. Resetting patience.\n",
            "\n",
            "--- Epoch 22/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Train: 100%|██████████| 234/234 [00:24<00:00,  9.54it/s, loss=6.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Average Training Loss: 6.2943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.11it/s, loss=6.55]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Average Evaluation Loss: 6.5301\n",
            "Evaluation loss improved to 6.5301. Resetting patience.\n",
            "\n",
            "--- Epoch 23/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Train: 100%|██████████| 234/234 [00:24<00:00,  9.55it/s, loss=6.61]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Average Training Loss: 6.2560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.05it/s, loss=6.53]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Average Evaluation Loss: 6.5080\n",
            "Evaluation loss improved to 6.5080. Resetting patience.\n",
            "\n",
            "--- Epoch 24/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Train: 100%|██████████| 234/234 [00:24<00:00,  9.54it/s, loss=6.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Average Training Loss: 6.2168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.87it/s, loss=6.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Average Evaluation Loss: 6.4774\n",
            "Evaluation loss improved to 6.4774. Resetting patience.\n",
            "\n",
            "--- Epoch 25/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=6.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Average Training Loss: 6.1828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.00it/s, loss=6.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Average Evaluation Loss: 6.4549\n",
            "Evaluation loss improved to 6.4549. Resetting patience.\n",
            "\n",
            "--- Epoch 26/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=6.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Average Training Loss: 6.1494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.84it/s, loss=6.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Average Evaluation Loss: 6.4266\n",
            "Evaluation loss improved to 6.4266. Resetting patience.\n",
            "\n",
            "--- Epoch 27/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=6.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Average Training Loss: 6.1162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.86it/s, loss=6.41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Average Evaluation Loss: 6.4050\n",
            "Evaluation loss improved to 6.4050. Resetting patience.\n",
            "\n",
            "--- Epoch 28/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=6.29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Average Training Loss: 6.0865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.03it/s, loss=6.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Average Evaluation Loss: 6.3835\n",
            "Evaluation loss improved to 6.3835. Resetting patience.\n",
            "\n",
            "--- Epoch 29/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Train: 100%|██████████| 234/234 [00:24<00:00,  9.52it/s, loss=6.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Average Training Loss: 6.0539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.83it/s, loss=6.36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Average Evaluation Loss: 6.3609\n",
            "Evaluation loss improved to 6.3609. Resetting patience.\n",
            "\n",
            "--- Epoch 30/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=6.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Average Training Loss: 6.0269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.93it/s, loss=6.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Average Evaluation Loss: 6.3422\n",
            "Evaluation loss improved to 6.3422. Resetting patience.\n",
            "\n",
            "--- Epoch 31/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Train: 100%|██████████| 234/234 [00:24<00:00,  9.52it/s, loss=5.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 Average Training Loss: 5.9965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.12it/s, loss=6.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 Average Evaluation Loss: 6.3266\n",
            "Evaluation loss improved to 6.3266. Resetting patience.\n",
            "\n",
            "--- Epoch 32/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=6.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 Average Training Loss: 5.9708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.08it/s, loss=6.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 Average Evaluation Loss: 6.3071\n",
            "Evaluation loss improved to 6.3071. Resetting patience.\n",
            "\n",
            "--- Epoch 33/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Train: 100%|██████████| 234/234 [00:24<00:00,  9.52it/s, loss=5.91]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 Average Training Loss: 5.9451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.03it/s, loss=6.29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 Average Evaluation Loss: 6.2940\n",
            "Evaluation loss improved to 6.2940. Resetting patience.\n",
            "\n",
            "--- Epoch 34/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Train: 100%|██████████| 234/234 [00:24<00:00,  9.51it/s, loss=5.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 Average Training Loss: 5.9178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.81it/s, loss=6.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 Average Evaluation Loss: 6.2740\n",
            "Evaluation loss improved to 6.2740. Resetting patience.\n",
            "\n",
            "--- Epoch 35/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Train: 100%|██████████| 234/234 [00:24<00:00,  9.51it/s, loss=5.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 Average Training Loss: 5.8930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.80it/s, loss=6.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 Average Evaluation Loss: 6.2633\n",
            "Evaluation loss improved to 6.2633. Resetting patience.\n",
            "\n",
            "--- Epoch 36/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Train: 100%|██████████| 234/234 [00:24<00:00,  9.51it/s, loss=6.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 Average Training Loss: 5.8695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.08it/s, loss=6.24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 Average Evaluation Loss: 6.2452\n",
            "Evaluation loss improved to 6.2452. Resetting patience.\n",
            "\n",
            "--- Epoch 37/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 Average Training Loss: 5.8455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.03it/s, loss=6.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 Average Evaluation Loss: 6.2315\n",
            "Evaluation loss improved to 6.2315. Resetting patience.\n",
            "\n",
            "--- Epoch 38/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Train: 100%|██████████| 234/234 [00:24<00:00,  9.52it/s, loss=5.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 Average Training Loss: 5.8242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.76it/s, loss=6.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 Average Evaluation Loss: 6.2200\n",
            "Evaluation loss improved to 6.2200. Resetting patience.\n",
            "\n",
            "--- Epoch 39/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 Average Training Loss: 5.8013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.96it/s, loss=6.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 Average Evaluation Loss: 6.2065\n",
            "Evaluation loss improved to 6.2065. Resetting patience.\n",
            "\n",
            "--- Epoch 40/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 Average Training Loss: 5.7799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.81it/s, loss=6.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 Average Evaluation Loss: 6.1939\n",
            "Evaluation loss improved to 6.1939. Resetting patience.\n",
            "\n",
            "--- Epoch 41/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41 Train: 100%|██████████| 234/234 [00:24<00:00,  9.52it/s, loss=5.66]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 Average Training Loss: 5.7600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.03it/s, loss=6.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 Average Evaluation Loss: 6.1792\n",
            "Evaluation loss improved to 6.1792. Resetting patience.\n",
            "\n",
            "--- Epoch 42/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 Average Training Loss: 5.7375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.88it/s, loss=6.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 Average Evaluation Loss: 6.1719\n",
            "Evaluation loss improved to 6.1719. Resetting patience.\n",
            "\n",
            "--- Epoch 43/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.67]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 Average Training Loss: 5.7207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.12it/s, loss=6.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 Average Evaluation Loss: 6.1564\n",
            "Evaluation loss improved to 6.1564. Resetting patience.\n",
            "\n",
            "--- Epoch 44/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44 Train: 100%|██████████| 234/234 [00:24<00:00,  9.55it/s, loss=5.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 Average Training Loss: 5.6994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44 Eval: 100%|██████████| 42/42 [00:01<00:00, 29.02it/s, loss=6.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 Average Evaluation Loss: 6.1526\n",
            "Evaluation loss improved to 6.1526. Resetting patience.\n",
            "\n",
            "--- Epoch 45/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.61]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 Average Training Loss: 5.6809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.92it/s, loss=6.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 Average Evaluation Loss: 6.1414\n",
            "Evaluation loss improved to 6.1414. Resetting patience.\n",
            "\n",
            "--- Epoch 46/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.61]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 Average Training Loss: 5.6621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.84it/s, loss=6.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 Average Evaluation Loss: 6.1306\n",
            "Evaluation loss improved to 6.1306. Resetting patience.\n",
            "\n",
            "--- Epoch 47/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47 Train: 100%|██████████| 234/234 [00:24<00:00,  9.54it/s, loss=5.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 Average Training Loss: 5.6443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.90it/s, loss=6.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 Average Evaluation Loss: 6.1194\n",
            "Evaluation loss improved to 6.1194. Resetting patience.\n",
            "\n",
            "--- Epoch 48/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48 Train: 100%|██████████| 234/234 [00:24<00:00,  9.53it/s, loss=5.53]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 Average Training Loss: 5.6276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.84it/s, loss=6.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 Average Evaluation Loss: 6.1146\n",
            "Evaluation loss improved to 6.1146. Resetting patience.\n",
            "\n",
            "--- Epoch 49/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49 Train: 100%|██████████| 234/234 [00:24<00:00,  9.52it/s, loss=5.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 Average Training Loss: 5.6103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49 Eval: 100%|██████████| 42/42 [00:01<00:00, 28.75it/s, loss=6.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 Average Evaluation Loss: 6.1044\n",
            "Evaluation loss improved to 6.1044. Resetting patience.\n",
            "\n",
            "--- Epoch 50/150 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50 Train:  48%|████▊     | 113/234 [00:11<00:12,  9.46it/s, loss=5.85]"
          ]
        }
      ]
    }
  ]
}
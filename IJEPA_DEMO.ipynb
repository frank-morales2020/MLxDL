{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPe21tGCE8F0dj/cg/JHqNm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/IJEPA_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IJEPA paper: https://arxiv.org/abs/2301.08243"
      ],
      "metadata": {
        "id": "oOSTbpXHL2rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBTafoc8LrCV",
        "outputId": "159fd1e3-7682-4c08-b3c5-a156dd98f762"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 31 06:41:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             48W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCKsJ9kyJABP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- 1. Define I-JEPA Components (Conceptual - Optimized for Single GPU PoC) ---\n",
        "\n",
        "class VisionTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Conceptual Vision Transformer (ViT) Encoder.\n",
        "    For this PoC, it simulates a ViT's role without actual complex layers.\n",
        "    The dummy_layer is just to ensure the model has parameters for the optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size=\"ViT-H/14\"):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing {model_size} Vision Transformer Encoder for PoC...\")\n",
        "        self.model_size = model_size\n",
        "        dummy_representation_dim = 1024 # Conceptual output dimension for ViT-H/14 features\n",
        "        # Minimal dummy layer with actual parameters. Output will be based on random input here.\n",
        "        self.encoder_linear_proj = nn.Linear(50, dummy_representation_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        # Generate random input for the dummy linear layer, ensuring it requires grad\n",
        "        dummy_input_for_encoder_linear = torch.randn(batch_size, 196, 50, device=x.device, requires_grad=True)\n",
        "        # Pass through the dummy linear layer to create a computational graph\n",
        "        conceptual_features = self.encoder_linear_proj(dummy_input_for_encoder_linear)\n",
        "        return conceptual_features\n",
        "\n",
        "class IJEPA_Predictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Conceptual I-JEPA Predictor Network.\n",
        "    Simulates the role of the predictor.\n",
        "    \"\"\"\n",
        "    def __init__(self, context_embedding_dim, predictor_embedding_dim=384, depth=12):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing I-JEPA Predictor for PoC (depth={depth}, width={predictor_embedding_dim})...\")\n",
        "        self.depth = depth\n",
        "        self.predictor_embedding_dim = predictor_embedding_dim\n",
        "        # This layer will now process 'context_output' conceptually\n",
        "        self.predictor_linear = nn.Linear(context_embedding_dim, context_embedding_dim)\n",
        "\n",
        "    def forward(self, context_output, mask_tokens_with_positional_embedding):\n",
        "        predicted_output = self.predictor_linear(context_output)\n",
        "\n",
        "        num_target_patches = mask_tokens_with_positional_embedding.shape[1]\n",
        "\n",
        "        return predicted_output[:, :num_target_patches, :].contiguous()\n",
        "\n",
        "# --- 2. Data Loading and Masking (Conceptual) ---\n",
        "\n",
        "class ImageNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Conceptual ImageNet Dataset. Generates dummy image tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "        self.num_dummy_images = 100\n",
        "        print(f\"Initializing ImageNet Dataset with {self.num_dummy_images} dummy images for PoC...\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_dummy_images\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.randn(3, 224, 224)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def i_jepa_mask_sampler(image_tensor_shape: tuple, num_target_blocks: int = 4,\n",
        "                        context_scale_range: tuple = (0.85, 1.0),\n",
        "                        target_scale_range: tuple = (0.15, 0.2)) -> tuple:\n",
        "    \"\"\"\n",
        "    Conceptual I-JEPA Multi-block Masking Strategy.\n",
        "    Generates boolean masks for context and target blocks for dummy data.\n",
        "    \"\"\"\n",
        "    _, H, W = image_tensor_shape\n",
        "    patch_size = 16\n",
        "    num_patches_h, num_patches_w = H // patch_size, W // patch_size\n",
        "    total_patches = num_patches_h * num_patches_w\n",
        "\n",
        "    context_mask = np.zeros(total_patches, dtype=bool)\n",
        "    target_masks = []\n",
        "    target_mask_locations = []\n",
        "\n",
        "    context_start_idx = np.random.randint(0, total_patches // 2)\n",
        "    context_end_idx = min(context_start_idx + int(total_patches * random.uniform(*context_scale_range)), total_patches)\n",
        "    context_mask[context_start_idx:context_end_idx] = True\n",
        "\n",
        "    for _ in range(num_target_blocks):\n",
        "        target_mask = np.zeros(total_patches, dtype=bool)\n",
        "        target_start_idx = np.random.randint(0, total_patches - 5)\n",
        "        target_end_idx = min(target_start_idx + int(total_patches * random.uniform(*target_scale_range)), total_patches)\n",
        "        target_mask[target_start_idx:target_end_idx] = True\n",
        "\n",
        "        target_masks.append(target_mask)\n",
        "        target_mask_locations.append(np.where(target_mask)[0])\n",
        "\n",
        "        # Remove overlapping regions from context\n",
        "        context_mask = context_mask & (~target_mask)\n",
        "\n",
        "    return context_mask, target_masks, target_mask_locations\n",
        "\n",
        "def collate_fn_with_masking(batch: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Conceptual batch collator to apply masking.\n",
        "    Processes dummy images and generates dummy masks.\n",
        "    \"\"\"\n",
        "    images = torch.stack(batch)\n",
        "\n",
        "    batched_context_masks = []\n",
        "    batched_target_masks = []\n",
        "    batched_target_mask_locations = []\n",
        "\n",
        "    for _ in range(images.shape[0]):\n",
        "        context_m, target_ms, target_m_locs = i_jepa_mask_sampler(images.shape[1:])\n",
        "        batched_context_masks.append(context_m)\n",
        "        batched_target_masks.append(target_ms)\n",
        "        batched_target_mask_locations.append(target_m_locs)\n",
        "\n",
        "    return images, batched_context_masks, batched_target_masks, batched_target_mask_locations\n",
        "\n",
        "# --- 3. Training Loop (Conceptual PoC) ---\n",
        "\n",
        "def run_ijepa_poc():\n",
        "    print(\"--- Starting Conceptual I-JEPA PoC Demo on Single GPU ---\")\n",
        "\n",
        "    # --- Device Configuration ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"CUDA not available. Running on CPU. This PoC is intended for GPU demonstration.\")\n",
        "\n",
        "    # Hyperparameters (Conceptual - to make it run, not for actual performance)\n",
        "    epochs = 10\n",
        "    batch_size = 4\n",
        "    learning_rate = 1e-3\n",
        "    weight_decay = 0.01\n",
        "    momentum = 0.996\n",
        "\n",
        "    # Model Initialization (Conceptual)\n",
        "    context_encoder = VisionTransformerEncoder(model_size=\"ViT-H/14\").to(device)\n",
        "    target_encoder = copy.deepcopy(context_encoder).to(device)\n",
        "    predictor = IJEPA_Predictor(context_embedding_dim=1024, depth=12).to(device)\n",
        "\n",
        "    # Set target_encoder to not require gradients initially for EMA update\n",
        "    for param in target_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Optimizer (Conceptual)\n",
        "    optimizer = optim.AdamW(list(context_encoder.parameters()) + list(predictor.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Dataset and DataLoader (Conceptual)\n",
        "    transform = transforms.Compose([\n",
        "        # No transforms.ToTensor() as dummy images are already tensors\n",
        "    ])\n",
        "    dataset = ImageNetDataset(transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_with_masking)\n",
        "\n",
        "    print(f\"\\nConceptual training loop starting for {epochs} epochs with batch size {batch_size}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        context_encoder.train()\n",
        "        predictor.train()\n",
        "\n",
        "        current_momentum = min(momentum + (1.0 - momentum) * (epoch / epochs), 1.0)\n",
        "\n",
        "        for batch_idx, (images, batch_context_masks, batch_target_masks, batch_target_mask_locations) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            context_representations = context_encoder(images)\n",
        "\n",
        "            # The target_encoder's parameters are updated by EMA, not directly by optimizer.\n",
        "            # Its output, conceptual_target_features_full, serves as a 'fixed' target for this step.\n",
        "            with torch.no_grad():\n",
        "                # In real I-JEPA, this is the output of the (momentum) target encoder applied to target patches.\n",
        "                # Here, we generate a conceptually fixed target that the predictor will try to match.\n",
        "                # We make it a slightly offset version of context_representations to create a \"learnable\" task\n",
        "                # that will show a decreasing loss.\n",
        "                conceptual_target_features_full = context_representations + (torch.randn_like(context_representations) * 0.1) # Add small noise\n",
        "                # Detach ensures no gradient flow back through this part if it was accidental.\n",
        "                # For this conceptual model, the requires_grad=True from context_encoder is what matters for loss backward.\n",
        "\n",
        "            total_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "            for i in range(images.shape[0]):\n",
        "                for target_block_idx, target_mask_locs in enumerate(batch_target_mask_locations[i]):\n",
        "                    if len(target_mask_locs) > 0:\n",
        "                        dummy_mask_tokens = torch.randn(1, len(target_mask_locs), predictor.predictor_embedding_dim, device=device)\n",
        "\n",
        "                        predicted_target_block_repr = predictor(context_representations[i:i+1], dummy_mask_tokens)\n",
        "\n",
        "                        # Actual target representation slice for the current batch item and target block\n",
        "                        # This 'target' is now derived from the context_representations plus noise,\n",
        "                        # making it something the predictor can actually learn to approximate.\n",
        "                        actual_target_block_repr = conceptual_target_features_full[i:i+1, torch.tensor(target_mask_locs, device=device)]\n",
        "\n",
        "                        # Calculate L2 loss (Mean Squared Error)\n",
        "                        # Scale down the loss by a factor to make the numbers smaller.\n",
        "                        loss = torch.mean(torch.norm(predicted_target_block_repr - actual_target_block_repr, p=2, dim=-1)**2) * 0.001 # <-- SCALING FACTOR\n",
        "                        total_loss += loss\n",
        "\n",
        "            if total_loss.item() > 0:\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Update target encoder weights via EMA (No gradient tracking for target_encoder params)\n",
        "            with torch.no_grad():\n",
        "                for param_q, param_k in zip(context_encoder.parameters(), target_encoder.parameters()):\n",
        "                    param_k.data.mul_(current_momentum).add_((1 - current_momentum) * param_q.data)\n",
        "\n",
        "            # Print frequently for conceptual demo\n",
        "            if batch_idx % 1 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs}, Batch {batch_idx}, Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "        print(f\"\\nEpoch {epoch} finished. Average Loss: {total_loss.item() / len(dataloader):.4f}\\n\")\n",
        "\n",
        "    print(\"Conceptual I-JEPA PoC pretraining finished.\")\n",
        "    print(\"\\n--- PoC Purpose and Limitations ---\")\n",
        "    print(\"This PoC successfully demonstrates the conceptual flow of I-JEPA training on a GPU,\")\n",
        "    print(\"now with a simulated decreasing loss to illustrate the learning process.\")\n",
        "    print(\"It shows how models and data are moved to the GPU, how the forward/backward passes occur,\")\n",
        "    print(\"and how the optimizer and EMA updates conceptually work.\")\n",
        "    print(\"\\nCrucially, this demo is NOT a substitute for actual I-JEPA pretraining because:\")\n",
        "    print(\"- It uses highly simplified dummy models and dummy data.\")\n",
        "    print(\"- The 'learning' here is a simplified approximation, not real feature extraction from images.\")\n",
        "    print(\"- Full-scale I-JEPA training (e.g., ViT-H/14 on ImageNet) requires significantly more GPU resources\")\n",
        "    print(\"  (e.g., multiple A100s, specialized distributed training setup) than a single A100 40GB can provide for full training.\")\n",
        "    print(\"This PoC validates the *concept* of the training loop on a GPU with a decreasing loss trend, not actual performance or learning capability.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_ijepa_poc()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I-JEPA PoC with ViT-S/16"
      ],
      "metadata": {
        "id": "st11oXbzSzUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# --- 1. Define I-JEPA Components (Executable Conceptual for ViT-S/16 PoC) ---\n",
        "\n",
        "class ExecutableViTEncoderConceptual(nn.Module):\n",
        "    \"\"\"\n",
        "    Executable Conceptual Vision Transformer (ViT) Encoder.\n",
        "    Uses a dummy linear layer to create a runnable computational graph.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, embed_dim=384):\n",
        "        super().__init__()\n",
        "        print(f\"Executable Conceptual: Initializing ViT-S/16 Encoder...\")\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.patch_projection = nn.Linear(3 * patch_size * patch_size, embed_dim)\n",
        "        self.dummy_transformer_block = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x_input_patches_flat):\n",
        "        batch_size = x_input_patches_flat.shape[0]\n",
        "        num_patches_in_input = x_input_patches_flat.shape[1]\n",
        "\n",
        "        projected_patches = self.patch_projection(x_input_patches_flat)\n",
        "        features = self.dummy_transformer_block(projected_patches)\n",
        "\n",
        "        return features.requires_grad_(True)\n",
        "\n",
        "class ExecutableIJEPAPredictorConceptual(nn.Module):\n",
        "    \"\"\"\n",
        "    Executable Conceptual I-JEPA Predictor Network.\n",
        "    Uses dummy linear layers to create a runnable computational graph.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_embed_dim=384, predictor_embed_dim=192):\n",
        "        super().__init__()\n",
        "        print(f\"Executable Conceptual: Initializing I-JEPA Predictor...\")\n",
        "        self.input_embed_dim = input_embed_dim\n",
        "        self.predictor_embed_dim = predictor_embed_dim\n",
        "\n",
        "        self.input_proj = nn.Linear(input_embed_dim, predictor_embed_dim)\n",
        "        self.predict_layer = nn.Linear(predictor_embed_dim, input_embed_dim)\n",
        "\n",
        "    def forward(self, context_features, target_mask_indices_for_predictor_dummy_tokens):\n",
        "        averaged_context_feature = context_features.mean(dim=1)\n",
        "\n",
        "        x = self.input_proj(averaged_context_feature)\n",
        "        predicted_output_base = self.predict_layer(x)\n",
        "\n",
        "        batch_size = predicted_output_base.shape[0]\n",
        "        num_target_patches_in_block = target_mask_indices_for_predictor_dummy_tokens.shape[1]\n",
        "\n",
        "        predicted_expanded = predicted_output_base.unsqueeze(1).repeat(1, num_target_patches_in_block, 1)\n",
        "\n",
        "        return predicted_expanded.requires_grad_(True)\n",
        "\n",
        "# --- 2. Data Loading and Masking Strategy (Executable Conceptual) ---\n",
        "\n",
        "CIFAR100_TRANSFORMS = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "class ExecutableCIFAR100DatasetConceptual(datasets.CIFAR100):\n",
        "    \"\"\"\n",
        "    Conceptual CIFAR-100 dataset that actually loads CIFAR-100 metadata\n",
        "    but simulates the image data to avoid dependency on actual dataset files for portability.\n",
        "    \"\"\"\n",
        "    def __init__(self, root='./data', train=True, download=True, transform=None):\n",
        "        super().__init__(root, train=train, download=True, transform=transform)\n",
        "        print(\"Executable Conceptual: Using CIFAR-100 metadata, but simulating image data.\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dummy_image = Image.new('RGB', (32, 32), color = 'red')\n",
        "        if self.transform:\n",
        "            dummy_image = self.transform(dummy_image)\n",
        "        return dummy_image, 0\n",
        "\n",
        "def i_jepa_collate_fn_executable(batch_list, patch_size=16, img_size=224, num_target_blocks=4,\n",
        "                                 context_scale_range=(0.85, 1.0), target_scale_range=(0.15, 0.2)):\n",
        "\n",
        "    batched_images = torch.stack([item[0] for item in batch_list])\n",
        "    batch_size = batched_images.shape[0]\n",
        "\n",
        "    num_patches_per_image = (img_size // patch_size) ** 2\n",
        "    patch_flat_dim = batched_images.shape[1] * patch_size * patch_size\n",
        "\n",
        "    all_patches_flat_conceptual = torch.randn(batch_size, num_patches_per_image, patch_flat_dim)\n",
        "\n",
        "    batched_context_patch_indices_list = []\n",
        "    batched_target_patch_indices_for_predictor_list = []\n",
        "    batched_target_patch_indices_for_target_encoder_list = []\n",
        "\n",
        "    for _ in range(batch_size):\n",
        "        context_mask_indices, target_block_mask_indices_list, _ = i_jepa_mask_sampler_conceptual(\n",
        "            (3, img_size, img_size), patch_size=patch_size,\n",
        "            num_target_blocks=num_target_blocks,\n",
        "            context_scale_range=context_scale_range,\n",
        "            target_scale_range=target_scale_range\n",
        "        )\n",
        "        batched_context_patch_indices_list.append(torch.tensor(context_mask_indices, dtype=torch.long))\n",
        "        batched_target_patch_indices_for_predictor_list.append([torch.tensor(idx_list, dtype=torch.long) for idx_list in target_block_mask_indices_list])\n",
        "\n",
        "        all_target_indices_for_image = np.concatenate(target_block_mask_indices_list)\n",
        "        batched_target_patch_indices_for_target_encoder_list.append(torch.tensor(all_target_indices_for_image, dtype=torch.long))\n",
        "\n",
        "    return (all_patches_flat_conceptual,\n",
        "            batched_context_patch_indices_list,\n",
        "            batched_target_patch_indices_for_predictor_list,\n",
        "            batched_target_patch_indices_for_target_encoder_list)\n",
        "\n",
        "def i_jepa_mask_sampler_conceptual(image_tensor_shape: tuple, patch_size: int, num_target_blocks: int,\n",
        "                                   context_scale_range: tuple, target_scale_range: tuple) -> tuple:\n",
        "    _, H, W = image_tensor_shape\n",
        "    num_patches = (H // patch_size) * (W // patch_size)\n",
        "    all_patch_indices = list(range(num_patches))\n",
        "\n",
        "    context_mask_indices = []\n",
        "    target_block_mask_indices_list = []\n",
        "\n",
        "    num_context_patches = int(num_patches * random.uniform(*context_scale_range))\n",
        "    context_mask_indices = random.sample(all_patch_indices, num_context_patches)\n",
        "    available_for_targets = [idx for idx in all_patch_indices if idx not in set(context_mask_indices)]\n",
        "\n",
        "    for _ in range(num_target_blocks):\n",
        "        num_target_patches_in_block = int(num_patches * random.uniform(*target_scale_range))\n",
        "        if len(available_for_targets) < num_target_patches_in_block:\n",
        "            selected_target_indices = random.sample(all_patch_indices, num_target_patches_in_block)\n",
        "        else:\n",
        "            selected_target_indices = random.sample(available_for_targets, num_target_patches_in_block)\n",
        "\n",
        "        target_block_mask_indices_list.append(selected_target_indices)\n",
        "        available_for_targets = [idx for idx in available_for_targets if idx not in set(selected_target_indices)]\n",
        "\n",
        "    return context_mask_indices, target_block_mask_indices_list, target_block_mask_indices_list\n",
        "\n",
        "# --- Early Stopping Class (Conceptual - Modified for direct trigger) ---\n",
        "class ConceptualEarlyStopping:\n",
        "    def __init__(self, patience=5, stop_threshold=0.0001, verbose=True):\n",
        "        self.patience = patience\n",
        "        self.stop_threshold = stop_threshold\n",
        "        self.verbose = verbose\n",
        "        self.num_epochs_below_threshold = 0\n",
        "        self.early_stop = False\n",
        "        self.best_loss = None\n",
        "\n",
        "    def __call__(self, current_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = current_loss\n",
        "            if self.verbose:\n",
        "                print(f\"Conceptual EarlyStop: Initial best loss set to {self.best_loss:.6f}\")\n",
        "\n",
        "        if current_loss <= self.stop_threshold:\n",
        "            self.num_epochs_below_threshold += 1\n",
        "            if self.verbose:\n",
        "                print(f\"Conceptual EarlyStop: Loss {current_loss:.8f} is below or at threshold {self.stop_threshold:.8f}. Consecutive epochs: {self.num_epochs_below_threshold}/{self.patience}\")\n",
        "            if self.num_epochs_below_threshold >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(\"Conceptual EarlyStop: Stopping training as loss consistently below threshold.\")\n",
        "        else:\n",
        "            self.num_epochs_below_threshold = 0\n",
        "            if self.verbose:\n",
        "                print(f\"Conceptual EarlyStop: Loss {current_loss:.8f} is above threshold {self.stop_threshold:.8f}. Resetting counter.\")\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "# --- 3. Training Loop (Executable Conceptual PoC) ---\n",
        "\n",
        "def run_ijepa_poc_vit_s_cifar100_executable():\n",
        "    print(\"--- Starting Executable Conceptual I-JEPA PoC Demo with ViT-S/16 on CIFAR-100 ---\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"CUDA not available. Running on CPU. This PoC is intended for GPU demonstration.\")\n",
        "\n",
        "    MODEL_EMBED_DIM = 384\n",
        "    MODEL_DEPTH = 12\n",
        "    MODEL_NUM_HEADS = 6\n",
        "    PATCH_SIZE = 16\n",
        "    IMAGE_SIZE = 224\n",
        "\n",
        "    epochs = 5\n",
        "    batch_size = 4\n",
        "    learning_rate = 1e-3\n",
        "    weight_decay = 0.05\n",
        "    momentum_target_encoder = 0.996\n",
        "\n",
        "    conceptual_early_stopper = ConceptualEarlyStopping(patience=1, stop_threshold=1e-5, verbose=True)\n",
        "\n",
        "    train_dataset = ExecutableCIFAR100DatasetConceptual(\n",
        "        root='./data', train=True, download=True, transform=CIFAR100_TRANSFORMS\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: i_jepa_collate_fn_executable(b, patch_size=PATCH_SIZE, img_size=IMAGE_SIZE)\n",
        "    )\n",
        "\n",
        "    print(f\"Loaded conceptual CIFAR-100 dataset. Number of batches per epoch: {len(train_loader)}\")\n",
        "\n",
        "    context_encoder = ExecutableViTEncoderConceptual(\n",
        "        img_size=IMAGE_SIZE, patch_size=PATCH_SIZE, embed_dim=MODEL_EMBED_DIM\n",
        "    ).to(device)\n",
        "\n",
        "    target_encoder = copy.deepcopy(context_encoder).to(device)\n",
        "    for param in target_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    predictor = ExecutableIJEPAPredictorConceptual(\n",
        "        input_embed_dim=MODEL_EMBED_DIM,\n",
        "        predictor_embed_dim=MODEL_EMBED_DIM // 2\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        list(context_encoder.parameters()) + list(predictor.parameters()),\n",
        "        lr=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Starting Executable Conceptual I-JEPA PoC Training Loop ---\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        context_encoder.train()\n",
        "        predictor.train()\n",
        "        total_loss_epoch = 0\n",
        "\n",
        "        current_momentum_ema = min(momentum_target_encoder + (1.0 - momentum_target_encoder) * (epoch / epochs), 1.0)\n",
        "\n",
        "        for batch_idx, (all_patches_flat_conceptual, context_indices_list,\n",
        "                        target_predictor_indices_list, target_encoder_indices_list) in enumerate(train_loader):\n",
        "\n",
        "            all_patches_flat_conceptual = all_patches_flat_conceptual.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            fixed_encoder_input_shape = (batch_size, context_encoder.num_patches, 3 * PATCH_SIZE * PATCH_SIZE)\n",
        "            conceptual_encoder_input = torch.randn(fixed_encoder_input_shape, device=device)\n",
        "\n",
        "            context_features = context_encoder(conceptual_encoder_input)\n",
        "\n",
        "            conceptual_target_encoder_input = torch.randn(fixed_encoder_input_shape, device=device)\n",
        "            with torch.no_grad():\n",
        "                target_features_full = target_encoder(conceptual_target_encoder_input)\n",
        "\n",
        "            batch_loss = 0.0\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                current_image_context_features_for_predictor = context_features[i:i+1]\n",
        "\n",
        "                for block_idx, target_mask_indices_for_predictor_block in enumerate(target_predictor_indices_list[i]):\n",
        "                    if len(target_mask_indices_for_predictor_block) > 0:\n",
        "                        dummy_predictor_mask_tokens = torch.randn(\n",
        "                            1, len(target_mask_indices_for_predictor_block), predictor.predictor_embed_dim, device=device\n",
        "                        )\n",
        "\n",
        "                        predicted_target_block_repr = predictor(current_image_context_features_for_predictor, dummy_predictor_mask_tokens)\n",
        "\n",
        "                        actual_target_block_repr = target_features_full[i:i+1, target_mask_indices_for_predictor_block]\n",
        "\n",
        "                        loss = nn.functional.mse_loss(predicted_target_block_repr, actual_target_block_repr) * 0.001\n",
        "                        batch_loss += loss\n",
        "\n",
        "            total_loss_epoch += batch_loss.item()\n",
        "\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for param_q, param_k in zip(context_encoder.parameters(), target_encoder.parameters()):\n",
        "                    param_k.data.mul_(current_momentum_ema).add_((1 - current_momentum_ema) * param_q.data)\n",
        "\n",
        "            if batch_idx % 1000 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {batch_loss.item():.4f}\")\n",
        "\n",
        "        avg_epoch_loss = total_loss_epoch / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch} finished. Average Loss: {avg_epoch_loss:.4f}\\n\")\n",
        "\n",
        "        if conceptual_early_stopper(avg_epoch_loss):\n",
        "            print(\"Conceptual Early Stopping triggered. Exiting training loop.\")\n",
        "            break\n",
        "\n",
        "    print(\"--- Executable Conceptual I-JEPA PoC Training Finished ---\")\n",
        "    print(\"\\n--- PoC Purpose and Limitations ---\")\n",
        "    print(\"This code is an EXECUTABLE CONCEPTUAL DEMO of I-JEPA training on a GPU.\")\n",
        "    print(\"It includes a CONCEPTUAL Early Stopping mechanism for demonstration purposes.\")\n",
        "    print(\"However, it is NOT a real deep learning training setup because:\")\n",
        "    print(\"1.  **Simplified Model Architectures:** The encoder and predictor use `nn.Linear` layers as their core, which are vastly simplified compared to full ViT architectures.\")\n",
        "    print(\"2.  **Simulated Data:** The 'patches' are derived from randomly generated tensors that are then transformed and normalized, not from actual meaningful image content.\")\n",
        "    print(\"3.  **Artificial Prediction Targets:** The prediction targets are designed based on an artificial relationship with the inputs to show a decreasing loss trend, not actual image semantics.\")\n",
        "    print(\"4.  **No Real Learning/Meaningful Loss:** The model does not learn meaningful visual features. Early stopping here is based on a loss that isn't indicative of real-world AI performance.\")\n",
        "    print(\"5.  **Conceptual Early Stopping:** The early stopping mechanism is functional, but its trigger (monitoring a numerically small, conceptual loss) is not based on actual model performance on a validation set.\")\n",
        "    print(\"This PoC is intended for illustrating the *flow* and *PyTorch GPU mechanics* of I-JEPA, including early stopping logic, not for achieving state-of-the-art results.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_ijepa_poc_vit_s_cifar100_executable()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJKkxWvSS1tF",
        "outputId": "d1a2cc2a-f711-4960-e187-71c9003a2da6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Executable Conceptual I-JEPA PoC Demo with ViT-S/16 on CIFAR-100 ---\n",
            "Using device: cuda\n",
            "Current GPU: NVIDIA A100-SXM4-40GB\n",
            "Executable Conceptual: Using CIFAR-100 metadata, but simulating image data.\n",
            "Loaded conceptual CIFAR-100 dataset. Number of batches per epoch: 12500\n",
            "Executable Conceptual: Initializing ViT-S/16 Encoder...\n",
            "Executable Conceptual: Initializing I-JEPA Predictor...\n",
            "\n",
            "--- Starting Executable Conceptual I-JEPA PoC Training Loop ---\n",
            "Epoch 0/5, Batch 0/12500, Loss: 0.0018\n",
            "Epoch 0/5, Batch 1000/12500, Loss: 0.0011\n",
            "Epoch 0/5, Batch 2000/12500, Loss: 0.0008\n",
            "Epoch 0/5, Batch 3000/12500, Loss: 0.0006\n",
            "Epoch 0/5, Batch 4000/12500, Loss: 0.0004\n",
            "Epoch 0/5, Batch 5000/12500, Loss: 0.0003\n",
            "Epoch 0/5, Batch 6000/12500, Loss: 0.0002\n",
            "Epoch 0/5, Batch 7000/12500, Loss: 0.0002\n",
            "Epoch 0/5, Batch 8000/12500, Loss: 0.0001\n",
            "Epoch 0/5, Batch 9000/12500, Loss: 0.0001\n",
            "Epoch 0/5, Batch 10000/12500, Loss: 0.0001\n",
            "Epoch 0/5, Batch 11000/12500, Loss: 0.0001\n",
            "Epoch 0/5, Batch 12000/12500, Loss: 0.0001\n",
            "\n",
            "Epoch 0 finished. Average Loss: 0.0004\n",
            "\n",
            "Conceptual EarlyStop: Initial best loss set to 0.000398\n",
            "Conceptual EarlyStop: Loss 0.00039774 is above threshold 0.00001000. Resetting counter.\n",
            "Epoch 1/5, Batch 0/12500, Loss: 0.0001\n",
            "Epoch 1/5, Batch 1000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 2000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 3000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 4000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 5000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 6000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 7000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 8000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 9000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 10000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 11000/12500, Loss: 0.0000\n",
            "Epoch 1/5, Batch 12000/12500, Loss: 0.0000\n",
            "\n",
            "Epoch 1 finished. Average Loss: 0.0000\n",
            "\n",
            "Conceptual EarlyStop: Loss 0.00002103 is above threshold 0.00001000. Resetting counter.\n",
            "Epoch 2/5, Batch 0/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 1000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 2000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 3000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 4000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 5000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 6000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 7000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 8000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 9000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 10000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 11000/12500, Loss: 0.0000\n",
            "Epoch 2/5, Batch 12000/12500, Loss: 0.0000\n",
            "\n",
            "Epoch 2 finished. Average Loss: 0.0000\n",
            "\n",
            "Conceptual EarlyStop: Loss 0.00000175 is below or at threshold 0.00001000. Consecutive epochs: 1/1\n",
            "Conceptual EarlyStop: Stopping training as loss consistently below threshold.\n",
            "Conceptual Early Stopping triggered. Exiting training loop.\n",
            "--- Executable Conceptual I-JEPA PoC Training Finished ---\n",
            "\n",
            "--- PoC Purpose and Limitations ---\n",
            "This code is an EXECUTABLE CONCEPTUAL DEMO of I-JEPA training on a GPU.\n",
            "It includes a CONCEPTUAL Early Stopping mechanism for demonstration purposes.\n",
            "However, it is NOT a real deep learning training setup because:\n",
            "1.  **Simplified Model Architectures:** The encoder and predictor use `nn.Linear` layers as their core, which are vastly simplified compared to full ViT architectures.\n",
            "2.  **Simulated Data:** The 'patches' are derived from randomly generated tensors that are then transformed and normalized, not from actual meaningful image content.\n",
            "3.  **Artificial Prediction Targets:** The prediction targets are designed based on an artificial relationship with the inputs to show a decreasing loss trend, not actual image semantics.\n",
            "4.  **No Real Learning/Meaningful Loss:** The model does not learn meaningful visual features. Early stopping here is based on a loss that isn't indicative of real-world AI performance.\n",
            "5.  **Conceptual Early Stopping:** The early stopping mechanism is functional, but its trigger (monitoring a numerically small, conceptual loss) is not based on actual model performance on a validation set.\n",
            "This PoC is intended for illustrating the *flow* and *PyTorch GPU mechanics* of I-JEPA, including early stopping logic, not for achieving state-of-the-art results.\n"
          ]
        }
      ]
    }
  ]
}
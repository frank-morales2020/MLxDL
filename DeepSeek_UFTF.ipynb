{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "FOIBubPGOwYF"
      ],
      "authorship_tag": "ABX9TyOyS7TxQ4jx7C2z0M6VMhaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f52bbd33776428b9b34f59194065f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2899927b4a734e349d21b84f93bb2e17",
              "IPY_MODEL_7713d34a5fbb45bb8cd8460415b5445f",
              "IPY_MODEL_cc2afc7b5a6847a78bad5d4344bfd5f9"
            ],
            "layout": "IPY_MODEL_3047a97d4a0247bfa403044e0b66802a"
          }
        },
        "2899927b4a734e349d21b84f93bb2e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62276a77028f43609fc6daa0916ff4f8",
            "placeholder": "​",
            "style": "IPY_MODEL_2c21f8d9d06e485597f62019de9bd635",
            "value": "Map: 100%"
          }
        },
        "7713d34a5fbb45bb8cd8460415b5445f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5220e6a4e7394181b2faccb9a35081ba",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47767c3e9d494ec497334ff9c5e7febe",
            "value": 100
          }
        },
        "cc2afc7b5a6847a78bad5d4344bfd5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad07fd10994a45559e5aa5deff73cce9",
            "placeholder": "​",
            "style": "IPY_MODEL_400b284df71f43d996e0142232e7ded3",
            "value": " 100/100 [00:00&lt;00:00, 3392.74 examples/s]"
          }
        },
        "3047a97d4a0247bfa403044e0b66802a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62276a77028f43609fc6daa0916ff4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c21f8d9d06e485597f62019de9bd635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5220e6a4e7394181b2faccb9a35081ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47767c3e9d494ec497334ff9c5e7febe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad07fd10994a45559e5aa5deff73cce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "400b284df71f43d996e0142232e7ded3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d5f061062fe47f699d93f4627294e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3d308c797c64c8d91bbc817f93136ca",
              "IPY_MODEL_bf58ccb433fe4ddd9d97af148257c232",
              "IPY_MODEL_e2de1abd63d74cfd9b7a7334575e4489"
            ],
            "layout": "IPY_MODEL_6f7618c3af6447caaadf6f23c4497b9d"
          }
        },
        "e3d308c797c64c8d91bbc817f93136ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b932710ecc7d47fbac631eb638bd4ecb",
            "placeholder": "​",
            "style": "IPY_MODEL_585dc2c5bf06438995c472a8eaa41dd0",
            "value": "Map: 100%"
          }
        },
        "bf58ccb433fe4ddd9d97af148257c232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_434e7b5c066246fd92a41d94ddb8a5d8",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94aac47c685c450190bfa4da00c482a9",
            "value": 25
          }
        },
        "e2de1abd63d74cfd9b7a7334575e4489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d704a82719e49c69e6632bf855a936c",
            "placeholder": "​",
            "style": "IPY_MODEL_5c14440e2cf6492386712879efd31a68",
            "value": " 25/25 [00:00&lt;00:00, 1406.07 examples/s]"
          }
        },
        "6f7618c3af6447caaadf6f23c4497b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b932710ecc7d47fbac631eb638bd4ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "585dc2c5bf06438995c472a8eaa41dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "434e7b5c066246fd92a41d94ddb8a5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94aac47c685c450190bfa4da00c482a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d704a82719e49c69e6632bf855a936c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c14440e2cf6492386712879efd31a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38a28775728d4fa99bdca87c7b4b83fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_825a8d18ccf74d8683c99a3d1f2a3100",
              "IPY_MODEL_56635e8ccfcd4fe58a6cf4db8f3b5980",
              "IPY_MODEL_03181a88dfd04757acb1bf2e0e901df6"
            ],
            "layout": "IPY_MODEL_6709642b700b497bb2de78fe50a0abb2"
          }
        },
        "825a8d18ccf74d8683c99a3d1f2a3100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af265f75b71848a9b3630e6d2de44aa7",
            "placeholder": "​",
            "style": "IPY_MODEL_8c102f0fb01f4980bbdedb0f2e534cc0",
            "value": "Map: 100%"
          }
        },
        "56635e8ccfcd4fe58a6cf4db8f3b5980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15f74de3ae2c47aaaea982aa677cd112",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecd1ffdeeddb4bd99b6fad5aa1f15532",
            "value": 100
          }
        },
        "03181a88dfd04757acb1bf2e0e901df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa4d0e36a8e459ca9c310b510083ebe",
            "placeholder": "​",
            "style": "IPY_MODEL_933807be97884d96967a1a7eab5fa23c",
            "value": " 100/100 [00:00&lt;00:00, 3494.58 examples/s]"
          }
        },
        "6709642b700b497bb2de78fe50a0abb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af265f75b71848a9b3630e6d2de44aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c102f0fb01f4980bbdedb0f2e534cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15f74de3ae2c47aaaea982aa677cd112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecd1ffdeeddb4bd99b6fad5aa1f15532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aa4d0e36a8e459ca9c310b510083ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933807be97884d96967a1a7eab5fa23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87f316120eb94699a789e828aa611cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d03239448b4f4db79c770c276f1e7788",
              "IPY_MODEL_015c25a7ef574513bc1e03c5c6db99b4",
              "IPY_MODEL_59d12f2763ee4f9999c4a402a72ebb2f"
            ],
            "layout": "IPY_MODEL_e5a7790adefd4fcc8a7c2996881b28ad"
          }
        },
        "d03239448b4f4db79c770c276f1e7788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d841a54d34a04f1182952d383bfa8de7",
            "placeholder": "​",
            "style": "IPY_MODEL_d29f8dfa1f7f408ea6b45ee5f344bbe3",
            "value": "Map: 100%"
          }
        },
        "015c25a7ef574513bc1e03c5c6db99b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2056f7662344465b8bef74022a5902aa",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e4a4be6cbb6466a9eb514f705489f45",
            "value": 25
          }
        },
        "59d12f2763ee4f9999c4a402a72ebb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77ae47489589436f8c3facef3c617272",
            "placeholder": "​",
            "style": "IPY_MODEL_15928a867d284cc1aea9d6bf4ce5ed05",
            "value": " 25/25 [00:00&lt;00:00, 1358.72 examples/s]"
          }
        },
        "e5a7790adefd4fcc8a7c2996881b28ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d841a54d34a04f1182952d383bfa8de7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d29f8dfa1f7f408ea6b45ee5f344bbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2056f7662344465b8bef74022a5902aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4a4be6cbb6466a9eb514f705489f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77ae47489589436f8c3facef3c617272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15928a867d284cc1aea9d6bf4ce5ed05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfc8ca8886ef48769d5c38a6e6256008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a61f28a67c344ee6af46b316ff4b792f",
              "IPY_MODEL_a8579fc7a71b432dae58cc2ae0c557d6",
              "IPY_MODEL_99614c6e879a4613a6b035a99ff24fd8"
            ],
            "layout": "IPY_MODEL_50d6617ff8804092a95194ffdf029e30"
          }
        },
        "a61f28a67c344ee6af46b316ff4b792f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b318e8919b0342fc9c246d5147d0c57f",
            "placeholder": "​",
            "style": "IPY_MODEL_c59c0fb3eddb446aa624b6cf08c73a6a",
            "value": "Map: 100%"
          }
        },
        "a8579fc7a71b432dae58cc2ae0c557d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6331cd8b254f4b9e96881b93ecff026e",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_451949c5b6504b7e8ed5cf874dd6753f",
            "value": 25
          }
        },
        "99614c6e879a4613a6b035a99ff24fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13b5d1ad8d8647458729f3d359c02cc6",
            "placeholder": "​",
            "style": "IPY_MODEL_d2af2ff63664427a927c5adbce61ab49",
            "value": " 25/25 [00:00&lt;00:00, 768.85 examples/s]"
          }
        },
        "50d6617ff8804092a95194ffdf029e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b318e8919b0342fc9c246d5147d0c57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59c0fb3eddb446aa624b6cf08c73a6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6331cd8b254f4b9e96881b93ecff026e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "451949c5b6504b7e8ed5cf874dd6753f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13b5d1ad8d8647458729f3d359c02cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2af2ff63664427a927c5adbce61ab49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5347db2a0b0c41d28eb3bf476271f68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64a6a94f4cd04adf88eefb6f5ba9cc7a",
              "IPY_MODEL_2cecd2f5dc074c5f93c15aae8edc2a7a",
              "IPY_MODEL_70ca418cfc654ba8a8d417df55db5602"
            ],
            "layout": "IPY_MODEL_6065ceecb2e14fea84deb1ddc0917026"
          }
        },
        "64a6a94f4cd04adf88eefb6f5ba9cc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed73cf674e748e8a03548aa3d2592f7",
            "placeholder": "​",
            "style": "IPY_MODEL_b680e560339f455697f8de8f6be36981",
            "value": "Map: 100%"
          }
        },
        "2cecd2f5dc074c5f93c15aae8edc2a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d469482af0e14ab1a69690413ea6dfe2",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c36035d44e294180b679232ea3108242",
            "value": 25
          }
        },
        "70ca418cfc654ba8a8d417df55db5602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07918a203e3a4ba9b83ec7cbf33d9a5e",
            "placeholder": "​",
            "style": "IPY_MODEL_cc502046aa294be293a2fe9c967c763c",
            "value": " 25/25 [00:00&lt;00:00, 352.21 examples/s]"
          }
        },
        "6065ceecb2e14fea84deb1ddc0917026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ed73cf674e748e8a03548aa3d2592f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b680e560339f455697f8de8f6be36981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d469482af0e14ab1a69690413ea6dfe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36035d44e294180b679232ea3108242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07918a203e3a4ba9b83ec7cbf33d9a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc502046aa294be293a2fe9c967c763c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/DeepSeek_UFTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enviroment Setup"
      ],
      "metadata": {
        "id": "epm2iZUYsct1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary modules\n",
        "!pip install transformers accelerate trl bitsandbytes datasets peft --quiet\n",
        "!pip install -U bitsandbytes -q\n"
      ],
      "metadata": {
        "id": "1bziEeh_hKVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8jo2f91qw7Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4bca08-cce8-434b-fd77-5cfba78a9b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 24 23:16:43 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             50W /  400W |    2447MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "!pip install transformers accelerate --quiet\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "import accelerate\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = accelerate.Accelerator()"
      ],
      "metadata": {
        "id": "3SfIlucXsUlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepSeekr1 - Unsloth"
      ],
      "metadata": {
        "id": "FOIBubPGOwYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B"
      ],
      "metadata": {
        "id": "ElfXXIzpOqCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub --quiet\n",
        "!pip install unsloth -q\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"You seem to be using the pipelines sequentially on GPU\")\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "LeIPCBEAI8Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\""
      ],
      "metadata": {
        "id": "MkxESLfYed3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = access_token_write,\n",
        ")"
      ],
      "metadata": {
        "id": "bDYgdzf4HBq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\""
      ],
      "metadata": {
        "id": "2-oqfgQlW1fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-LZraZIWqW2",
        "outputId": "65efc29e-f7e1-4fe6-fa46-07b91ddd194a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I need to figure out what cystometry would show for this 61-year-old woman. Let me start by breaking down the information given.\n",
            "\n",
            "First, the patient has a history of involuntary urine loss during activities like coughing or sneezing but no leakage at night. That makes me think of stress urinary incontinence. Stress incontinence usually happens when the urethral muscles aren't strong enough to prevent urine from leaking out when there's increased pressure, like from coughing or sneezing.\n",
            "\n",
            "She undergoes a gynecological exam and a Q-tip test. I'm not entirely sure about the Q-tip test, but I think it's a common test used to assess urethral function. From what I remember, the Q-tip is a small catheter that's placed in the urethra, and the doctor measures the closure pressure. If the pressure is low, it might indicate that the urethral sphincter isn't functioning well, contributing to incontinence.\n",
            "\n",
            "Now, considering the findings from these tests, what would cystometry show? Cystometry is a more detailed test that looks at how the bladder responds under different conditions. It measures things like how much urine is left in the bladder (residual volume) and how the detrusor muscle contracts when it's stimulated.\n",
            "\n",
            "Given that she has stress incontinence, during the cystometry, they might fill the bladder and then release some urine to see the residual volume. If her residual volume is low, that suggests her bladder can empty effectively, which is good. But if the residual volume is high, it might mean her bladder isn't emptying properly, contributing to her symptoms.\n",
            "\n",
            "Next, they'll probably stimulate the detrusor muscle to see how it contracts. Normally, the detrusor contracts to push urine out. If the contraction is weak, it might explain why she's having issues with incontinence. But if the contraction is strong and normal, then the issue might be elsewhere, like with the urethral sphincter.\n",
            "\n",
            "Putting it all together, if the Q-tip test shows low closure pressure, suggesting weak urethral function, and if her history points towards stress incontinence, then during cystometry, her residual volume might be normal because her bladder can empty, but the detrusor contractions might be normal too. Wait, but the question is about residual volume and detrusor contractions. Maybe I'm mixing things up.\n",
            "\n",
            "Alternatively, if the Q-tip shows low pressure, that points to a weak urethral sphincter. So during cystometry, even if the bladder is emptying well, the urethral sphincter can't hold the urine, leading to incontinence. So the residual volume might be normal, but the detrusor contractions could be normal as well. Hmm, I'm not entirely sure.\n",
            "\n",
            "Wait, maybe the residual volume would be low because the bladder is emptying effectively, but the issue is that the sphincter isn't opening properly. So the detrusor contractions might be normal, but the sphincter isn't opening, leading to incontinence.\n",
            "\n",
            "I think I'm getting confused. Let me try to structure this.\n",
            "\n",
            "1. The patient has stress incontinence, so urethral sphincter function is impaired.\n",
            "2. Q-tip test likely shows low closure pressure, indicating weak sphincter.\n",
            "3. Cystometry would assess residual volume and detrusor contractions.\n",
            "   - Residual volume: If the bladder is emptying well, residual volume would be low.\n",
            "   - Detrusor contractions: Normally, they should contract when stimulated. If they do, it suggests the issue is elsewhere, like the sphincter.\n",
            "4. So, in this case, cystometry would likely show normal detrusor contractions (they contract as they should) and low residual volume because the bladder empties well, but the sphincter issue leads to incontinence.\n",
            "\n",
            "Wait, but the question is asking what cystometry would most likely reveal about her residual volume and detrusor contractions. So the answer would be that residual volume is low and detrusor contractions are normal.\n",
            "\n",
            "I think that makes sense. So the answer is that cystometry would show low residual volume and normal detrusor contractions.\n",
            "</think>\n",
            "\n",
            "Cystometry for this 61-year-old woman would most likely reveal a **low residual volume** and **normal detrusor contractions**. The low residual volume indicates that her bladder empties effectively, while the normal detrusor contractions suggest that the issue lies elsewhere, likely with the urethral sphincter, contributing to her stress urinary incontinence.<｜end▁of▁sentence｜>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UFTF DeepSeekr1"
      ],
      "metadata": {
        "id": "bOcy5szPhxuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, uninstall all the problematic libraries.\n",
        "!pip uninstall -y torch torchvision torchaudio transformers accelerate datasets peft bitsandbytes trl unsloth\n",
        "\n",
        "# Check the current CUDA version (if available).\n",
        "#!nvidia-smi\n",
        "\n",
        "# Now, install a specific PyTorch version with its compatible dependencies.\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Upgrade Libraries\n",
        "!pip install -U transformers==4.36.0 accelerate datasets peft bitsandbytes trl -q\n",
        "# Install Unsloth.\n",
        "!pip install \"unsloth[hf]\" -q\n",
        "\n",
        "# Check the installed versions\n",
        "!pip show torch\n",
        "!pip show transformers\n",
        "\n",
        "# Check if the CUDA driver is correct:\n",
        "#!nvidia-smi"
      ],
      "metadata": {
        "id": "zgyqn3Oi0K-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UFTF WITH 1 dataset"
      ],
      "metadata": {
        "id": "YJaVhrwA95Jz"
      }
    },
    {
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import gc\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import Trainer\n",
        "import copy\n",
        "\n",
        "# Import from Unsloth\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from unsloth.kernels import cross_entropy_loss\n",
        "\n",
        "# Import SFTTrainer from TRL\n",
        "from trl import SFTTrainer\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "\n",
        "class FineTuningAgent:\n",
        "    def __init__(self, model_id, dataset_name, config=None):\n",
        "        \"\"\"\n",
        "        Initializes the FineTuningAgent.\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.dataset_name = dataset_name\n",
        "        if config is None:\n",
        "            config = {}\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_args = None\n",
        "        self.peft_config = None\n",
        "        self.dataset = None\n",
        "        self.counter = 0\n",
        "        self.data_collator = None\n",
        "        self.max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "\n",
        "    def _observe(self):\n",
        "        \"\"\"\n",
        "        Loads the model, tokenizer, and dataset.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(f\"Starting Observe ...\")\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        quantization_config = None\n",
        "        # Determine if Unsloth is used.\n",
        "        is_unsloth_model = self.config.get(\"use_unsloth\", False)\n",
        "\n",
        "        if self.config.get(\"quantization\") and not is_unsloth_model:\n",
        "            if \"mistral\" in self.model_id.lower():\n",
        "                print(\"Mistral model detected. Using 4-bit quantization.\")\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                )\n",
        "            else:\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float32,\n",
        "                )\n",
        "        model_downloaded = False\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "        while not model_downloaded and retry_count < max_retries:\n",
        "            try:\n",
        "                # Determine the correct model class based on architecture\n",
        "                if \"bert\" in self.model_id.lower() and not is_unsloth_model:\n",
        "                    self.model = AutoModelForSequenceClassification.from_pretrained(  # Use correct model type\n",
        "                        self.model_id,\n",
        "                        num_labels=2,  # For MRPC, which is binary classification\n",
        "                        quantization_config=quantization_config,\n",
        "                        trust_remote_code=True,\n",
        "                    )\n",
        "                elif \"mistral\" in self.model_id.lower() and not is_unsloth_model:\n",
        "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                        self.model_id,\n",
        "                        quantization_config=quantization_config,\n",
        "                        trust_remote_code=True,\n",
        "                    )\n",
        "                # Load Model with Unsloth\n",
        "                elif is_unsloth_model:\n",
        "                    print(\"Loading model using Unsloth...\")\n",
        "                    # This is the correct model ID to use with Unsloth\n",
        "                    # Corrected Model ID.\n",
        "                    unsloth_model_id = self.config.get(\n",
        "                        \"unsloth_model_id\", \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "                    )\n",
        "                    max_seq_length = self.config.get(\"max_seq_length\", 2048)  # You can tune this.\n",
        "                    dtype = self.config.get(\"dtype\", None)  # You can tune this.\n",
        "                    load_in_4bit = self.config.get(\"load_in_4bit\", True)  # You can tune this.\n",
        "                    access_token = self.config.get(\"access_token\", None)\n",
        "                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                        model_name=unsloth_model_id,\n",
        "                        max_seq_length=max_seq_length,\n",
        "                        dtype=dtype,\n",
        "                        load_in_4bit=load_in_4bit,\n",
        "                        token=access_token,\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    print(f\"Model {self.model_id} not supported.\")\n",
        "                    return\n",
        "\n",
        "                model_downloaded = True\n",
        "            except KeyboardInterrupt:\n",
        "                print(\n",
        "                    f\"Model download interrupted. Retrying... (Attempt {retry_count + 1}/{max_retries})\"\n",
        "                )\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during model download: {e}\")\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "        # Load Tokenizer with HF library if it is not an unsloth model.\n",
        "        if not is_unsloth_model:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_id, trust_remote_code=True\n",
        "            )\n",
        "\n",
        "        # Add padding token if it does not exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        # Move model to device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Load Dataset (using dataset name from Hugging Face Hub)\n",
        "        dataset = load_dataset(self.dataset_name, split=\"train\")\n",
        "        self.dataset = dataset.shuffle().select(\n",
        "            range(self.config.get(\"dataset_size\", 125))\n",
        "        )  # Set a default dataset size of 125\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Observe finished.\")\n",
        "\n",
        "    def _orient(self):\n",
        "        \"\"\"\n",
        "        Orients the agent by formatting the dataset and preparing training arguments.\n",
        "        \"\"\"\n",
        "        print(\"\\n\")\n",
        "        self.counter += 1\n",
        "        print(f\"Starting Orient ...\")\n",
        "        if self.dataset_name == \"SetFit/mrpc\":\n",
        "            print(\"Dataset: SetFit/mrpc\")\n",
        "            preprocessing_function = self._preprocess_function_mrpc\n",
        "            dataset_text_field = None  # No need for dataset_text_field for mrpc\n",
        "        elif self.dataset_name == \"b-mc2/sql-create-context\":\n",
        "            print(\"Dataset: b-mc2/sql-create-context\")\n",
        "            preprocessing_function = self._preprocess_function_sql_create_context\n",
        "            dataset_text_field = \"text\"  # We use the text field\n",
        "        elif self.dataset_name == \"anthropic/hh-rlhf\":\n",
        "            print(\"Dataset: anthropic/hh-rlhf\")\n",
        "            preprocessing_function = self._preprocess_function_anthropic_hh_rlhf\n",
        "            dataset_text_field = \"text\"  # We use the text field\n",
        "        else:\n",
        "            print(f\"Dataset: {self.dataset_name} not supported.\")\n",
        "            return\n",
        "\n",
        "        # Set the train/test split.\n",
        "        test_size_percentage = self.config.get(\"test_split_percentage\", 0.2)  # Set a default test size to 20%\n",
        "        self.dataset = self.dataset.train_test_split(\n",
        "            test_size=test_size_percentage\n",
        "        )\n",
        "\n",
        "        self.dataset = self.dataset.map(\n",
        "            preprocessing_function,\n",
        "            batched=True,\n",
        "            remove_columns=self.dataset[\"train\"].column_names,\n",
        "        )\n",
        "        self.dataset_text_field = dataset_text_field\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient Dataset: {self.dataset}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient finished.\")\n",
        "\n",
        "    def _decide(self):\n",
        "        \"\"\"\n",
        "        Decides on the fine-tuning strategy, including LoRA configuration.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(f\"Starting Decide ...\")\n",
        "        clear_memory()\n",
        "        # PEFT Configuration (LoRA)\n",
        "        if self.config.get(\"lora\"):\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "            if \"bert\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,  # You can tune this.\n",
        "                    lora_dropout=0.1,  # You can tune this.\n",
        "                    r=64,  # You can tune this.\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Correct target modules for BERT\n",
        "                    task_type=\"SEQ_CLS\",  # correct task type\n",
        "                )\n",
        "            elif \"mistral\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            # If we are using unsloth, we will use this config\n",
        "            elif self.config.get(\"use_unsloth\", False):\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=64,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "                print(\"\\n\")\n",
        "                print(f\"LORA: {peft_config}\")\n",
        "            else:\n",
        "                print(f\"Model {self.model_id} not supported.\")\n",
        "                return\n",
        "\n",
        "            self.peft_config = peft_config\n",
        "            self.model = get_peft_model(self.model, peft_config)\n",
        "\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        print('\\n')\n",
        "        print(f\"Decide finished.\")\n",
        "\n",
        "    def _act(self):\n",
        "        \"\"\"\n",
        "        Acts by preprocessing the dataset and initializing the training loop.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(f\"Starting Act ...\")\n",
        "        clear_memory()\n",
        "\n",
        "        try:\n",
        "            if \"train\" not in self.dataset or \"test\" not in self.dataset:\n",
        "                print(f\"Missing train or test split for {self.dataset_name}\")\n",
        "                return\n",
        "\n",
        "            print(\"Dataset preprocessed successfully.\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Create TrainingArguments with the desired parameters\n",
        "            training_args_config = self.config.get(\"training_args\", {})\n",
        "            self.training_args = TrainingArguments(\n",
        "                output_dir=training_args_config.get(\"output_dir\", \"./output\"),\n",
        "                per_device_train_batch_size=training_args_config.get(\n",
        "                    \"per_device_train_batch_size\", 2\n",
        "                ),\n",
        "                gradient_accumulation_steps=training_args_config.get(\n",
        "                    \"gradient_accumulation_steps\", 4\n",
        "                ),\n",
        "                warmup_steps=training_args_config.get(\"warmup_steps\", 5),\n",
        "                max_steps=training_args_config.get(\"max_steps\", 60),\n",
        "                learning_rate=training_args_config.get(\"learning_rate\", 2e-4),\n",
        "                fp16=training_args_config.get(\"fp16\", not is_bfloat16_supported()),\n",
        "                bf16=training_args_config.get(\"bf16\", is_bfloat16_supported()),\n",
        "                logging_steps=training_args_config.get(\"logging_steps\", 10),\n",
        "                optim=training_args_config.get(\"optim\", \"adamw_8bit\"),\n",
        "                weight_decay=training_args_config.get(\"weight_decay\", 0.01),\n",
        "                lr_scheduler_type=training_args_config.get(\"lr_scheduler_type\", \"linear\"),\n",
        "                seed=training_args_config.get(\"seed\", 3407),\n",
        "                evaluation_strategy=training_args_config.get(\"evaluation_strategy\", \"steps\"),  # Now we use a strategy\n",
        "                eval_steps=training_args_config.get(\"eval_steps\", 20),  # Now we use eval steps\n",
        "                # report_to=training_args_config.get(\"report_to\", None), # Not needed this parameter\n",
        "            )\n",
        "\n",
        "            # Initialize Trainer\n",
        "            print(\"Initializing Trainer...\")\n",
        "            # Use the Trainer class instead of SFTTrainer\n",
        "            # Use SFTTrainer\n",
        "            self.trainer = SFTTrainer(\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                train_dataset=self.dataset[\"train\"],\n",
        "                eval_dataset=self.dataset[\"test\"],  # We add the eval dataset\n",
        "                dataset_text_field=self.dataset_text_field,\n",
        "                max_seq_length=self.max_seq_length,\n",
        "                dataset_num_proc=self.config.get(\"dataset_num_proc\", 2),\n",
        "                args=self.training_args,\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in _act(): {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Act finished.\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Executes the OODA loop and fine-tunes the language model.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(f\"Starting Run ...\")\n",
        "        clear_memory()\n",
        "        self._observe()\n",
        "        if self.model is None:\n",
        "            print(\"Model loading failed, skipping _orient, _decide and _act\")\n",
        "            return\n",
        "        self._orient()\n",
        "        self._decide()\n",
        "        self._act()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Run Dataset: {self.dataset}\")\n",
        "        # Add this part\n",
        "        if self.dataset and \"test\" in self.dataset:\n",
        "          print(f\"Test Dataset Size: {len(self.dataset['test'])}\")\n",
        "        else:\n",
        "          print(\"No test dataset found or dataset is None.\")\n",
        "        print(f\"Eval Batch Size: {self.trainer.args.per_device_eval_batch_size}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if self.trainer is not None:\n",
        "            try:\n",
        "                # Train the model\n",
        "                self.trainer.train()\n",
        "                print(\"\\n\")\n",
        "                print(\"Evaluation:\")\n",
        "                eval_results = self.evaluate()\n",
        "                print(\"\\n\")\n",
        "                print(eval_results)\n",
        "                print(\"\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during training or evaluation: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"Trainer is None. Skipping training and evaluation.\")\n",
        "\n",
        "        print(f\"Run  finished.\")\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the fine-tuned language model.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            eval_results = self.trainer.evaluate()\n",
        "            return eval_results\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in evaluate(): {e}\")\n",
        "            raise\n",
        "\n",
        "    def _preprocess_function_mrpc(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the SetFit/mrpc dataset.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: SetFit/mrpc\")\n",
        "        inputs = self.tokenizer(\n",
        "            examples[\"text1\"],\n",
        "            examples[\"text2\"],\n",
        "            max_length=128,  # Adjust as needed\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "        # Corrected labels for classification models\n",
        "        inputs[\"labels\"] = examples[\"label\"]\n",
        "        return inputs\n",
        "\n",
        "    def _preprocess_function_sql_create_context(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the b-mc2/sql-create-context dataset.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: b-mc2/sql-create-context\")\n",
        "        # Tokenize inputs and labels\n",
        "        inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "        model_inputs = self.tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        # Tokenize labels\n",
        "        labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        # Assign labels to model_inputs\n",
        "        model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        # Add 'text' field for the text in the model.\n",
        "        model_inputs[\"text\"] = inputs\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def _preprocess_function_anthropic_hh_rlhf(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the anthropic/hh-rlhf dataset.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: anthropic/hh-rlhf\")\n",
        "        # Construct \"question\" and \"context\" using the 'text' column for b-mc2/sql-create-context\n",
        "        inputs = examples[\"chosen\"]\n",
        "\n",
        "        model_inputs = self.tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "        # Tokenize labels\n",
        "        labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "        model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        # Add 'text' field for the text in the model.\n",
        "        model_inputs[\"text\"] = inputs\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "# Configuration for experiments\n",
        "RL_PAIRS = [\n",
        "    # mrpc\n",
        "    {\n",
        "        \"model_id\": \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "        \"dataset_name\": \"SetFit/mrpc\",\n",
        "        \"config\": {\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",  # Corrected model ID for Unsloth\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"training_args\": {\n",
        "                \"evaluation_strategy\": \"steps\",  # Now we use a strategy\n",
        "                \"eval_steps\": 20,  # Now we use eval steps\n",
        "                \"num_train_epochs\": 1,\n",
        "                \"max_steps\": 60,\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "for pair in RL_PAIRS:\n",
        "    model_id = pair[\"model_id\"]\n",
        "    dataset_name = pair[\"dataset_name\"]\n",
        "    config = pair[\"config\"]\n",
        "\n",
        "    agent = FineTuningAgent(model_id, dataset_name, config)\n",
        "    agent.run()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4f52bbd33776428b9b34f59194065f0b",
            "2899927b4a734e349d21b84f93bb2e17",
            "7713d34a5fbb45bb8cd8460415b5445f",
            "cc2afc7b5a6847a78bad5d4344bfd5f9",
            "3047a97d4a0247bfa403044e0b66802a",
            "62276a77028f43609fc6daa0916ff4f8",
            "2c21f8d9d06e485597f62019de9bd635",
            "5220e6a4e7394181b2faccb9a35081ba",
            "47767c3e9d494ec497334ff9c5e7febe",
            "ad07fd10994a45559e5aa5deff73cce9",
            "400b284df71f43d996e0142232e7ded3",
            "9d5f061062fe47f699d93f4627294e6d",
            "e3d308c797c64c8d91bbc817f93136ca",
            "bf58ccb433fe4ddd9d97af148257c232",
            "e2de1abd63d74cfd9b7a7334575e4489",
            "6f7618c3af6447caaadf6f23c4497b9d",
            "b932710ecc7d47fbac631eb638bd4ecb",
            "585dc2c5bf06438995c472a8eaa41dd0",
            "434e7b5c066246fd92a41d94ddb8a5d8",
            "94aac47c685c450190bfa4da00c482a9",
            "3d704a82719e49c69e6632bf855a936c",
            "5c14440e2cf6492386712879efd31a68"
          ]
        },
        "id": "nqGeNpez_z15",
        "outputId": "500ac2a9-b691-4831-d38b-446c1f233f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Loading model using Unsloth...\n",
            "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.46.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "deepseek-ai/deepseek-coder-1.3b-base does not have a padding token! Will use pad_token = <pad>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Observe finished.\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f52bbd33776428b9b34f59194065f0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d5f061062fe47f699d93f4627294e6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "\n",
            "\n",
            "LORA: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'up_proj', 'o_proj', 'down_proj', 'gate_proj', 'q_proj', 'k_proj', 'v_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 59,965,440 || all params: 1,406,437,376 || trainable%: 4.2636\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "Test Dataset Size: 25\n",
            "Eval Batch Size: 2\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 59,965,440\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:42, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.981900</td>\n",
              "      <td>6.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.334400</td>\n",
              "      <td>5.182404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.833600</td>\n",
              "      <td>5.084407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Evaluation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'eval_loss': 5.083302974700928, 'eval_runtime': 1.4732, 'eval_samples_per_second': 16.97, 'eval_steps_per_second': 8.824, 'epoch': 4.8}\n",
            "\n",
            "\n",
            "Run  finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTFT WITH 3 DATASETS"
      ],
      "metadata": {
        "id": "0jOLIsftGCES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio transformers accelerate datasets peft bitsandbytes trl unsloth\n",
        "\n",
        "# Check the current CUDA version (if available).\n",
        "#!nvidia-smi\n",
        "\n",
        "# Now, install a specific PyTorch version with its compatible dependencies.\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Upgrade Libraries\n",
        "!pip install -U transformers==4.36.0 accelerate datasets peft bitsandbytes trl -q\n",
        "# Install Unsloth.\n",
        "!pip install \"unsloth[hf]\" -q\n",
        "\n",
        "# Check the installed versions\n",
        "!pip show torch\n",
        "!pip show transformers\n",
        "\n",
        "# Check if the CUDA driver is correct:\n",
        "#!nvidia-smi\n"
      ],
      "metadata": {
        "id": "Hxu-CuxdIZwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import gc\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import Trainer\n",
        "import copy\n",
        "\n",
        "# Import from Unsloth\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from unsloth.kernels import cross_entropy_loss\n",
        "\n",
        "# Import SFTTrainer from TRL\n",
        "from trl import SFTTrainer\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "\n",
        "class FineTuningAgent:\n",
        "    def __init__(self, model_id, dataset_name, config=None):\n",
        "        \"\"\"\n",
        "        Initializes the FineTuningAgent.\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.dataset_name = dataset_name\n",
        "        if config is None:\n",
        "            config = {}\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_args = None\n",
        "        self.peft_config = None\n",
        "        self.dataset = None\n",
        "        self.counter = 0\n",
        "        self.data_collator = None\n",
        "        self.max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "        self.dataset_text_field = None # New variable\n",
        "\n",
        "    def _observe(self):\n",
        "        \"\"\"\n",
        "        Loads the model, tokenizer, and dataset.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(f\"Starting Observe ...\")\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        quantization_config = None\n",
        "        # Determine if Unsloth is used.\n",
        "        is_unsloth_model = self.config.get(\"use_unsloth\", False)\n",
        "\n",
        "        if self.config.get(\"quantization\") and not is_unsloth_model:\n",
        "            if \"mistral\" in self.model_id.lower():\n",
        "                print(\"Mistral model detected. Using 4-bit quantization.\")\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                )\n",
        "            else:\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float32,\n",
        "                )\n",
        "        model_downloaded = False\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "        while not model_downloaded and retry_count < max_retries:\n",
        "            try:\n",
        "                # Determine the correct model class based on architecture\n",
        "                if \"bert\" in self.model_id.lower() and not is_unsloth_model:\n",
        "                    self.model = AutoModelForSequenceClassification.from_pretrained(  # Use correct model type\n",
        "                        self.model_id,\n",
        "                        num_labels=2,  # For MRPC, which is binary classification\n",
        "                        quantization_config=quantization_config,\n",
        "                        trust_remote_code=True,\n",
        "                    )\n",
        "                elif \"mistral\" in self.model_id.lower() and not is_unsloth_model:\n",
        "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                        self.model_id,\n",
        "                        quantization_config=quantization_config,\n",
        "                        trust_remote_code=True,\n",
        "                    )\n",
        "                # Load Model with Unsloth\n",
        "                elif is_unsloth_model:\n",
        "                    print(\"Loading model using Unsloth...\")\n",
        "                    # This is the correct model ID to use with Unsloth\n",
        "                    # Corrected Model ID.\n",
        "                    unsloth_model_id = self.config.get(\n",
        "                        \"unsloth_model_id\", \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "                    )\n",
        "                    max_seq_length = self.config.get(\"max_seq_length\", 2048)  # You can tune this.\n",
        "                    dtype = self.config.get(\"dtype\", None)  # You can tune this.\n",
        "                    load_in_4bit = self.config.get(\"load_in_4bit\", True)  # You can tune this.\n",
        "                    access_token = self.config.get(\"access_token\", None)\n",
        "                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                        model_name=unsloth_model_id,\n",
        "                        max_seq_length=max_seq_length,\n",
        "                        dtype=dtype,\n",
        "                        load_in_4bit=load_in_4bit,\n",
        "                        token=access_token,\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    print(f\"Model {self.model_id} not supported.\")\n",
        "                    return\n",
        "\n",
        "                model_downloaded = True\n",
        "            except KeyboardInterrupt:\n",
        "                print(\n",
        "                    f\"Model download interrupted. Retrying... (Attempt {retry_count + 1}/{max_retries})\"\n",
        "                )\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during model download: {e}\")\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "        # Load Tokenizer with HF library if it is not an unsloth model.\n",
        "        if not is_unsloth_model:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_id, trust_remote_code=True\n",
        "            )\n",
        "\n",
        "        # Add padding token if it does not exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        # Move model to device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Load Dataset (using dataset name from Hugging Face Hub)\n",
        "        dataset = load_dataset(self.dataset_name, split=\"train\")\n",
        "        self.dataset = dataset.shuffle().select(\n",
        "            range(self.config.get(\"dataset_size\", 125))\n",
        "        )  # Set a default dataset size of 125\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Observe finished.\")\n",
        "\n",
        "    def _orient(self):\n",
        "        \"\"\"\n",
        "        Orients the agent by formatting the dataset and preparing training arguments.\n",
        "        \"\"\"\n",
        "        print(\"\\n\")\n",
        "        self.counter += 1\n",
        "        print(f\"Starting Orient ...\")\n",
        "        if self.dataset_name == \"SetFit/mrpc\":\n",
        "            print(\"Dataset: SetFit/mrpc\")\n",
        "            preprocessing_function = self._preprocess_function_mrpc\n",
        "            self.dataset_text_field = None  # No need for dataset_text_field for mrpc\n",
        "        elif self.dataset_name == \"b-mc2/sql-create-context\":\n",
        "            print(\"Dataset: b-mc2/sql-create-context\")\n",
        "            preprocessing_function = self._preprocess_function_sql_create_context\n",
        "            self.dataset_text_field = \"text\"  # We use the text field\n",
        "        elif self.dataset_name == \"anthropic/hh-rlhf\":\n",
        "            print(\"Dataset: anthropic/hh-rlhf\")\n",
        "            preprocessing_function = self._preprocess_function_anthropic_hh_rlhf\n",
        "            self.dataset_text_field = \"text\"  # We use the text field\n",
        "        else:\n",
        "            print(f\"Dataset: {self.dataset_name} not supported.\")\n",
        "            return\n",
        "\n",
        "        # Set the train/test split.\n",
        "        test_size_percentage = self.config.get(\"test_split_percentage\", 0.2)  # Set a default test size to 20%\n",
        "        self.dataset = self.dataset.train_test_split(\n",
        "            test_size=test_size_percentage\n",
        "        )\n",
        "\n",
        "        self.dataset = self.dataset.map(\n",
        "            preprocessing_function,\n",
        "            batched=True,\n",
        "            remove_columns=self.dataset[\"train\"].column_names,\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient Dataset: {self.dataset}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient finished.\")\n",
        "\n",
        "    def _decide(self):\n",
        "        \"\"\"\n",
        "        Decides on the fine-tuning strategy, including LoRA configuration.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(f\"Starting Decide ...\")\n",
        "        clear_memory()\n",
        "        # PEFT Configuration (LoRA)\n",
        "        if self.config.get(\"lora\"):\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "            if \"bert\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,  # You can tune this.\n",
        "                    lora_dropout=0.1,  # You can tune this.\n",
        "                    r=64,  # You can tune this.\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Correct target modules for BERT\n",
        "                    task_type=\"SEQ_CLS\",  # correct task type\n",
        "                )\n",
        "            elif \"mistral\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            # If we are using unsloth, we will use this config\n",
        "            elif self.config.get(\"use_unsloth\", False):\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=64,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "                print(\"\\n\")\n",
        "                print(f\"LORA: {peft_config}\")\n",
        "            else:\n",
        "                print(f\"Model {self.model_id} not supported.\")\n",
        "                return\n",
        "\n",
        "            self.peft_config = peft_config\n",
        "            self.model = get_peft_model(self.model, peft_config)\n",
        "\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        print('\\n')\n",
        "        print(f\"Decide finished.\")\n",
        "\n",
        "    def _act(self):\n",
        "        \"\"\"\n",
        "        Acts by preprocessing the dataset and initializing the training loop.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(f\"Starting Act ...\")\n",
        "        clear_memory()\n",
        "\n",
        "        try:\n",
        "            if \"train\" not in self.dataset or \"test\" not in self.dataset:\n",
        "                print(f\"Missing train or test split for {self.dataset_name}\")\n",
        "                return\n",
        "\n",
        "            print(\"Dataset preprocessed successfully.\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Create TrainingArguments with the desired parameters\n",
        "            training_args_config = self.config.get(\"training_args\", {})\n",
        "            self.training_args = TrainingArguments(\n",
        "                output_dir=training_args_config.get(\"output_dir\", \"./output\"),\n",
        "                per_device_train_batch_size=training_args_config.get(\n",
        "                    \"per_device_train_batch_size\", 2\n",
        "                ),\n",
        "                gradient_accumulation_steps=training_args_config.get(\n",
        "                    \"gradient_accumulation_steps\", 4\n",
        "                ),\n",
        "                warmup_steps=training_args_config.get(\"warmup_steps\", 5),\n",
        "                max_steps=training_args_config.get(\"max_steps\", 60),\n",
        "                learning_rate=training_args_config.get(\"learning_rate\", 2e-4),\n",
        "                fp16=training_args_config.get(\"fp16\", not is_bfloat16_supported()),\n",
        "                bf16=training_args_config.get(\"bf16\", is_bfloat16_supported()),\n",
        "                logging_steps=training_args_config.get(\"logging_steps\", 10),\n",
        "                optim=training_args_config.get(\"optim\", \"adamw_8bit\"),\n",
        "                weight_decay=training_args_config.get(\"weight_decay\", 0.01),\n",
        "                lr_scheduler_type=training_args_config.get(\"lr_scheduler_type\", \"linear\"),\n",
        "                seed=training_args_config.get(\"seed\", 3407),\n",
        "                evaluation_strategy=training_args_config.get(\"evaluation_strategy\", \"steps\"), # we need this\n",
        "                eval_steps=training_args_config.get(\"eval_steps\", 20), # We need this\n",
        "                # report_to=training_args_config.get(\"report_to\", None), # Not needed this parameter\n",
        "            )\n",
        "\n",
        "            # Initialize Trainer\n",
        "            print(\"Initializing Trainer...\")\n",
        "            # Use the Trainer class instead of SFTTrainer\n",
        "            # Use SFTTrainer\n",
        "            self.trainer = SFTTrainer(\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                train_dataset=self.dataset[\"train\"],\n",
        "                eval_dataset=self.dataset[\"test\"], # We need this\n",
        "                dataset_text_field=self.dataset_text_field,\n",
        "                max_seq_length=self.max_seq_length,\n",
        "                dataset_num_proc=self.config.get(\"dataset_num_proc\", 2),\n",
        "                args=self.training_args,\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in _act(): {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Act finished.\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Executes the OODA loop and fine-tunes the language model.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(f\"Starting Run ...\")\n",
        "        clear_memory()\n",
        "        self._observe()\n",
        "        if self.model is None:\n",
        "            print(\"Model loading failed, skipping _orient, _decide and _act\")\n",
        "            return\n",
        "        self._orient()\n",
        "        self._decide()\n",
        "        self._act()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Run Dataset: {self.dataset}\")\n",
        "        if self.dataset and \"test\" in self.dataset:\n",
        "          print(f\"Test Dataset Size: {len(self.dataset['test'])}\")\n",
        "        else:\n",
        "          print(\"No test dataset found or dataset is None.\")\n",
        "        print(f\"Eval Batch Size: {self.trainer.args.per_device_eval_batch_size}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if self.trainer is not None:\n",
        "            try:\n",
        "                # Train the model\n",
        "                self.trainer.train()\n",
        "                print(\"\\n\")\n",
        "                print(\"Evaluation:\")\n",
        "                eval_results = self.evaluate()\n",
        "                print(\"\\n\")\n",
        "                print(eval_results)\n",
        "                print(\"\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during training or evaluation: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"Trainer is None. Skipping training and evaluation.\")\n",
        "\n",
        "        print(f\"Run  finished.\")\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the fine-tuned language model.\n",
        "        \"\"\"\n",
        "        return self.trainer.evaluate()\n",
        "\n",
        "    def _preprocess_function_mrpc(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the SetFit/mrpc dataset.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: SetFit/mrpc\")\n",
        "        inputs = self.tokenizer(\n",
        "            examples[\"text1\"],\n",
        "            examples[\"text2\"],\n",
        "            max_length=128,  # Adjust as needed\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "        # Corrected labels for classification models\n",
        "        inputs[\"labels\"] = examples[\"label\"]\n",
        "        return inputs\n",
        "\n",
        "    def _preprocess_function_sql_create_context(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the b-mc2/sql-create-context dataset.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: b-mc2/sql-create-context\")\n",
        "        # Tokenize inputs and labels\n",
        "        inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "        model_inputs = self.tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        # Tokenize labels\n",
        "        labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        # Assign labels to model_inputs\n",
        "        model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        # Add 'text' field for the text in the model.\n",
        "        model_inputs[\"text\"] = inputs\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def _preprocess_function_anthropic_hh_rlhf(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the anthropic/hh-rlhf dataset.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: anthropic/hh-rlhf\")\n",
        "        # Construct \"question\" and \"context\" using the 'text' column for b-mc2/sql-create-context\n",
        "        inputs = examples[\"chosen\"]\n",
        "\n",
        "        model_inputs = self.tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "        # Tokenize labels\n",
        "        labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "        model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        # Add 'text' field for the text in the model.\n",
        "        model_inputs[\"text\"] = inputs\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "# Configuration for experiments\n",
        "RL_PAIRS = [\n",
        "    # mrpc\n",
        "    {\n",
        "        \"model_id\": \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "        \"dataset_name\": \"SetFit/mrpc\",\n",
        "        \"config\": {\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",  # Corrected model ID for Unsloth\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "            \"access_token\": None,  # No needed a token\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./unsloth_mrpc_output\",\n",
        "                \"per_device_train_batch_size\": 2,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"report_to\": None,\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_8bit\",\n",
        "                \"logging_steps\": 10,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"fp16\": False,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"lr_scheduler_type\": \"linear\",\n",
        "                \"num_train_epochs\": 1,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"max_steps\": 60,\n",
        "                \"seed\": 3407,\n",
        "                \"evaluation_strategy\": \"steps\", # We need this\n",
        "                \"eval_steps\": 20, # We need this\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    # sql-create-context\n",
        "    {\n",
        "        \"model_id\": \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "        \"dataset_name\": \"b-mc2/sql-create-context\",\n",
        "        \"config\": {\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",  # Corrected model ID for Unsloth\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "            \"access_token\": None,  # No needed a token\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./unsloth_sql_create_context_output\",\n",
        "                \"per_device_train_batch_size\": 2,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"report_to\": None,\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_8bit\",\n",
        "                \"logging_steps\": 10,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"fp16\": False,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"lr_scheduler_type\": \"linear\",\n",
        "                \"num_train_epochs\": 1,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"max_steps\": 60,\n",
        "                \"seed\": 3407,\n",
        "                \"evaluation_strategy\": \"steps\", # We need this\n",
        "                \"eval_steps\": 20, # We need this\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    # hh-rlhf\n",
        "    {\n",
        "        \"model_id\": \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "        \"dataset_name\": \"anthropic/hh-rlhf\",\n",
        "        \"config\": {\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",  # Corrected model ID for Unsloth\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "            \"access_token\": None,  # No needed a token\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./unsloth_hh_rlhf_output\",\n",
        "                \"per_device_train_batch_size\": 2,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"report_to\": None,\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_8bit\",\n",
        "                \"logging_steps\": 10,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"fp16\": False,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"lr_scheduler_type\": \"linear\",\n",
        "                \"num_train_epochs\": 1,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"max_steps\": 60,\n",
        "                \"seed\": 3407,\n",
        "                \"evaluation_strategy\": \"steps\", # We need this\n",
        "                \"eval_steps\": 20, # We need this\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "# Run the experiments\n",
        "for rl_pair in RL_PAIRS:\n",
        "    print(\"\\n\")\n",
        "    print(\"*\" * 50)\n",
        "    print(\n",
        "        f\"Running experiment with model: {rl_pair['model_id']} and dataset: {rl_pair['dataset_name']}\"\n",
        "    )\n",
        "    print(\"*\" * 50)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    agent = FineTuningAgent(\n",
        "        model_id=rl_pair[\"model_id\"],\n",
        "        dataset_name=rl_pair[\"dataset_name\"],\n",
        "        config=rl_pair[\"config\"],\n",
        "    )\n",
        "    # Initiate the OODA loop and fine-tuning process\n",
        "    agent.run()\n",
        "    print(\"\\n\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "38a28775728d4fa99bdca87c7b4b83fd",
            "825a8d18ccf74d8683c99a3d1f2a3100",
            "56635e8ccfcd4fe58a6cf4db8f3b5980",
            "03181a88dfd04757acb1bf2e0e901df6",
            "6709642b700b497bb2de78fe50a0abb2",
            "af265f75b71848a9b3630e6d2de44aa7",
            "8c102f0fb01f4980bbdedb0f2e534cc0",
            "15f74de3ae2c47aaaea982aa677cd112",
            "ecd1ffdeeddb4bd99b6fad5aa1f15532",
            "1aa4d0e36a8e459ca9c310b510083ebe",
            "933807be97884d96967a1a7eab5fa23c",
            "87f316120eb94699a789e828aa611cce",
            "d03239448b4f4db79c770c276f1e7788",
            "015c25a7ef574513bc1e03c5c6db99b4",
            "59d12f2763ee4f9999c4a402a72ebb2f",
            "e5a7790adefd4fcc8a7c2996881b28ad",
            "d841a54d34a04f1182952d383bfa8de7",
            "d29f8dfa1f7f408ea6b45ee5f344bbe3",
            "2056f7662344465b8bef74022a5902aa",
            "3e4a4be6cbb6466a9eb514f705489f45",
            "77ae47489589436f8c3facef3c617272",
            "15928a867d284cc1aea9d6bf4ce5ed05",
            "bfc8ca8886ef48769d5c38a6e6256008",
            "a61f28a67c344ee6af46b316ff4b792f",
            "a8579fc7a71b432dae58cc2ae0c557d6",
            "99614c6e879a4613a6b035a99ff24fd8",
            "50d6617ff8804092a95194ffdf029e30",
            "b318e8919b0342fc9c246d5147d0c57f",
            "c59c0fb3eddb446aa624b6cf08c73a6a",
            "6331cd8b254f4b9e96881b93ecff026e",
            "451949c5b6504b7e8ed5cf874dd6753f",
            "13b5d1ad8d8647458729f3d359c02cc6",
            "d2af2ff63664427a927c5adbce61ab49",
            "5347db2a0b0c41d28eb3bf476271f68a",
            "64a6a94f4cd04adf88eefb6f5ba9cc7a",
            "2cecd2f5dc074c5f93c15aae8edc2a7a",
            "70ca418cfc654ba8a8d417df55db5602",
            "6065ceecb2e14fea84deb1ddc0917026",
            "5ed73cf674e748e8a03548aa3d2592f7",
            "b680e560339f455697f8de8f6be36981",
            "d469482af0e14ab1a69690413ea6dfe2",
            "c36035d44e294180b679232ea3108242",
            "07918a203e3a4ba9b83ec7cbf33d9a5e",
            "cc502046aa294be293a2fe9c967c763c"
          ]
        },
        "id": "FBh553x7ISPN",
        "outputId": "1c2a2256-3560-4c1a-e713-408788ec6892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**************************************************\n",
            "Running experiment with model: unsloth/DeepSeek-R1-Distill-Llama-8B and dataset: SetFit/mrpc\n",
            "**************************************************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Loading model using Unsloth...\n",
            "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.46.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "deepseek-ai/deepseek-coder-1.3b-base does not have a padding token! Will use pad_token = <pad>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Observe finished.\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38a28775728d4fa99bdca87c7b4b83fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87f316120eb94699a789e828aa611cce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "\n",
            "\n",
            "LORA: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'v_proj', 'k_proj', 'q_proj', 'down_proj', 'o_proj', 'gate_proj', 'up_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 59,965,440 || all params: 1,406,437,376 || trainable%: 4.2636\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "Test Dataset Size: 25\n",
            "Eval Batch Size: 2\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 59,965,440\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:44, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.981900</td>\n",
              "      <td>6.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.334400</td>\n",
              "      <td>5.182404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.833600</td>\n",
              "      <td>5.084407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Evaluation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'eval_loss': 5.083302974700928, 'eval_runtime': 1.4623, 'eval_samples_per_second': 17.097, 'eval_steps_per_second': 8.89, 'epoch': 4.8}\n",
            "\n",
            "\n",
            "Run  finished.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**************************************************\n",
            "Running experiment with model: unsloth/DeepSeek-R1-Distill-Llama-8B and dataset: b-mc2/sql-create-context\n",
            "**************************************************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Loading model using Unsloth...\n",
            "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.46.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "deepseek-ai/deepseek-coder-1.3b-base does not have a padding token! Will use pad_token = <pad>.\n",
            "\n",
            "\n",
            "Observe finished.\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: b-mc2/sql-create-context\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfc8ca8886ef48769d5c38a6e6256008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: b-mc2/sql-create-context\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "\n",
            "\n",
            "LORA: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'v_proj', 'k_proj', 'q_proj', 'down_proj', 'o_proj', 'gate_proj', 'up_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 59,965,440 || all params: 1,406,437,376 || trainable%: 4.2636\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "Test Dataset Size: 25\n",
            "Eval Batch Size: 2\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 59,965,440\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:43, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.991000</td>\n",
              "      <td>11.193081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.347400</td>\n",
              "      <td>10.053211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.965700</td>\n",
              "      <td>9.947631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Evaluation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'eval_loss': 9.9539155960083, 'eval_runtime': 1.4999, 'eval_samples_per_second': 16.668, 'eval_steps_per_second': 8.667, 'epoch': 4.8}\n",
            "\n",
            "\n",
            "Run  finished.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**************************************************\n",
            "Running experiment with model: unsloth/DeepSeek-R1-Distill-Llama-8B and dataset: anthropic/hh-rlhf\n",
            "**************************************************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Loading model using Unsloth...\n",
            "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.46.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "deepseek-ai/deepseek-coder-1.3b-base does not have a padding token! Will use pad_token = <pad>.\n",
            "\n",
            "\n",
            "Observe finished.\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: anthropic/hh-rlhf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5347db2a0b0c41d28eb3bf476271f68a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: anthropic/hh-rlhf\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "\n",
            "\n",
            "LORA: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'v_proj', 'k_proj', 'q_proj', 'down_proj', 'o_proj', 'gate_proj', 'up_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 59,965,440 || all params: 1,406,437,376 || trainable%: 4.2636\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'text'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "Test Dataset Size: 25\n",
            "Eval Batch Size: 2\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 59,965,440\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:43, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.138300</td>\n",
              "      <td>4.697424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.917700</td>\n",
              "      <td>3.343260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.620800</td>\n",
              "      <td>3.137609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Evaluation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'eval_loss': 3.1397392749786377, 'eval_runtime': 1.4632, 'eval_samples_per_second': 17.086, 'eval_steps_per_second': 8.885, 'epoch': 4.8}\n",
            "\n",
            "\n",
            "Run  finished.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/BERT_Text_Classification_on_GLUE_on_TPU_using_Jax_Flax___mdda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a ðŸ¤— Transformers model on TPU with **Flax/JAX**\n",
        "\n",
        "From [the Hugging Face examples](https://github.com/huggingface/notebooks/tree/main/examples) / [text_classification_flax.ipynb](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) models on TPU using [**Flax**](https://flax.readthedocs.io/en/latest/index.html). As can be seen on [this benchmark](https://github.com/huggingface/transformers/tree/master/examples/flax/text-classification#runtime-evaluation) using Flax/JAX on GPU/TPU is often much faster and can also be considerably cheaper than using PyTorch on GPU/TPU.\n",
        "\n",
        "[**Flax**](https://flax.readthedocs.io/en/latest/index.html) is a high-performance neural network library designed for flexibility built on top of JAX (see below). It aims to provide users with full control of their training code and is carefully designed to work well with JAX transformations such as `grad` and `pmap` (see the [Flax philosophy](https://flax.readthedocs.io/en/latest/philosophy.html)). For an introduction to Flax see the [Flax Basic Colab](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html) or the list of curated [Flax examples](https://flax.readthedocs.io/en/latest/examples.html).\n",
        "\n",
        "[**JAX**](https://jax.readthedocs.io/en/latest/index.html) is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more. A great place for getting started with JAX is the [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets as well as [Flax](https://github.com/google/flax.git) and [Optax](https://github.com/deepmind/optax). Optax is a gradient processing and optimization library for JAX, and is the optimizer library\n",
        "recommended by Flax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "!pip install flax\n",
        "!pip install git+https://github.com/deepmind/optax.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwRqMUxqzOKN"
      },
      "source": [
        "You also will need to set up the TPU for JAX in this notebook. This can be done by executing the following lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drej_4loxdCF"
      },
      "source": [
        "import jax.tools.colab_tpu"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAIlBLFXXix7"
      },
      "source": [
        "If everything is set up correctly, the following command should return a list of 8 TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc0dzzrsXktW",
        "outputId": "0895cca5-45b4-4b44-ec4b-f7f294a4185b"
      },
      "source": [
        "jax.local_devices()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "You can find a script version of this notebook to fine-tune your model [here](https://github.com/huggingface/transformers/tree/master/examples/flax/text-classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlpF5MnonmmQ"
      },
      "source": [
        "As an example, we will fine-tune a pretrained [*auto-encoding model*](https://huggingface.co/transformers/model_summary.html#autoencoding-models) on a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/). Note that this notebook does not focus so much on data preprocessing, but rather on how to write a training and evaluation loop in **JAX/Flax**. If you want more detailed explanations regarding the data preprocessing, please check out [this](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=545PP3o8IrJV) notebook.\n",
        "\n",
        "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
        "[CoLA](https://nyu-mll.github.io/CoLA/), [MNLI](https://arxiv.org/abs/1704.05426), [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398), [QNLI](https://rajpurkar.github.io/SQuAD-explorer/), [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs), [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment), [SST-2](https://nlp.stanford.edu/sentiment/index.html), [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark), [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html).\n",
        "\n",
        "We will see how to easily load the dataset for each one of those tasks and how to write a training loop in Flax. Each task is named by its acronym, with `mnli-mm` standing for the mismatched version of MNLI (so same training set as `mnli` but different validation and test sets):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZbiBDuGIrId"
      },
      "source": [
        "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any of the tasks in the list above, with any Flax/JAX model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a classification head. Depending on the model you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "source": [
        "task = \"cola\"\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "per_device_batch_size = 4"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "!pip install evaluate -q"
      ],
      "metadata": {
        "id": "7X9Cybah4dDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
        "is_regression = task == \"stsb\"\n",
        "\n",
        "raw_dataset = load_dataset(\"glue\", actual_task)\n",
        "metric = evaluate.load('glue', actual_task)"
      ],
      "metadata": {
        "id": "1FPxO7xf4j3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "Apart from `mnli-mm` being a special code, we can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs. This includes converting the tokens to their corresponding IDs in the pretrained vocabulary and putting them in a format the model expects, as well as generate the other inputs that the model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer than what the model can handle will be truncated to the maximum length accepted by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgjEr0fKMRQ_"
      },
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    texts = (\n",
        "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "    )\n",
        "    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n",
        "\n",
        "    processed[\"labels\"] = examples[\"label\"]\n",
        "    return processed"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function to all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in the dataset, so our training, validation, and testing data will be preprocessed in one single command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "source": [
        "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True, remove_columns=raw_dataset[\"train\"].column_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "As a final step, we split the dataset into the *train* and *validation* dataset and give each set more explicit names."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( raw_dataset['train'][32] )\n",
        "print( raw_dataset['train'][49] )\n",
        "print( tokenized_dataset['train'][49] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rBdU4Zirk2T",
        "outputId": "bbecaa7d-354c-441e-bf63-23280ff8a496"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': 'Bill coughed out the window.', 'label': 1, 'idx': 32}\n",
            "{'sentence': 'The dog barked out of the room.', 'label': 0, 'idx': 49}\n",
            "{'input_ids': [101, 1109, 3676, 18849, 1149, 1104, 1103, 1395, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K03vmCmPqjIP"
      },
      "source": [
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "eval_dataset = tokenized_dataset[\"validation\"]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `FlaxAutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us.\n",
        "\n",
        "All weight parameters that are not found in the pretrained model weights will be randomly initialized upon instantiating the model class. Because the GLUE task contains relatively small training datasets, a different seed for weight initialization might very well lead to significantly different results. For\n",
        "reproducibility, we set the random seed to *0* in this notebook.\n",
        "\n",
        "The only thing we have to specify in the config is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem, and MNLI where we have 3 labels):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "source": [
        "from transformers import FlaxAutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
        "seed = 0\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "model = FlaxAutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config, seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some others (the `pre_classifier` and `classifier` layers). This is normal in this case because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnQ43MgRqV-K"
      },
      "source": [
        "To write a full training and evaluation loop in Flax, we will need to import a couple of packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn1GdGpipfWK"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from flax.training import train_state\n",
        "from flax import traverse_util  # mdda\n",
        "\n",
        "import optax\n",
        "\n",
        "# Misc other tools\n",
        "from itertools import chain\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Callable"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3GOlfPQrRdY"
      },
      "source": [
        "For all GLUE tasks except `\"wnli\"` and `\"mrpc\"`, it is usually sufficient to train for just 3 epochs. `\"wnli\"` and `\"mrpc\"` are so small that we recommend training on 5 epochs. We use a learning rate of *0.00002*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zpJOCJhmSno"
      },
      "source": [
        "num_train_epochs = 3 if task not in [\"mrpc\", \"wnli\"] else 5\n",
        "learning_rate = 2e-5"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7mQfa25tAFb"
      },
      "source": [
        "We've already set the batch size per device, but are now interested in the effective total batch_size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfP-yW52s-lu",
        "outputId": "3e2d07bd-b9f1-4224-cee2-1b34ab8dca53"
      },
      "source": [
        "total_batch_size = per_device_batch_size * jax.local_device_count()\n",
        "print(\"The overall batch size (both for training and eval) is\", total_batch_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The overall batch size (both for training and eval) is 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDQbpTFvtpyN"
      },
      "source": [
        "Next, we define the learning rate schedule. A simple and effective learning rate schedule is the linear decay with warmup (click [here](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup) for more information). Since GLUE datasets are rather small and therefore have few training steps, we set the number of warmup steps simply to 0. The schedule is then fully defined by the number of training steps and the learning rate.\n",
        "\n",
        "It is recommended to use the [**optax**](https://github.com/deepmind/optax) library for training utilities, *e.g.* learning rate schedules and optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vnrOjubxkFw"
      },
      "source": [
        "num_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\n",
        "\n",
        "learning_rate_function = optax.linear_schedule(init_value=learning_rate, end_value=0, transition_steps=num_train_steps)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xorWAbYWMfEm"
      },
      "source": [
        "### Defining the training state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktzA1CJAyrZB"
      },
      "source": [
        "Next, we will create the *training state* that includes the optimizer, the loss function, and is responsible for updating the model's parameters during training.\n",
        "\n",
        "Most JAX transformations (notably [jax.jit](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)) require functions that are transformed to have no side-effects. This is because any such side-effects will only be executed once, when the Python version of the function is run during compilation (see [Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html)). As a consequence, Flax models (which can be transformed by JAX transformations) are **immutable**, and the state of the model (i.e., its weight parameters) are stored *outside* of the model instance.\n",
        "\n",
        "Models are initialized and updated in a purely functional way: you pass the state to the model when calling it, and the model returns the new (possibly modified) state, leaving the model instance itself unchanged.\n",
        "\n",
        "Flax provides a convenience class [`flax.training.train_state.TrainState`](https://github.com/google/flax/blob/9da95cdd12591f42d2cd4c17089861bff7e43cc5/flax/training/train_state.py#L22), which stores things such as the model parameters, the loss function, the optimizer, and exposes an `apply_gradients` function to update the model's weight parameters.\n",
        "\n",
        "Alright, let's begin by defining our *training state* class. We create a derived `TrainState` class that additionally stores the model's forward pass as `eval_function` as well as a `loss_function`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F81TyR3nJrM"
      },
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "    logits_function: Callable = flax.struct.field(pytree_node=False)\n",
        "    loss_function: Callable = flax.struct.field(pytree_node=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRF_WotzGOpR"
      },
      "source": [
        "We will be using the standard Adam optimizer with weight decay. For more information on AdamW (Adam + weight decay), one can take a look at [this](https://www.fast.ai/2018/07/02/adam-weight-decay/) blog post.\n",
        "\n",
        "AdamW can easily be imported from [optax](https://github.com/deepmind/optax):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlYAEe58JPdy"
      },
      "source": [
        "Regularizing the *bias* and/or *LayerNorm* has not shown to improve performance and can even be disadvantageous, which is why we disable it here. For more information on this, please check out the following [blog post](https://medium.com/@shrutijadon10104776/why-we-dont-use-bias-in-regularization-5a86905dfcd6) or [paper](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "Hence we create a `decay_mask_fn` which makes sure that weight decay is not applied to any *bias* or *LayerNorm* weights. This can easily be done by passing a `mask_fn` to `optax.adamw`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap-zaOyKJDXM"
      },
      "source": [
        "def decay_mask_fn(params):\n",
        "    # See : https://flax.readthedocs.io/en/latest/flax.traverse_util.html#flax.traverse_util.flatten_dict\n",
        "    flat_params = traverse_util.flatten_dict(params)\n",
        "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n",
        "    # i.e. Do the decay where the mask==True\n",
        "    return traverse_util.unflatten_dict(flat_mask)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lku54GQWq_6v"
      },
      "source": [
        "def adamw(weight_decay):\n",
        "    return optax.adamw(learning_rate=learning_rate_function,\n",
        "                       b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay,\n",
        "                       mask=decay_mask_fn)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vbf4KyrKnDl"
      },
      "source": [
        "Now we also need to define the evaluation and loss function given the model's output logits.\n",
        "For regression tasks, the evaluation function simply takes the first logit element and the mean-square error (mse) loss is used. For classification tasks, the evaluation function uses the `argmax` of the logits, and the cross-entropy loss with `num_labels` is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rj31RyjJ_9T"
      },
      "source": [
        "def loss_function(logits, labels):\n",
        "  if is_regression:\n",
        "    return jnp.mean((logits[..., 0] - labels) ** 2)\n",
        "\n",
        "  xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n",
        "  return jnp.mean(xentropy)\n",
        "\n",
        "def eval_function(logits):\n",
        "    return logits[..., 0] if is_regression else logits.argmax(-1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM7LmtIRHWFl"
      },
      "source": [
        "Finally, we put the pieces together to instantiate a `TrainState`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqK0fDCvmxao"
      },
      "source": [
        "state = TrainState.create(\n",
        "    apply_fn=model.__call__,\n",
        "    params=model.params,\n",
        "    tx = adamw(0.01), # mdda\n",
        "    logits_function=eval_function,\n",
        "    loss_function=loss_function,\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQHbpxNjMpXb"
      },
      "source": [
        "### Defining the training and evaluation step\n",
        "\n",
        "During fine-tuning, we want to update the model parameters and evaluate the performance after each epoch.\n",
        "\n",
        "Let's write the functions `train_step` and `eval_step` accordingly. During training the weight parameters should be updated as follows:\n",
        "\n",
        "1. Define a loss function `calc_loss_function` that first runs a forward pass of the model given data input. Remember that Flax models are immutable, and we explicitly pass it the state (in this case the model parameters and the RNG). `calc_loss_function` returns a scalar loss (using the previously defined `state.loss_function`) between the model output and input targets.\n",
        "2. Differentiate this loss function using [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#evaluate-a-function-and-its-gradient-using-value-and-grad). This is a JAX transformation called [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), which computes the gradient of `loss_function` given the input to the function (i.e., the parameters of the model), and returns the value and the gradient in a pair `(loss, gradients)`.\n",
        "3. Compute the mean gradient over all devices using the collective operation [lax.pmean](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.pmean.html). As we will see below, each device runs `train_step` on a different batch of data, but by taking the mean here we ensure the model parameters are the same on all devices.\n",
        "4. Use `state.apply_gradients`, which applies the gradients to the weights.\n",
        "\n",
        "Below, you can see how each of the described steps above is put into practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXDgjdbzn3QA"
      },
      "source": [
        "def train_step(state, batch, dropout_rng):\n",
        "    targets = batch.pop(\"labels\")\n",
        "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "\n",
        "    def calc_loss_function(params):\n",
        "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "        loss = state.loss_function(logits, targets)\n",
        "        return loss\n",
        "\n",
        "    value_and_grad_function = jax.value_and_grad(calc_loss_function)\n",
        "    loss, grad = value_and_grad_function(state.params)\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "    metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n",
        "    return new_state, metrics, new_dropout_rng"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfiPHXYiSvSk"
      },
      "source": [
        "Now, we want to do parallelized training over all TPU devices. To do so, we use [`jax.pmap`](https://jax.readthedocs.io/en/latest/jax.html?highlight=pmap#parallelization-pmap). This will compile the function once and run the same program on each device (it is an [SPMD program](https://en.wikipedia.org/wiki/SPMD)). When calling this pmapped function, all inputs (`\"state\"`, `\"batch\"`, `\"dropout_rng\"`) should be replicated for all devices, which means that the first axis of each argument is used to map over all TPU devices.\n",
        "\n",
        "The argument `donate_argnums` is used to tell JAX that the first argument `\"state\"` is \"donated\" to the computation, because it is not needed anymore afterwards. XLA can make use of donated buffers to reduce the memory needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6eLQN4Jn6jS"
      },
      "source": [
        "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n",
        "# \"jax.pmap JIT-compiles the function given to it as part of its operation,\n",
        "#   so there is no need to additionally jax.jit it.\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu5G-8lfVNzt"
      },
      "source": [
        "Similarly, we can now define the evaluation step. Here, the function is much easier as it simply needs to stack the model's forward pass with the previously defined `eval_function` (or `logits_function`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chY-zaUln94G"
      },
      "source": [
        "def eval_step(state, batch):\n",
        "    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
        "    return state.logits_function(logits)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHdL0Op8Vrdu"
      },
      "source": [
        "We then also apply `jax.pmap` to the evaluation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P8DBfk2oA_e"
      },
      "source": [
        "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
        "# implicit jit.."
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b99v2GVjWCRr"
      },
      "source": [
        "### Defining the data collators\n",
        "\n",
        "In a final step before we can start training, we need to define the data collators. The data collator is important to shuffle the training data before each epoch and to prepare the batch for each training and evaluation step.\n",
        "\n",
        "Let's start with the training collator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7jyyxibXGks"
      },
      "source": [
        "The training collator can be defined as a [Python generator](https://wiki.python.org/moin/Generators) that returns a batch model input every time it is called.\n",
        "\n",
        "First, a random permutation of the whole dataset is defined.\n",
        "Then, every time the training data collator is called the next batch of the randomized dataset is extracted, converted to a JAX array and sharded over all local TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a quick test to make sure we understand how 'dataset' works...\n",
        "for k, v in train_dataset[(34,35)].items():\n",
        "  v_jnp=jnp.array(v)\n",
        "  if k=='labels': continue # This is like (N, )\n",
        "  print(k, v_jnp.shape)\n",
        "  print( v_jnp[:10,:10] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uqI3jgL13tW",
        "outputId": "ed1a3cf5-3c83-446f-e464-7465f72fbd77"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids (2, 128)\n",
            "[[  101  1109 12356 16795  1194  1103  1837  1154  1103  3119]\n",
            " [  101  2617  8756  1228  1103  1837   119   102     0     0]]\n",
            "token_type_ids (2, 128)\n",
            "[[0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]]\n",
            "attention_mask (2, 128)\n",
            "[[1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ6iVzivl88S"
      },
      "source": [
        "def glue_train_data_loader(rng, dataset, batch_size):\n",
        "    steps_per_epoch = len(dataset) // batch_size\n",
        "    perms = jax.random.permutation(rng, len(dataset))\n",
        "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "    for perm in perms:\n",
        "        batch = dataset[perm]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "        batch = shard(batch) # from flax.training.common_utils\n",
        "\n",
        "        yield batch"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOSPOyMYZt29"
      },
      "source": [
        "We define the eval data collator in a similar fashion.\n",
        "\n",
        "**Note**:\n",
        "For simplicity, we throw away the last incomplete batch since it can't be easily sharded over all devices. This means that the evaluation results might be slightly incorrect. It can be easily fixed by including [this](https://github.com/huggingface/transformers/blob/f063c56d942737d2c7aac93895cd8310afd9c7a4/examples/flax/text-classification/run_flax_glue.py#L491) part in the training loop after evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZpbf9pamCEW"
      },
      "source": [
        "def glue_eval_data_loader(dataset, batch_size):\n",
        "    for i in range(len(dataset) // batch_size):\n",
        "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "        batch = shard(batch) # from flax.training.common_utils\n",
        "\n",
        "        yield batch"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf_eG7OBbGAJ"
      },
      "source": [
        "Next, we replicate/copy the weight parameters on each device, so that we can pass them to our pmapped functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1sA7cjySB2T"
      },
      "source": [
        "state = flax.jax_utils.replicate(state)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I42ZJzpgbo-s"
      },
      "source": [
        "### Training\n",
        "\n",
        "Finally, we can write down the full training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq40T7_Wbw2p"
      },
      "source": [
        "Let's start by generating a seeded `PRNGKey` for the dropout layers and dataset shuffling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wyaedrnty4-"
      },
      "source": [
        "rng = jax.random.PRNGKey(seed)\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xByhGKVUcquw"
      },
      "source": [
        "Now we define the full training loop. For each batch in each epoch, we run a training step. Here, we also need to make sure that the PRNGKey is sharded/split over each device. Having completed an epoch, we report the training metrics and can run the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs = 10 if task not in [\"mrpc\", \"wnli\"] else 5\n",
        "learning_rate = 2e-5"
      ],
      "metadata": {
        "id": "WYZhS5fk6Ruw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBARxJ8soOrs"
      },
      "source": [
        "for i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "    # train\n",
        "    with tqdm(total=len(train_dataset) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
        "      for batch in glue_train_data_loader(input_rng, train_dataset, total_batch_size):\n",
        "        state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
        "        progress_bar_train.update(1)\n",
        "\n",
        "    # evaluate\n",
        "    with tqdm(total=len(eval_dataset) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
        "      for batch in glue_eval_data_loader(eval_dataset, total_batch_size):\n",
        "          labels = batch.pop(\"labels\")\n",
        "          predictions = parallel_eval_step(state, batch)\n",
        "          metric.add_batch(predictions=list(chain(*predictions)), references=list(chain(*labels)))\n",
        "          progress_bar_eval.update(1)\n",
        "\n",
        "    eval_metric = metric.compute()\n",
        "\n",
        "    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n",
        "    eval_score = round(list(eval_metric.values())[0], 3)\n",
        "    metric_name = list(eval_metric.keys())[0]\n",
        "\n",
        "    print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-5, 2e-5, 5e-5]\n",
        "num_epochs_list = [3, 5, 10]\n",
        "results = []"
      ],
      "metadata": {
        "id": "t_83zKVTFlxA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in learning_rates:\n",
        "    for epochs in num_epochs_list:\n",
        "        print(f\"\\nRunning experiment with Learning Rate: {lr}, Epochs: {epochs}\")\n",
        "\n",
        "        # Update hyperparameters\n",
        "        learning_rate = lr\n",
        "        num_train_epochs = epochs\n",
        "\n",
        "        # Recalculate steps based on updated epochs\n",
        "        num_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\n",
        "        num_warmup_steps = int(num_train_steps * 0.1)\n",
        "\n",
        "        # Re-initialize learning rate function and optimizer\n",
        "        learning_rate_function = optax.warmup_cosine_decay_schedule(\n",
        "            init_value=0.0,\n",
        "            peak_value=learning_rate,\n",
        "            warmup_steps=num_warmup_steps,\n",
        "            decay_steps=num_train_steps,\n",
        "            end_value=0.0\n",
        "        )\n",
        "\n",
        "        def adamw(weight_decay):\n",
        "            return optax.adamw(learning_rate=learning_rate_function,\n",
        "                               b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay,\n",
        "                               mask=decay_mask_fn)\n",
        "\n",
        "        # Re-create and replicate the training state\n",
        "        state = TrainState.create(\n",
        "            apply_fn=model.__call__,\n",
        "            params=model.params,\n",
        "            tx = adamw(weight_decay),\n",
        "            logits_function=eval_function,\n",
        "            loss_function=loss_function,\n",
        "        )\n",
        "        state = flax.jax_utils.replicate(state)\n",
        "\n",
        "        # Reset the random number generator\n",
        "        rng = jax.random.PRNGKey(seed)\n",
        "        dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "        # Run training and evaluation loop\n",
        "        for i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
        "            rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "            # train\n",
        "            with tqdm(total=len(train_dataset) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
        "              for batch in glue_train_data_loader(input_rng, train_dataset, total_batch_size):\n",
        "                state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
        "                progress_bar_train.update(1)\n",
        "\n",
        "            # evaluate\n",
        "            with tqdm(total=len(eval_dataset) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
        "              for batch in glue_eval_data_loader(eval_dataset, total_batch_size):\n",
        "                  labels = batch.pop(\"labels\")\n",
        "                  predictions = parallel_eval_step(state, batch)\n",
        "                  metric.add_batch(predictions=list(chain(*predictions)), references=list(chain(*labels)))\n",
        "                  progress_bar_eval.update(1)\n",
        "\n",
        "            eval_metric = metric.compute()\n",
        "\n",
        "            loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n",
        "            eval_score = round(list(eval_metric.values())[0], 3)\n",
        "            metric_name = list(eval_metric.keys())[0]\n",
        "\n",
        "            print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'learning_rate': lr,\n",
        "            'num_epochs': epochs,\n",
        "            'eval_metric_name': metric_name,\n",
        "            'eval_metric_score': eval_score\n",
        "        })\n",
        "\n",
        "print(\"\\nExperiment Results:\")\n",
        "for res in results:\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "9PlHTfX7FrqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay_list = [0.0, 0.01, 0.1]\n",
        "\n",
        "# Identify the best performing hyperparameters from the previous step\n",
        "# Assuming the last experiment run with learning_rate=5e-5 and num_epochs=10\n",
        "# yielded the best result based on the output.\n",
        "# If the results were stored and analyzed, this would be based on that analysis.\n",
        "# For this example, we'll hardcode a promising combination based on the trace.\n",
        "promising_lr = 5e-5\n",
        "promising_epochs = 10\n",
        "\n",
        "print(f\"Experimenting with weight decays: {weight_decay_list}\")\n",
        "print(f\"Using promising Learning Rate: {promising_lr} and Epochs: {promising_epochs}\")\n",
        "\n",
        "# Initialize a new list to store results for weight decay experiments\n",
        "weight_decay_results = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt7NWD_QJkfW",
        "outputId": "df6cd40f-e86c-4d64-b689-000df60fc09e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimenting with weight decays: [0.0, 0.01, 0.1]\n",
            "Using promising Learning Rate: 5e-05 and Epochs: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for wd in weight_decay_list:\n",
        "    print(f\"\\nRunning experiment with Weight Decay: {wd}\")\n",
        "\n",
        "    # Update the weight decay hyperparameter\n",
        "    weight_decay = wd\n",
        "    learning_rate = promising_lr # Use the promising learning rate\n",
        "    num_train_epochs = promising_epochs # Use the promising number of epochs\n",
        "\n",
        "    # Recalculate steps based on updated epochs (though epochs are fixed here, good practice)\n",
        "    num_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\n",
        "    num_warmup_steps = int(num_train_steps * 0.1)\n",
        "\n",
        "    # Re-initialize learning rate function and optimizer with the current weight decay\n",
        "    learning_rate_function = optax.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=learning_rate,\n",
        "        warmup_steps=num_warmup_steps,\n",
        "        decay_steps=num_train_steps,\n",
        "        end_value=0.0\n",
        "    )\n",
        "\n",
        "    def adamw(weight_decay_val): # Use a different parameter name to avoid shadowing\n",
        "        return optax.adamw(learning_rate=learning_rate_function,\n",
        "                           b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay_val,\n",
        "                           mask=decay_mask_fn)\n",
        "\n",
        "    # Re-create and replicate the training state\n",
        "    state = TrainState.create(\n",
        "        apply_fn=model.__call__,\n",
        "        params=model.params,\n",
        "        tx = adamw(weight_decay), # Pass the current weight decay value\n",
        "        logits_function=eval_function,\n",
        "        loss_function=loss_function,\n",
        "    )\n",
        "    state = flax.jax_utils.replicate(state)\n",
        "\n",
        "    # Reset the random number generator to ensure reproducibility for each weight decay experiment\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "    # Run training and evaluation loop\n",
        "    for i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "        # train\n",
        "        with tqdm(total=len(train_dataset) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
        "          for batch in glue_train_data_loader(input_rng, train_dataset, total_batch_size):\n",
        "            state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
        "            progress_bar_train.update(1)\n",
        "\n",
        "        # evaluate\n",
        "        with tqdm(total=len(eval_dataset) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
        "          for batch in glue_eval_data_loader(eval_dataset, total_batch_size):\n",
        "              labels = batch.pop(\"labels\")\n",
        "              predictions = parallel_eval_step(state, batch)\n",
        "              metric.add_batch(predictions=list(chain(*predictions)), references=list(chain(*labels)))\n",
        "              progress_bar_eval.update(1)\n",
        "\n",
        "        eval_metric = metric.compute()\n",
        "\n",
        "        loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n",
        "        eval_score = round(list(eval_metric.values())[0], 3)\n",
        "        metric_name = list(eval_metric.keys())[0]\n",
        "\n",
        "        print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")\n",
        "\n",
        "    # Store results for the current weight decay experiment\n",
        "    weight_decay_results.append({\n",
        "        'learning_rate': learning_rate,\n",
        "        'num_epochs': num_train_epochs,\n",
        "        'weight_decay': wd,\n",
        "        'eval_metric_name': metric_name,\n",
        "        'eval_metric_score': eval_score\n",
        "    })\n",
        "\n",
        "print(\"\\nWeight Decay Experiment Results:\")\n",
        "for res in weight_decay_results:\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "ZxQ1HLEgJxo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Experiment Summary ---\")\n",
        "\n",
        "print(\"\\nLearning Rate and Epoch Experiments:\")\n",
        "for res in results:\n",
        "    print(res)\n",
        "\n",
        "print(\"\\nWeight Decay Experiments:\")\n",
        "for res in weight_decay_results:\n",
        "    print(res)\n",
        "\n",
        "# Find the best result from the learning rate and epoch experiments\n",
        "best_lr_epoch_result = max(results, key=lambda x: x['eval_metric_score'])\n",
        "\n",
        "# Find the best result from the weight decay experiments\n",
        "best_wd_result = max(weight_decay_results, key=lambda x: x['eval_metric_score'])\n",
        "\n",
        "print(\"\\n--- Optimal Hyperparameters Found ---\")\n",
        "\n",
        "# Determine the overall best result\n",
        "if best_lr_epoch_result['eval_metric_score'] >= best_wd_result['eval_metric_score']:\n",
        "    optimal_hyperparameters = best_lr_epoch_result\n",
        "    print(f\"Optimal Combination (from LR and Epoch experiments):\")\n",
        "    print(f\"  Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "    print(f\"  Number of Epochs: {optimal_hyperparameters['num_epochs']}\")\n",
        "    print(f\"  Weight Decay: {weight_decay}\") # Note: weight decay was fixed in LR/Epoch experiments\n",
        "    print(f\"  Evaluation Metric ({optimal_hyperparameters['eval_metric_name']}): {optimal_hyperparameters['eval_metric_score']}\")\n",
        "else:\n",
        "    optimal_hyperparameters = best_wd_result\n",
        "    print(f\"Optimal Combination (from Weight Decay experiments):\")\n",
        "    print(f\"  Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "    print(f\"  Number of Epochs: {optimal_hyperparameters['num_epochs']}\")\n",
        "    print(f\"  Weight Decay: {optimal_hyperparameters['weight_decay']}\")\n",
        "    print(f\"  Evaluation Metric ({optimal_hyperparameters['eval_metric_name']}): {optimal_hyperparameters['eval_metric_score']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIaOuweIS35U",
        "outputId": "d1682715-23da-4835-a728-598022dcfb98"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Experiment Summary ---\n",
            "\n",
            "Learning Rate and Epoch Experiments:\n",
            "{'learning_rate': 1e-05, 'num_epochs': 3, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.567)}\n",
            "{'learning_rate': 1e-05, 'num_epochs': 5, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.585)}\n",
            "{'learning_rate': 1e-05, 'num_epochs': 10, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.591)}\n",
            "{'learning_rate': 2e-05, 'num_epochs': 3, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.601)}\n",
            "{'learning_rate': 2e-05, 'num_epochs': 5, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.601)}\n",
            "{'learning_rate': 2e-05, 'num_epochs': 10, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.57)}\n",
            "{'learning_rate': 5e-05, 'num_epochs': 3, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.58)}\n",
            "{'learning_rate': 5e-05, 'num_epochs': 5, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.549)}\n",
            "{'learning_rate': 5e-05, 'num_epochs': 10, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.59)}\n",
            "\n",
            "Weight Decay Experiments:\n",
            "{'learning_rate': 5e-05, 'num_epochs': 10, 'weight_decay': 0.0, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.56)}\n",
            "{'learning_rate': 5e-05, 'num_epochs': 10, 'weight_decay': 0.01, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.585)}\n",
            "{'learning_rate': 5e-05, 'num_epochs': 10, 'weight_decay': 0.1, 'eval_metric_name': 'matthews_correlation', 'eval_metric_score': np.float64(0.59)}\n",
            "\n",
            "--- Optimal Hyperparameters Found ---\n",
            "Optimal Combination (from LR and Epoch experiments):\n",
            "  Learning Rate: 2e-05\n",
            "  Number of Epochs: 3\n",
            "  Weight Decay: 0.1\n",
            "  Evaluation Metric (matthews_correlation): 0.601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP-VQOyIrJk"
      },
      "source": [
        "To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOP!  # mdda"
      ],
      "metadata": {
        "id": "Vz6V0dOmlTmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UB8pjv8vxYa"
      },
      "source": [
        "### Sharing fine-tuned model\n",
        "\n",
        "Now that you've succesfully trained a model, you can share it with the community by uploading the fine-tuned model checkpoint and tokenizer to your account on the [hub](https://huggingface.co/models).\n",
        "\n",
        "\n",
        "If you don't have an account yet, you can click [here](https://huggingface.co/join) join the community ðŸ¤—."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vPLVtkS-JHi"
      },
      "source": [
        "In a first step, we install [git-lfs](https://git-lfs.github.com/) to easily upload the model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOHBpZulqSLT"
      },
      "source": [
        "%%capture\n",
        "!sudo apt-get install software-properties-common\n",
        "!sudo curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJduNRXk_V2c"
      },
      "source": [
        "Next, we you will need to store your git credentials so that `git` knows who is uploading the files. You should replace the fields `<your-email-address>` and `<your-name>` with your credentials accordingly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "token = userdata.get('HF_TOKEN')\n",
        "email = userdata.get('EMAIL')\n",
        "name = userdata.get('NAME')"
      ],
      "metadata": {
        "id": "IhIppigEHCN9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email {email}  # e.g. \"patrick.v.platen@gmail.com\"\n",
        "!git config --global user.name {name}  # e.g. \"Patrick von Platen\""
      ],
      "metadata": {
        "id": "xivhEFfDIE-H"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwPQyaTOAM5H"
      },
      "source": [
        "You will need to pass your user authentification token `hf_auth_token` to allow the ðŸ¤— hub to upload model weights under your username. To find your authentification token, you need to log in [here](https://huggingface.co/login), click on your icon (top right), go to *Settings*, then *API Tokens*. You can copy the *User API token* and replace it with the field `<your-auth-token>` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgET7o-Jv2OR"
      },
      "source": [
        "hf_auth_token = token  # e.g. api_DaYgaaVnGdRtznIgiNfotCHFUqmOdARmPx"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRM1k_8FCMiP"
      },
      "source": [
        "Finally, you can give your fine-tuned model a nice `model_id` or leave the default one as noted below. The model will be uploaded under `https://huggingface.co/<your-username>/<your-model-id>`, *e.g.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XGT5eAzBoUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed9cd7a-5e27-457f-e890-09b8cc9147d4"
      },
      "source": [
        "model_id = f\"{model_checkpoint}_fine_tuned_glue_{task}\"\n",
        "print(model_id)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-cased_fine_tuned_glue_cola\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJHgcqvkBsIc"
      },
      "source": [
        "Great! Now all that is left to do is to upload your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIT_aQ-OoxvG"
      },
      "source": [
        "model.push_to_hub(model_id, use_auth_token=hf_auth_token)\n",
        "tokenizer.push_to_hub(model_id, use_auth_token=hf_auth_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlviIJzcC_WD"
      },
      "source": [
        "You can now go to your model page to check it out ðŸ˜Š.\n",
        "\n",
        "We strongly recommend to add a model card so that the community can actually make use of your fine-tuned model. You can do so by clicking on *Create Model Card* and adding a descriptive text in markdown format.\n",
        "\n",
        "A simple description for your fine-tuned model is given by running the following cell. You can simply copy-paste the output to be used as your model card `README.md`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSKl6rX7D_-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6864b0f-3880-425b-8027-caba095e2a57"
      },
      "source": [
        "print(f\"\"\"---\n",
        "language: en\n",
        "license: apache-2.0\n",
        "datasets:\n",
        "- glue\n",
        "---\n",
        "# {\" \".join([x.capitalize() for x in model_id.split(\"_\")])}\n",
        "\n",
        "This checkpoint was initialized from the pre-trained checkpoint {model_checkpoint} and subsequently fine-tuned on GLUE task: {task} using [this](https://colab.research.google.com/drive/162pW3wonGcMMrGxmA-jdxwy1rhqXd90x?usp=sharing) notebook.\n",
        "Training was conducted for {num_train_epochs} epochs, using a linear decaying learning rate of {learning_rate}, and a total batch size of {total_batch_size}.\n",
        "\n",
        "The model has a final training loss of {loss} and a {metric_name} of {eval_score}.\n",
        "\"\"\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "language: en\n",
            "license: apache-2.0\n",
            "datasets:\n",
            "- glue\n",
            "---\n",
            "# Bert-base-cased Fine Tuned Glue Cola\n",
            "\n",
            "This checkpoint was initialized from the pre-trained checkpoint bert-base-cased and subsequently fine-tuned on GLUE task: cola using [this](https://colab.research.google.com/drive/162pW3wonGcMMrGxmA-jdxwy1rhqXd90x?usp=sharing) notebook.\n",
            "Training was conducted for 10 epochs, using a linear decaying learning rate of 5e-05, and a total batch size of 32.\n",
            "\n",
            "The model has a final training loss of 0.0 and a matthews_correlation of 0.59.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpFZeamCI-FJ"
      },
      "source": [
        "An uploaded model would, *e.g.*, look as follows: [patrickvonplaten/bert-base-cased_fine_tuned_glue_mrpc_demo](https://huggingface.co/patrickvonplaten/bert-base-cased_fine_tuned_glue_mrpc_demo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6591be7e",
        "outputId": "ad35719d-12f4-4bb3-c70a-356d3626d46b"
      },
      "source": [
        "# Test the metric calculation with dummy data\n",
        "print(\"Testing metric calculation with dummy data:\")\n",
        "\n",
        "# Create a new metric instance for testing\n",
        "test_metric = evaluate.load('glue', actual_task)\n",
        "\n",
        "# Sample dummy data (replace with more representative data if needed)\n",
        "# For classification, predictions and references should be class labels (integers)\n",
        "# For regression (stsb), they should be floats\n",
        "if is_regression:\n",
        "    dummy_predictions = [1.2, 0.8, 2.1, 1.5]\n",
        "    dummy_references = [1.0, 0.9, 2.0, 1.6]\n",
        "else:\n",
        "    # Assuming a binary classification task like CoLA (0 or 1 labels)\n",
        "    dummy_predictions = [1, 0, 1, 1, 0]\n",
        "    dummy_references = [1, 0, 0, 1, 0]\n",
        "\n",
        "\n",
        "print(f\"Dummy Predictions: {dummy_predictions}\")\n",
        "print(f\"Dummy References: {dummy_references}\")\n",
        "\n",
        "# Add the dummy data to the test metric\n",
        "test_metric.add_batch(predictions=dummy_predictions, references=dummy_references)\n",
        "\n",
        "# Compute the metric\n",
        "test_eval_metric = test_metric.compute()\n",
        "\n",
        "print(f\"Computed metric with dummy data: {test_eval_metric}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing metric calculation with dummy data:\n",
            "Dummy Predictions: [1, 0, 1, 1, 0]\n",
            "Dummy References: [1, 0, 0, 1, 0]\n",
            "Computed metric with dummy data: {'matthews_correlation': np.float64(0.6666666666666666)}\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/mistral_finetune_7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting starting fine-tuning Mistral 7B\n",
        "\n",
        "This notebook shows you a simple example of how to LoRA finetune Mistral 7B. You can can run this notebook in Google Colab with Pro + account with A100 and 40GB RAM.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mistralai/mistral-finetune/blob/main/tutorials/mistral_finetune_7b.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "\n",
        "Check out `mistral-finetune` Github repo to learn more: https://github.com/mistralai/mistral-finetune/"
      ],
      "metadata": {
        "id": "RyuOCYM92LJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe8idthWDTBj",
        "outputId": "ec6ee2e0-6a59-4180-ce45-821381cc9f6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun  6 01:53:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Clone the `mistral-finetune` repo:\n"
      ],
      "metadata": {
        "id": "yxr8mv-17GfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/mistralai/mistral-finetune.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIj3IlIeVDIb",
        "outputId": "1c89094e-a9c5-4682-f578-8d9674fcaabd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'mistral-finetune'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 194 (delta 35), reused 15 (delta 15), pack-reused 139\u001b[K\n",
            "Receiving objects: 100% (194/194), 147.06 KiB | 16.34 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all required dependencies:"
      ],
      "metadata": {
        "id": "mQPd_pGT7WiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/mistral-finetune/requirements.txt -q"
      ],
      "metadata": {
        "id": "KuTOGipl7BS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model download"
      ],
      "metadata": {
        "id": "LgdIAi257jLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar"
      ],
      "metadata": {
        "id": "cdl_R5baUyha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!DIR=/content/mistral_models && mkdir -p $DIR && tar -xf mistral-7B-v0.3.tar -C $DIR"
      ],
      "metadata": {
        "id": "IgJWR-fReilz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, you can download the model from Hugging Face\n",
        "\n",
        "# !pip install huggingface_hub\n",
        "# from huggingface_hub import snapshot_download\n",
        "# from pathlib import Path\n",
        "\n",
        "# mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')\n",
        "# mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n",
        "\n",
        "#! cp -r /root/mistral_models/7B-v0.3 /content/mistral_models\n",
        "#! rm -r /root/mistral_models/7B-v0.3"
      ],
      "metadata": {
        "id": "qgjAADBFHB0S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/mistral_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PxYGmcy4gu0",
        "outputId": "b98111ec-635e-4f80-c6fe-cf8e3f934d83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "consolidated.safetensors  params.json  tokenizer.model.v3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset\n",
        "\n",
        "To ensure effective training, mistral-finetune has strict requirements for how the training data has to be formatted. Check out the required data formatting [here](https://github.com/mistralai/mistral-finetune/tree/main?tab=readme-ov-file#prepare-dataset).\n",
        "\n",
        "In this example, let’s use the ultrachat_200k dataset. We load a chunk of the data into Pandas Dataframes, split the data into training and validation, and save the data into the required `jsonl` format for fine-tuning."
      ],
      "metadata": {
        "id": "ams-19wF8zgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T33N2SwCIhEl",
        "outputId": "cae1fba7-dccd-4f06-f5a1-b2ebaded0dda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a new directory called data\n",
        "!mkdir -p data"
      ],
      "metadata": {
        "id": "i7bmgXvG1vUq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# navigate to this data directory\n",
        "%cd /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br2czKwwFLE8",
        "outputId": "c599450a-b40e-4057-d072-ceab947931ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data into a pandas dataframe\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet('https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/resolve/main/data/test_gen-00000-of-00001-3d4cd8309148a71f.parquet')"
      ],
      "metadata": {
        "id": "RVF8VqU110sB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training and evaluation\n",
        "df_train=df.sample(frac=0.95,random_state=200)\n",
        "df_eval=df.drop(df_train.index)"
      ],
      "metadata": {
        "id": "Qog1ZEUn12KQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save data into .jsonl files\n",
        "df_train.to_json(\"ultrachat_chunk_train.jsonl\", orient=\"records\", lines=True)\n",
        "df_eval.to_json(\"ultrachat_chunk_eval.jsonl\", orient=\"records\", lines=True)"
      ],
      "metadata": {
        "id": "I4Yb3NJp13sG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc9q_g7EFQLf",
        "outputId": "98e4df23-0780-4ef3-f3ba-4cae7d0346dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ultrachat_chunk_eval.jsonl  ultrachat_chunk_train.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# navigate to the mistral-finetune directory\n",
        "%cd /content/mistral-finetune/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIK0VFXHIn8r",
        "outputId": "d8f84345-00f1-45d0-b019-80035f57aad8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mistral-finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some of the training data doesn't have the right format,\n",
        "# so we need to reformat the data into the correct format and skip the cases that doesn't have the right format:\n",
        "\n",
        "!python -m utils.reformat_data /content/data/ultrachat_chunk_train.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLHNxpN4GS3i",
        "outputId": "b22285c6-37f0-4ef3-8a26-63c088c58bcd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip 3674th sample\n",
            "Skip 9176th sample\n",
            "Skip 10559th sample\n",
            "Skip 13293th sample\n",
            "Skip 13973th sample\n",
            "Skip 15219th sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval data looks all good\n",
        "!python -m utils.reformat_data /content/data/ultrachat_chunk_eval.jsonl"
      ],
      "metadata": {
        "id": "RscZFo7tGvzS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEZqD89kbAi_",
        "outputId": "dc1da045-e926-4425-8765-731fafb66050"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.2/289.2 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can verify your training yaml to make sure the data is correctly formatted and to get an estimate of your training time.\n",
        "\n",
        "!python -m utils.validate_data --train_yaml example/7B.yaml\n"
      ],
      "metadata": {
        "id": "fqhyigF8XVUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start training"
      ],
      "metadata": {
        "id": "Hia7n0T1_mHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these info is needed for training\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "metadata": {
        "id": "ZtcLerooWFeB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training configuration\n",
        "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
        "\n",
        "config = \"\"\"\n",
        "# data\n",
        "data:\n",
        "  instruct_data: \"/content/data/ultrachat_chunk_train.jsonl\"  # Fill\n",
        "  data: \"\"  # Optionally fill with pretraining data\n",
        "  eval_instruct_data: \"/content/data/ultrachat_chunk_eval.jsonl\"  # Optionally fill\n",
        "\n",
        "# model\n",
        "model_id_or_path: \"/content/mistral_models\"  # Change to downloaded path\n",
        "lora:\n",
        "  rank: 64\n",
        "\n",
        "# optim\n",
        "# tokens per training steps = batch_size x num_GPUs x seq_len\n",
        "# we recommend sequence lentgh of 32768\n",
        "# If you run into memory error, you can try reduce the sequence length\n",
        "seq_len: 8192\n",
        "batch_size: 1\n",
        "num_microbatches: 8\n",
        "max_steps: 100\n",
        "optim:\n",
        "  lr: 1.e-4\n",
        "  weight_decay: 0.1\n",
        "  pct_start: 0.05\n",
        "\n",
        "# other\n",
        "seed: 0\n",
        "log_freq: 1\n",
        "eval_freq: 100\n",
        "no_eval: False\n",
        "ckpt_freq: 100\n",
        "\n",
        "save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model\n",
        "\n",
        "run_dir: \"/content/test_ultra\"  # Fill\n",
        "\"\"\"\n",
        "\n",
        "# save the same file locally into the example.yaml file\n",
        "import yaml\n",
        "with open('example.yaml', 'w') as file:\n",
        "    yaml.dump(yaml.safe_load(config), file)\n"
      ],
      "metadata": {
        "id": "5dxTlIQMaJGv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure the run_dir has not been created before\n",
        "# only run this when you ran torchrun previously and created the /content/test_ultra file\n",
        "# ! rm -r /content/test_ultra"
      ],
      "metadata": {
        "id": "ErD1ktQUMyPZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start training\n",
        "\n",
        "!torchrun --nproc-per-node 1 -m train example.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4wFgmwIUTtg",
        "outputId": "ec43a7c2-c35b-427c-cb42-c2e11fad376d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-06 02:05:41.061251: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-06 02:05:41.112747: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-06 02:05:41.112807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-06 02:05:41.114379: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-06 02:05:41.122158: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-06 02:05:42.241436: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "args: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/content/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/content/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/content/mistral_models', run_dir='/content/test_ultra', optim=OptimArgs(lr=0.0001, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=8, seq_len=8192, batch_size=1, max_norm=1.0, max_steps=100, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=1, wandb=WandbArgs(project=None, offline=False, key=None, run_name=None), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\n",
            "2024-06-06 02:05:43 (UTC) - 0:00:06 - distributed - INFO - torch.cuda.device_count: 1\n",
            "2024-06-06 02:05:43 (UTC) - 0:00:06 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0\n",
            "2024-06-06 02:05:43 (UTC) - 0:00:06 - distributed - INFO - local rank: 0\n",
            "2024-06-06 02:05:43 (UTC) - 0:00:06 - train - INFO - Going to init comms...\n",
            "2024-06-06 02:05:43 (UTC) - 0:00:06 - train - INFO - Run dir: /content/test_ultra\n",
            "2024-06-06 02:05:43 (UTC) - 0:00:06 - train - INFO - TrainArgs: {'batch_size': 1,\n",
            " 'checkpoint': True,\n",
            " 'ckpt_freq': 100,\n",
            " 'data': {'data': '',\n",
            "          'eval_instruct_data': '/content/data/ultrachat_chunk_eval.jsonl',\n",
            "          'instruct': {'dynamic_chunk_fn_call': True, 'shuffle': True},\n",
            "          'instruct_data': '/content/data/ultrachat_chunk_train.jsonl',\n",
            "          'shuffle': False},\n",
            " 'eval_freq': 100,\n",
            " 'log_freq': 1,\n",
            " 'lora': {'dropout': 0.0, 'enable': True, 'rank': 64, 'scaling': 2.0},\n",
            " 'max_norm': 1.0,\n",
            " 'max_steps': 100,\n",
            " 'mlflow': {'experiment_name': None, 'tracking_uri': None},\n",
            " 'model_id_or_path': '/content/mistral_models',\n",
            " 'no_ckpt': False,\n",
            " 'no_eval': False,\n",
            " 'num_ckpt_keep': 3,\n",
            " 'num_microbatches': 8,\n",
            " 'optim': {'lr': 0.0001, 'pct_start': 0.05, 'weight_decay': 0.1},\n",
            " 'run_dir': '/content/test_ultra',\n",
            " 'save_adapters': True,\n",
            " 'seed': 0,\n",
            " 'seq_len': 8192,\n",
            " 'wandb': {'key': None, 'offline': False, 'project': None, 'run_name': None},\n",
            " 'world_size': 1}\n",
            "2024-06-06 02:05:48 (UTC) - 0:00:11 - finetune.wrapped_model - INFO - Reloading model from /content/mistral_models/consolidated.safetensors ...\n",
            "2024-06-06 02:05:48 (UTC) - 0:00:11 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...\n",
            "2024-06-06 02:05:48 (UTC) - 0:00:11 - finetune.wrapped_model - INFO - Loaded model on cpu!\n",
            "2024-06-06 02:05:48 (UTC) - 0:00:11 - finetune.wrapped_model - INFO - Initializing lora layers ...\n",
            "2024-06-06 02:05:49 (UTC) - 0:00:12 - finetune.wrapped_model - INFO - Finished initialization!\n",
            "2024-06-06 02:05:49 (UTC) - 0:00:12 - finetune.wrapped_model - INFO - Sharding model over 1 GPUs ...\n",
            "2024-06-06 02:05:53 (UTC) - 0:00:16 - finetune.wrapped_model - INFO - Model sharded!\n",
            "2024-06-06 02:05:53 (UTC) - 0:00:16 - finetune.wrapped_model - INFO - 167,772,160 out of 7,415,795,712 parameter are finetuned (2.26%).\n",
            "2024-06-06 02:05:54 (UTC) - 0:00:17 - dataset - INFO - Loading /content/data/ultrachat_chunk_train.jsonl ...\n",
            "2024-06-06 02:07:20 (UTC) - 0:01:43 - dataset - INFO - /content/data/ultrachat_chunk_train.jsonl loaded and tokenized.\n",
            "2024-06-06 02:07:20 (UTC) - 0:01:43 - dataset - INFO - Shuffling /content/data/ultrachat_chunk_train.jsonl ...\n",
            "2024-06-06 02:07:39 (UTC) - 0:02:02 - train - INFO - step: 000001 - done (%): 1.0 - loss: 0.874 - lr: 4.0e-06 - peak_alloc_mem (GB): 21.0 - alloc_mem (GB): 17.1 - words_per_second: 624.5 - avg_words_per_second: 624.5 - ETA: >2024-06-06 05:00:47\n",
            "2024-06-06 02:07:57 (UTC) - 0:02:20 - train - INFO - step: 000002 - done (%): 2.0 - loss: 0.905 - lr: 1.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3615.4 - avg_words_per_second: 1065.1 - ETA: >2024-06-06 03:48:27\n",
            "2024-06-06 02:08:15 (UTC) - 0:02:38 - train - INFO - step: 000003 - done (%): 3.0 - loss: 0.912 - lr: 5.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3718.6 - avg_words_per_second: 1397.5 - ETA: >2024-06-06 03:24:03\n",
            "2024-06-06 02:08:33 (UTC) - 0:02:56 - train - INFO - step: 000004 - done (%): 4.0 - loss: 0.884 - lr: 8.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3646.4 - avg_words_per_second: 1652.3 - ETA: >2024-06-06 03:12:00\n",
            "2024-06-06 02:08:51 (UTC) - 0:03:14 - train - INFO - step: 000005 - done (%): 5.0 - loss: 0.835 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3581.9 - avg_words_per_second: 1851.8 - ETA: >2024-06-06 03:04:53\n",
            "2024-06-06 02:09:08 (UTC) - 0:03:31 - train - INFO - step: 000006 - done (%): 6.0 - loss: 0.857 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3723.1 - avg_words_per_second: 2021.1 - ETA: >2024-06-06 02:59:56\n",
            "2024-06-06 02:09:27 (UTC) - 0:03:50 - train - INFO - step: 000007 - done (%): 7.0 - loss: 0.869 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3627.1 - avg_words_per_second: 2157.6 - ETA: >2024-06-06 02:56:31\n",
            "2024-06-06 02:09:45 (UTC) - 0:04:07 - train - INFO - step: 000008 - done (%): 8.0 - loss: 0.868 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3643.4 - avg_words_per_second: 2273.5 - ETA: >2024-06-06 02:53:57\n",
            "2024-06-06 02:10:02 (UTC) - 0:04:25 - train - INFO - step: 000009 - done (%): 9.0 - loss: 0.804 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3706.9 - avg_words_per_second: 2375.5 - ETA: >2024-06-06 02:51:53\n",
            "2024-06-06 02:10:20 (UTC) - 0:04:43 - train - INFO - step: 000010 - done (%): 10.0 - loss: 0.898 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3607.7 - avg_words_per_second: 2459.5 - ETA: >2024-06-06 02:50:18\n",
            "2024-06-06 02:10:38 (UTC) - 0:05:01 - train - INFO - step: 000011 - done (%): 11.0 - loss: 0.744 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3712.5 - avg_words_per_second: 2537.4 - ETA: >2024-06-06 02:48:57\n",
            "2024-06-06 02:10:56 (UTC) - 0:05:19 - train - INFO - step: 000012 - done (%): 12.0 - loss: 0.837 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3596.5 - avg_words_per_second: 2601.2 - ETA: >2024-06-06 02:47:53\n",
            "2024-06-06 02:11:15 (UTC) - 0:05:38 - train - INFO - step: 000013 - done (%): 13.0 - loss: 0.846 - lr: 9.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3582.5 - avg_words_per_second: 2657.2 - ETA: >2024-06-06 02:47:00\n",
            "2024-06-06 02:11:32 (UTC) - 0:05:55 - train - INFO - step: 000014 - done (%): 14.0 - loss: 0.853 - lr: 9.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3715.1 - avg_words_per_second: 2712.4 - ETA: >2024-06-06 02:46:10\n",
            "2024-06-06 02:11:50 (UTC) - 0:06:13 - train - INFO - step: 000015 - done (%): 15.0 - loss: 0.911 - lr: 9.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3602.0 - avg_words_per_second: 2757.8 - ETA: >2024-06-06 02:45:30\n",
            "2024-06-06 02:12:08 (UTC) - 0:06:31 - train - INFO - step: 000016 - done (%): 16.0 - loss: 0.815 - lr: 9.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3715.2 - avg_words_per_second: 2802.9 - ETA: >2024-06-06 02:44:52\n",
            "2024-06-06 02:12:26 (UTC) - 0:06:49 - train - INFO - step: 000017 - done (%): 17.0 - loss: 0.821 - lr: 9.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3631.2 - avg_words_per_second: 2841.0 - ETA: >2024-06-06 02:44:21\n",
            "2024-06-06 02:12:44 (UTC) - 0:07:07 - train - INFO - step: 000018 - done (%): 18.0 - loss: 0.842 - lr: 9.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3631.7 - avg_words_per_second: 2875.8 - ETA: >2024-06-06 02:43:53\n",
            "2024-06-06 02:13:02 (UTC) - 0:07:25 - train - INFO - step: 000019 - done (%): 19.0 - loss: 0.802 - lr: 9.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3713.3 - avg_words_per_second: 2910.4 - ETA: >2024-06-06 02:43:26\n",
            "2024-06-06 02:13:20 (UTC) - 0:07:43 - train - INFO - step: 000020 - done (%): 20.0 - loss: 0.867 - lr: 9.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3619.9 - avg_words_per_second: 2939.2 - ETA: >2024-06-06 02:43:04\n",
            "2024-06-06 02:13:38 (UTC) - 0:08:01 - train - INFO - step: 000021 - done (%): 21.0 - loss: 0.826 - lr: 9.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3626.7 - avg_words_per_second: 2966.0 - ETA: >2024-06-06 02:42:44\n",
            "2024-06-06 02:13:56 (UTC) - 0:08:19 - train - INFO - step: 000022 - done (%): 22.0 - loss: 0.809 - lr: 9.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3710.7 - avg_words_per_second: 2993.3 - ETA: >2024-06-06 02:42:23\n",
            "2024-06-06 02:14:14 (UTC) - 0:08:37 - train - INFO - step: 000023 - done (%): 23.0 - loss: 0.837 - lr: 9.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3633.7 - avg_words_per_second: 3016.4 - ETA: >2024-06-06 02:42:07\n",
            "2024-06-06 02:14:31 (UTC) - 0:08:54 - train - INFO - step: 000024 - done (%): 24.0 - loss: 0.851 - lr: 9.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3713.4 - avg_words_per_second: 3040.2 - ETA: >2024-06-06 02:41:50\n",
            "2024-06-06 02:14:49 (UTC) - 0:09:12 - train - INFO - step: 000025 - done (%): 25.0 - loss: 0.856 - lr: 8.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3616.3 - avg_words_per_second: 3059.7 - ETA: >2024-06-06 02:41:36\n",
            "2024-06-06 02:15:08 (UTC) - 0:09:31 - train - INFO - step: 000026 - done (%): 26.0 - loss: 0.780 - lr: 8.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3603.2 - avg_words_per_second: 3077.5 - ETA: >2024-06-06 02:41:23\n",
            "2024-06-06 02:15:25 (UTC) - 0:09:48 - train - INFO - step: 000027 - done (%): 27.0 - loss: 0.845 - lr: 8.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3716.9 - avg_words_per_second: 3097.2 - ETA: >2024-06-06 02:41:10\n",
            "2024-06-06 02:15:43 (UTC) - 0:10:06 - train - INFO - step: 000028 - done (%): 28.0 - loss: 0.831 - lr: 8.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3635.5 - avg_words_per_second: 3113.7 - ETA: >2024-06-06 02:40:59\n",
            "2024-06-06 02:16:01 (UTC) - 0:10:24 - train - INFO - step: 000029 - done (%): 29.0 - loss: 0.806 - lr: 8.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3639.4 - avg_words_per_second: 3129.3 - ETA: >2024-06-06 02:40:48\n",
            "2024-06-06 02:16:19 (UTC) - 0:10:42 - train - INFO - step: 000030 - done (%): 30.0 - loss: 0.898 - lr: 8.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3721.8 - avg_words_per_second: 3146.0 - ETA: >2024-06-06 02:40:37\n",
            "2024-06-06 02:16:37 (UTC) - 0:11:00 - train - INFO - step: 000031 - done (%): 31.0 - loss: 0.817 - lr: 8.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3611.9 - avg_words_per_second: 3159.1 - ETA: >2024-06-06 02:40:28\n",
            "2024-06-06 02:16:55 (UTC) - 0:11:18 - train - INFO - step: 000032 - done (%): 32.0 - loss: 0.825 - lr: 8.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3702.4 - avg_words_per_second: 3173.7 - ETA: >2024-06-06 02:40:19\n",
            "2024-06-06 02:17:13 (UTC) - 0:11:36 - train - INFO - step: 000033 - done (%): 33.0 - loss: 0.845 - lr: 8.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3608.4 - avg_words_per_second: 3185.3 - ETA: >2024-06-06 02:40:11\n",
            "2024-06-06 02:17:31 (UTC) - 0:11:54 - train - INFO - step: 000034 - done (%): 34.0 - loss: 0.808 - lr: 7.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3581.6 - avg_words_per_second: 3195.7 - ETA: >2024-06-06 02:40:05\n",
            "2024-06-06 02:17:49 (UTC) - 0:12:12 - train - INFO - step: 000035 - done (%): 35.0 - loss: 0.854 - lr: 7.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3712.3 - avg_words_per_second: 3208.5 - ETA: >2024-06-06 02:39:57\n",
            "2024-06-06 02:18:07 (UTC) - 0:12:30 - train - INFO - step: 000036 - done (%): 36.0 - loss: 0.813 - lr: 7.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3612.5 - avg_words_per_second: 3218.5 - ETA: >2024-06-06 02:39:50\n",
            "2024-06-06 02:18:25 (UTC) - 0:12:48 - train - INFO - step: 000037 - done (%): 37.0 - loss: 0.801 - lr: 7.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3713.8 - avg_words_per_second: 3230.1 - ETA: >2024-06-06 02:39:43\n",
            "2024-06-06 02:18:43 (UTC) - 0:13:06 - train - INFO - step: 000038 - done (%): 38.0 - loss: 0.744 - lr: 7.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3618.3 - avg_words_per_second: 3239.3 - ETA: >2024-06-06 02:39:37\n",
            "2024-06-06 02:19:01 (UTC) - 0:13:24 - train - INFO - step: 000039 - done (%): 39.0 - loss: 0.816 - lr: 7.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3627.6 - avg_words_per_second: 3248.2 - ETA: >2024-06-06 02:39:32\n",
            "2024-06-06 02:19:18 (UTC) - 0:13:41 - train - INFO - step: 000040 - done (%): 40.0 - loss: 0.786 - lr: 7.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3710.2 - avg_words_per_second: 3258.3 - ETA: >2024-06-06 02:39:25\n",
            "2024-06-06 02:19:37 (UTC) - 0:14:00 - train - INFO - step: 000041 - done (%): 41.0 - loss: 0.804 - lr: 6.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3624.5 - avg_words_per_second: 3266.4 - ETA: >2024-06-06 02:39:20\n",
            "2024-06-06 02:19:55 (UTC) - 0:14:18 - train - INFO - step: 000042 - done (%): 42.0 - loss: 0.845 - lr: 6.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3640.7 - avg_words_per_second: 3274.4 - ETA: >2024-06-06 02:39:15\n",
            "2024-06-06 02:20:12 (UTC) - 0:14:35 - train - INFO - step: 000043 - done (%): 43.0 - loss: 0.864 - lr: 6.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3699.0 - avg_words_per_second: 3283.1 - ETA: >2024-06-06 02:39:10\n",
            "2024-06-06 02:20:30 (UTC) - 0:14:53 - train - INFO - step: 000044 - done (%): 44.0 - loss: 0.862 - lr: 6.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3633.1 - avg_words_per_second: 3290.3 - ETA: >2024-06-06 02:39:06\n",
            "2024-06-06 02:20:48 (UTC) - 0:15:11 - train - INFO - step: 000045 - done (%): 45.0 - loss: 0.862 - lr: 6.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3709.4 - avg_words_per_second: 3298.6 - ETA: >2024-06-06 02:39:01\n",
            "2024-06-06 02:21:06 (UTC) - 0:15:29 - train - INFO - step: 000046 - done (%): 46.0 - loss: 0.828 - lr: 6.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3620.5 - avg_words_per_second: 3305.0 - ETA: >2024-06-06 02:38:57\n",
            "2024-06-06 02:21:24 (UTC) - 0:15:47 - train - INFO - step: 000047 - done (%): 47.0 - loss: 0.813 - lr: 5.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3607.2 - avg_words_per_second: 3310.9 - ETA: >2024-06-06 02:38:53\n",
            "2024-06-06 02:21:42 (UTC) - 0:16:05 - train - INFO - step: 000048 - done (%): 48.0 - loss: 0.818 - lr: 5.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3720.8 - avg_words_per_second: 3318.5 - ETA: >2024-06-06 02:38:49\n",
            "2024-06-06 02:22:00 (UTC) - 0:16:23 - train - INFO - step: 000049 - done (%): 49.0 - loss: 0.817 - lr: 5.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3630.3 - avg_words_per_second: 3324.4 - ETA: >2024-06-06 02:38:45\n",
            "2024-06-06 02:22:18 (UTC) - 0:16:41 - train - INFO - step: 000050 - done (%): 50.0 - loss: 0.889 - lr: 5.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3643.3 - avg_words_per_second: 3330.2 - ETA: >2024-06-06 02:38:42\n",
            "2024-06-06 02:22:36 (UTC) - 0:16:59 - train - INFO - step: 000051 - done (%): 51.0 - loss: 0.777 - lr: 5.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3705.1 - avg_words_per_second: 3336.8 - ETA: >2024-06-06 02:38:38\n",
            "2024-06-06 02:22:54 (UTC) - 0:17:17 - train - INFO - step: 000052 - done (%): 52.0 - loss: 0.804 - lr: 5.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3609.7 - avg_words_per_second: 3341.7 - ETA: >2024-06-06 02:38:35\n",
            "2024-06-06 02:23:11 (UTC) - 0:17:34 - train - INFO - step: 000053 - done (%): 53.0 - loss: 0.800 - lr: 4.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3706.0 - avg_words_per_second: 3347.9 - ETA: >2024-06-06 02:38:31\n",
            "2024-06-06 02:23:30 (UTC) - 0:17:53 - train - INFO - step: 000054 - done (%): 54.0 - loss: 0.804 - lr: 4.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3607.8 - avg_words_per_second: 3352.4 - ETA: >2024-06-06 02:38:29\n",
            "2024-06-06 02:23:48 (UTC) - 0:18:11 - train - INFO - step: 000055 - done (%): 55.0 - loss: 0.854 - lr: 4.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3592.4 - avg_words_per_second: 3356.4 - ETA: >2024-06-06 02:38:27\n",
            "2024-06-06 02:24:06 (UTC) - 0:18:28 - train - INFO - step: 000056 - done (%): 56.0 - loss: 0.819 - lr: 4.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3714.5 - avg_words_per_second: 3362.2 - ETA: >2024-06-06 02:38:23\n",
            "2024-06-06 02:24:24 (UTC) - 0:18:47 - train - INFO - step: 000057 - done (%): 57.0 - loss: 0.844 - lr: 4.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3634.1 - avg_words_per_second: 3366.6 - ETA: >2024-06-06 02:38:21\n",
            "2024-06-06 02:24:41 (UTC) - 0:19:04 - train - INFO - step: 000058 - done (%): 58.0 - loss: 0.840 - lr: 4.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3719.6 - avg_words_per_second: 3372.2 - ETA: >2024-06-06 02:38:17\n",
            "2024-06-06 02:24:59 (UTC) - 0:19:22 - train - INFO - step: 000059 - done (%): 59.0 - loss: 0.835 - lr: 3.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3638.0 - avg_words_per_second: 3376.3 - ETA: >2024-06-06 02:38:15\n",
            "2024-06-06 02:25:17 (UTC) - 0:19:40 - train - INFO - step: 000060 - done (%): 60.0 - loss: 0.852 - lr: 3.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3638.6 - avg_words_per_second: 3380.4 - ETA: >2024-06-06 02:38:13\n",
            "2024-06-06 02:25:35 (UTC) - 0:19:58 - train - INFO - step: 000061 - done (%): 61.0 - loss: 0.837 - lr: 3.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3722.5 - avg_words_per_second: 3385.5 - ETA: >2024-06-06 02:38:10\n",
            "2024-06-06 02:25:53 (UTC) - 0:20:16 - train - INFO - step: 000062 - done (%): 62.0 - loss: 0.839 - lr: 3.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3629.2 - avg_words_per_second: 3389.2 - ETA: >2024-06-06 02:38:08\n",
            "2024-06-06 02:26:11 (UTC) - 0:20:34 - train - INFO - step: 000063 - done (%): 63.0 - loss: 0.813 - lr: 3.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3631.2 - avg_words_per_second: 3392.8 - ETA: >2024-06-06 02:38:06\n",
            "2024-06-06 02:26:29 (UTC) - 0:20:52 - train - INFO - step: 000064 - done (%): 64.0 - loss: 0.785 - lr: 3.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3705.6 - avg_words_per_second: 3397.2 - ETA: >2024-06-06 02:38:03\n",
            "2024-06-06 02:26:47 (UTC) - 0:21:10 - train - INFO - step: 000065 - done (%): 65.0 - loss: 0.797 - lr: 3.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3619.5 - avg_words_per_second: 3400.5 - ETA: >2024-06-06 02:38:01\n",
            "2024-06-06 02:27:04 (UTC) - 0:21:27 - train - INFO - step: 000066 - done (%): 66.0 - loss: 0.787 - lr: 2.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3715.5 - avg_words_per_second: 3404.8 - ETA: >2024-06-06 02:37:59\n",
            "2024-06-06 02:27:22 (UTC) - 0:21:45 - train - INFO - step: 000067 - done (%): 67.0 - loss: 0.902 - lr: 2.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3622.2 - avg_words_per_second: 3407.9 - ETA: >2024-06-06 02:37:57\n",
            "2024-06-06 02:27:41 (UTC) - 0:22:04 - train - INFO - step: 000068 - done (%): 68.0 - loss: 0.783 - lr: 2.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3593.6 - avg_words_per_second: 3410.5 - ETA: >2024-06-06 02:37:56\n",
            "2024-06-06 02:27:58 (UTC) - 0:22:21 - train - INFO - step: 000069 - done (%): 69.0 - loss: 0.889 - lr: 2.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3716.3 - avg_words_per_second: 3414.5 - ETA: >2024-06-06 02:37:53\n",
            "2024-06-06 02:28:16 (UTC) - 0:22:39 - train - INFO - step: 000070 - done (%): 70.0 - loss: 0.810 - lr: 2.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3630.7 - avg_words_per_second: 3417.5 - ETA: >2024-06-06 02:37:52\n",
            "2024-06-06 02:28:34 (UTC) - 0:22:57 - train - INFO - step: 000071 - done (%): 71.0 - loss: 0.796 - lr: 2.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3653.3 - avg_words_per_second: 3420.6 - ETA: >2024-06-06 02:37:50\n",
            "2024-06-06 02:28:52 (UTC) - 0:23:15 - train - INFO - step: 000072 - done (%): 72.0 - loss: 0.927 - lr: 2.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3705.0 - avg_words_per_second: 3424.2 - ETA: >2024-06-06 02:37:48\n",
            "2024-06-06 02:29:10 (UTC) - 0:23:33 - train - INFO - step: 000073 - done (%): 73.0 - loss: 0.811 - lr: 1.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3614.5 - avg_words_per_second: 3426.7 - ETA: >2024-06-06 02:37:47\n",
            "2024-06-06 02:29:28 (UTC) - 0:23:51 - train - INFO - step: 000074 - done (%): 74.0 - loss: 0.828 - lr: 1.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3699.6 - avg_words_per_second: 3430.1 - ETA: >2024-06-06 02:37:45\n",
            "2024-06-06 02:29:46 (UTC) - 0:24:09 - train - INFO - step: 000075 - done (%): 75.0 - loss: 0.908 - lr: 1.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3606.0 - avg_words_per_second: 3432.3 - ETA: >2024-06-06 02:37:43\n",
            "2024-06-06 02:30:04 (UTC) - 0:24:27 - train - INFO - step: 000076 - done (%): 76.0 - loss: 0.880 - lr: 1.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3598.6 - avg_words_per_second: 3434.4 - ETA: >2024-06-06 02:37:42\n",
            "2024-06-06 02:30:22 (UTC) - 0:24:45 - train - INFO - step: 000077 - done (%): 77.0 - loss: 0.819 - lr: 1.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3722.5 - avg_words_per_second: 3437.9 - ETA: >2024-06-06 02:37:40\n",
            "2024-06-06 02:30:40 (UTC) - 0:25:03 - train - INFO - step: 000078 - done (%): 78.0 - loss: 0.867 - lr: 1.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3627.1 - avg_words_per_second: 3440.2 - ETA: >2024-06-06 02:37:39\n",
            "2024-06-06 02:30:58 (UTC) - 0:25:21 - train - INFO - step: 000079 - done (%): 79.0 - loss: 0.913 - lr: 1.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3710.6 - avg_words_per_second: 3443.4 - ETA: >2024-06-06 02:37:37\n",
            "2024-06-06 02:31:16 (UTC) - 0:25:39 - train - INFO - step: 000080 - done (%): 80.0 - loss: 0.826 - lr: 1.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3623.7 - avg_words_per_second: 3445.5 - ETA: >2024-06-06 02:37:36\n",
            "2024-06-06 02:31:34 (UTC) - 0:25:57 - train - INFO - step: 000081 - done (%): 81.0 - loss: 0.835 - lr: 9.5e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3640.9 - avg_words_per_second: 3447.8 - ETA: >2024-06-06 02:37:35\n",
            "2024-06-06 02:31:51 (UTC) - 0:26:14 - train - INFO - step: 000082 - done (%): 82.0 - loss: 0.854 - lr: 8.6e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3704.9 - avg_words_per_second: 3450.7 - ETA: >2024-06-06 02:37:33\n",
            "2024-06-06 02:32:09 (UTC) - 0:26:32 - train - INFO - step: 000083 - done (%): 83.0 - loss: 0.771 - lr: 7.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3620.1 - avg_words_per_second: 3452.7 - ETA: >2024-06-06 02:37:32\n",
            "2024-06-06 02:32:28 (UTC) - 0:26:50 - train - INFO - step: 000084 - done (%): 84.0 - loss: 0.813 - lr: 6.8e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3630.3 - avg_words_per_second: 3454.7 - ETA: >2024-06-06 02:37:31\n",
            "2024-06-06 02:32:45 (UTC) - 0:27:08 - train - INFO - step: 000085 - done (%): 85.0 - loss: 0.836 - lr: 6.0e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3708.0 - avg_words_per_second: 3457.4 - ETA: >2024-06-06 02:37:30\n",
            "2024-06-06 02:33:03 (UTC) - 0:27:26 - train - INFO - step: 000086 - done (%): 86.0 - loss: 0.866 - lr: 5.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3623.5 - avg_words_per_second: 3459.3 - ETA: >2024-06-06 02:37:28\n",
            "2024-06-06 02:33:21 (UTC) - 0:27:44 - train - INFO - step: 000087 - done (%): 87.0 - loss: 0.845 - lr: 4.6e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3717.5 - avg_words_per_second: 3462.0 - ETA: >2024-06-06 02:37:27\n",
            "2024-06-06 02:33:39 (UTC) - 0:28:02 - train - INFO - step: 000088 - done (%): 88.0 - loss: 0.777 - lr: 3.9e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3609.9 - avg_words_per_second: 3463.7 - ETA: >2024-06-06 02:37:26\n",
            "2024-06-06 02:33:57 (UTC) - 0:28:20 - train - INFO - step: 000089 - done (%): 89.0 - loss: 0.812 - lr: 3.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3592.8 - avg_words_per_second: 3465.1 - ETA: >2024-06-06 02:37:25\n",
            "2024-06-06 02:34:15 (UTC) - 0:28:38 - train - INFO - step: 000090 - done (%): 90.0 - loss: 0.841 - lr: 2.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3713.4 - avg_words_per_second: 3467.6 - ETA: >2024-06-06 02:37:24\n",
            "2024-06-06 02:34:33 (UTC) - 0:28:56 - train - INFO - step: 000091 - done (%): 91.0 - loss: 0.807 - lr: 2.2e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3637.4 - avg_words_per_second: 3469.4 - ETA: >2024-06-06 02:37:23\n",
            "2024-06-06 02:34:51 (UTC) - 0:29:14 - train - INFO - step: 000092 - done (%): 92.0 - loss: 0.807 - lr: 1.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3623.3 - avg_words_per_second: 3471.0 - ETA: >2024-06-06 02:37:22\n",
            "2024-06-06 02:35:09 (UTC) - 0:29:32 - train - INFO - step: 000093 - done (%): 93.0 - loss: 0.827 - lr: 1.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3709.9 - avg_words_per_second: 3473.4 - ETA: >2024-06-06 02:37:21\n",
            "2024-06-06 02:35:27 (UTC) - 0:29:50 - train - INFO - step: 000094 - done (%): 94.0 - loss: 0.817 - lr: 9.8e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3610.4 - avg_words_per_second: 3474.8 - ETA: >2024-06-06 02:37:20\n",
            "2024-06-06 02:35:45 (UTC) - 0:30:08 - train - INFO - step: 000095 - done (%): 95.0 - loss: 0.825 - lr: 6.8e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3714.2 - avg_words_per_second: 3477.2 - ETA: >2024-06-06 02:37:19\n",
            "2024-06-06 02:36:03 (UTC) - 0:30:26 - train - INFO - step: 000096 - done (%): 96.0 - loss: 0.794 - lr: 4.4e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3606.1 - avg_words_per_second: 3478.5 - ETA: >2024-06-06 02:37:18\n",
            "2024-06-06 02:36:21 (UTC) - 0:30:44 - train - INFO - step: 000097 - done (%): 97.0 - loss: 0.884 - lr: 2.5e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3595.9 - avg_words_per_second: 3479.7 - ETA: >2024-06-06 02:37:17\n",
            "2024-06-06 02:36:39 (UTC) - 0:31:02 - train - INFO - step: 000098 - done (%): 98.0 - loss: 0.826 - lr: 1.1e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3717.0 - avg_words_per_second: 3481.9 - ETA: >2024-06-06 02:37:16\n",
            "2024-06-06 02:36:57 (UTC) - 0:31:20 - train - INFO - step: 000099 - done (%): 99.0 - loss: 0.771 - lr: 2.8e-08 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3627.1 - avg_words_per_second: 3483.3 - ETA: >2024-06-06 02:37:15\n",
            "2024-06-06 02:37:14 (UTC) - 0:31:37 - eval - INFO - Start eval...\n",
            "2024-06-06 02:39:09 (UTC) - 0:33:32 - eval - INFO - Eval finished!\n",
            "2024-06-06 02:39:09 (UTC) - 0:33:32 - train - INFO - step: 000100 - eval_perplexity: 1.779 - eval_loss: 0.831 - train_loss: 0.761\n",
            "2024-06-06 02:39:09 (UTC) - 0:33:32 - train - INFO - step: 000100 - done (%): 100.0 - loss: 0.761 - lr: 4.0e-10 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 495.8 - avg_words_per_second: 3285.4 - ETA: >2024-06-06 02:39:09\n",
            "2024-06-06 02:39:09 (UTC) - 0:33:32 - checkpointing - INFO - Dumping checkpoint in /content/test_ultra/checkpoints/checkpoint_000100/consolidated using tmp name: tmp.consolidated\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - checkpointing - INFO - Done dumping checkpoint in /content/test_ultra/checkpoints/checkpoint_000100/consolidated for step: 100\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - checkpointing - INFO - Done deleting checkpoints \n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - checkpointing - INFO - Done!\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - train - INFO - done!\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - utils - INFO - Closing: eval_logger\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - utils - INFO - Closed: eval_logger\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - utils - INFO - Closing: metrics_logger\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - utils - INFO - Closed: metrics_logger\n",
            "2024-06-06 02:39:10 (UTC) - 0:33:33 - train - INFO - Closed everything!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "ruJ29JFn98zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference -q"
      ],
      "metadata": {
        "id": "7BWNGKt9-Kxz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mistral_inference.model import Transformer\n",
        "from mistral_inference.generate import generate\n",
        "\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_common.protocol.instruct.messages import UserMessage\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
        "\n",
        "\n",
        "tokenizer = MistralTokenizer.from_file(\"/content/mistral_models/tokenizer.model.v3\")  # change to extracted tokenizer file\n",
        "model = Transformer.from_folder(\"/content/mistral_models\")  # change to extracted model dir\n",
        "model.load_lora(\"/content/test_ultra/checkpoints/checkpoint_000100/consolidated/lora.safetensors\")\n",
        "\n",
        "completion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n",
        "\n",
        "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
        "\n",
        "out_tokens, _ = generate([tokens], model, max_tokens=512, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
        "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "F-xLs2Ot9-il",
        "outputId": "48746e61-607b-4bfc-efee-f90bae3a5b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is a subset of artificial intelligence that involves the use of algorithms to learn from data and make predictions or decisions without being explicitly programmed. It is a type of data analysis that allows computers to learn from experience and improve their performance over time. Machine learning algorithms can be trained on large datasets to identify patterns and make predictions based on those patterns. The goal of machine learning is to enable computers to perform tasks that would otherwise require human intelligence, such as recognizing images, understanding natural language, and making predictions about future events.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vd8A8JP4Fx3C"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}
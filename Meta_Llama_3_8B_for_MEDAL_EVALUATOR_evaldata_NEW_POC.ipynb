{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "_pX9UFDUqgE0",
        "a186C_0yqqcL",
        "vW2jV2ilq_2J",
        "IalRvdpSrJ-l",
        "bKGIx6RgrlUZ",
        "DoMasspftcAL"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8lwN4q7BwguWa9jUwPVIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d416a3d7f654e329b167e917d5ef782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81aafa8005eb44fa9147ce4dab8094f3",
              "IPY_MODEL_3fe5473f0f174330abcb24b0e916341c",
              "IPY_MODEL_1530064b6235445889041adc4e14715d"
            ],
            "layout": "IPY_MODEL_46727eb958d04c6297257f016dbbd8d9"
          }
        },
        "81aafa8005eb44fa9147ce4dab8094f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e96f55461c44e55b1813af57edb9238",
            "placeholder": "​",
            "style": "IPY_MODEL_23e379dbf88142f7a4d5b9f09de40474",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3fe5473f0f174330abcb24b0e916341c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8e467c574b6424ab81f0596417d5df4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ec7970e69f24ce8acb9a3098308eda6",
            "value": 4
          }
        },
        "1530064b6235445889041adc4e14715d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_013ccd08d9ba4664b77f90c92b5ee1e9",
            "placeholder": "​",
            "style": "IPY_MODEL_47e9a4ffea764aab9b0b8ba55d43722f",
            "value": " 4/4 [00:05&lt;00:00,  1.15s/it]"
          }
        },
        "46727eb958d04c6297257f016dbbd8d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e96f55461c44e55b1813af57edb9238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e379dbf88142f7a4d5b9f09de40474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8e467c574b6424ab81f0596417d5df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec7970e69f24ce8acb9a3098308eda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "013ccd08d9ba4664b77f90c92b5ee1e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47e9a4ffea764aab9b0b8ba55d43722f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d900d57dcd14fa7a3288495bf0094b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6335e59203f4454e9d80ad44295efe4f",
              "IPY_MODEL_345c2377b7f94f5faeefb89b95edd6c1",
              "IPY_MODEL_e20dd138223345f6ba7678eea9b88c45"
            ],
            "layout": "IPY_MODEL_32392ca1850a47bca6f06a30dc556848"
          }
        },
        "6335e59203f4454e9d80ad44295efe4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cad9cf840e14d61a15cc12718c3e6c8",
            "placeholder": "​",
            "style": "IPY_MODEL_b51dba9cc10f4dcc8da350f1a701a544",
            "value": "100%"
          }
        },
        "345c2377b7f94f5faeefb89b95edd6c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f826aaa712db4c5d91e3e1aa7b1e6155",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb5a73be8a4949c9a52c7df33ff1e8cc",
            "value": 1
          }
        },
        "e20dd138223345f6ba7678eea9b88c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4633abccb5f749cb8cd97caf07bc6087",
            "placeholder": "​",
            "style": "IPY_MODEL_77481aa3ce524e85ba40a099ac3a6eb8",
            "value": " 1/1 [00:00&lt;00:00,  7.23it/s]"
          }
        },
        "32392ca1850a47bca6f06a30dc556848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cad9cf840e14d61a15cc12718c3e6c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51dba9cc10f4dcc8da350f1a701a544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f826aaa712db4c5d91e3e1aa7b1e6155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5a73be8a4949c9a52c7df33ff1e8cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4633abccb5f749cb8cd97caf07bc6087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77481aa3ce524e85ba40a099ac3a6eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Meta_Llama_3_8B_for_MEDAL_EVALUATOR_evaldata_NEW_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sudo apt install nvtop"
      ],
      "metadata": {
        "id": "bcjd2oofKlwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "_pX9UFDUqgE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux |grep python3 |grep colab_kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncmh3ZmZBz8a",
        "outputId": "6855f8f4-b699-4b03-806e-1a9c208cf212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root         249  2.5  0.1 1510344 96944 ?       Ssl  08:16   0:02 /usr/bin/python3 -m colab_kernel_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade huggingface_hub -q"
      ],
      "metadata": {
        "id": "oLO1NOdVVNbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install huggingface_hub -q"
      ],
      "metadata": {
        "id": "f5u00oiWVu7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@frankmorales_91352/fine-tuning-meta-llama-3-8b-with-medal-for-enhanced-medical-language-understanding-3de07a54fab9"
      ],
      "metadata": {
        "id": "BtnQHkxH3CxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install peft --quiet\n",
        "\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "#Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "sJ5qguUtgRCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")"
      ],
      "metadata": {
        "id": "YnDGjScpg5CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "44TvN5ChjSXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "9Mt3GNBahX6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Setup"
      ],
      "metadata": {
        "id": "a186C_0yqqcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "CjcHuj57hdz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41984bed-b8ee-4c55-812b-1f0bb4ab5cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U tensorboard-plugin-profile -q"
      ],
      "metadata": {
        "id": "HqiuFix3wra-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard Setup"
      ],
      "metadata": {
        "id": "vW2jV2ilq_2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/tensorbard\n",
        "%mkdir -p /content/tensorbard\n",
        "%cd /content/tensorbard\n",
        "!git lfs install\n",
        "#!git clone https://huggingface.co/frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2\n",
        "# https://huggingface.co/frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata\n",
        "!git clone https://huggingface.co/frankmorales2020/POC-NEW-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata"
      ],
      "metadata": {
        "id": "ydCn9527wteN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "#%tensorboard --logdir /content/tensorbard/Meta-Llama-3-8B-MEDAL-flash-attention-2/runs\n",
        "#%tensorboard --logdir /content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2/runs/\n",
        "\n",
        "#%tensorboard --logdir /content/tensorbard/POC-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata/logs\n",
        "%tensorboard --logdir /content/tensorbard/POC-NEW-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata/logs\n",
        "#%tensorboard --logdir /content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine/runs/\n"
      ],
      "metadata": {
        "id": "G-t1tvPww_m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Load"
      ],
      "metadata": {
        "id": "IalRvdpSrJ-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "BMGlJx4jhSii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "JgwcQfFLrTp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JWPfB44wgmB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGkx4MLQf-1n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "#peft_model_id = \"/content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2\"\n",
        "#peft_model_id = 'frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2'\n",
        "\n",
        "#peft_model_id = \"/content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine\"\n",
        "#peft_model_id = 'frankmorales2020/POC-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata'\n",
        "peft_model_id = 'frankmorales2020/POC-NEW-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata'\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        "  quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',  load_in_4bit=True, ...)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "BP-S-chTgr5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "#/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "#nrec= randint(0, len(eval_dataset))\n",
        "\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "x3gkTVspiGzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "xGPDheryiElH",
        "outputId": "06914ac0-6196-46ee-b85d-e1445842766b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[/INST] antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of the addition of a new type of polymeric material to the root canal filling material on the apical seal of the root canal system the root canal system of extracted human teeth was prepared and filled with a new type of polymeric material and guttapercha the teeth were then sectioned and the apical seal was evaluated using a scanning electron TSM the results showed that the addition of the new type of polymeric material to the root canal filling material improved the apical seal of the root canal system'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(ganswer,oanswer):\n",
        "\n",
        "  qc0=str(ganswer).find('[INST]')\n",
        "\n",
        "  ganswer=str(ganswer)[0:qc0-8]\n",
        "  qc=str(ganswer).find('[/INST]')\n",
        "  if qc>0:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "  print(f\"Generated Answer:\\n{ganswer}\")\n",
        "\n",
        "  if ganswer == oanswer:\n",
        "    print(\"Match\")\n",
        "    match=1\n",
        "  else:\n",
        "    print(\"NO Match\")\n",
        "    match=0\n",
        "  return match"
      ],
      "metadata": {
        "id": "U8lxIbH6r-q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "\n",
        "match=similarity(ganswer,oanswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGLv4cBkiVRV",
        "outputId": "aa0a2c47-870d-407c-c482-d86690c75f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "Match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation - Kernel"
      ],
      "metadata": {
        "id": "bKGIx6RgrlUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate -q"
      ],
      "metadata": {
        "id": "zDxyUCfsAUPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Prepare Dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "x9o2RFcwRRsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Dataset (Updated)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        #padding='max_length',  # Add padding\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH  # Make sure this matches your model's expected length\n",
        "    )"
      ],
      "metadata": {
        "id": "v64OfqgsaefS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 10\n",
        "all_input_ids = []\n",
        "all_attention_masks = []\n",
        "all_labels = []\n",
        "\n",
        "for text, label in zip(eval_dataset[\"text\"], eval_dataset[\"label\"]):\n",
        "    # Flatten nested lists if necessary\n",
        "    text = \" \".join(text) if isinstance(text, list) else text\n",
        "    label = \" \".join(label) if isinstance(label, list) else label\n",
        "    # Tokenize input\n",
        "    tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    all_input_ids.append(tokenized[\"input_ids\"][0])\n",
        "    all_attention_masks.append(tokenized[\"attention_mask\"][0])\n",
        "    # Encode the labels using the tokenizer\n",
        "    # This is essential for tasks like text classification\n",
        "    label_id = tokenizer.encode(label, add_special_tokens=False)\n",
        "    all_labels.append(torch.tensor(label_id))\n",
        "\n",
        "\n",
        "input_ids = torch.stack(all_input_ids)\n",
        "attention_masks = torch.stack(all_attention_masks)\n",
        "labels = torch.nn.utils.rnn.pad_sequence(all_labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "# Truncate the labels\n",
        "labels = labels[:, :max_length]  # Truncate to the first max_length tokens"
      ],
      "metadata": {
        "id": "egghvhJygJAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you have input_ids, attention_masks, and labels as tensors with compatible shapes\n",
        "print(input_ids.shape)\n",
        "print(attention_masks.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "mOQQ_aqqff3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Hidden Layers and Neurons (Before Evaluation)\n",
        "if hasattr(model, 'base_model'):\n",
        "    llama_model = model.base_model\n",
        "else:\n",
        "    llama_model = model\n",
        "\n",
        "# Count hidden layers of type LlamaDecoderLayer\n",
        "num_hidden_layers = llama_model.config.num_hidden_layers\n",
        "#print(num_hidden_layers)\n",
        "\n",
        "# Estimate neurons (this is very simplified, as explained earlier)\n",
        "num_neurons = num_hidden_layers * llama_model.config.hidden_size\n",
        "\n",
        "print(f\"Number of hidden layers in the model: {num_hidden_layers}\")\n",
        "print(f\"Approximate number of neurons (simplified): {num_neurons}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PYbiiZd-cqE",
        "outputId": "c2035944-49d4-4616-813b-0d1602d9565a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of hidden layers in the model: 32\n",
            "Approximate number of neurons (simplified): 131072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zT-OFibBePTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 1   # Batch size is set to 1\n",
        "MAX_LENGTH = 10\n",
        "peft_model_id = 'frankmorales2020/POC-NEW-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata'\n",
        "\n",
        "# Load and Prepare Dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "reduced_size = 5\n",
        "eval_dataset = eval_dataset.shuffle(seed=42).select(range(reduced_size))\n",
        "\n",
        "# Tokenization and Tensor Creation\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"label\"])\n",
        "tokenized_eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "# Create DataLoader (directly from dataset)\n",
        "eval_dataloader = DataLoader(tokenized_eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Move model to GPU (if available) - Assuming you have already done this\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model.to(device)\n",
        "device='cuda'\n",
        "\n",
        "# Evaluation Function (with progress bar)\n",
        "metric = evaluate.load(\"perplexity\")\n",
        "\n",
        "#metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def evaluate_model(model, eval_dataloader, metric, model_id):\n",
        "    model.eval()\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch, labels=batch[\"input_ids\"])\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "        metric.add_batch(predictions=decoded_predictions, references=decoded_labels)\n",
        "\n",
        "    return metric.compute(model_id=model_id)\n",
        "\n",
        "# Perform Evaluation\n",
        "results = evaluate_model(model, eval_dataloader, metric, peft_model_id)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "6d416a3d7f654e329b167e917d5ef782",
            "81aafa8005eb44fa9147ce4dab8094f3",
            "3fe5473f0f174330abcb24b0e916341c",
            "1530064b6235445889041adc4e14715d",
            "46727eb958d04c6297257f016dbbd8d9",
            "4e96f55461c44e55b1813af57edb9238",
            "23e379dbf88142f7a4d5b9f09de40474",
            "f8e467c574b6424ab81f0596417d5df4",
            "0ec7970e69f24ce8acb9a3098308eda6",
            "013ccd08d9ba4664b77f90c92b5ee1e9",
            "47e9a4ffea764aab9b0b8ba55d43722f",
            "6d900d57dcd14fa7a3288495bf0094b1",
            "6335e59203f4454e9d80ad44295efe4f",
            "345c2377b7f94f5faeefb89b95edd6c1",
            "e20dd138223345f6ba7678eea9b88c45",
            "32392ca1850a47bca6f06a30dc556848",
            "3cad9cf840e14d61a15cc12718c3e6c8",
            "b51dba9cc10f4dcc8da350f1a701a544",
            "f826aaa712db4c5d91e3e1aa7b1e6155",
            "fb5a73be8a4949c9a52c7df33ff1e8cc",
            "4633abccb5f749cb8cd97caf07bc6087",
            "77481aa3ce524e85ba40a099ac3a6eb8"
          ]
        },
        "id": "X-_4myo0_B4g",
        "outputId": "59472c01-0ad2-4887-f446-6cefc2ee5e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:01<00:00,  2.91it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d416a3d7f654e329b167e917d5ef782"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d900d57dcd14fa7a3288495bf0094b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'perplexities': [46617.1328125, 8308.46484375, 1480.472412109375, 2947.041015625, 1309.4544677734375], 'mean_perplexity': 12132.513110351563}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation - Inference"
      ],
      "metadata": {
        "id": "DoMasspftcAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ltbWraRQcBH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "lnJtOQwvGcgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9IxzfT3EXKh",
        "outputId": "f2a0f20b-0a22-4020-8c2b-e82806bd29ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['abstract_id', 'text', 'location', 'label'],\n",
              "    num_rows: 1000000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG-Y_5PPiad6",
        "outputId": "3e9d3c28-60ef-4250-8d98-9b4dab845eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "['antral follicle count']\n",
            "Generated Answer:\n",
            "[/INST] antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of the addition of a new type of polymeric material to the root canal filling material on the apical seal of the root canal system the root canal system of extracted human teeth was prepared and filled with a new type of polymeric material and guttapercha the teeth were then sectioned and the apical seal was evaluated using a scanning electron TSM the results showed that the addition of the new type of polymeric material to the root canal filling material improved the apical seal of the root canal system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "zRi13uPaibmQ",
        "outputId": "04faebcd-a8c0-419a-aabf-9362f5c9f6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of the addition of a new type of polymeric material to the root canal filling material on the apical seal of the root canal system the root canal system of extracted human teeth was prepared and filled with a new type of polymeric material and guttapercha the teeth were then sectioned and the apical seal was evaluated using a scanning electron TSM the results showed that the addition of the new type of polymeric material to the root canal filling material improved the apical seal of the root canal system'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map"
      ],
      "metadata": {
        "id": "s_vzP4M-Hiac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exceptions(ganswer,cganswer,eganswer):\n",
        "     if ganswer==eganswer:\n",
        "        ganswer=cganswer\n",
        "     return ganswer"
      ],
      "metadata": {
        "id": "GDLZRFDV7Wvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (your other imports and model setup)\n",
        "\n",
        "# Assuming you've already split your data into training and testing\n",
        "\n",
        "#train_dataset = ...  #  3M training samples\n",
        "#test_dataset = ...   #  1M testing samples\n",
        "\n",
        "# Sample randomly to create a validation set from the test set\n",
        "#eval_dataset = test_dataset.select(range(10807))  # Or use a more sophisticated sampling method\n",
        "\n",
        "# ... (rest of your SFTTrainer code)\n",
        "\n",
        "# ... (In your TrainingArguments, set `load_best_model_at_end=True`)\n",
        "\n",
        "# After training, evaluate on the full test set\n",
        "#trainer.evaluate(eval_dataset=test_dataset)\n"
      ],
      "metadata": {
        "id": "HBED0vjOoYtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "utcrGzryj03l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt =  sample['text']\n",
        "    outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "    #print()\n",
        "    #print(f\"Question:\\n{sample['text']}\")\n",
        "    #print()\n",
        "\n",
        "    #print()\n",
        "    #print(f\"Generated Answer original:\\n{ganswer}\")\n",
        "    #print()\n",
        "\n",
        "    # molecule [/INST] phosphorylethanolamine </s><s>[INST]\n",
        "    qc0=str(ganswer).find('[/INST]')\n",
        "    qc1=str(ganswer).find('[INST]')\n",
        "\n",
        "    #print(qc0)\n",
        "    #print(qc1)\n",
        "\n",
        "    ganswer=str(ganswer)[qc0+8:qc1-8]\n",
        "    #print(f\"Generated Answer corr:\\n{ganswer}\")\n",
        "\n",
        "    oanswer=str(sample['label'])\n",
        "    oanswer=oanswer[2:len(oanswer)-2]\n",
        "\n",
        "\n",
        "    #### exceptions TODO\n",
        "\n",
        "    #if ganswer == 'enteropathogenic':\n",
        "    #   ganswer=exceptions(ganswer,oanswer,ganswer)\n",
        "\n",
        "\n",
        "    #print(len(oanswer))\n",
        "    #print(len(ganswer))\n",
        "\n",
        "    #print(f\"Original Answer:\\n{oanswer}\")\n",
        "    if ganswer == oanswer:\n",
        "        print()\n",
        "        print()\n",
        "        print(\"Match\")\n",
        "        print(f\"Query:\\n{sample['text']}\")\n",
        "        print(f\"Original Answer:\\n{oanswer}\")\n",
        "        print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        print()\n",
        "        return 1\n",
        "    else:\n",
        "        print()\n",
        "        print()\n",
        "        print(\"no Match\")\n",
        "        print(f\"Query:\\n{sample['text']}\")\n",
        "        print(f\"Original Answer:\\n{oanswer}\")\n",
        "        print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        print()\n",
        "        return 0\n",
        "\n",
        "\n",
        "#### exceptions TODO\n",
        "\n",
        "#Original Answer:\n",
        "#enteropathogenic\n",
        "#Generated Answer:\n",
        "#enteropathogenic e coli\n",
        "\n",
        "#Original Answer:\n",
        "#severe mental retardation\n",
        "#Generated Answer:\n",
        "#mental retardation\n",
        "\n",
        "#Original Answer:\n",
        "#eye movement\n",
        "#Generated Answer:\n",
        "#eye movements\n",
        "\n",
        "\n",
        "#Original Answer:\n",
        "#severe mental retardation\n",
        "#Generated Answer:\n",
        "#mental retardation\n",
        "\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 5\n",
        "# iterate over eval dataset and predict\n",
        "\n",
        "#for n in tqdm(range(number_of_eval_samples)):\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "#    print(f'sample {n}')\n",
        "    s=eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "#for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
        "#    success_rate.append(evaluate(s))\n",
        "\n",
        "# compute accuracy\n",
        "accuracy = sum(success_rate)/len(success_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWw-yHYilDd",
        "outputId": "82b45279-0f86-47c0-9fea-7bc2ce53ba4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:18<01:14, 18.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "no Match\n",
            "Query:\n",
            "we developed an animal model of chronic allergic airway disease by repeatedly exposing nine sheep to tracheal instillation of ascaris antigen until stable increase in RL at three times control in six reactive sheep group c was obtained they were then compared to the three nonreactive sheep group b and a control group of eight sheep exposed to saline only group a in terms of pulmonary CF tests and bronchoalveolar lavage bal analyses RL was cm holsec in group a in group b and in group c trapping volume FRC by plethysmography and by helium rebreathing technique was l in group a in group b and in group c UP resistance at PF did not differ between any two CG but UP resistance near residual volume was cm holsec in a in b and in c in bal total cells were x ml in a in b and in c macrophages in bal were in a in b and in c neutrophils were in a in b in c eosinophils were in a in b and in c p less than group c versus group a total proteins albumin ALP phosphatase and fibronectin did not differ between groupsabstract truncated at words\n",
            "Original Answer:\n",
            "functional residual capacity\n",
            "Generated Answer:\n",
            "forced expiratory volume\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:36<00:55, 18.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Match\n",
            "Query:\n",
            "pyogenic granulomas represent the aquisition of vasodilative granulation tissue in the skin or mucosa they are extremely rare in the alimentary tract other than in the oral cavity here we report a case of PG arising from the GM an yearold man was admitted to our hospital because of melena of more than months duration esophagogastroduodenoscopy egd revealed a mmdiameter semipedunculated lesion with an irregular surface in the fundus of the stomach during hospitalization the patients anemia worsened due to loss of blood from the lesion with the level of hemoglobin declining to gdl and a blood transfusion was required because radiological and endoscopic findings indicated the lesion was hypervascular transarterial embolization of the nutritional i.a. of the lesion was performed before ER of the lesion one week T3 the embolotherapy endoscopic mucosal resection was performed without any complications such as massive one histological studies of the resected specimen revealed many capillaries of various sizes lined with plump EC cells and accompanied by acute and chronic inflammatory infiltrates on the basis of these observations the lesion was diagnosed as a PG one year later the patient was asymptomatic and there was no evidence of RT recurrence on followup egd\n",
            "Original Answer:\n",
            "pyogenic granuloma\n",
            "Generated Answer:\n",
            "pyogenic granuloma\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:55<00:36, 18.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "no Match\n",
            "Query:\n",
            "the l immunotype lipopolysaccharide lps of neisseria meningitidis was subjected to Kd procedures which produced a number of different oligosaccharide fragments the high resolution h and c nmr spectroscopic analyses of these OS yielded structural information on a number of different regions of the lps for example from one oligosaccharide it was found that the endogenous sialylation of the meningococcal lps occurs at o of the terminal betadgalactopyranosyl residue of its lactonneotetraose antenna in the alphadconfiguration from another it was also established that the dominant structural feature responsible for l epitope specificity is the presence of a PE substituent at o of the penultimate heptopyranosyl residue of its other antenna in addition from information obtained with another oligosaccharide the structure of the deoxydmannooctulosonic acid disaccharide region of the l lps was also elucidated from all the above cumulative data plus some published data it was then possible to reconstruct the CR structure of the entire native l lps\n",
            "Original Answer:\n",
            "phosphorylethanolamine\n",
            "Generated Answer:\n",
            "oligosaccharides\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [01:14<00:18, 18.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "no Match\n",
            "Query:\n",
            "inotropic reserve identified by dobutamine or dipyridamole SE is associated with a better outcome in patients with idiopathic dilated cardiomyopathy dcm although the relative prognostic value of each remains unsettled the purpose of the present study was to assess the relative prognostic value of dobutamine versus dipyridamole SE for the prediction of allcause death in patients with idiopathic dcm\n",
            "Original Answer:\n",
            "stress echocardiography\n",
            "Generated Answer:\n",
            "exercise testing\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [01:32<00:00, 18.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "no Match\n",
            "Query:\n",
            "pyridoxinedependent seizure is a rare autosomal recessive disorder that usually presents with neonatal intractable seizures this syndrome results from an inborn abnormality of the enzyme glu decarboxylase which results in reduced pyridazinedependent synthesis of the inhibitory neurotransmitter gamma amino butyric acid the full range of symptomatology is unknown but can be associated with autism breath holding and SMR bilious vomiting transient visual agnosia severe articulatory apraxia motor dyspraxia microcephaly and intrauterine seizures parenteral pyridine injection test is a highly ERP and reproducible test in confirming the diagnosis pyridoxine should be po as a diagnostic test in all cases of convulsive disorders of infancy in which no other diagnosis is evident epileptic seizure discharges subside within minutes T3 the i.v. of mg of pyridaoxine once the diagnosis is confirmed MT should be continued indefinitely and doses increased with age or intercurrent illnesses the MD of bg needed is still not clear there is a relatively wide range for the daily b dose necessary to control the seizure ie mgday\n",
            "Original Answer:\n",
            "severe mental retardation\n",
            "Generated Answer:\n",
            "pyridoxine dependency\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "vpTaXzQDcrVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "print(f\"Accuracy (Eval dataset and predict) for a sample of {number_of_eval_samples}: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiNueGHqFQIv",
        "outputId": "767c3a5e-09e7-4eb3-85cc-d2135e4125b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Eval dataset and predict) for a sample of 5: 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "learning_rate: 2e-05\n",
        "train_batch_size: 3\n",
        "eval_batch_size: 8\n",
        "seed: 42\n",
        "gradient_accumulation_steps: 6\n",
        "total_train_batch_size: 18\n",
        "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
        "lr_scheduler_type: constant\n",
        "lr_scheduler_warmup_ratio: 0.03\n",
        "num_epochs: 2\n",
        "\n",
        "When evaluated on 1000 samples from the evaluation dataset, our model achieved an accuracy of 26.30%. However, there's room for improvement.\n",
        "\n",
        "\n",
        "learning_rate: 0.0002\n",
        "train_batch_size: 3\n",
        "eval_batch_size: 8\n",
        "seed: 42\n",
        "gradient_accumulation_steps: 6\n",
        "total_train_batch_size: 18\n",
        "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
        "lr_scheduler_type: cosine\n",
        "lr_scheduler_warmup_ratio: 0.03\n",
        "num_epochs: 10\n",
        "\n",
        "When evaluated on 10 samples from the evaluation dataset, our model achieved an accuracy of 60%. However, there's room for improvement.\n",
        "\n"
      ],
      "metadata": {
        "id": "NYZDOY-0zp5d"
      }
    }
  ]
}
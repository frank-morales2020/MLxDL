{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+aS/XyPsQV4F45FoVIEMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/RAG_GOOGLE_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37dvyahNxkPd",
        "outputId": "46430845-de27-4ad7-e04e-a02da1afee59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini client configured for **gemini-3-pro-preview**.\n",
            "Attempting to download PDF from: https://arxiv.org/pdf/1706.03762.pdf\n",
            "Successfully downloaded and saved to: Attention_Is_All_You_Need.pdf\n",
            "\n",
            "Creating File Search Store...\n",
            "Store created: fileSearchStores/transformerragstore-wy6tksi3ftzd\n",
            "Uploading and indexing file: Attention_Is_All_You_Need.pdf...\n",
            "Waiting for indexing to complete (This may take a moment)...\n",
            "Indexing in progress...\n",
            "Indexing complete.\n",
            "\n",
            "Configuring model with RAG tool...\n",
            "\n",
            "User Query (using high thinking level): Summarize the major components of the model as described in the abstract.\n",
            "\n",
            "--- AI Response (Retrieval Augmented Generation) ---\n",
            "Based on the abstract of the paper \"Attention Is All You Need,\" the major components and characteristics of the model are summarized as follows:\n",
            "\n",
            "*   **Transformer Architecture:** The abstract introduces a new, simple network architecture called the **Transformer**.\n",
            "*   **Sole Reliance on Attention Mechanisms:** Unlike dominant sequence transduction models of the time, this architecture is based **solely on attention mechanisms**. It connects the encoder and decoder through attention rather than processing sequences sequentially.\n",
            "*   **Absence of Recurrence and Convolutions:** The model explicitly **dispenses with recurrence and convolutions** entirely, which were standard in previous complex neural networks (RNNs and CNNs).\n",
            "\n",
            "The abstract highlights that by using these components, the model becomes more parallelizable, requires significantly less training time, and achieves superior quality in machine translation tasks.\n",
            "\n",
            "--- Model Thoughts ---\n",
            "Model thoughts were requested but not returned by the current API structure.\n",
            "\n",
            "--- RAG Citations (Source Document) ---\n",
            "The concepts were grounded by retrieving the following chunks from the indexed PDF:\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "\n",
            "--- Cleanup ---\n",
            "Forcing deletion of File Search Store: fileSearchStores/transformerragstore-wy6tksi3ftzd\n",
            "Deleting local file: Attention_Is_All_You_Need.pdf\n",
            "Cleanup complete.\n"
          ]
        }
      ],
      "source": [
        "# 1. IMPORTS\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- Secure API Client Initialization using Userdata/Environment Variables ---\n",
        "GEMINI_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI')\n",
        "except (ImportError, KeyError):\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "REQUESTED_MODEL_ID = 'gemini-3-pro-preview'\n",
        "client = None\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    try:\n",
        "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "        # Check to ensure client initialized successfully before proceeding\n",
        "        if client:\n",
        "            print(f\"✅ Gemini client configured for **{REQUESTED_MODEL_ID}**.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Client initialization failed: {e}\")\n",
        "        client = None\n",
        "else:\n",
        "    print(\"❌ API Key not found. Please ensure your key is set up.\")\n",
        "\n",
        "if not client:\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. AGENT CONFIGURATIONS ---\n",
        "def get_low_think_config():\n",
        "    \"\"\"Config for fast, low-reasoning tasks (low latency).\"\"\"\n",
        "    return types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=\"low\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_high_think_config():\n",
        "    \"\"\"Config for complex, high-reasoning tasks (high quality, slower).\"\"\"\n",
        "    return types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=\"high\",\n",
        "            include_thoughts=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "# --- 3. RAG DEMO CONFIGURATION ---\n",
        "PDF_URL = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "FILE_PATH = \"Attention_Is_All_You_Need.pdf\"\n",
        "STORE_DISPLAY_NAME = \"Transformer_RAG_Store\"\n",
        "USER_QUERY = \"Summarize the major components of the model as described in the abstract.\"\n",
        "FILE_DISPLAY_NAME = \"Attention Is All You Need Paper\" # Defined for citation clarity\n",
        "\n",
        "# --- 4. CORE FUNCTIONS ---\n",
        "\n",
        "def download_pdf(url, filepath):\n",
        "    \"\"\"Downloads a PDF from a URL and saves it locally.\"\"\"\n",
        "    print(f\"Attempting to download PDF from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(filepath, 'wb') as pdf_file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                pdf_file.write(chunk)\n",
        "        print(f\"Successfully downloaded and saved to: {filepath}\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"[ERROR] Failed to download file: {e}\")\n",
        "        return False\n",
        "\n",
        "def run_rag_demo():\n",
        "    # Attempt to download the file first\n",
        "    if not download_pdf(PDF_URL, FILE_PATH):\n",
        "        print(\"Exiting RAG demo due to download failure.\")\n",
        "        return\n",
        "\n",
        "    file_search_store = None\n",
        "    uploaded_op = None\n",
        "\n",
        "    try:\n",
        "        # --- RAG STEP 1: Create a FileSearchStore ---\n",
        "        print(\"\\nCreating File Search Store...\")\n",
        "        file_search_store = client.file_search_stores.create(\n",
        "            config={\"display_name\": STORE_DISPLAY_NAME}\n",
        "        )\n",
        "        print(f\"Store created: {file_search_store.name}\")\n",
        "\n",
        "        # --- RAG STEP 2: Upload File and Import into Store ---\n",
        "        print(f\"Uploading and indexing file: {FILE_PATH}...\")\n",
        "\n",
        "        uploaded_op = client.file_search_stores.upload_to_file_search_store(\n",
        "            file=FILE_PATH,\n",
        "            file_search_store_name=file_search_store.name,\n",
        "            config={\"display_name\": FILE_DISPLAY_NAME}\n",
        "        )\n",
        "\n",
        "        print(\"Waiting for indexing to complete (This may take a moment)...\")\n",
        "        while not uploaded_op.done:\n",
        "            time.sleep(5)\n",
        "            uploaded_op = client.operations.get(uploaded_op)\n",
        "            print(\"Indexing in progress...\")\n",
        "\n",
        "        print(\"Indexing complete.\")\n",
        "\n",
        "        # --- RAG STEP 3: Query the Model using the FileSearchTool ---\n",
        "        print(\"\\nConfiguring model with RAG tool...\")\n",
        "\n",
        "        high_think_config = get_high_think_config()\n",
        "\n",
        "        # Using the stable configuration for the tool\n",
        "        high_think_config.tools = [\n",
        "            types.Tool(\n",
        "                file_search=types.FileSearch(\n",
        "                    file_search_store_names=[file_search_store.name]\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nUser Query (using high thinking level): {USER_QUERY}\")\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model=REQUESTED_MODEL_ID,\n",
        "            contents=USER_QUERY,\n",
        "            config=high_think_config\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- AI Response (Retrieval Augmented Generation) ---\")\n",
        "        print(response.text)\n",
        "\n",
        "        # FIX 1: Safely check for thinking_output\n",
        "        if hasattr(response, 'thinking_output') and response.thinking_output:\n",
        "            print(\"\\n--- Model's Internal Thoughts ---\")\n",
        "            print(str(response.thinking_output))\n",
        "        else:\n",
        "             print(\"\\n--- Model Thoughts ---\")\n",
        "             print(\"Model thoughts were requested but not returned by the current API structure.\")\n",
        "\n",
        "        # FINAL ROBUST FIX: Citation Access\n",
        "        grounding_metadata = response.candidates[0].grounding_metadata\n",
        "\n",
        "        chunks = None\n",
        "        if hasattr(grounding_metadata, 'grounding_chunks'):\n",
        "            chunks = grounding_metadata.grounding_chunks\n",
        "        elif hasattr(grounding_metadata, 'retrieval_chunks'):\n",
        "             chunks = grounding_metadata.retrieval_chunks\n",
        "\n",
        "        if chunks:\n",
        "            print(\"\\n--- RAG Citations (Source Document) ---\")\n",
        "            print(\"The concepts were grounded by retrieving the following chunks from the indexed PDF:\")\n",
        "\n",
        "            #\n",
        "\n",
        "            for chunk in chunks:\n",
        "                # The robust check for page number\n",
        "                page = 'N/A'\n",
        "                if hasattr(chunk, 'page_number') and chunk.page_number is not None:\n",
        "                    page = chunk.page_number\n",
        "                elif hasattr(chunk, 'rag_chunk') and chunk.rag_chunk and hasattr(chunk.rag_chunk, 'page_number') and chunk.rag_chunk.page_number is not None:\n",
        "                    page = chunk.rag_chunk.page_number\n",
        "\n",
        "                print(f\"- Source: {FILE_DISPLAY_NAME} (Page: {page})\")\n",
        "        else:\n",
        "             print(\"\\n--- RAG Citations (Source Document) ---\")\n",
        "             print(\"Grounding metadata was found, but the structure of the citation chunks is not recognized by this SDK version.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An API error occurred during the process: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # --- 5. Clean up ---\n",
        "        print(\"\\n--- Cleanup ---\")\n",
        "\n",
        "        if file_search_store:\n",
        "            try:\n",
        "                print(f\"Forcing deletion of File Search Store: {file_search_store.name}\")\n",
        "                client.file_search_stores.delete(\n",
        "                    name=file_search_store.name,\n",
        "                    config={'force': True}\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to delete store (Manual deletion may be required): {e}\")\n",
        "\n",
        "        if os.path.exists(FILE_PATH):\n",
        "            print(f\"Deleting local file: {FILE_PATH}\")\n",
        "            os.remove(FILE_PATH)\n",
        "\n",
        "        print(\"Cleanup complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_rag_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code with Complex Query"
      ],
      "metadata": {
        "id": "QIcC68A32sWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. IMPORTS\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- Secure API Client Initialization using Userdata/Environment Variables ---\n",
        "GEMINI_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI')\n",
        "except (ImportError, KeyError):\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "REQUESTED_MODEL_ID = 'gemini-3-pro-preview'\n",
        "client = None\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    try:\n",
        "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "        if client:\n",
        "            print(f\"✅ Gemini client configured for **{REQUESTED_MODEL_ID}**.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Client initialization failed: {e}\")\n",
        "        client = None\n",
        "else:\n",
        "    print(\"❌ API Key not found. Please ensure your key is set up.\")\n",
        "\n",
        "if not client:\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. AGENT CONFIGURATIONS ---\n",
        "def get_low_think_config():\n",
        "    \"\"\"Config for fast, low-reasoning tasks (low latency).\"\"\"\n",
        "    return types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=\"low\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_high_think_config():\n",
        "    \"\"\"Config for complex, high-reasoning tasks (high quality, slower).\"\"\"\n",
        "    return types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=\"high\",\n",
        "            include_thoughts=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "# --- 3. RAG DEMO CONFIGURATION ---\n",
        "PDF_URL = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "FILE_PATH = \"Attention_Is_All_You_Need.pdf\"\n",
        "STORE_DISPLAY_NAME = \"Transformer_RAG_Store\"\n",
        "# --- UPDATED COMPLEX QUERY ---\n",
        "USER_QUERY = \"What is the complexity per layer for the self-attention mechanism, and how does this compare to recurrent layers, as described in Section 3.1 and 4?\"\n",
        "# -----------------------------\n",
        "FILE_DISPLAY_NAME = \"Attention Is All You Need Paper\"\n",
        "\n",
        "# --- 4. CORE FUNCTIONS ---\n",
        "\n",
        "def download_pdf(url, filepath):\n",
        "    \"\"\"Downloads a PDF from a URL and saves it locally.\"\"\"\n",
        "    print(f\"Attempting to download PDF from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(filepath, 'wb') as pdf_file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                pdf_file.write(chunk)\n",
        "        print(f\"Successfully downloaded and saved to: {filepath}\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"[ERROR] Failed to download file: {e}\")\n",
        "        return False\n",
        "\n",
        "def run_rag_demo():\n",
        "    # Attempt to download the file first\n",
        "    if not download_pdf(PDF_URL, FILE_PATH):\n",
        "        print(\"Exiting RAG demo due to download failure.\")\n",
        "        return\n",
        "\n",
        "    file_search_store = None\n",
        "    uploaded_op = None\n",
        "\n",
        "    try:\n",
        "        # --- RAG STEP 1: Create a FileSearchStore ---\n",
        "        print(\"\\nCreating File Search Store...\")\n",
        "        file_search_store = client.file_search_stores.create(\n",
        "            config={\"display_name\": STORE_DISPLAY_NAME}\n",
        "        )\n",
        "        print(f\"Store created: {file_search_store.name}\")\n",
        "\n",
        "        # --- RAG STEP 2: Upload File and Import into Store ---\n",
        "        print(f\"Uploading and indexing file: {FILE_PATH}...\")\n",
        "\n",
        "        uploaded_op = client.file_search_stores.upload_to_file_search_store(\n",
        "            file=FILE_PATH,\n",
        "            file_search_store_name=file_search_store.name,\n",
        "            config={\"display_name\": FILE_DISPLAY_NAME}\n",
        "        )\n",
        "\n",
        "        print(\"Waiting for indexing to complete (This may take a moment)...\")\n",
        "        while not uploaded_op.done:\n",
        "            time.sleep(5)\n",
        "            uploaded_op = client.operations.get(uploaded_op)\n",
        "            print(\"Indexing in progress...\")\n",
        "\n",
        "        print(\"Indexing complete.\")\n",
        "\n",
        "        # --- RAG STEP 3: Query the Model using the FileSearchTool ---\n",
        "        print(\"\\nConfiguring model with RAG tool...\")\n",
        "\n",
        "        high_think_config = get_high_think_config()\n",
        "\n",
        "        # Using the stable configuration for the tool\n",
        "        high_think_config.tools = [\n",
        "            types.Tool(\n",
        "                file_search=types.FileSearch(\n",
        "                    file_search_store_names=[file_search_store.name]\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nUser Query (using high thinking level): {USER_QUERY}\")\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model=REQUESTED_MODEL_ID,\n",
        "            contents=USER_QUERY,\n",
        "            config=high_think_config\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- AI Response (Retrieval Augmented Generation) ---\")\n",
        "        print(response.text)\n",
        "\n",
        "        # FIX 1: Safely check for thinking_output\n",
        "        if hasattr(response, 'thinking_output') and response.thinking_output:\n",
        "            print(\"\\n--- Model's Internal Thoughts ---\")\n",
        "            print(str(response.thinking_output))\n",
        "        else:\n",
        "             print(\"\\n--- Model Thoughts ---\")\n",
        "             print(\"Model thoughts were requested but not returned by the current API structure.\")\n",
        "\n",
        "        # FINAL ROBUST FIX: Citation Access\n",
        "        grounding_metadata = response.candidates[0].grounding_metadata\n",
        "\n",
        "        chunks = None\n",
        "        if hasattr(grounding_metadata, 'grounding_chunks'):\n",
        "            chunks = grounding_metadata.grounding_chunks\n",
        "        elif hasattr(grounding_metadata, 'retrieval_chunks'):\n",
        "             chunks = grounding_metadata.retrieval_chunks\n",
        "\n",
        "        if chunks:\n",
        "            print(\"\\n--- RAG Citations (Source Document) ---\")\n",
        "            print(\"The concepts were grounded by retrieving the following chunks from the indexed PDF:\")\n",
        "\n",
        "            #\n",
        "\n",
        "            for chunk in chunks:\n",
        "                # The robust check for page number\n",
        "                page = 'N/A'\n",
        "                if hasattr(chunk, 'page_number') and chunk.page_number is not None:\n",
        "                    page = chunk.page_number\n",
        "\n",
        "                # Check on the internal rag_chunk (if it exists)\n",
        "                elif hasattr(chunk, 'rag_chunk') and chunk.rag_chunk and hasattr(chunk.rag_chunk, 'page_number') and chunk.rag_chunk.page_number is not None:\n",
        "                    page = chunk.rag_chunk.page_number\n",
        "\n",
        "                print(f\"- Source: {FILE_DISPLAY_NAME} (Page: {page})\")\n",
        "        else:\n",
        "             print(\"\\n--- RAG Citations (Source Document) ---\")\n",
        "             print(\"Grounding metadata was found, but the structure of the citation chunks is not recognized by this SDK version.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An API error occurred during the process: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # --- 5. Clean up ---\n",
        "        print(\"\\n--- Cleanup ---\")\n",
        "\n",
        "        if file_search_store:\n",
        "            try:\n",
        "                print(f\"Forcing deletion of File Search Store: {file_search_store.name}\")\n",
        "                client.file_search_stores.delete(\n",
        "                    name=file_search_store.name,\n",
        "                    config={'force': True}\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to delete store (Manual deletion may be required): {e}\")\n",
        "\n",
        "        if os.path.exists(FILE_PATH):\n",
        "            print(f\"Deleting local file: {FILE_PATH}\")\n",
        "            os.remove(FILE_PATH)\n",
        "\n",
        "        print(\"Cleanup complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_rag_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGW3MOQz2un6",
        "outputId": "3cad8cd0-547e-4b61-f538-5ae78c83fc40"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini client configured for **gemini-3-pro-preview**.\n",
            "Attempting to download PDF from: https://arxiv.org/pdf/1706.03762.pdf\n",
            "Successfully downloaded and saved to: Attention_Is_All_You_Need.pdf\n",
            "\n",
            "Creating File Search Store...\n",
            "Store created: fileSearchStores/transformerragstore-c6v2y1jzwm1p\n",
            "Uploading and indexing file: Attention_Is_All_You_Need.pdf...\n",
            "Waiting for indexing to complete (This may take a moment)...\n",
            "Indexing in progress...\n",
            "Indexing complete.\n",
            "\n",
            "Configuring model with RAG tool...\n",
            "\n",
            "User Query (using high thinking level): What is the complexity per layer for the self-attention mechanism, and how does this compare to recurrent layers, as described in Section 3.1 and 4?\n",
            "\n",
            "--- AI Response (Retrieval Augmented Generation) ---\n",
            "Based on **Section 4** (specifically Table 1 and the accompanying text) of the \"Attention Is All You Need\" paper, here is the complexity per layer for the self-attention mechanism and its comparison to recurrent layers:\n",
            "\n",
            "### Complexity per Layer\n",
            "*   **Self-Attention Layer:** The computational complexity per layer is **$O(n^2 \\cdot d)$**.\n",
            "*   **Recurrent Layer:** The computational complexity per layer is **$O(n \\cdot d^2)$**.\n",
            "\n",
            "*(Where $n$ is the sequence length and $d$ is the representation dimension.)*\n",
            "\n",
            "### Comparison\n",
            "The paper compares these two mechanisms based on three main criteria:\n",
            "\n",
            "1.  **Computational Complexity:**\n",
            "    *   Self-attention layers are computationally faster than recurrent layers when the sequence length $n$ is smaller than the representation dimensionality $d$ ($n < d$).\n",
            "    *   The authors note that this condition ($n < d$) is frequently met in state-of-the-art models for machine translation, where sentence representations (like word-piece or byte-pair) are used.\n",
            "\n",
            "2.  **Sequential Operations (Parallelization):**\n",
            "    *   **Self-Attention:** Requires **$O(1)$** sequential operations. This allows for significantly more parallelization, as the mechanism connects all positions with a constant number of sequentially executed operations.\n",
            "    *   **Recurrent:** Requires **$O(n)$** sequential operations, limiting parallelization because it must process the sequence step-by-step.\n",
            "\n",
            "3.  **Path Length (Long-Range Dependencies):**\n",
            "    *   **Self-Attention:** The maximum path length between any two positions in the network is **$O(1)$**. This shorter path length makes it easier for the model to learn long-range dependencies between distant positions in the input and output sequences.\n",
            "    *   **Recurrent:** The maximum path length is **$O(n)$**, meaning signals may have to traverse a long path (forward and backward) to connect distant positions.\n",
            "\n",
            "--- Model Thoughts ---\n",
            "Model thoughts were requested but not returned by the current API structure.\n",
            "\n",
            "--- RAG Citations (Source Document) ---\n",
            "The concepts were grounded by retrieving the following chunks from the indexed PDF:\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "\n",
            "--- Cleanup ---\n",
            "Forcing deletion of File Search Store: fileSearchStores/transformerragstore-c6v2y1jzwm1p\n",
            "Deleting local file: Attention_Is_All_You_Need.pdf\n",
            "Cleanup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code with Multi-Head Attention Query"
      ],
      "metadata": {
        "id": "upOgkqBl3328"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. IMPORTS\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- Secure API Client Initialization using Userdata/Environment Variables ---\n",
        "GEMINI_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI')\n",
        "except (ImportError, KeyError):\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "REQUESTED_MODEL_ID = 'gemini-3-pro-preview'\n",
        "client = None\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    try:\n",
        "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "        if client:\n",
        "            print(f\"✅ Gemini client configured for **{REQUESTED_MODEL_ID}**.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Client initialization failed: {e}\")\n",
        "        client = None\n",
        "else:\n",
        "    print(\"❌ API Key not found. Please ensure your key is set up.\")\n",
        "\n",
        "if not client:\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. AGENT CONFIGURATIONS ---\n",
        "def get_low_think_config():\n",
        "    \"\"\"Config for fast, low-reasoning tasks (low latency).\"\"\"\n",
        "    return types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=\"low\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_high_think_config():\n",
        "    \"\"\"Config for complex, high-reasoning tasks (high quality, slower).\"\"\"\n",
        "    return types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_level=\"high\",\n",
        "            include_thoughts=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "# --- 3. RAG DEMO CONFIGURATION ---\n",
        "PDF_URL = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "FILE_PATH = \"Attention_Is_All_You_Need.pdf\"\n",
        "STORE_DISPLAY_NAME = \"Transformer_RAG_Store\"\n",
        "# --- UPDATED COMPLEX QUERY ---\n",
        "USER_QUERY = \"What is the motivation behind using Multi-Head Attention instead of single-head attention, and what are the specific benefits of dividing the attention space? (Reference sections 3.2.2 and 3.2.3)\"\n",
        "# -----------------------------\n",
        "FILE_DISPLAY_NAME = \"Attention Is All You Need Paper\"\n",
        "\n",
        "# --- 4. CORE FUNCTIONS ---\n",
        "\n",
        "def download_pdf(url, filepath):\n",
        "    \"\"\"Downloads a PDF from a URL and saves it locally.\"\"\"\n",
        "    print(f\"Attempting to download PDF from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(filepath, 'wb') as pdf_file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                pdf_file.write(chunk)\n",
        "        print(f\"Successfully downloaded and saved to: {filepath}\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"[ERROR] Failed to download file: {e}\")\n",
        "        return False\n",
        "\n",
        "def run_rag_demo():\n",
        "    # Attempt to download the file first\n",
        "    if not download_pdf(PDF_URL, FILE_PATH):\n",
        "        print(\"Exiting RAG demo due to download failure.\")\n",
        "        return\n",
        "\n",
        "    file_search_store = None\n",
        "    uploaded_op = None\n",
        "\n",
        "    try:\n",
        "        # --- RAG STEP 1: Create a FileSearchStore ---\n",
        "        print(\"\\nCreating File Search Store...\")\n",
        "        file_search_store = client.file_search_stores.create(\n",
        "            config={\"display_name\": STORE_DISPLAY_NAME}\n",
        "        )\n",
        "        print(f\"Store created: {file_search_store.name}\")\n",
        "\n",
        "        # --- RAG STEP 2: Upload File and Import into Store ---\n",
        "        print(f\"Uploading and indexing file: {FILE_PATH}...\")\n",
        "\n",
        "        uploaded_op = client.file_search_stores.upload_to_file_search_store(\n",
        "            file=FILE_PATH,\n",
        "            file_search_store_name=file_search_store.name,\n",
        "            config={\"display_name\": FILE_DISPLAY_NAME}\n",
        "        )\n",
        "\n",
        "        print(\"Waiting for indexing to complete (This may take a moment)...\")\n",
        "        while not uploaded_op.done:\n",
        "            time.sleep(5)\n",
        "            uploaded_op = client.operations.get(uploaded_op)\n",
        "            print(\"Indexing in progress...\")\n",
        "\n",
        "        print(\"Indexing complete.\")\n",
        "\n",
        "        # --- RAG STEP 3: Query the Model using the FileSearchTool ---\n",
        "        print(\"\\nConfiguring model with RAG tool...\")\n",
        "\n",
        "        high_think_config = get_high_think_config()\n",
        "\n",
        "        # Using the stable configuration for the tool\n",
        "        high_think_config.tools = [\n",
        "            types.Tool(\n",
        "                file_search=types.FileSearch(\n",
        "                    file_search_store_names=[file_search_store.name]\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nUser Query (using high thinking level): {USER_QUERY}\")\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model=REQUESTED_MODEL_ID,\n",
        "            contents=USER_QUERY,\n",
        "            config=high_think_config\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- AI Response (Retrieval Augmented Generation) ---\")\n",
        "        print(response.text)\n",
        "\n",
        "        # FIX 1: Safely check for thinking_output\n",
        "        if hasattr(response, 'thinking_output') and response.thinking_output:\n",
        "            print(\"\\n--- Model's Internal Thoughts ---\")\n",
        "            print(str(response.thinking_output))\n",
        "        else:\n",
        "             print(\"\\n--- Model Thoughts ---\")\n",
        "             print(\"Model thoughts were requested but not returned by the current API structure.\")\n",
        "\n",
        "        # FINAL ROBUST FIX: Citation Access\n",
        "        grounding_metadata = response.candidates[0].grounding_metadata\n",
        "\n",
        "        chunks = None\n",
        "        if hasattr(grounding_metadata, 'grounding_chunks'):\n",
        "            chunks = grounding_metadata.grounding_chunks\n",
        "        elif hasattr(grounding_metadata, 'retrieval_chunks'):\n",
        "             chunks = grounding_metadata.retrieval_chunks\n",
        "\n",
        "        if chunks:\n",
        "            print(\"\\n--- RAG Citations (Source Document) ---\")\n",
        "            print(\"The concepts were grounded by retrieving the following chunks from the indexed PDF:\")\n",
        "\n",
        "            for chunk in chunks:\n",
        "                # The robust check for page number\n",
        "                page = 'N/A'\n",
        "                if hasattr(chunk, 'page_number') and chunk.page_number is not None:\n",
        "                    page = chunk.page_number\n",
        "\n",
        "                # Check on the internal rag_chunk (if it exists)\n",
        "                elif hasattr(chunk, 'rag_chunk') and chunk.rag_chunk and hasattr(chunk.rag_chunk, 'page_number') and chunk.rag_chunk.page_number is not None:\n",
        "                    page = chunk.rag_chunk.page_number\n",
        "\n",
        "                print(f\"- Source: {FILE_DISPLAY_NAME} (Page: {page})\")\n",
        "        else:\n",
        "             print(\"\\n--- RAG Citations (Source Document) ---\")\n",
        "             print(\"Grounding metadata was found, but the structure of the citation chunks is not recognized by this SDK version.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An API error occurred during the process: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # --- 5. Clean up ---\n",
        "        print(\"\\n--- Cleanup ---\")\n",
        "\n",
        "        if file_search_store:\n",
        "            try:\n",
        "                print(f\"Forcing deletion of File Search Store: {file_search_store.name}\")\n",
        "                client.file_search_stores.delete(\n",
        "                    name=file_search_store.name,\n",
        "                    config={'force': True}\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to delete store (Manual deletion may be required): {e}\")\n",
        "\n",
        "        if os.path.exists(FILE_PATH):\n",
        "            print(f\"Deleting local file: {FILE_PATH}\")\n",
        "            os.remove(FILE_PATH)\n",
        "\n",
        "        print(\"Cleanup complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_rag_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQN-XrCR36El",
        "outputId": "c2543430-8a26-4f95-961f-72a8fcba4bf7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini client configured for **gemini-3-pro-preview**.\n",
            "Attempting to download PDF from: https://arxiv.org/pdf/1706.03762.pdf\n",
            "Successfully downloaded and saved to: Attention_Is_All_You_Need.pdf\n",
            "\n",
            "Creating File Search Store...\n",
            "Store created: fileSearchStores/transformerragstore-0hrq1vopweyt\n",
            "Uploading and indexing file: Attention_Is_All_You_Need.pdf...\n",
            "Waiting for indexing to complete (This may take a moment)...\n",
            "Indexing in progress...\n",
            "Indexing complete.\n",
            "\n",
            "Configuring model with RAG tool...\n",
            "\n",
            "User Query (using high thinking level): What is the motivation behind using Multi-Head Attention instead of single-head attention, and what are the specific benefits of dividing the attention space? (Reference sections 3.2.2 and 3.2.3)\n",
            "\n",
            "--- AI Response (Retrieval Augmented Generation) ---\n",
            "Based on **Section 3.2.2 (Multi-Head Attention)** of the \"Attention Is All You Need\" paper, the motivation and benefits are as follows:\n",
            "\n",
            "### Motivation\n",
            "The primary motivation for using Multi-Head Attention instead of a single attention function is that it allows the model to capture a richer variety of information. Instead of performing a single attention function with full-dimensional keys, values, and queries ($d_{model}$), the authors found it beneficial to project these inputs linearly $h$ times into smaller dimensions ($d_k$, $d_v$) using different, learned linear projections.\n",
            "\n",
            "### Specific Benefits of Dividing the Attention Space\n",
            "Dividing the attention space into multiple heads provides two main benefits:\n",
            "\n",
            "1.  **Joint Attention to Different Subspaces:** It allows the model to \"jointly attend to information from different representation subspaces at different positions.\" This means the model can focus on different types of relationships or features (e.g., grammatical structure vs. semantic meaning) simultaneously.\n",
            "2.  **Avoiding the \"Averaging\" Problem:** With a single attention head, the model is forced to average the attention weights across all potential subspaces. This averaging process inhibits the model's ability to focus on distinct aspects of the input simultaneously. Multi-head attention overcomes this limitation.\n",
            "\n",
            "Additionally, the paper notes that despite having multiple heads, the total computational cost remains similar to that of single-head attention with full dimensionality, because each head operates on a reduced dimension (e.g., $d_{model}/h$).\n",
            "\n",
            "--- Model Thoughts ---\n",
            "Model thoughts were requested but not returned by the current API structure.\n",
            "\n",
            "--- RAG Citations (Source Document) ---\n",
            "The concepts were grounded by retrieving the following chunks from the indexed PDF:\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "- Source: Attention Is All You Need Paper (Page: N/A)\n",
            "\n",
            "--- Cleanup ---\n",
            "Forcing deletion of File Search Store: fileSearchStores/transformerragstore-0hrq1vopweyt\n",
            "Deleting local file: Attention_Is_All_You_Need.pdf\n",
            "Cleanup complete.\n"
          ]
        }
      ]
    }
  ]
}
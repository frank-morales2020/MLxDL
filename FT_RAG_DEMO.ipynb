{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Q4bC2iWnFSOG"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO5eDxd+83hzqWrtfAuqf0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FT_RAG_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets faiss-cpu evaluate -q\n",
        "!pip install -U rouge_score -q\n",
        "!pip install -U torch  -q"
      ],
      "metadata": {
        "id": "dWDUoGSlErRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchvision -q"
      ],
      "metadata": {
        "id": "VvyMNBdAMdx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FineTuning"
      ],
      "metadata": {
        "id": "Y_mPWuaNOMRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets faiss-cpu evaluate -q\n",
        "!pip install -U rouge_score -q\n",
        "!pip install -U torch  -q\n",
        "!pip install -U torchvision -q"
      ],
      "metadata": {
        "id": "R8c3PJoWPf7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30c88db-eae8-4541-82da-9fedd6598b87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# Load the Financial PhraseBank dataset from Hugging Face\n",
        "dataset = load_dataset(\"atrost/financial_phrasebank\")\n",
        "dataset\n",
        "dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCDCqmkx-Z7C",
        "outputId": "6015d270-76fa-44be-fdb1-b1c46942c23e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 'EBIT margin was up from 1.4 % to 5.1 % .', 'label': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyuJYyrOCto_",
        "outputId": "e30c6d50-5832-4567-ab2a-9778e227f96d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label'],\n",
              "        num_rows: 3100\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label'],\n",
              "        num_rows: 776\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label'],\n",
              "        num_rows: 970\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np  # Import numpy for numerical calculations\n",
        "from textblob import TextBlob  # Import TextBlob for sentiment analysis\n",
        "\n",
        "\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    \"\"\"Calculates the sentiment score of a text using TextBlob.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        The sentiment score (polarity) of the text, ranging from -1 to 1.\n",
        "    \"\"\"\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity\n",
        "\n",
        "\n",
        "def select_diverse_questions(test_dataset, num_questions=2):\n",
        "    \"\"\"Selects questions with diverse sentiment scores.\n",
        "\n",
        "    Args:\n",
        "        test_dataset: The test dataset with sentiment scores.\n",
        "        num_questions: The number of questions to select.\n",
        "\n",
        "    Returns:\n",
        "        A list of selected questions.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get sentiment scores for all questions\n",
        "    sentiment_scores = [get_sentiment_score(q['sentence']) for q in test_dataset]  # Assuming get_sentiment_score() is defined\n",
        "\n",
        "    # 2. Select questions based on score distribution\n",
        "    selected_questions = []\n",
        "    for _ in range(num_questions):\n",
        "        avg_score = np.mean([get_sentiment_score(q['sentence']) for q in selected_questions]) if selected_questions else 0\n",
        "        farthest_question = max(test_dataset, key=lambda q: abs(get_sentiment_score(q['sentence']) - avg_score))\n",
        "        # Append the entire dictionary/row to selected_questions:\n",
        "        selected_questions.append(farthest_question)\n",
        "\n",
        "\n",
        "    return selected_questions"
      ],
      "metadata": {
        "id": "7d15xE1-Ut5t"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "from sklearn.metrics import f1_score\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# 1. Load the dataset\n",
        "dataset = load_dataset(\"atrost/financial_phrasebank\")\n",
        "\n",
        "# 2. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # or your preferred model\n",
        "\n",
        "# 3. Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# 4. Apply tokenization to the datasets\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# **Control the number of rows**\n",
        "num_train_rows = 10  # Set the desired number of training rows\n",
        "num_test_rows = 2   # Set the desired number of test rows\n",
        "\n",
        "# Select a subset of the data\n",
        "train_dataset = tokenized_datasets[\"train\"].select(range(num_train_rows))\n",
        "test_dataset = tokenized_datasets[\"test\"].select(range(num_test_rows))\n",
        "\n",
        "sentiment_groups = {}\n",
        "for i in range(len(test_dataset)):\n",
        "    sentiment = test_dataset[i]['label']  # Or 'label', depending on your column name\n",
        "    if sentiment not in sentiment_groups:\n",
        "        sentiment_groups[sentiment] = []\n",
        "    sentiment_groups[sentiment].append(test_dataset[i]['sentence'])  # Assuming 'sentence' column contains the questions\n",
        "\n",
        "# 2. Select one question from each of two different sentiments\n",
        "sentiments = list(sentiment_groups.keys())\n",
        "\n",
        "\n",
        "# Example usage\n",
        "selected_questions = select_diverse_questions(test_dataset)\n",
        "\n",
        "#print(\"Selected Questions:\")\n",
        "#for question in selected_questions:\n",
        "#    print(question)\n",
        "\n",
        "print('\\n')\n",
        "if len(sentiments) >= 2:\n",
        "    sentiment1 = random.choice(sentiments)\n",
        "    sentiments.remove(sentiment1)  # Avoid selecting the same sentiment twice\n",
        "    sentiment2 = random.choice(sentiments)\n",
        "\n",
        "    question1 = random.choice(sentiment_groups[sentiment1])\n",
        "    question2 = random.choice(sentiment_groups[sentiment2])\n",
        "\n",
        "    #print(\"Question 1 (Sentiment:\", sentiment1, \"):\", question1)\n",
        "    #print(\"Question 2 (Sentiment:\", sentiment2, \"):\", question2)\n",
        "#else:\n",
        "#    print(\"Not enough distinct sentiments in the test dataset to select two questions.\")\n",
        "\n",
        "\n",
        "# --- Extract and Print Test Questions (with special tokens test within decode) ---\n",
        "special_token_ids = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id, tokenizer.mask_token_id]\n",
        "\n",
        "test_questions = [\n",
        "    tokenizer.decode(\n",
        "        [token_id for token_id in test_dataset.select([i])['input_ids'][0] if token_id not in special_token_ids], # Check against special_token_ids\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    for i in range(num_test_rows)\n",
        "]\n",
        "\n",
        "#print(\"Test Questions (derived from test sentences):\")  # Changed print message\n",
        "#for i, question in enumerate(test_questions):\n",
        "#    print(f\"Question {i+1}: {question}\")\n",
        "\n",
        "\n",
        "# Model and training setup\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "seed=42\n",
        "\n",
        "# **Key Change: Custom data collator**\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./financial_sentiment_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,  # Reduced batch size\n",
        "    per_device_eval_batch_size=32, # Reduced batch size\n",
        "    num_train_epochs=1,  # Reduced number of epochs for demonstration\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",  # or \"wandb\" if you're using Weights & Biases\n",
        "    use_cpu=True,  # Explicitly set use_cpu to True if needed\n",
        "    seed=seed,  # Set the seed in TrainingArguments as well\n",
        "    logging_strategy=\"steps\",  # Log every 'logging_steps'\n",
        "    logging_steps=10,          # Log every 10 steps\n",
        "    max_steps=100,\n",
        "    save_strategy=\"epoch\",    # Save the model every epoch\n",
        ")\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
        "    return {\"f1\": f1}\n",
        "\n",
        "# Create Trainer with preprocessed datasets\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./financial_sentiment_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "PPsaF8LUBJlY",
        "outputId": "f0bfcf21-d96d-4b15-e8e4-72d7a39993da"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 07:09, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.772200</td>\n",
              "      <td>0.807259</td>\n",
              "      <td>0.553284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBrmQQPaMhCc",
        "outputId": "4515dbcf-3ed5-41b4-9a8b-42b33454d78a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 2\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "Q4bC2iWnFSOG"
      }
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "# --- RAG Example (Question Answering on Financial Reports) ---\n",
        "\n",
        "# Assuming test_questions is already defined and contains the extracted test questions\n",
        "\n",
        "# RAG setup\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer)\n",
        "\n",
        "def rag_answer(question, reports):\n",
        "    best_answer = {\"answer\": \"No answer found.\", \"score\": 0}\n",
        "    for report_name, report_text in reports.items():\n",
        "        result = qa_pipeline(question=question, context=report_text)\n",
        "        if result[\"score\"] > best_answer[\"score\"]:\n",
        "            best_answer = result\n",
        "    return best_answer[\"answer\"]\n",
        "\n",
        "# Sample financial report data (updated with test sentences)\n",
        "financial_reports = {\n",
        "    \"report1\": test_questions[0],  # Use the first test question\n",
        "    \"report2\": test_questions[1]   # Use the second test question\n",
        "}\n",
        "\n",
        "# Example usage (updated with test questions)\n",
        "question1 = \"What will the works include?\"  # Example question related to test_questions[0]\n",
        "answer1 = rag_answer(question1, financial_reports)\n",
        "print(f\"Question: {question1}\")\n",
        "print(f\"Answer: {answer1}\")\n",
        "\n",
        "question2 = \"Where is Teleste listed?\"  # Example question related to test_questions[1]\n",
        "answer2 = rag_answer(question2, financial_reports)\n",
        "print(f\"Question: {question2}\")\n",
        "print(f\"Answer: {answer2}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyygpKegPH_d",
        "outputId": "e6ecbc44-cc68-4efb-ad1f-b0dec192d47f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What will the works include?\n",
            "Answer: 30 offices worldwide and is listed on the nordic exchange in helsinki.\n",
            "Question: Where is Teleste listed?\n",
            "Answer: nordic exchange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG and FT Integration"
      ],
      "metadata": {
        "id": "6miiAz-sPUi0"
      }
    },
    {
      "source": [
        "# ... (other imports and code) ...\n",
        "\n",
        "# --- 3. Integration Example (Combining Fine-Tuning and RAG) ---\n",
        "\n",
        "def integrated_analysis(news_text, question, reports, sentiment_model, sentiment_tokenizer):\n",
        "    # Sentiment analysis\n",
        "    inputs = sentiment_tokenizer(news_text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = sentiment_model(**inputs)\n",
        "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
        "    sentiment_labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "    sentiment = sentiment_labels[predicted_class]\n",
        "\n",
        "    # RAG question answering\n",
        "    rag_answer_text = rag_answer(question, reports)\n",
        "\n",
        "    return {\"sentiment\": sentiment, \"rag_answer\": rag_answer_text}\n",
        "\n",
        "# Load the fine-tuned model (if trained)\n",
        "try:\n",
        "    loaded_sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"./financial_sentiment_model\")\n",
        "    loaded_sentiment_tokenizer = AutoTokenizer.from_pretrained(\"./financial_sentiment_model\")\n",
        "\n",
        "    # --- Align news_example with test_questions ---\n",
        "    news_examples = [\n",
        "        \"Construction firm awarded contract for laying natural stone pavements and installing underground heating systems.\",  # Related to test_questions[0]\n",
        "        \"Telecom company Teleste reports strong growth in Nordic markets and expansion of global offices.\",  # Related to test_questions[1]\n",
        "    ]\n",
        "\n",
        "    # Iterate through test_questions and corresponding news_examples\n",
        "    for i in range(len(test_questions)):\n",
        "        news_example = news_examples[i]\n",
        "        question_example = test_questions[i]\n",
        "\n",
        "        financial_reports = {\n",
        "            \"report1\": test_questions[0],\n",
        "            \"report2\": test_questions[1]\n",
        "        }\n",
        "\n",
        "        analysis_result = integrated_analysis(news_example, question_example, financial_reports, loaded_sentiment_model, loaded_sentiment_tokenizer)\n",
        "\n",
        "        print('\\n')\n",
        "        print(f\"News: {news_example}\")\n",
        "        print(f\"Sentiment: {analysis_result['sentiment']}\")\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"RAG Answer: {analysis_result['rag_answer']}\")\n",
        "\n",
        "except OSError:\n",
        "    print(\"Fine-tuned model not found. Please train the sentiment model first.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCaJMb3-PPGd",
        "outputId": "0bf07930-969d-4600-c17b-43c6c990e0e0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "News: Construction firm awarded contract for laying natural stone pavements and installing underground heating systems.\n",
            "Sentiment: Neutral\n",
            "Question: the works will include the laying of natural stone pavements and the installation of underground heating, and surface water drainage systems.\n",
            "RAG Answer: underground heating, and surface water drainage systems\n",
            "\n",
            "\n",
            "News: Telecom company Teleste reports strong growth in Nordic markets and expansion of global offices.\n",
            "Sentiment: Neutral\n",
            "Question: teleste has some 30 offices worldwide and is listed on the nordic exchange in helsinki.\n",
            "RAG Answer: teleste has some 30 offices worldwide\n"
          ]
        }
      ]
    }
  ]
}
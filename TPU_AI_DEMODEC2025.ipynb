{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyNoYYkiYA2oWKHPcpaeeejt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/TPU_AI_DEMODEC2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ORgSvPftJm",
        "outputId": "3be43f22-9818-466b-b6ed-243a77d7c7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸ¤– Agentic AI Demo: TPU-Accelerated Loop\n",
            "======================================================================\n",
            "INFO: Starting JAX Profiler to capture trace data...\n",
            "INFO: JAX Profiler trace saved successfully to directory: /tmp/tpu_agent_trace_output\n",
            "\n",
            "======================================================================\n",
            "AGENT DEMO RESULT SUMMARY\n",
            "======================================================================\n",
            "Final Answer (TPU-Enhanced):\n",
            "Prompt: 'I need the current weather in London to decide on my outfit.'\n",
            "Tool Output: {\"city\": \"London\", \"temp_c\": 12, \"condition\": \"Cloudy\", \"source\": \"API\"}\n",
            "Model Synthesis Score: -1.6875\n",
            "Conclusion: The agent used the weather tool and synthesized the final result based on the data.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, random\n",
        "from flax import linen as nn\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "from typing import Any, Tuple\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Configuration ---\n",
        "DTYPE = jnp.bfloat16\n",
        "D_MODEL = 512\n",
        "SEQ_LEN = 64\n",
        "BATCH_SIZE = 1\n",
        "# Directory where the TPU profile data will be saved\n",
        "TRACE_OUTPUT_DIR = \"/tmp/tpu_agent_trace_output\"\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- ðŸ–¥ï¸ TPU Resource Logging ---\n",
        "\n",
        "def log_tpu_details():\n",
        "    \"\"\"Checks and logs the details of the available JAX devices (TPUs).\"\"\"\n",
        "    devices = jax.devices()\n",
        "    if not devices:\n",
        "        logging.error(\"No JAX devices found. Ensure the TPU is configured correctly.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"--- ðŸ–¥ï¸ TPU RESOURCE DETAILS ---\")\n",
        "    logging.info(f\"Total JAX Devices (Units/Cores): {len(devices)}\")\n",
        "\n",
        "    first_device = devices[0]\n",
        "    logging.info(f\"Device Kind (TPU Type): {first_device.device_kind}\")\n",
        "    logging.info(\"-\" * 35)\n",
        "\n",
        "# --- 1. TPU-Accelerated LLM Core (using JAX/Flax) ---\n",
        "\n",
        "class TinyLLM(nn.Module):\n",
        "    \"\"\"A simplified Transformer block for demonstration, optimized for TPU.\"\"\"\n",
        "    d_model: int = D_MODEL\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        w_q = self.param('w_q', nn.initializers.glorot_uniform(), (self.d_model, self.d_model), DTYPE)\n",
        "        q = jnp.dot(x, w_q)\n",
        "\n",
        "        w1 = self.param('w1', nn.initializers.glorot_uniform(), (self.d_model, self.d_model * 4), DTYPE)\n",
        "        b1 = self.param('b1', nn.initializers.zeros, (self.d_model * 4,), DTYPE)\n",
        "\n",
        "        h = nn.gelu(jnp.dot(q, w1) + b1)\n",
        "\n",
        "        w2 = self.param('w2', nn.initializers.glorot_uniform(), (self.d_model * 4, self.d_model), DTYPE)\n",
        "        b2 = self.param('b2', nn.initializers.zeros, (self.d_model,), DTYPE)\n",
        "        output = jnp.dot(h, w2) + b2\n",
        "\n",
        "        return output\n",
        "\n",
        "@jit\n",
        "def llm_inference(params: Any, inputs: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"Performs one step of LLM inference on the TPU.\"\"\"\n",
        "    model = TinyLLM()\n",
        "    raw_output = model.apply({'params': params}, inputs)\n",
        "\n",
        "    tool_score = jnp.mean(raw_output) * 10.0\n",
        "\n",
        "    return tool_score, raw_output\n",
        "\n",
        "# --- 2. Agentic Tool (Run on Host CPU) ---\n",
        "\n",
        "def execute_external_api_call(city: str) -> str:\n",
        "    \"\"\"A tool to simulate fetching real-time weather data (runs on host CPU).\"\"\"\n",
        "    logging.info(f\"Tool executing on CPU: Fetching data for {city}...\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    if \"london\" in city.lower():\n",
        "        result_data = {\"city\": \"London\", \"temp_c\": 12, \"condition\": \"Cloudy\", \"source\": \"API\"}\n",
        "    else:\n",
        "        result_data = {\"error\": \"City data not found\", \"source\": \"API\"}\n",
        "\n",
        "    return json.dumps(result_data)\n",
        "\n",
        "# --- 3. Agent Orchestration ---\n",
        "\n",
        "def run_agent(prompt: str, params: Any) -> Tuple[str, jnp.ndarray]:\n",
        "    input_tensor = jnp.ones((BATCH_SIZE, SEQ_LEN, D_MODEL), dtype=DTYPE)\n",
        "\n",
        "    logging.info(\"\\n[1] Running initial LLM reasoning on TPU...\")\n",
        "    tool_score, _ = llm_inference(params, input_tensor)\n",
        "    tool_score_val = tool_score.item()\n",
        "\n",
        "    # MODIFIED THRESHOLD: Ensures tool use for the demo\n",
        "    if tool_score_val > -2.0:\n",
        "        tool_name = \"execute_external_api_call\"\n",
        "        tool_args = {\"city\": \"London\"}\n",
        "\n",
        "        logging.info(f\"\\n[2] LLM decision (TPU Score {tool_score_val:.4f}): USE TOOL: {tool_name}\")\n",
        "\n",
        "        tool_result_json = execute_external_api_call(tool_args[\"city\"])\n",
        "\n",
        "        logging.info(f\"\\n[3] Tool Result received. Rerunning LLM on TPU to generate final answer...\")\n",
        "\n",
        "        context_tensor = jnp.ones((BATCH_SIZE, SEQ_LEN, D_MODEL), dtype=DTYPE) * 0.2\n",
        "        new_input_tensor = input_tensor + context_tensor\n",
        "        final_score, _ = llm_inference(params, new_input_tensor)\n",
        "\n",
        "        output_string = (f\"Final Answer (TPU-Enhanced):\\n\"\n",
        "                         f\"Prompt: '{prompt}'\\n\"\n",
        "                         f\"Tool Output: {tool_result_json}\\n\"\n",
        "                         f\"Model Synthesis Score: {final_score.item():.4f}\\n\"\n",
        "                         f\"Conclusion: The agent used the weather tool and synthesized the final result based on the data.\")\n",
        "        return output_string, final_score\n",
        "\n",
        "    else:\n",
        "        logging.info(f\"\\n[2] LLM decision (TPU Score {tool_score_val:.4f}): NO TOOL NEEDED.\")\n",
        "        output_string = (f\"Final Answer (Direct):\\n\"\n",
        "                         f\"Prompt: '{prompt}'\\n\"\n",
        "                         f\"Model Synthesis Score: {tool_score_val:.4f}\\n\"\n",
        "                         f\"Conclusion: The agent provided a direct answer without external tool use.\")\n",
        "        return output_string, tool_score\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- STEP 0: Log Resources ---\n",
        "    log_tpu_details()\n",
        "\n",
        "    # Initialize the LLM parameters\n",
        "    key = random.PRNGKey(42)\n",
        "    init_rng, params_rng = random.split(key)\n",
        "\n",
        "    model = TinyLLM(d_model=D_MODEL)\n",
        "    params = model.init({'params': params_rng},\n",
        "                        jnp.ones((BATCH_SIZE, SEQ_LEN, D_MODEL), dtype=DTYPE))['params']\n",
        "\n",
        "    # Clean up previous trace directory and create a new one\n",
        "    if os.path.exists(TRACE_OUTPUT_DIR):\n",
        "        shutil.rmtree(TRACE_OUTPUT_DIR)\n",
        "    os.makedirs(TRACE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ¤– Agentic AI Demo: TPU-Accelerated Loop\")\n",
        "    print(\"======================================================================\")\n",
        "\n",
        "    # --- STEP 4: Start JAX Profiler (Non-blocking) ---\n",
        "    print(f\"INFO: Starting JAX Profiler to capture trace data...\")\n",
        "    jax.profiler.start_trace(TRACE_OUTPUT_DIR)\n",
        "\n",
        "    # Run the agent demo\n",
        "    final_output_string, final_score_tensor = run_agent(\"I need the current weather in London to decide on my outfit.\", params)\n",
        "\n",
        "    # Block until the final computation is complete\n",
        "    final_score_tensor.block_until_ready()\n",
        "\n",
        "    # --- STEP 5: Stop JAX Profiler and Finalize ---\n",
        "    jax.profiler.stop_trace()\n",
        "    print(f\"INFO: JAX Profiler trace saved successfully to directory: {TRACE_OUTPUT_DIR}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AGENT DEMO RESULT SUMMARY\")\n",
        "    print(\"======================================================================\")\n",
        "    print(final_output_string)\n",
        "    print(\"======================================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, random\n",
        "from flax import linen as nn\n",
        "from jax.sharding import NamedSharding, PartitionSpec as P\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "from typing import Any, Tuple\n",
        "\n",
        "# --- Configuration ---\n",
        "DTYPE = jnp.bfloat16\n",
        "D_MODEL = 512\n",
        "SEQ_LEN = 64\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- ðŸ–¥ï¸ TPU Resource Logging ---\n",
        "def log_tpu_details():\n",
        "    \"\"\"Checks and logs the details of the available JAX devices (TPUs), including specific type info.\"\"\"\n",
        "    devices = jax.devices()\n",
        "    if not devices:\n",
        "        logging.error(\"No JAX devices found. Ensure the TPU is configured correctly.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"--- ðŸ–¥ï¸ TPU RESOURCE DETAILS ---\")\n",
        "    first_device = devices[0]\n",
        "    logging.info(f\"Total JAX Devices (Units/Cores): {len(devices)}\")\n",
        "    logging.info(f\"Device Kind: {first_device.device_kind}\")\n",
        "    logging.info(f\"Full Device Description: {repr(first_device)}\")\n",
        "    logging.info(\"-\" * 35)\n",
        "\n",
        "# --- 1. TPU-Accelerated LLM Core (using JAX/Flax) ---\n",
        "\n",
        "class TinyLLM(nn.Module):\n",
        "    d_model: int = D_MODEL\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        w_q = self.param('w_q', nn.initializers.glorot_uniform(), (self.d_model, self.d_model), DTYPE)\n",
        "        q = jnp.dot(x, w_q)\n",
        "        w1 = self.param('w1', nn.initializers.glorot_uniform(), (self.d_model, self.d_model * 4), DTYPE)\n",
        "        b1 = self.param('b1', nn.initializers.zeros, (self.d_model * 4,), DTYPE)\n",
        "        h = nn.gelu(jnp.dot(q, w1) + b1)\n",
        "        w2 = self.param('w2', nn.initializers.glorot_uniform(), (self.d_model * 4, self.d_model), DTYPE)\n",
        "        b2 = self.param('b2', nn.initializers.zeros, (self.d_model,), DTYPE)\n",
        "        output = jnp.dot(h, w2) + b2\n",
        "        return output\n",
        "\n",
        "@jit\n",
        "def llm_inference(params: Any, inputs: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    model = TinyLLM()\n",
        "    raw_output = model.apply({'params': params}, inputs)\n",
        "    tool_score = jnp.mean(raw_output) * 10.0\n",
        "    return tool_score, raw_output\n",
        "\n",
        "# --- 2. Agentic Tool (Run on Host CPU) ---\n",
        "\n",
        "def execute_external_api_call(city: str) -> str:\n",
        "    logging.info(f\"Tool executing on CPU: Fetching data for {city}...\")\n",
        "    time.sleep(0.5)\n",
        "    if \"london\" in city.lower():\n",
        "        result_data = {\"city\": \"London\", \"temp_c\": 12, \"condition\": \"Cloudy\", \"source\": \"API\"}\n",
        "    else:\n",
        "        result_data = {\"error\": \"City data not found\", \"source\": \"API\"}\n",
        "    return json.dumps(result_data)\n",
        "\n",
        "# --- 3. Agent Orchestration ---\n",
        "\n",
        "def run_agent(prompt: str, params: Any) -> str:\n",
        "    input_tensor = jnp.ones((BATCH_SIZE, SEQ_LEN, D_MODEL), dtype=DTYPE)\n",
        "\n",
        "    logging.info(\"\\n[A1] Running initial LLM reasoning on TPU...\")\n",
        "    tool_score, _ = llm_inference(params, input_tensor)\n",
        "    tool_score_val = tool_score.item()\n",
        "\n",
        "    if tool_score_val > -2.0:\n",
        "        tool_name = \"execute_external_api_call\"\n",
        "        tool_args = {\"city\": \"London\"}\n",
        "\n",
        "        logging.info(f\"\\n[A2] LLM decision (TPU Score {tool_score_val:.4f}): USE TOOL: {tool_name}\")\n",
        "\n",
        "        tool_result_json = execute_external_api_call(tool_args[\"city\"])\n",
        "\n",
        "        logging.info(f\"\\n[A3] Tool Result received. Rerunning LLM on TPU to generate final answer...\")\n",
        "\n",
        "        context_tensor = jnp.ones((BATCH_SIZE, SEQ_LEN, D_MODEL), dtype=DTYPE) * 0.2\n",
        "        new_input_tensor = input_tensor + context_tensor\n",
        "        final_score, _ = llm_inference(params, new_input_tensor)\n",
        "\n",
        "        return (f\"Final Answer (TPU-Enhanced):\\n\"\n",
        "                f\"Prompt: '{prompt}'\\n\"\n",
        "                f\"Tool Output: {tool_result_json}\\n\"\n",
        "                f\"Model Synthesis Score: {final_score.item():.4f}\\n\"\n",
        "                f\"Conclusion: The agent used the weather tool and synthesized the final result based on the data.\")\n",
        "\n",
        "    else:\n",
        "        logging.info(f\"\\n[A2] LLM decision (TPU Score {tool_score_val:.4f}): NO TOOL NEEDED.\")\n",
        "        return (f\"Final Answer (Direct):\\n\"\n",
        "                f\"Prompt: '{prompt}'\\n\"\n",
        "                f\"Model Synthesis Score: {tool_score_val:.4f}\\n\"\n",
        "                f\"Conclusion: The agent provided a direct answer without external tool use.\")\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- STEP 0: Log Resources ---\n",
        "    log_tpu_details()\n",
        "\n",
        "    # --- JAX SHARDING INTEGRATION ---\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"âœ¨ JAX SHARDING DEMONSTRATION (Single Device Setup)\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    device_count = jax.device_count()\n",
        "    print(f\"INFO: Found {device_count} devices for sharding setup.\")\n",
        "\n",
        "    # --- 1. Corrected Mesh Creation ---\n",
        "    print(\"--- 1. Creating the Device Mesh (1x1 Logical Grid) ---\")\n",
        "    mesh = jax.make_mesh((device_count,), ('X',))\n",
        "    print(f\"Mesh created with axes: {mesh.axis_names}\")\n",
        "    print(f\"Number of physical devices in mesh: {mesh.size}\")\n",
        "\n",
        "    # --- 2. Create the Array ---\n",
        "    key = jax.random.key(0)\n",
        "    x = jax.random.normal(key, (256, 256), dtype=jnp.float32)\n",
        "    print(f\"\\n--- 2. Initial Array Created: {x.shape} ({x.dtype}) ---\")\n",
        "\n",
        "    # --- 3. Define the Sharding Specification ---\n",
        "    sharding_spec = P('X', None)\n",
        "    sharding_rule = NamedSharding(mesh, sharding_spec)\n",
        "    print(f\"--- 3. Sharding Rule Defined: Partitioning {sharding_spec} ---\")\n",
        "\n",
        "    # --- 4. Distribute the Array ---\n",
        "    y = jax.device_put(x, sharding_rule)\n",
        "    print(f\"\\n--- 4. Array Distributed ('y') ---\")\n",
        "    print(f\"Resulting array shape: {y.shape}\")\n",
        "    print(f\"Partition size per device: {y.addressable_shards[0].data.shape}\")\n",
        "\n",
        "    # --- 5. Visualize the Sharding ---\n",
        "    print(\"\\n--- 5. Visualizing Array Sharding ---\")\n",
        "    jax.debug.visualize_array_sharding(y)\n",
        "\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"ðŸ¤– STARTING TPU AGENT DEMO EXECUTION\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    # --- TPU AGENT EXECUTION ---\n",
        "\n",
        "    # Initialize the LLM parameters\n",
        "    key = random.PRNGKey(42)\n",
        "    init_rng, params_rng = random.split(key)\n",
        "\n",
        "    model = TinyLLM(d_model=D_MODEL)\n",
        "    params = model.init({'params': params_rng},\n",
        "                        jnp.ones((BATCH_SIZE, SEQ_LEN, D_MODEL), dtype=DTYPE))['params']\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ¤– Agentic AI Demo: TPU-Accelerated Loop\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run the agent demo\n",
        "    result = run_agent(\"I need the current weather in London to decide on my outfit.\", params)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AGENT DEMO RESULT SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(result)\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "TYjkoAGMq4t1",
        "outputId": "0e001174-61a6-4b1c-f367-1d06f4b46b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "âœ¨ JAX SHARDING DEMONSTRATION (Single Device Setup)\n",
            "######################################################################\n",
            "INFO: Found 1 devices for sharding setup.\n",
            "--- 1. Creating the Device Mesh (1x1 Logical Grid) ---\n",
            "Mesh created with axes: ('X',)\n",
            "Number of physical devices in mesh: 1\n",
            "\n",
            "--- 2. Initial Array Created: (256, 256) (float32) ---\n",
            "--- 3. Sharding Rule Defined: Partitioning PartitionSpec('X', None) ---\n",
            "\n",
            "--- 4. Array Distributed ('y') ---\n",
            "Resulting array shape: (256, 256)\n",
            "Partition size per device: (256, 256)\n",
            "\n",
            "--- 5. Visualizing Array Sharding ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">          TPU 0          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "ðŸ¤– STARTING TPU AGENT DEMO EXECUTION\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "ðŸ¤– Agentic AI Demo: TPU-Accelerated Loop\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "AGENT DEMO RESULT SUMMARY\n",
            "======================================================================\n",
            "Final Answer (TPU-Enhanced):\n",
            "Prompt: 'I need the current weather in London to decide on my outfit.'\n",
            "Tool Output: {\"city\": \"London\", \"temp_c\": 12, \"condition\": \"Cloudy\", \"source\": \"API\"}\n",
            "Model Synthesis Score: -1.6875\n",
            "Conclusion: The agent used the weather tool and synthesized the final result based on the data.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from google import genai\n",
        "from typing import Any, Tuple\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from jax.sharding import NamedSharding, PartitionSpec as P\n",
        "from google.colab import userdata\n",
        "from google.genai import types\n",
        "\n",
        "# --- WARNING: This code runs your client initialization first ---\n",
        "\n",
        "# --- GEMINI CLIENT SETUP ---\n",
        "try:\n",
        "    # This block requires 'google.colab.userdata' to be available.\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "    if not GOOGLE_API_KEY:\n",
        "        raise ValueError(\"Error: 'GEMINI' API key not found in Colab Secrets.\")\n",
        "\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    # Using the stable model for chat\n",
        "    GEMINI_MODEL_ID = 'gemini-2.5-pro'\n",
        "\n",
        "    print(f\"Gemini client successfully configured for model: {GEMINI_MODEL_ID}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Configuration Error: {e}\")\n",
        "    raise SystemExit(\"Exiting script due to configuration failure.\")\n",
        "\n",
        "# --- Agent Configuration ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "tool_function_map = {\n",
        "    \"execute_external_api_call\": lambda city: execute_external_api_call(city)\n",
        "}\n",
        "\n",
        "# --- ðŸ–¥ï¸ TPU Resource Logging ---\n",
        "\n",
        "def log_tpu_details():\n",
        "    \"\"\"Checks and logs the details of the available JAX devices (TPUs), including specific type info.\"\"\"\n",
        "    devices = jax.devices()\n",
        "    if not devices:\n",
        "        logging.error(\"No JAX devices found.\")\n",
        "        return\n",
        "    logging.info(\"--- ðŸ–¥ï¸ TPU RESOURCE DETAILS ---\")\n",
        "    first_device = devices[0]\n",
        "    logging.info(f\"Total JAX Devices (Units/Cores): {len(devices)}\")\n",
        "    logging.info(f\"Device Kind: {first_device.device_kind}\")\n",
        "    logging.info(f\"Full Device Description: {repr(first_device)}\")\n",
        "    logging.info(\"-\" * 35)\n",
        "\n",
        "\n",
        "# --- 1. Agentic Tool (Runs on Host CPU) ---\n",
        "\n",
        "def execute_external_api_call(city: str) -> str:\n",
        "    \"\"\"A tool to simulate fetching real-time weather data. Runs on the Host CPU.\"\"\"\n",
        "    logging.info(f\"Tool executing on CPU: Fetching data for {city}...\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    if \"london\" in city.lower():\n",
        "        result_data = {\"city\": \"London\", \"temp_c\": 12, \"condition\": \"Cloudy\", \"source\": \"API\"}\n",
        "    else:\n",
        "        result_data = {\"error\": \"City data not found\", \"source\": \"API\"}\n",
        "\n",
        "    return json.dumps(result_data)\n",
        "\n",
        "\n",
        "# --- 2. Agent Orchestration (Using Gemini API with Chat) ---\n",
        "\n",
        "def run_gemini_agent(prompt: str) -> str:\n",
        "\n",
        "    tools_config = [execute_external_api_call]\n",
        "\n",
        "    # Use AUTO mode, as Chat sessions handle the context needed for this decision well\n",
        "    tool_config = types.ToolConfig(\n",
        "        function_calling_config=types.FunctionCallingConfig(mode=\"AUTO\")\n",
        "    )\n",
        "\n",
        "    # 1. Create a chat session with the tools attached\n",
        "    chat = client.chats.create(\n",
        "        model=GEMINI_MODEL_ID,\n",
        "        config=types.GenerateContentConfig(\n",
        "            tools=tools_config,\n",
        "            tool_config=tool_config\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 2. First Message: Ask Gemini for a decision (Reasoning)\n",
        "    logging.info(\"\\n[A1] Sending prompt to Gemini chat for initial reasoning and tool decision...\")\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "    # --- Check for Function Calls ---\n",
        "    function_calls = response.candidates[0].function_calls if response.candidates and hasattr(response.candidates[0], 'function_calls') else None\n",
        "\n",
        "    # 3. Check for Tool Call\n",
        "    if function_calls:\n",
        "        # Use the first function call suggested\n",
        "        fc = function_calls[0]\n",
        "        tool_name = fc.name\n",
        "        tool_args = dict(fc.args)\n",
        "\n",
        "        logging.info(f\"\\n[A2] Gemini decided to use Tool: {tool_name} with args: {tool_args}\")\n",
        "\n",
        "        # 4. Execute the tool locally (on the Host CPU)\n",
        "        if tool_name in tool_function_map:\n",
        "            tool_func = tool_function_map[tool_name]\n",
        "            tool_result = tool_func(**tool_args)\n",
        "\n",
        "            logging.info(f\"Tool Result (CPU): {tool_result}\")\n",
        "\n",
        "            # 5. Second Message: Send result back to Gemini for synthesis (Chat handles context)\n",
        "            logging.info(\"\\n[A3] Sending Tool Result back to Gemini chat for final synthesis...\")\n",
        "\n",
        "            function_response_part = genai.types.Part.from_function_response(\n",
        "                name=tool_name,\n",
        "                response={\"result\": tool_result}\n",
        "            )\n",
        "\n",
        "            second_response = chat.send_message([function_response_part])\n",
        "\n",
        "            # Ensure text extraction\n",
        "            final_conclusion = second_response.text if second_response.text else \"Model Synthesis Successful, Text Unavailable (SDK issue).\"\n",
        "\n",
        "            return (f\"Final Answer (Gemini-Enhanced via Chat):\\n\"\n",
        "                    f\"Model: {GEMINI_MODEL_ID} (Chat Function Calling)\\\\n\"\\\n",
        "                    f\"Prompt: '{prompt}'\\\\n\"\\\n",
        "                    f\"Tool Output: {tool_result}\\\\n\"\\\n",
        "                    f\"Conclusion: {final_conclusion}\")\n",
        "        else:\n",
        "            return f\"Error: Agent tried to call unknown tool: {tool_name}\"\n",
        "\n",
        "    else:\n",
        "        # 6. Direct Answer (Model decided to answer directly)\n",
        "        logging.info(\"\\n[A2] Gemini provided a direct answer without tool use.\")\n",
        "        direct_conclusion = response.text if response.text else \"Model provided a structured response that lacks text.\"\n",
        "\n",
        "        return (f\"Final Answer (Direct Answer via Chat):\\n\"\n",
        "                f\"Model: {GEMINI_MODEL_ID}\\\\n\"\\\n",
        "                f\"Prompt: '{prompt}'\\\\n\"\\\n",
        "                f\"Conclusion: {direct_conclusion}\")\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- Log Resources ---\n",
        "    log_tpu_details()\n",
        "\n",
        "    # --- JAX SHARDING DEMONSTRATION (Your Code Integrated) ---\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"âœ¨ JAX SHARDING DEMONSTRATION (TPU Array Initialization)\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    device_count = jax.device_count()\n",
        "    print(f\"INFO: Found {device_count} devices for sharding setup.\")\n",
        "\n",
        "    # 1. Mesh Creation (Adapted to available devices)\n",
        "    print(\"--- 1. Creating the Device Mesh (1x1 Logical Grid) ---\")\n",
        "    mesh = jax.make_mesh((device_count,), ('X',))\n",
        "    print(f\"Mesh created with axes: {mesh.axis_names}\")\n",
        "    print(f\"Number of physical devices in mesh: {mesh.size}\")\n",
        "\n",
        "    # 2. Create the Array (Using small array and float32 for simplicity)\n",
        "    key = jax.random.key(0)\n",
        "    x = jax.random.normal(key, (256, 256), dtype=jnp.float32)\n",
        "    print(f\"\\n--- 2. Initial Array Created: {x.shape} ({x.dtype}) ---\")\n",
        "\n",
        "    # 3. Define the Sharding Specification (Partitions across the single 'X' axis)\n",
        "    sharding_spec = P('X', None)\n",
        "    sharding_rule = NamedSharding(mesh, sharding_spec)\n",
        "    print(f\"--- 3. Sharding Rule Defined: Partitioning {sharding_spec} ---\")\n",
        "\n",
        "    # 4. Distribute the Array\n",
        "    y = jax.device_put(x, sharding_rule)\n",
        "    print(f\"\\n--- 4. Array Distributed ('y') ---\")\n",
        "    print(f\"Partition size per device: {y.addressable_shards[0].data.shape}\")\n",
        "\n",
        "    # 5. Visualize the Sharding\n",
        "    print(\"\\n--- 5. Visualizing Array Sharding ---\")\n",
        "    jax.debug.visualize_array_sharding(y)\n",
        "\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"ðŸ¤– STARTING GEMINI API AGENT EXECUTION (CHAT FIXED)\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    # --- GEMINI AGENT EXECUTION ---\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"ðŸ¤– Agentic AI Demo: Gemini Function Calling Loop\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run the agent demo\n",
        "    result = run_gemini_agent(\"I need the current weather in London to decide on my outfit.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AGENT DEMO RESULT SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(result)\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "jXLq0aw9uyou",
        "outputId": "728379f6-0fc4-4764-87ef-212ff4afef0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini client successfully configured for model: gemini-2.5-pro.\n",
            "\n",
            "######################################################################\n",
            "âœ¨ JAX SHARDING DEMONSTRATION (TPU Array Initialization)\n",
            "######################################################################\n",
            "INFO: Found 1 devices for sharding setup.\n",
            "--- 1. Creating the Device Mesh (1x1 Logical Grid) ---\n",
            "Mesh created with axes: ('X',)\n",
            "Number of physical devices in mesh: 1\n",
            "\n",
            "--- 2. Initial Array Created: (256, 256) (float32) ---\n",
            "--- 3. Sharding Rule Defined: Partitioning PartitionSpec('X', None) ---\n",
            "\n",
            "--- 4. Array Distributed ('y') ---\n",
            "Partition size per device: (256, 256)\n",
            "\n",
            "--- 5. Visualizing Array Sharding ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">          TPU 0          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "ðŸ¤– STARTING GEMINI API AGENT EXECUTION (CHAT FIXED)\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "ðŸ¤– Agentic AI Demo: Gemini Function Calling Loop\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "AGENT DEMO RESULT SUMMARY\n",
            "======================================================================\n",
            "Final Answer (Direct Answer via Chat):\n",
            "Model: gemini-2.5-pro\\nPrompt: 'I need the current weather in London to decide on my outfit.'\\nConclusion: The weather in London is currently cloudy with a temperature of 12 degrees Celsius.\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from google import genai\n",
        "from typing import Any, Tuple\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from jax.sharding import NamedSharding, PartitionSpec as P\n",
        "from google.colab import userdata\n",
        "from google.genai import types\n",
        "\n",
        "# --- WARNING: This code runs your client initialization first ---\n",
        "\n",
        "# --- GEMINI CLIENT SETUP ---\n",
        "try:\n",
        "    # This block requires 'google.colab.userdata' to be available.\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "    if not GOOGLE_API_KEY:\n",
        "        raise ValueError(\"Error: 'GEMINI' API key not found in Colab Secrets.\")\n",
        "\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    # Using the stable model for chat\n",
        "    GEMINI_MODEL_ID = 'gemini-2.5-pro'\n",
        "\n",
        "    print(f\"Gemini client successfully configured for model: {GEMINI_MODEL_ID}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Configuration Error: {e}\")\n",
        "    raise SystemExit(\"Exiting script due to configuration failure.\")\n",
        "\n",
        "# --- Agent Configuration ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "tool_function_map = {\n",
        "    \"execute_external_api_call\": lambda city: execute_external_api_call(city)\n",
        "}\n",
        "\n",
        "# --- ðŸ–¥ï¸ TPU Resource Logging ---\n",
        "\n",
        "def log_tpu_details():\n",
        "    \"\"\"Checks and logs the details of the available JAX devices (TPUs), including specific type info.\"\"\"\n",
        "    devices = jax.devices()\n",
        "    if not devices:\n",
        "        logging.error(\"No JAX devices found.\")\n",
        "        return\n",
        "    logging.info(\"--- ðŸ–¥ï¸ TPU RESOURCE DETAILS ---\")\n",
        "    first_device = devices[0]\n",
        "    logging.info(f\"Total JAX Devices (Units/Cores): {len(devices)}\")\n",
        "    logging.info(f\"Device Kind: {first_device.device_kind}\")\n",
        "    logging.info(f\"Full Device Description: {repr(first_device)}\")\n",
        "    logging.info(\"-\" * 35)\n",
        "\n",
        "\n",
        "# --- 1. Agentic Tool (Runs on Host CPU - MODIFIED to return Vostok data for specific demo) ---\n",
        "\n",
        "def execute_external_api_call(city: str) -> str:\n",
        "    \"\"\"A tool to simulate fetching real-time weather data. Runs on the Host CPU.\"\"\"\n",
        "    logging.info(f\"Tool executing on CPU: Fetching data for {city}...\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    # Force a unique, tool-dependent result to prove tool was used for the new prompt\n",
        "    if \"vostok\" in city.lower():\n",
        "        result_data = {\"city\": \"Vostok Station, Antarctica\", \"temp_c\": -60, \"condition\": \"Severe Clear Cold\", \"source\": \"API\"}\n",
        "    else:\n",
        "        # Fallback to original London data\n",
        "        result_data = {\"city\": \"London\", \"temp_c\": 12, \"condition\": \"Cloudy\", \"source\": \"API\"}\n",
        "\n",
        "    return json.dumps(result_data)\n",
        "\n",
        "\n",
        "# --- 2. Agent Orchestration (Using Gemini API with Chat) ---\n",
        "\n",
        "def run_gemini_agent(prompt: str) -> str:\n",
        "\n",
        "    tools_config = [execute_external_api_call]\n",
        "\n",
        "    # Use AUTO mode, as Chat sessions handle the context needed for this decision well\n",
        "    tool_config = types.ToolConfig(\n",
        "        function_calling_config=types.FunctionCallingConfig(mode=\"AUTO\")\n",
        "    )\n",
        "\n",
        "    # 1. Create a chat session with the tools attached\n",
        "    chat = client.chats.create(\n",
        "        model=GEMINI_MODEL_ID,\n",
        "        config=types.GenerateContentConfig(\n",
        "            tools=tools_config,\n",
        "            tool_config=tool_config\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 2. First Message: Ask Gemini for a decision (Reasoning)\n",
        "    logging.info(\"\\n[A1] Sending prompt to Gemini chat for initial reasoning and tool decision...\")\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "    # --- Check for Function Calls ---\n",
        "    function_calls = response.candidates[0].function_calls if response.candidates and hasattr(response.candidates[0], 'function_calls') else None\n",
        "\n",
        "    # 3. Check for Tool Call\n",
        "    if function_calls:\n",
        "        # Use the first function call suggested\n",
        "        fc = function_calls[0]\n",
        "        tool_name = fc.name\n",
        "        tool_args = dict(fc.args)\n",
        "\n",
        "        logging.info(f\"\\n[A2] Gemini decided to use Tool: {tool_name} with args: {tool_args}\")\n",
        "\n",
        "        # 4. Execute the tool locally (on the Host CPU)\n",
        "        if tool_name in tool_function_map:\n",
        "            tool_func = tool_function_map[tool_name]\n",
        "            # Extract city argument, defaulting to a test city if missing\n",
        "            city_arg = tool_args.get(\"city\", \"London\")\n",
        "            tool_result = tool_func(city=city_arg)\n",
        "\n",
        "            logging.info(f\"Tool Result (CPU): {tool_result}\")\n",
        "\n",
        "            # 5. Second Message: Send result back to Gemini for synthesis (Chat handles context)\n",
        "            logging.info(\"\\n[A3] Sending Tool Result back to Gemini chat for final synthesis...\")\n",
        "\n",
        "            function_response_part = genai.types.Part.from_function_response(\n",
        "                name=tool_name,\n",
        "                response={\"result\": tool_result}\n",
        "            )\n",
        "\n",
        "            # Send the tool result back as a part of the chat history\n",
        "            second_response = chat.send_message([function_response_part])\n",
        "\n",
        "            # Ensure text extraction\n",
        "            final_conclusion = second_response.text if second_response.text else \"Model Synthesis Successful, Text Unavailable (SDK issue).\"\n",
        "\n",
        "            return (f\"Final Answer (Gemini-Enhanced via Chat):\\n\"\n",
        "                    f\"Model: {GEMINI_MODEL_ID} (Chat Function Calling)\\\\n\"\\\n",
        "                    f\"Prompt: '{prompt}'\\\\n\"\\\n",
        "                    f\"Tool Output: {tool_result}\\\\n\"\\\n",
        "                    f\"Conclusion: {final_conclusion}\")\n",
        "        else:\n",
        "            return f\"Error: Agent tried to call unknown tool: {tool_name}\"\n",
        "\n",
        "    else:\n",
        "        # 6. Direct Answer (Model decided to answer directly)\n",
        "        logging.info(\"\\n[A2] Gemini provided a direct answer without tool use.\")\n",
        "        direct_conclusion = response.text if response.text else \"Model provided a structured response that lacks text.\"\n",
        "\n",
        "        return (f\"Final Answer (Direct Answer via Chat):\\n\"\n",
        "                f\"Model: {GEMINI_MODEL_ID}\\\\n\"\\\n",
        "                f\"Prompt: '{prompt}'\\\\n\"\\\n",
        "                f\"Conclusion: {direct_conclusion}\")\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- Log Resources ---\n",
        "    log_tpu_details()\n",
        "\n",
        "    # --- JAX SHARDING DEMONSTRATION (Your Code Integrated) ---\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"âœ¨ JAX SHARDING DEMONSTRATION (TPU Array Initialization)\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    device_count = jax.device_count()\n",
        "    print(f\"INFO: Found {device_count} devices for sharding setup.\")\n",
        "\n",
        "    # 1. Mesh Creation (Adapted to available devices)\n",
        "    print(\"--- 1. Creating the Device Mesh (1x1 Logical Grid) ---\")\n",
        "    mesh = jax.make_mesh((device_count,), ('X',))\n",
        "    print(f\"Mesh created with axes: {mesh.axis_names}\")\n",
        "    print(f\"Number of physical devices in mesh: {mesh.size}\")\n",
        "\n",
        "    # 2. Create the Array (Using small array and float32 for simplicity)\n",
        "    key = jax.random.key(0)\n",
        "    x = jax.random.normal(key, (256, 256), dtype=jnp.float32)\n",
        "    print(f\"\\n--- 2. Initial Array Created: {x.shape} ({x.dtype}) ---\")\n",
        "\n",
        "    # 3. Define the Sharding Specification (Partitions across the single 'X' axis)\n",
        "    sharding_spec = P('X', None)\n",
        "    sharding_rule = NamedSharding(mesh, sharding_spec)\n",
        "    print(f\"--- 3. Sharding Rule Defined: Partitioning {sharding_spec} ---\")\n",
        "\n",
        "    # 4. Distribute the Array\n",
        "    y = jax.device_put(x, sharding_rule)\n",
        "    print(f\"\\n--- 4. Array Distributed ('y') ---\")\n",
        "    print(f\"Partition size per device: {y.addressable_shards[0].data.shape}\")\n",
        "\n",
        "    # 5. Visualize the Sharding\n",
        "    print(\"\\n--- 5. Visualizing Array Sharding ---\")\n",
        "    jax.debug.visualize_array_sharding(y)\n",
        "\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"ðŸ¤– STARTING GEMINI API AGENT EXECUTION (CHAT FIXED, PROMPT MODIFIED)\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    # --- GEMINI AGENT EXECUTION ---\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"ðŸ¤– Agentic AI Demo: Gemini Function Calling Loop\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run the agent demo with the specific prompt\n",
        "    # NEW PROMPT: Designed to force the model to call the tool because it's highly specific\n",
        "    result = run_gemini_agent(\"What is the current temperature in Vostok Station, Antarctica? I need this exact temperature reading.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AGENT DEMO RESULT SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(result)\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "txFY-Qv1ALdw",
        "outputId": "bb7980bb-a0b1-4a74-a589-bafc0cabe21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini client successfully configured for model: gemini-2.5-pro.\n",
            "\n",
            "######################################################################\n",
            "âœ¨ JAX SHARDING DEMONSTRATION (TPU Array Initialization)\n",
            "######################################################################\n",
            "INFO: Found 1 devices for sharding setup.\n",
            "--- 1. Creating the Device Mesh (1x1 Logical Grid) ---\n",
            "Mesh created with axes: ('X',)\n",
            "Number of physical devices in mesh: 1\n",
            "\n",
            "--- 2. Initial Array Created: (256, 256) (float32) ---\n",
            "--- 3. Sharding Rule Defined: Partitioning PartitionSpec('X', None) ---\n",
            "\n",
            "--- 4. Array Distributed ('y') ---\n",
            "Partition size per device: (256, 256)\n",
            "\n",
            "--- 5. Visualizing Array Sharding ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">          TPU 0          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "ðŸ¤– STARTING GEMINI API AGENT EXECUTION (CHAT FIXED, PROMPT MODIFIED)\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "ðŸ¤– Agentic AI Demo: Gemini Function Calling Loop\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "AGENT DEMO RESULT SUMMARY\n",
            "======================================================================\n",
            "Final Answer (Direct Answer via Chat):\n",
            "Model: gemini-2.5-pro\\nPrompt: 'What is the current temperature in Vostok Station, Antarctica? I need this exact temperature reading.'\\nConclusion: I am sorry, but I cannot provide the exact temperature for Vostok Station, Antarctica, as my weather data is not that precise. It is a remote research station, and I can only provide weather information for larger cities.\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/pytorch_tpu_starter_mistral_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch"
      ],
      "metadata": {
        "id": "6KibpHHXbTJP"
      },
      "id": "6KibpHHXbTJP"
    },
    {
      "cell_type": "markdown",
      "id": "1fd29e81",
      "metadata": {
        "papermill": {
          "duration": 0.024857,
          "end_time": "2022-06-07T10:50:17.130071",
          "exception": false,
          "start_time": "2022-06-07T10:50:17.105214",
          "status": "completed"
        },
        "tags": [],
        "id": "1fd29e81"
      },
      "source": [
        "\n",
        "# Introduction\n",
        "PyTorch XLA is a PyTorch library for XLA support. XLA (Accelerated Linear Algebra) is a domain-specific compiler that was originally meant for compiling and accelerating TensorFlow models. However, other packages, like JAX and now PyTorch XLA can compile program with XLA to accelerate code. TPUs can be programmed with XLA programs and PyTorch XLA provides this interface with TPUs by compiling our PyTorch code as XLA programs to run on TPU devices.\n",
        "\n",
        "In this kernel, I provide an in-depth look into how you can use PyTorch XLA to train a PyTorch model on the TPU for the Feedback Effectiveness Prize competition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c88356",
      "metadata": {
        "papermill": {
          "duration": 0.023205,
          "end_time": "2022-06-07T10:50:17.178816",
          "exception": false,
          "start_time": "2022-06-07T10:50:17.155611",
          "status": "completed"
        },
        "tags": [],
        "id": "42c88356"
      },
      "source": [
        "## Installs & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "nNspNTXERrn1"
      },
      "id": "nNspNTXERrn1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b376fc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:51:23.298724Z",
          "iopub.status.busy": "2022-06-07T10:51:23.294026Z",
          "iopub.status.idle": "2022-06-07T10:51:55.840875Z",
          "shell.execute_reply": "2022-06-07T10:51:55.840282Z",
          "shell.execute_reply.started": "2022-06-07T08:32:26.52533Z"
        },
        "papermill": {
          "duration": 32.584908,
          "end_time": "2022-06-07T10:51:55.841034",
          "exception": false,
          "start_time": "2022-06-07T10:51:23.256126",
          "status": "completed"
        },
        "tags": [],
        "id": "b9b376fc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install --upgrade huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import warnings, transformers, logging, torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch.optim.adamw as AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.disable(logging.WARNING)\n",
        "import datasets\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from sklearn.metrics import log_loss\n",
        "from pathlib import Path\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# PyTorch XLA specific imports\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "bfJSgZ0mMXOf"
      },
      "id": "bfJSgZ0mMXOf",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ae479d24",
      "metadata": {
        "papermill": {
          "duration": 0.046322,
          "end_time": "2022-06-07T10:52:04.869045",
          "exception": false,
          "start_time": "2022-06-07T10:52:04.822723",
          "status": "completed"
        },
        "tags": [],
        "id": "ae479d24"
      },
      "source": [
        "## Dataset creation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/TPU/input/"
      ],
      "metadata": {
        "id": "SS8tGnge_rID"
      },
      "id": "SS8tGnge_rID",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "955c74f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:05.070022Z",
          "iopub.status.busy": "2022-06-07T10:52:05.069406Z",
          "iopub.status.idle": "2022-06-07T10:52:05.549891Z",
          "shell.execute_reply": "2022-06-07T10:52:05.549192Z",
          "shell.execute_reply.started": "2022-06-07T08:33:10.14231Z"
        },
        "papermill": {
          "duration": 0.530692,
          "end_time": "2022-06-07T10:52:05.550038",
          "exception": false,
          "start_time": "2022-06-07T10:52:05.019346",
          "status": "completed"
        },
        "tags": [],
        "id": "955c74f6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/TPU/input/train.csv')\n",
        "test_df = pd.read_csv('/content/gdrive/MyDrive/TPU/input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "23297086",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:05.651856Z",
          "iopub.status.busy": "2022-06-07T10:52:05.650851Z",
          "iopub.status.idle": "2022-06-07T10:52:10.542641Z",
          "shell.execute_reply": "2022-06-07T10:52:10.542121Z",
          "shell.execute_reply.started": "2022-06-07T08:33:10.480063Z"
        },
        "papermill": {
          "duration": 4.94532,
          "end_time": "2022-06-07T10:52:10.542789",
          "exception": false,
          "start_time": "2022-06-07T10:52:05.597469",
          "status": "completed"
        },
        "tags": [],
        "id": "23297086"
      },
      "outputs": [],
      "source": [
        "model_id = '/content/gdrive/MyDrive/TPU/Mistral-7B-v0.1'\n",
        "#tokz = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large', model_max_length=512)\n",
        "tokz = AutoTokenizer.from_pretrained(model_id, model_max_length=512)\n",
        "\n",
        "# Check if sep_token is None and use eos_token or a space as fallback\n",
        "sep = tokz.sep_token if tokz.sep_token is not None else (tokz.eos_token if tokz.eos_token is not None else \" \")\n",
        "\n",
        "df['inputs'] = df.discourse_type + sep +df.discourse_text\n",
        "new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}\n",
        "df = df.replace(new_label)\n",
        "df = df.rename(columns = {\"discourse_effectiveness\": \"labels\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e54db31",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:10.747733Z",
          "iopub.status.busy": "2022-06-07T10:52:10.746757Z",
          "iopub.status.idle": "2022-06-07T10:52:12.353979Z",
          "shell.execute_reply": "2022-06-07T10:52:12.353436Z",
          "shell.execute_reply.started": "2022-06-07T08:33:16.058144Z"
        },
        "papermill": {
          "duration": 1.658898,
          "end_time": "2022-06-07T10:52:12.354113",
          "exception": false,
          "start_time": "2022-06-07T10:52:10.695215",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e54db31",
        "outputId": "b34f13bd-03d0-4c28-978b-3d7f91138659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7181, 29584)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "essay_ids = df.essay_id.unique()\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(essay_ids)\n",
        "essay_ids[:5]\n",
        "val_prop = 0.2\n",
        "val_sz = int(len(essay_ids)*val_prop)\n",
        "val_essay_ids = essay_ids[:val_sz]\n",
        "is_val = np.isin(df.essay_id, val_essay_ids)\n",
        "idxs = np.arange(len(df))\n",
        "val_idxs = idxs[ is_val]\n",
        "trn_idxs = idxs[~is_val]\n",
        "len(val_idxs),len(trn_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8cf1ac64",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:12.455396Z",
          "iopub.status.busy": "2022-06-07T10:52:12.454493Z",
          "iopub.status.idle": "2022-06-07T10:52:12.460005Z",
          "shell.execute_reply": "2022-06-07T10:52:12.460595Z",
          "shell.execute_reply.started": "2022-06-07T08:33:17.69128Z"
        },
        "papermill": {
          "duration": 0.057865,
          "end_time": "2022-06-07T10:52:12.460758",
          "exception": false,
          "start_time": "2022-06-07T10:52:12.402893",
          "status": "completed"
        },
        "tags": [],
        "id": "8cf1ac64"
      },
      "outputs": [],
      "source": [
        "def get_dds(df, train=True):\n",
        "    ds = Dataset.from_pandas(df)\n",
        "    to_remove = ['discourse_text','discourse_type','inputs','discourse_id','essay_id']\n",
        "    tok_ds = ds.map(tok_func, batched=True, remove_columns=to_remove)\n",
        "    if train:\n",
        "        return DatasetDict({\"train\":tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})\n",
        "    else:\n",
        "        return tok_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a2447b",
      "metadata": {
        "papermill": {
          "duration": 0.049668,
          "end_time": "2022-06-07T10:52:23.501089",
          "exception": false,
          "start_time": "2022-06-07T10:52:23.451421",
          "status": "completed"
        },
        "tags": [],
        "id": "e2a2447b"
      },
      "source": [
        "## Training code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define global variables\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-2\n",
        "num_epochs = 10\n",
        "num_warmup_epochs = 10\n",
        "BATCH_SIZE = 2 # Reduced batch size\n",
        "EPOCHS = 1\n",
        "NUM_WORKERS = 1\n",
        "OPTIM = \"AdamW\"\n",
        "LR = 1e-5\n",
        "WD = 0.01\n",
        "WARMUP_PCT = 0.1\n",
        "\n",
        "import os\n",
        "os.environ['XLA_FLAGS'] = '--xla_debug_info'\n",
        "\n",
        "# Dataset creation\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/TPU/input/train.csv')\n",
        "test_df = pd.read_csv('/content/gdrive/MyDrive/TPU/input/test.csv')\n",
        "\n",
        "model_id = '/content/gdrive/MyDrive/TPU/Mistral-7B-v0.1'\n",
        "tokz = AutoTokenizer.from_pretrained(model_id, model_max_length=512)\n",
        "sep = tokz.sep_token if tokz.sep_token is not None else (tokz.eos_token if tokz.eos_token is not None else ' ')\n",
        "df['inputs'] = df.discourse_type + sep + df.discourse_text\n",
        "new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}\n",
        "df = df.replace(new_label)\n",
        "df = df.rename(columns={\"discourse_effectiveness\": \"labels\"})\n",
        "essay_ids = df.essay_id.unique()\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(essay_ids)\n",
        "val_prop = 0.2\n",
        "val_sz = int(len(essay_ids) * val_prop)\n",
        "val_essay_ids = essay_ids[:val_sz]\n",
        "is_val = np.isin(df.essay_id, val_essay_ids)\n",
        "idxs = np.arange(len(df))\n",
        "val_idxs = idxs[is_val]\n",
        "trn_idxs = idxs[~is_val]\n",
        "\n",
        "# Training code\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.save_pretrained('tokenizer')\n",
        "\n",
        "def tok_func(x):\n",
        "    return tokenizer(x[\"inputs\"], padding='max_length', truncation=True)\n",
        "\n",
        "def get_dds(df, train=True):\n",
        "    ds = Dataset.from_pandas(df)\n",
        "    to_remove = ['discourse_text', 'discourse_type', 'inputs', 'discourse_id']\n",
        "    tok_ds = ds.map(tok_func, batched=True, remove_columns=to_remove)\n",
        "    if train:\n",
        "        return DatasetDict({\"train\": tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})\n",
        "    else:\n",
        "        return tok_ds\n",
        "\n",
        "dds = get_dds(df)\n",
        "dds.save_to_disk('dds')\n",
        "\n",
        "# Let's now define our training and validation functions.\n",
        "def train_loop_fn(data_loader, loss_fn, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    for bi, d in enumerate(data_loader):\n",
        "        for k, v in d.items():\n",
        "            d[k] = v.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**d)\n",
        "        loss = loss_fn(outputs['logits'], d['labels'])\n",
        "\n",
        "        if bi % 50 == 0:\n",
        "            loss_reduced = xm.mesh_reduce('loss_reduce', loss, lambda x: sum(x) / len(x))\n",
        "            xm.master_print(f'bi={bi}, loss={loss_reduced}')\n",
        "\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "    model.eval()\n",
        "\n",
        "def eval_loop_fn(data_loader, loss_fn, model, device):\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    for bi, d in enumerate(data_loader):\n",
        "        for k, v in d.items():\n",
        "            d[k] = v.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**d)\n",
        "\n",
        "        targets = d['labels'].cpu().detach().tolist()\n",
        "        outputs = outputs['logits'].cpu().detach().tolist()\n",
        "\n",
        "        fin_targets.extend(targets)\n",
        "        fin_outputs.extend(outputs)\n",
        "\n",
        "        del targets, outputs\n",
        "        gc.collect()\n",
        "\n",
        "    loss = loss_fn(torch.tensor(fin_outputs), torch.tensor(fin_targets))\n",
        "    loss_reduced = xm.mesh_reduce('loss_reduce', loss, lambda x: sum(x) / len(x))\n",
        "    xm.master_print(f'val. loss={loss_reduced}')\n",
        "\n",
        "# Finally, we define a main function that we will run on each of the 8 cores\n",
        "def main():\n",
        "    dds = datasets.load_from_disk('dds')\n",
        "    dds.set_format('torch')\n",
        "    train_dataset = dds['train']\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True)\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    valid_dataset = dds['test']\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        valid_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False)\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=valid_sampler,\n",
        "        drop_last=False,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    device = xm.xla_device()\n",
        "\n",
        "    # Instantiate the model within the main function and enable gradient checkpointing\n",
        "    # This is crucial for multi-processing with PyTorch XLA\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=3, gradient_checkpointing=True)\n",
        "    model.to(device)\n",
        "    xm.master_print('done loading model and dataloader')\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)]},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)]}\n",
        "    ]\n",
        "\n",
        "    lr = LR * xm.xrt_world_size()\n",
        "    num_train_steps = int(len(train_dataset) / BATCH_SIZE / xm.xrt_world_size())\n",
        "\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(num_train_steps * WARMUP_PCT),\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
        "\n",
        "    # --- Start Isolation ---\n",
        "    xm.master_print(f'Process {xm.get_ordinal()}: Starting training loop...')\n",
        "    # for epoch in range(EPOCHS):\n",
        "    #     gc.collect()\n",
        "    #     train_loop_fn(train_data_loader, loss_fn, model, optimizer, device, scheduler)\n",
        "    #     gc.collect()\n",
        "    #     eval_loop_fn(valid_data_loader, loss_fn, model, device)\n",
        "    #     gc.collect()\n",
        "\n",
        "    #     xm.rendezvous('save_model')\n",
        "    #     xm.master_print('save model')\n",
        "    #     xm.save(model.state_dict(), f'xla_trained_model_epoch_{epoch}.pth')\n",
        "    # --- End Isolation ---\n",
        "\n",
        "# Kick off the training process using the main function and xmp.spawn\n",
        "def _mp_fn(rank):\n",
        "    main()"
      ],
      "metadata": {
        "id": "7S0yvnvFHVEf"
      },
      "id": "7S0yvnvFHVEf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    #xmp.spawn(_mp_fn, args=(), nprocs=None, start_method='spawn') # Changed start_method to 'spawn'\n",
        "    xmp.spawn(_mp_fn, args=(), nprocs=None, start_method='forkserver')"
      ],
      "metadata": {
        "id": "w2rS8TxIJOYd"
      },
      "id": "w2rS8TxIJOYd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3945.12844,
      "end_time": "2022-06-07T11:55:54.371213",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-06-07T10:50:09.242773",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
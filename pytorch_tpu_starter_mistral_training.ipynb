{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/pytorch_tpu_starter_mistral_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch"
      ],
      "metadata": {
        "id": "6KibpHHXbTJP"
      },
      "id": "6KibpHHXbTJP"
    },
    {
      "cell_type": "markdown",
      "id": "1fd29e81",
      "metadata": {
        "papermill": {
          "duration": 0.024857,
          "end_time": "2022-06-07T10:50:17.130071",
          "exception": false,
          "start_time": "2022-06-07T10:50:17.105214",
          "status": "completed"
        },
        "tags": [],
        "id": "1fd29e81"
      },
      "source": [
        "\n",
        "# Introduction\n",
        "PyTorch XLA is a PyTorch library for XLA support. XLA (Accelerated Linear Algebra) is a domain-specific compiler that was originally meant for compiling and accelerating TensorFlow models. However, other packages, like JAX and now PyTorch XLA can compile program with XLA to accelerate code. TPUs can be programmed with XLA programs and PyTorch XLA provides this interface with TPUs by compiling our PyTorch code as XLA programs to run on TPU devices.\n",
        "\n",
        "In this kernel, I provide an in-depth look into how you can use PyTorch XLA to train a PyTorch model on the TPU for the Feedback Effectiveness Prize competition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c88356",
      "metadata": {
        "papermill": {
          "duration": 0.023205,
          "end_time": "2022-06-07T10:50:17.178816",
          "exception": false,
          "start_time": "2022-06-07T10:50:17.155611",
          "status": "completed"
        },
        "tags": [],
        "id": "42c88356"
      },
      "source": [
        "## Installs & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "nNspNTXERrn1"
      },
      "id": "nNspNTXERrn1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b376fc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:51:23.298724Z",
          "iopub.status.busy": "2022-06-07T10:51:23.294026Z",
          "iopub.status.idle": "2022-06-07T10:51:55.840875Z",
          "shell.execute_reply": "2022-06-07T10:51:55.840282Z",
          "shell.execute_reply.started": "2022-06-07T08:32:26.52533Z"
        },
        "papermill": {
          "duration": 32.584908,
          "end_time": "2022-06-07T10:51:55.841034",
          "exception": false,
          "start_time": "2022-06-07T10:51:23.256126",
          "status": "completed"
        },
        "tags": [],
        "id": "b9b376fc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install --upgrade huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7d59b685",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:51:56.040240Z",
          "iopub.status.busy": "2022-06-07T10:51:56.039258Z",
          "iopub.status.idle": "2022-06-07T10:52:04.478338Z",
          "shell.execute_reply": "2022-06-07T10:52:04.477566Z",
          "shell.execute_reply.started": "2022-06-07T08:33:00.858006Z"
        },
        "papermill": {
          "duration": 8.497158,
          "end_time": "2022-06-07T10:52:04.478517",
          "exception": false,
          "start_time": "2022-06-07T10:51:55.981359",
          "status": "completed"
        },
        "tags": [],
        "id": "7d59b685"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import warnings,transformers,logging,torch\n",
        "from transformers import TrainingArguments,Trainer\n",
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
        "import torch.optim.adamw as AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from sklearn.metrics import log_loss\n",
        "from pathlib import Path\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "\n",
        "\n",
        "# PyTorch XLA-specific imports\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.debug.metrics as met"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n"
      ],
      "metadata": {
        "id": "eaeokmXPH821"
      },
      "id": "eaeokmXPH821",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN()"
      ],
      "metadata": {
        "id": "vduLRkZIIYWL"
      },
      "id": "vduLRkZIIYWL",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-2\n",
        "num_epochs = 10 # number of epochs\n",
        "\n",
        "num_warmup_epochs=10"
      ],
      "metadata": {
        "id": "dxtd1dmiIERg"
      },
      "id": "dxtd1dmiIERg",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8849d4ef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:04.771437Z",
          "iopub.status.busy": "2022-06-07T10:52:04.770825Z",
          "iopub.status.idle": "2022-06-07T10:52:04.774756Z",
          "shell.execute_reply": "2022-06-07T10:52:04.775294Z",
          "shell.execute_reply.started": "2022-06-07T08:33:10.113095Z"
        },
        "papermill": {
          "duration": 0.054149,
          "end_time": "2022-06-07T10:52:04.775585",
          "exception": false,
          "start_time": "2022-06-07T10:52:04.721436",
          "status": "completed"
        },
        "tags": [],
        "id": "8849d4ef"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=8\n",
        "EPOCHS=1\n",
        "NUM_WORKERS=1\n",
        "OPTIM = AdamW\n",
        "LR=1e-5\n",
        "WD = 0.01\n",
        "WARMUP_PCT=0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae479d24",
      "metadata": {
        "papermill": {
          "duration": 0.046322,
          "end_time": "2022-06-07T10:52:04.869045",
          "exception": false,
          "start_time": "2022-06-07T10:52:04.822723",
          "status": "completed"
        },
        "tags": [],
        "id": "ae479d24"
      },
      "source": [
        "## Dataset creation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/TPU/input/"
      ],
      "metadata": {
        "id": "SS8tGnge_rID"
      },
      "id": "SS8tGnge_rID",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "955c74f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:05.070022Z",
          "iopub.status.busy": "2022-06-07T10:52:05.069406Z",
          "iopub.status.idle": "2022-06-07T10:52:05.549891Z",
          "shell.execute_reply": "2022-06-07T10:52:05.549192Z",
          "shell.execute_reply.started": "2022-06-07T08:33:10.14231Z"
        },
        "papermill": {
          "duration": 0.530692,
          "end_time": "2022-06-07T10:52:05.550038",
          "exception": false,
          "start_time": "2022-06-07T10:52:05.019346",
          "status": "completed"
        },
        "tags": [],
        "id": "955c74f6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/TPU/input/train.csv')\n",
        "test_df = pd.read_csv('/content/gdrive/MyDrive/TPU/input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "23297086",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:05.651856Z",
          "iopub.status.busy": "2022-06-07T10:52:05.650851Z",
          "iopub.status.idle": "2022-06-07T10:52:10.542641Z",
          "shell.execute_reply": "2022-06-07T10:52:10.542121Z",
          "shell.execute_reply.started": "2022-06-07T08:33:10.480063Z"
        },
        "papermill": {
          "duration": 4.94532,
          "end_time": "2022-06-07T10:52:10.542789",
          "exception": false,
          "start_time": "2022-06-07T10:52:05.597469",
          "status": "completed"
        },
        "tags": [],
        "id": "23297086"
      },
      "outputs": [],
      "source": [
        "model_id = '/content/gdrive/MyDrive/TPU/Mistral-7B-v0.1'\n",
        "#tokz = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large', model_max_length=512)\n",
        "tokz = AutoTokenizer.from_pretrained(model_id, model_max_length=512)\n",
        "\n",
        "# Check if sep_token is None and use eos_token or a space as fallback\n",
        "sep = tokz.sep_token if tokz.sep_token is not None else (tokz.eos_token if tokz.eos_token is not None else \" \")\n",
        "\n",
        "df['inputs'] = df.discourse_type + sep +df.discourse_text\n",
        "new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}\n",
        "df = df.replace(new_label)\n",
        "df = df.rename(columns = {\"discourse_effectiveness\": \"labels\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4e54db31",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:10.747733Z",
          "iopub.status.busy": "2022-06-07T10:52:10.746757Z",
          "iopub.status.idle": "2022-06-07T10:52:12.353979Z",
          "shell.execute_reply": "2022-06-07T10:52:12.353436Z",
          "shell.execute_reply.started": "2022-06-07T08:33:16.058144Z"
        },
        "papermill": {
          "duration": 1.658898,
          "end_time": "2022-06-07T10:52:12.354113",
          "exception": false,
          "start_time": "2022-06-07T10:52:10.695215",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e54db31",
        "outputId": "e93d2d7b-9bad-4f68-c31b-6c56b995fce3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7181, 29584)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "essay_ids = df.essay_id.unique()\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(essay_ids)\n",
        "essay_ids[:5]\n",
        "val_prop = 0.2\n",
        "val_sz = int(len(essay_ids)*val_prop)\n",
        "val_essay_ids = essay_ids[:val_sz]\n",
        "is_val = np.isin(df.essay_id, val_essay_ids)\n",
        "idxs = np.arange(len(df))\n",
        "val_idxs = idxs[ is_val]\n",
        "trn_idxs = idxs[~is_val]\n",
        "len(val_idxs),len(trn_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8cf1ac64",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-07T10:52:12.455396Z",
          "iopub.status.busy": "2022-06-07T10:52:12.454493Z",
          "iopub.status.idle": "2022-06-07T10:52:12.460005Z",
          "shell.execute_reply": "2022-06-07T10:52:12.460595Z",
          "shell.execute_reply.started": "2022-06-07T08:33:17.69128Z"
        },
        "papermill": {
          "duration": 0.057865,
          "end_time": "2022-06-07T10:52:12.460758",
          "exception": false,
          "start_time": "2022-06-07T10:52:12.402893",
          "status": "completed"
        },
        "tags": [],
        "id": "8cf1ac64"
      },
      "outputs": [],
      "source": [
        "def get_dds(df, train=True):\n",
        "    ds = Dataset.from_pandas(df)\n",
        "    to_remove = ['discourse_text','discourse_type','inputs','discourse_id','essay_id']\n",
        "    tok_ds = ds.map(tok_func, batched=True, remove_columns=to_remove)\n",
        "    if train:\n",
        "        return DatasetDict({\"train\":tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})\n",
        "    else:\n",
        "        return tok_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a2447b",
      "metadata": {
        "papermill": {
          "duration": 0.049668,
          "end_time": "2022-06-07T10:52:23.501089",
          "exception": false,
          "start_time": "2022-06-07T10:52:23.451421",
          "status": "completed"
        },
        "tags": [],
        "id": "e2a2447b"
      },
      "source": [
        "## Training code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e13c39b5"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_id = '/content/gdrive/MyDrive/TPU/Mistral-7B-v0.1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# Set PAD token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# save tokenizer to load offline during inference\n",
        "tokenizer.save_pretrained('tokenizer')\n",
        "\n",
        "\n",
        "def tok_func(x): return tokenizer(x[\"inputs\"], padding='max_length', truncation=True)\n",
        "\n",
        "dds = get_dds(df)\n",
        "dds.save_to_disk('dds')\n",
        "\n",
        "\n",
        "\n",
        "# e2a2447b\n",
        "# Let's start training! To do so, we start by initializing the model. We use the `xmp.MpModelWrapper` provided by PyTorch XLA to save memory when initializing the model.\n",
        "\n",
        "# 7eb44021\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=3)\n",
        "mx = xmp.MpModelWrapper(model)\n",
        "\n",
        "# da38b15c\n",
        "# Let's now define our training and validation functions.\n",
        "\n",
        "# 6eeac157\n",
        "def train_loop_fn(data_loader, loss_fn, model, optimizer, device, scheduler=None):\n",
        "    model.train() # put model in training mode\n",
        "    for bi, d in enumerate(data_loader): # enumerate through the dataloader\n",
        "\n",
        "        # put tensors onto desired device, in this case the TPU core\n",
        "        for k,v in d.items():\n",
        "            d[k] = v.to(device)\n",
        "\n",
        "        # pass ids to model\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**d)\n",
        "        # calculate loss\n",
        "        loss = loss_fn(outputs['logits'], d['labels'])\n",
        "        if bi % 50 == 0:\n",
        "            # since the loss is on all 8 cores, reduce the loss values and print the average\n",
        "            loss_reduced = xm.mesh_reduce('loss_reduce',loss, lambda x: sum(x) / len(x))\n",
        "            # master_print will only print once (not from all 8 cores)\n",
        "            xm.master_print(f'bi={bi}, loss={loss_reduced}')\n",
        "\n",
        "        # backpropagate\n",
        "        loss.backward()\n",
        "\n",
        "        # Use PyTorch XLA optimizer stepping\n",
        "        xm.optimizer_step(optimizer)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    model.eval() # put model in eval mode for later use\n",
        "\n",
        "def eval_loop_fn(data_loader, loss_fn, model, device):\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    for bi, d in enumerate(data_loader): # enumerate through dataloader\n",
        "\n",
        "        # put tensors onto desired device, in this case the TPU core\n",
        "        for k,v in d.items():\n",
        "            d[k] = v.to(device)\n",
        "\n",
        "        # pass ids to model\n",
        "        with torch.no_grad(): outputs = model(**d)\n",
        "\n",
        "        # Add the outputs and targets to a list\n",
        "        targets = d['labels'].cpu().detach().tolist()\n",
        "        outputs = outputs['logits'].cpu().detach().tolist()\n",
        "        fin_targets.extend(targets)\n",
        "        fin_outputs.extend(outputs)\n",
        "        del targets, outputs\n",
        "        gc.collect() # delete for memory conservation\n",
        "\n",
        "    # calculate loss\n",
        "    loss = loss_fn(torch.tensor(fin_outputs), torch.tensor(fin_targets))\n",
        "    # since the loss is on all 8 cores, reduce the loss values and print the average\n",
        "    loss_reduced = xm.mesh_reduce('loss_reduce',loss, lambda x: sum(x) / len(x))\n",
        "    # master_print will only print once (not from all 8 cores)\n",
        "    xm.master_print(f'val. loss={loss_reduced}')\n",
        "\n",
        "# ad763b55\n",
        "# Finally, we define a main function that we will run on each of the 8 cores of the TPU.\n",
        "\n",
        "# 1ba39779\n",
        "def main(mx):\n",
        "    dds = datasets.load_from_disk('dds') # load dataset from disk\n",
        "    dds.set_format('torch') # set the dataset to return torch Tensors\n",
        "\n",
        "    train_dataset = dds['train']\n",
        "    # defining data samplers and loaders\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          train_dataset,\n",
        "          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n",
        "          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n",
        "          shuffle=True)\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    valid_dataset = dds['test']\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          valid_dataset,\n",
        "          num_replicas=xm.xrt_world_size(),\n",
        "          rank=xm.get_ordinal(),\n",
        "          shuffle=False)\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=valid_sampler,\n",
        "        drop_last=False,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    device = xm.xla_device() # our device (single TPU core)\n",
        "\n",
        "    # PyTorch XLA-specific dataloading\n",
        "    train_loader = pl.MpDeviceLoader(train_data_loader, device)\n",
        "    valid_loader = pl.MpDeviceLoader(valid_data_loader, device)\n",
        "\n",
        "    model = mx.to(device) # put model onto the current TPU core\n",
        "    xm.master_print('done loading model and dataloader')\n",
        "\n",
        "\n",
        "    param_optimizer = list(model.named_parameters()) # model parameters to optimize\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    # apply to weight decay\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': WD},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "    xm.master_print('training on train dataset')\n",
        "\n",
        "    lr = LR * xm.xrt_world_size() # scale the learning rate\n",
        "    # calculate the total number of training steps\n",
        "    num_train_steps = int(len(train_dataset) / BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
        "\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr) # define our optimizer\n",
        "\n",
        "    # a scheduler can be used if desired\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(num_train_steps*WARMUP_PCT),\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
        "\n",
        "    # Let's start training on the train set!\n",
        "    for epoch in range(EPOCHS):\n",
        "        gc.collect() # I use a lot of gc.collect() statement to hopefully prevent OOM problems\n",
        "        # call training loop:\n",
        "        train_loop_fn(train_loader, loss_fn, model, optimizer, device, scheduler=scheduler)\n",
        "        gc.collect()\n",
        "        # call evaluation loop:\n",
        "        eval_loop_fn(valid_loader, loss_fn, model, device)\n",
        "        gc.collect()\n",
        "\n",
        "    xm.rendezvous('save_model')\n",
        "    xm.master_print('save model')\n",
        "    xm.save(model.state_dict(), f'xla_trained_model_epoch_{epoch}.pth')"
      ],
      "id": "e13c39b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c93a6e0"
      },
      "source": [
        "# Kick off the training process using the main function and xmp.spawn\n",
        "def _mp_fn(rank): # Removed mx argument\n",
        "    main() # Removed mx argument in the call\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This is the standard way to launch PyTorch XLA training on multiple cores\n",
        "    # Changed nprocs to None as per the error message\n",
        "    xmp.spawn(_mp_fn, args=(), nprocs=None, start_method='spawn') # Removed mx from args"
      ],
      "id": "1c93a6e0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3945.12844,
      "end_time": "2022-06-07T11:55:54.371213",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-06-07T10:50:09.242773",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
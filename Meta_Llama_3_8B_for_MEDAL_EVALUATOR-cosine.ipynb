{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMaPloVpkg4+TJzeVKEI9CG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Meta_Llama_3_8B_for_MEDAL_EVALUATOR-cosine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip show huggingface_hub"
      ],
      "metadata": {
        "id": "d-iE3R7fVBDP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade huggingface_hub -q"
      ],
      "metadata": {
        "id": "oLO1NOdVVNbV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install huggingface_hub -q"
      ],
      "metadata": {
        "id": "f5u00oiWVu7k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@frankmorales_91352/fine-tuning-meta-llama-3-8b-with-medal-for-enhanced-medical-language-understanding-3de07a54fab9"
      ],
      "metadata": {
        "id": "BtnQHkxH3CxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install peft --quiet\n",
        "\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "#Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "sJ5qguUtgRCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")"
      ],
      "metadata": {
        "id": "YnDGjScpg5CP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "44TvN5ChjSXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9c8983-4a86-4742-852d-ff16e8da3a07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "9Mt3GNBahX6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76d911c-09d4-4b01-e79e-c7f5c8c0c6cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "CjcHuj57hdz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ceed3d0-6e99-4beb-f9a1-e625209e6c11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U tensorboard-plugin-profile -q"
      ],
      "metadata": {
        "id": "HqiuFix3wra-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/tensorbard\n",
        "%mkdir -p /content/tensorbard\n",
        "%cd /content/tensorbard\n",
        "!git lfs install\n",
        "#!git clone https://huggingface.co/frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2\n",
        "!git clone https://huggingface.co/frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine"
      ],
      "metadata": {
        "id": "ydCn9527wteN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "#%tensorboard --logdir /content/tensorbard/Meta-Llama-3-8B-MEDAL-flash-attention-2/runs\n",
        "#%tensorboard --logdir /content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2/runs/\n",
        "\n",
        "%tensorboard --logdir /content/tensorbard/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine/runs\n",
        "#%tensorboard --logdir /content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine/runs/\n"
      ],
      "metadata": {
        "id": "G-t1tvPww_m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "BMGlJx4jhSii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGkx4MLQf-1n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "#peft_model_id = \"/content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2\"\n",
        "#peft_model_id = 'frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2'\n",
        "\n",
        "#peft_model_id = \"/content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine\"\n",
        "peft_model_id = 'frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine'\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        "  #load_in_4bit=True\n",
        "  quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',  load_in_4bit=True, ...)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "#/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "#nrec= randint(0, len(eval_dataset))\n",
        "\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "x3gkTVspiGzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "xGPDheryiElH",
        "outputId": "aab262bd-d6ad-4d34-fbcc-7fcb3b89884c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for the management of this debilitating condition [/INST] antral follicle count </s><s>[INST] the purpose of this study was to investigate the effect of the novel antifolate aminopterin on the survival of mu and human glioma cells in vitro and in vivo aminopterin was more effective than methotrexate mtx in inhibiting the growth of a murine glioma cell L1 in vitro the ic for aminopterin was much lower than that of mtx nm vs nm aminopterin also inhibited the growth of a human glioma cell L1 in vitro the ic was nm the survival'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(ganswer,oanswer):\n",
        "\n",
        "  qc0=str(ganswer).find('[INST]')\n",
        "\n",
        "  ganswer=str(ganswer)[0:qc0-8]\n",
        "  qc=str(ganswer).find('[/INST]')\n",
        "  if qc>0:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "  print(f\"Generated Answer:\\n{ganswer}\")\n",
        "\n",
        "  if ganswer == oanswer:\n",
        "    print(\"Match\")\n",
        "    match=1\n",
        "  else:\n",
        "    print(\"NO Match\")\n",
        "    match=0\n",
        "  return match"
      ],
      "metadata": {
        "id": "U8lxIbH6r-q2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "\n",
        "match=similarity(ganswer,oanswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGLv4cBkiVRV",
        "outputId": "743cee5a-1b3d-45af-f7f3-d5b31f989b0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "Match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG-Y_5PPiad6",
        "outputId": "2281f56b-48a8-4f46-93f6-b8794f768473"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "['antral follicle count']\n",
            "Generated Answer:\n",
            "for the management of this debilitating condition [/INST] antral follicle count </s><s>[INST] the purpose of this study was to investigate the effect of the novel antifolate aminopterin on the survival of mu and human glioma cells in vitro and in vivo aminopterin was more effective than methotrexate mtx in inhibiting the growth of a murine glioma cell L1 in vitro the ic for aminopterin was much lower than that of mtx nm vs nm aminopterin also inhibited the growth of a human glioma cell L1 in vitro the ic was nm the survival\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "zRi13uPaibmQ",
        "outputId": "56c6f54a-c71b-4dee-f69d-93878f43772b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the purpose of this study was to investigate the effect of the novel antifolate aminopterin on the survival of mu and human glioma cells in vitro and in vivo aminopterin was more effective than methotrexate mtx in inhibiting the growth of a murine glioma cell L1 in vitro the ic for aminopterin was much lower than that of mtx nm vs nm aminopterin also inhibited the growth of a human glioma cell L1 in vitro the ic was nm the survival'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map"
      ],
      "metadata": {
        "id": "s_vzP4M-Hiac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exceptions(ganswer,cganswer,eganswer):\n",
        "     if ganswer==eganswer:\n",
        "        ganswer=cganswer\n",
        "     return ganswer"
      ],
      "metadata": {
        "id": "GDLZRFDV7Wvz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "#prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "#outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "#                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "#                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt =  sample['text']\n",
        "    outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "    #print()\n",
        "    #print(f\"Question:\\n{sample['text']}\")\n",
        "    #print()\n",
        "\n",
        "    #print()\n",
        "    #print(f\"Generated Answer original:\\n{ganswer}\")\n",
        "    #print()\n",
        "\n",
        "    # molecule [/INST] phosphorylethanolamine </s><s>[INST]\n",
        "    qc0=str(ganswer).find('[/INST]')\n",
        "    qc1=str(ganswer).find('[INST]')\n",
        "\n",
        "    #print(qc0)\n",
        "    #print(qc1)\n",
        "\n",
        "    ganswer=str(ganswer)[qc0+8:qc1-8]\n",
        "    #print(f\"Generated Answer corr:\\n{ganswer}\")\n",
        "\n",
        "    oanswer=str(sample['label'])\n",
        "    oanswer=oanswer[2:len(oanswer)-2]\n",
        "\n",
        "\n",
        "    #### exceptions TODO\n",
        "\n",
        "    #if ganswer == 'enteropathogenic':\n",
        "    #   ganswer=exceptions(ganswer,oanswer,ganswer)\n",
        "\n",
        "\n",
        "    #print(len(oanswer))\n",
        "    #print(len(ganswer))\n",
        "\n",
        "    #print(f\"Original Answer:\\n{oanswer}\")\n",
        "    if ganswer == oanswer:\n",
        "        #print()\n",
        "        #print(\"Match\")\n",
        "        #print(f\"Query:\\n{sample['text']}\")\n",
        "        #print(f\"Original Answer:\\n{oanswer}\")\n",
        "        #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        #print()\n",
        "        return 1\n",
        "    else:\n",
        "        #print()\n",
        "        #print(\"no Match\")\n",
        "        #print(f\"Query:\\n{sample['text']}\")\n",
        "        #print(f\"Original Answer:\\n{oanswer}\")\n",
        "        #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        #print()\n",
        "        return 0\n",
        "\n",
        "\n",
        "#### exceptions TODO\n",
        "\n",
        "#Original Answer:\n",
        "#enteropathogenic\n",
        "#Generated Answer:\n",
        "#enteropathogenic e coli\n",
        "\n",
        "#Original Answer:\n",
        "#severe mental retardation\n",
        "#Generated Answer:\n",
        "#mental retardation\n",
        "\n",
        "#Original Answer:\n",
        "#eye movement\n",
        "#Generated Answer:\n",
        "#eye movements\n",
        "\n",
        "\n",
        "#Original Answer:\n",
        "#severe mental retardation\n",
        "#Generated Answer:\n",
        "#mental retardation\n",
        "\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 10\n",
        "# iterate over eval dataset and predict\n",
        "\n",
        "#for n in tqdm(range(number_of_eval_samples)):\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "#    print(f'sample {n}')\n",
        "    s=eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "#for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
        "#    success_rate.append(evaluate(s))\n",
        "\n",
        "# compute accuracy\n",
        "accuracy = sum(success_rate)/len(success_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWw-yHYilDd",
        "outputId": "c7e9ffdb-d350-4244-baf1-ee8a63f3c731"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:06<00:00, 18.68s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "print(f\"Accuracy (Eval dataset and predict) for a sample of {number_of_eval_samples}: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiNueGHqFQIv",
        "outputId": "d326f415-d1f7-473b-f641-02863a8214a7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Eval dataset and predict) for a sample of 10: 60.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "learning_rate: 2e-05\n",
        "train_batch_size: 3\n",
        "eval_batch_size: 8\n",
        "seed: 42\n",
        "gradient_accumulation_steps: 6\n",
        "total_train_batch_size: 18\n",
        "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
        "lr_scheduler_type: constant\n",
        "lr_scheduler_warmup_ratio: 0.03\n",
        "num_epochs: 2\n",
        "\n",
        "When evaluated on 1000 samples from the evaluation dataset, our model achieved an accuracy of 26.30%. However, there's room for improvement.\n",
        "\n",
        "\n",
        "learning_rate: 0.0002\n",
        "train_batch_size: 3\n",
        "eval_batch_size: 8\n",
        "seed: 42\n",
        "gradient_accumulation_steps: 6\n",
        "total_train_batch_size: 18\n",
        "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
        "lr_scheduler_type: cosine\n",
        "lr_scheduler_warmup_ratio: 0.03\n",
        "num_epochs: 10\n",
        "\n",
        "When evaluated on 10 samples from the evaluation dataset, our model achieved an accuracy of 60%. However, there's room for improvement.\n",
        "\n"
      ],
      "metadata": {
        "id": "NYZDOY-0zp5d"
      }
    }
  ]
}
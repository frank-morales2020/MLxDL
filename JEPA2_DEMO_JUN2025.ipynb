{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPyzttxBHCIyqKIZg9G+HtJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/JEPA2_DEMO_JUN2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run the uninstallation and installation cells to ensure a clean install of the latest version.\n",
        "# Make sure to restart the kernel after the installation cells have completed successfully.\n",
        "\n",
        "# original code from user's notebook:\n",
        "# from IPython import get_ipython\n",
        "# from IPython.display import display\n",
        "# %%\n",
        "!pip uninstall transformers -y\n",
        "# If it prompts about multiple packages or dependencies, confirm 'y'\n",
        "# %%\n",
        "!pip install --upgrade pip -q\n",
        "# %%\n",
        "!pip install av -q\n",
        "# %%\n",
        "!pip install git+https://github.com/huggingface/transformers.git -q\n",
        "# %%\n",
        "# AFTER RUNNING THE ABOVE INSTALLATION CELLS, RESTART THE JUPYTER KERNEL.\n",
        "# THEN, RUN THIS CELL AND THE SUBSEQUENT CELLS.\n"
      ],
      "metadata": {
        "id": "RUFgWQEUEChN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "import torch\n",
        "import av # PyAV for video loading\n",
        "\n",
        "hf_repo = \"facebook/vjepa2-vitg-fpc64-256\"\n",
        "\n",
        "# Load the model and processor\n",
        "model = AutoModel.from_pretrained(hf_repo)\n",
        "processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "\n",
        "# --- Example Video Loading (replace with your actual video file) ---\n",
        "# For demonstration, let's assume you have a video file named 'your_video.mp4'\n",
        "# You would typically load a video using a library like PyAV or OpenCV\n",
        "\n",
        "# Dummy video creation for demonstration (replace with actual video loading)\n",
        "# This creates a dummy video tensor of shape (num_frames, channels, height, width)\n",
        "# A real video would have varying pixel values.\n",
        "num_frames = 16\n",
        "height, width = 256, 256\n",
        "dummy_video = torch.rand(num_frames, 3, height, width) # Example: 16 frames, 3 color channels, 256x256 pixels\n",
        "\n",
        "# Preprocess the video\n",
        "# The processor will handle resizing, normalization, and converting to the correct format\n",
        "inputs = processor(videos=list(dummy_video), return_tensors=\"pt\")\n",
        "\n",
        "# Pass the preprocessed video through the model\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# The 'last_hidden_state' typically contains the rich feature representations\n",
        "# The shape will depend on the model architecture, but it represents the learned features.\n",
        "video_features = outputs.last_hidden_state\n",
        "print(f\"Shape of extracted video features: {video_features.shape}\")\n",
        "\n",
        "# You can then use these features for downstream tasks (e.g., classification, anomaly detection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sooue7WQTY65",
        "outputId": "9522e44c-c2de-406f-87bb-81f575315385"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of extracted video features: torch.Size([1, 2048, 1408])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "import torch\n",
        "\n",
        "hf_repo = \"facebook/vjepa2-vitg-fpc64-256\"\n",
        "model = AutoModel.from_pretrained(hf_repo)\n",
        "processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "\n",
        "# Dummy video (same as above)\n",
        "num_frames = 16\n",
        "height, width = 256, 256\n",
        "dummy_video = torch.rand(num_frames, 3, height, width)\n",
        "\n",
        "# Preprocess\n",
        "inputs = processor(videos=list(dummy_video), return_tensors=\"pt\")\n",
        "\n",
        "# Get model outputs - these might include predicted features for masked regions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Depending on the model configuration, you might get different output keys.\n",
        "# Common ones are 'last_hidden_state' (for general features)\n",
        "# or potentially 'prediction_logits' if it's explicitly set up for a specific prediction task.\n",
        "print(outputs.keys())\n",
        "# For V-JEPA, 'last_hidden_state' is usually the most useful output for downstream tasks."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6e9NzQ2TuRl",
        "outputId": "63c03158-4e11-4d17-ecb9-cbf76855fa83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['last_hidden_state', 'masked_hidden_state', 'predictor_output'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "import torch\n",
        "import av # PyAV library for video loading\n",
        "import numpy as np\n",
        "import os # To check if the file exists\n",
        "\n",
        "# --- 1. Define the model and processor ---\n",
        "hf_repo = \"facebook/vjepa2-vitg-fpc64-256\"\n",
        "model = AutoModel.from_pretrained(hf_repo)\n",
        "processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "\n",
        "# --- 2. Specify the path to your video file ---\n",
        "video_path = '/content/airplane-landing.mp4'\n",
        "\n",
        "# --- Check if the video file exists ---\n",
        "if not os.path.exists(video_path):\n",
        "    print(f\"Error: The video file '{video_path}' was not found.\")\n",
        "    print(\"Please ensure the video is uploaded to your Colab environment or the path is correct.\")\n",
        "else:\n",
        "    # --- 3. Load and process the video ---\n",
        "    frames = []\n",
        "    try:\n",
        "        container = av.open(video_path)\n",
        "        # Sample frames evenly (e.g., aiming for 16 frames as common for V-JEPA)\n",
        "        total_frames_in_video = container.streams.video[0].frames\n",
        "        num_frames_to_sample = 16\n",
        "        sampling_interval = max(1, total_frames_in_video // num_frames_to_sample)\n",
        "\n",
        "        print(f\"Loading video from: {video_path}\")\n",
        "        print(f\"Total frames in video: {total_frames_in_video}\")\n",
        "        print(f\"Sampling interval: {sampling_interval} frames\")\n",
        "\n",
        "        for i, frame in enumerate(container.decode(video=0)):\n",
        "            if len(frames) >= num_frames_to_sample:\n",
        "                break\n",
        "            if i % sampling_interval == 0:\n",
        "                img = frame.to_rgb().to_ndarray() # Convert to NumPy array (H, W, C)\n",
        "                frames.append(img)\n",
        "\n",
        "        if not frames:\n",
        "            print(f\"Error: No frames could be loaded from '{video_path}'. Check video integrity.\")\n",
        "        elif len(frames) < num_frames_to_sample:\n",
        "            print(f\"Warning: Only {len(frames)} frames loaded. Model might expect {num_frames_to_sample}.\")\n",
        "\n",
        "        # The processor expects a list of NumPy arrays (H, W, C)\n",
        "        inputs = processor(videos=frames, return_tensors=\"pt\")\n",
        "\n",
        "        # --- 4. Pass the processed video through the model ---\n",
        "        print(f\"Extracting features from {len(frames)} frames...\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        video_features = outputs.last_hidden_state\n",
        "        print(f\"Successfully extracted video features with shape: {video_features.shape}\")\n",
        "\n",
        "        print(\"\\n--- Next Steps for Description ---\")\n",
        "        print(\"These 'video_features' are the model's numerical understanding of your video.\")\n",
        "        print(\"To get a human-readable description, you would need:\")\n",
        "        print(\"1.  **A Video Classification Model:** Train a classifier on a dataset of videos (or their V-JEPA features) labeled with categories (e.g., 'airplane landing', 'airplane takeoff', 'airport ground operations'). This classifier would then predict the most likely category for your video.\")\n",
        "        print(\"2.  **A Video Captioning Model:** Train a more advanced model that takes V-JEPA features as input and generates a descriptive sentence (e.g., 'A commercial airplane descends onto a runway and touches down.').\")\n",
        "        print(\"\\nWithout such a pre-trained downstream model, I cannot provide a textual description directly from these numerical features.\")\n",
        "\n",
        "    except av.FFmpegError as e:\n",
        "        print(f\"Error loading video with PyAV: {e}\")\n",
        "        print(\"This might indicate an issue with the video file itself or FFmpeg installation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        print(\"Ensure 'av' library is installed (`pip install av`) and the video format is supported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LinG_2AuUhJP",
        "outputId": "af34054b-cfa1-42ea-db91-e484e7ab0dd3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading video from: /content/airplane-landing.mp4\n",
            "Total frames in video: 983\n",
            "Sampling interval: 61 frames\n",
            "Extracting features from 16 frames...\n",
            "Successfully extracted video features with shape: torch.Size([1, 2048, 1408])\n",
            "\n",
            "--- Next Steps for Description ---\n",
            "These 'video_features' are the model's numerical understanding of your video.\n",
            "To get a human-readable description, you would need:\n",
            "1.  **A Video Classification Model:** Train a classifier on a dataset of videos (or their V-JEPA features) labeled with categories (e.g., 'airplane landing', 'airplane takeoff', 'airport ground operations'). This classifier would then predict the most likely category for your video.\n",
            "2.  **A Video Captioning Model:** Train a more advanced model that takes V-JEPA features as input and generates a descriptive sentence (e.g., 'A commercial airplane descends onto a runway and touches down.').\n",
            "\n",
            "Without such a pre-trained downstream model, I cannot provide a textual description directly from these numerical features.\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhkU4Jrb93S/+oVco1mrBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Mistral_in_AWS_with_TWOMODELS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREPARATION AND INTRODUCTION TBD##"
      ],
      "metadata": {
        "id": "uJSDkT0Zjlgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEEbeCwo3sz8",
        "outputId": "d85e04f1-c22c-48ef-a5d9-8120e5003384"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sagemaker\n",
            "  Downloading sagemaker-2.200.1-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker) (23.1.0)\n",
            "Collecting boto3<2.0,>=1.33.3 (from sagemaker)\n",
            "  Downloading boto3-1.34.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker) (2.2.1)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker) (1.23.5)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.12 in /usr/local/lib/python3.10/dist-packages (from sagemaker) (3.20.3)\n",
            "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker)\n",
            "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker) (1.5.3)\n",
            "Collecting pathos (from sagemaker)\n",
            "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting schema (from sagemaker)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: PyYAML~=6.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker) (6.0.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker) (4.19.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker) (4.1.0)\n",
            "Collecting tblib==1.7.0 (from sagemaker)\n",
            "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting urllib3<1.27 (from sagemaker)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn==0.22.0 (from sagemaker)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi==0.95.2 (from sagemaker)\n",
            "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m877.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sagemaker) (2.31.0)\n",
            "Collecting docker (from sagemaker)\n",
            "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sagemaker) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from sagemaker) (5.9.5)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.95.2->sagemaker) (1.10.13)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi==0.95.2->sagemaker)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn==0.22.0->sagemaker) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn==0.22.0->sagemaker)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.2 (from boto3<2.0,>=1.33.3->sagemaker)\n",
            "  Downloading botocore-1.34.2-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.33.3->sagemaker)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.10.0,>=0.9.0 (from boto3<2.0,>=1.33.3->sagemaker)\n",
            "  Downloading s3transfer-0.9.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sagemaker) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sagemaker) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sagemaker) (2023.11.17)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from google-pasta->sagemaker) (1.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker) (2023.3.post1)\n",
            "Collecting ppft>=1.7.6.7 (from pathos->sagemaker)\n",
            "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.7 (from pathos->sagemaker)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.3 (from pathos->sagemaker)\n",
            "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.15 (from pathos->sagemaker)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema->sagemaker) (21.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.95.2->sagemaker) (4.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.2.0)\n",
            "Installing collected packages: urllib3, tblib, smdebug-rulesconfig, schema, ppft, pox, jmespath, importlib-metadata, h11, dill, uvicorn, starlette, multiprocess, botocore, s3transfer, pathos, fastapi, docker, boto3, sagemaker\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: tblib\n",
            "    Found existing installation: tblib 3.0.0\n",
            "    Uninstalling tblib-3.0.0:\n",
            "      Successfully uninstalled tblib-3.0.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.34.2 botocore-1.34.2 dill-0.3.7 docker-7.0.0 fastapi-0.95.2 h11-0.14.0 importlib-metadata-6.11.0 jmespath-1.0.1 multiprocess-0.70.15 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7 s3transfer-0.9.0 sagemaker-2.200.1 schema-0.7.5 smdebug-rulesconfig-1.0.1 starlette-0.27.0 tblib-1.7.0 urllib3-1.26.18 uvicorn-0.22.0\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.2)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.2 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.2->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.2->boto3) (1.26.18)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.2->boto3) (1.16.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (1.26.18)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.18\n",
            "    Uninstalling urllib3-1.26.18:\n",
            "      Successfully uninstalled urllib3-1.26.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "botocore 1.34.2 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
            "sagemaker 2.200.1 requires urllib3<1.27, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-2.1.0\n",
            "Collecting colab-env\n",
            "  Downloading colab-env-0.2.0.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-dotenv<1.0,>=0.10.0 (from colab-env)\n",
            "  Downloading python_dotenv-0.21.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: colab-env\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colab-env: filename=colab_env-0.2.0-py3-none-any.whl size=3805 sha256=30a5095ebf427edd2ba1b90da4b945bf2a651245cab60fc37cb54b2c23a9d4a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/36/4f/466c2cd4db5d08f317893a920c4a0f58a81459ee3bdb136d35\n",
            "Successfully built colab-env\n",
            "Installing collected packages: python-dotenv, colab-env\n",
            "Successfully installed colab-env-0.2.0 python-dotenv-0.21.1\n",
            "Mounted at /content/gdrive\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
            "huggingface-llm-mistral-7b-instruct - JumpStartModel\n",
            "---------!\n",
            "\n",
            "meta-textgeneration-llama-2-7b-f - JumpStartModel\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
            "WARNING:sagemaker.jumpstart:For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------!"
          ]
        }
      ],
      "source": [
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "# https://github.com/mistralai/mistral-src\n",
        "\n",
        "#It looks great\n",
        "# https://mistral.ai/news/mixtral-of-experts/\n",
        "\n",
        "# https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html\n",
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "!pip install sagemaker\n",
        "!pip install boto3\n",
        "!pip install --upgrade urllib3\n",
        "!pip install colab-env --upgrade\n",
        "\n",
        "import colab_env\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "print('huggingface-llm-mistral-7b-instruct - JumpStartModel')\n",
        "model_version='2.0.0'\n",
        "model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", model_version='2.0.0', role=ROLE_ARN)\n",
        "predictor = model.deploy()\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "print('meta-textgeneration-llama-2-7b-f - JumpStartModel')\n",
        "model_id, model_version = \"meta-textgeneration-llama-2-7b-f\", \"2.0.4\"\n",
        "\n",
        "(model_id, model_version,) = (\n",
        "    \"meta-textgeneration-llama-2-7b\",\n",
        "    \"2.1.8\",\n",
        ")\n",
        "\n",
        "model_llama = JumpStartModel(model_id=model_id, model_version=model_version, role=ROLE_ARN)\n",
        "predictor_llama = model_llama.deploy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-text-completion.ipynb\n",
        "\n",
        "def print_response(payload, response):\n",
        "    print(payload[\"inputs\"])\n",
        "    print(f\"> {response[0]['generation']}\")\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "#\"inputs\": \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\",\n",
        "\n",
        "%time\n",
        "payload = {\n",
        "    \"inputs\": \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\",\n",
        "    #\"inputs\": \"I believe the meaning of life is\",\n",
        "    \"parameters\": {\n",
        "        \"max_new_tokens\": 1024,\n",
        "        \"top_p\": 0.9,\n",
        "        \"temperature\": 0.6,\n",
        "        \"return_full_text\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = predictor_llama.predict(payload, custom_attributes='accept_eula=true')\n",
        "    print_response(payload, response)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "wbIbzXvnPKZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PROMPT AND VALIDATION\"\""
      ],
      "metadata": {
        "id": "XjkX2lqFjUen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_sentiment(prompt,predictor,modelid):\n",
        "    if modelid == 0:\n",
        "       INPUT= \"<s>[INST]  \" + prompt + \" [/INST]\"\n",
        "    else:\n",
        "       INPUT= \"%s\"%prompt\n",
        "\n",
        "    payload = {\n",
        "        #\"inputs\": \"<s>[INST]  \" + prompt + \" [/INST]\",\n",
        "        \"inputs\": \"%s\"%INPUT,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.9,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"return_full_text\": False,\n",
        "            #\"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
        "        },\n",
        "    }\n",
        "    return predictor.predict(payload,custom_attributes=\"accept_eula=true\")\n",
        "\n",
        "prompt0 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "#prompt2 = \"I am currently have two bananas. i ate one yestesday. how many do i have now?\"\n",
        "print()\n",
        "print()\n",
        "print('TEST0: Predict the following words in the sequence')\n",
        "#prompt2 = \"This new music video was incredibile\"\n",
        "#prompt2 = \"I currently have two bananas and one apple in my basket. i ate the apple now. how many items are in my basket?\"\n",
        "print()\n",
        "print('Prompt #2: %s'%prompt0)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt0,predictor,0)\n",
        "#print(sentiment)\n",
        "print('OUTPUT PROMPT0:')\n",
        "print()\n",
        "print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwOuOKp1BQe3",
        "outputId": "feabb77e-2d9f-4f49-bab5-b6abc156b0bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TEST0: Predict the following words in the sequence\n",
            "\n",
            "Prompt #2: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "\n",
            "OUTPUT PROMPT0:\n",
            "\n",
            "Answer:  Certainly! Regularization is a technique used in machine learning to prevent overfitting of models. It involves adding a penalty term to the loss function, which discourages the model from fitting the training data too closely and instead encourages it to have a more general solution that can generalize to new, unseen data.\n",
            "\n",
            "Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on new data. Regularization works by adding a penalty term to the loss function, which encourages the model to have a simpler solution that is less likely to overfit.\n",
            "\n",
            "There are several types of regularization, including L1 (Lasso) regularization, L2 (Ridge) regularization, and Elastic Net regularization. Each type of regularization works by adding a different penalty term to the loss function, and the choice of regularization type depends on the specific problem and data at hand.\n",
            "\n",
            "Overall, regularization is an important technique in machine learning that can help prevent overfitting and improve the generalization performance of models.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#payload = {\"inputs\": \"<s>[INST] Hello! [/INST]\"}\n",
        "#print(predictor.predict(payload))\n",
        "\n",
        "#prompt = \"Tell me about Amazon SageMaker.\"\n",
        "\n",
        "#endpoint_name=model.endpoint_name\n",
        "\n",
        "import json\n",
        "\n",
        "def predict_sentiment(prompt,predictor,modelid):\n",
        "    if modelid == 0:\n",
        "       INPUT= \"<s>[INST]  \" + prompt + \" [/INST]\"\n",
        "    else:\n",
        "       INPUT= \"%s\"%prompt\n",
        "\n",
        "    payload = {\n",
        "        #\"inputs\": \"<s>[INST]  \" + prompt + \" [/INST]\",\n",
        "        \"inputs\": \"%s\"%INPUT,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.9,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"return_full_text\": False,\n",
        "            #\"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
        "        },\n",
        "    }\n",
        "    return predictor.predict(payload,custom_attributes=\"accept_eula=true\")\n",
        "\n",
        "\n",
        "print('TESTING of the Mistral LLM running hosted in AWS WITH THE CODE RUNNING FROM GOOGLE CLOUD with Google Colab')\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print('TEST1 MISTRAL: Mathematics and reasoning')\n",
        "prompt1='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "print()\n",
        "print('Prompt #1: %s'%prompt1)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt1,predictor,0)\n",
        "#s=sentiment[0]\n",
        "print('OUTPUT PROMPT1:')\n",
        "print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "#print(s['generated_text'])\n",
        "#print(sentiment)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print('TEST1 - LLAMA2: Mathematics and reasoning')\n",
        "prompt1='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "print()\n",
        "print('Prompt #1: %s'%prompt1)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt1,predictor_llama,1)\n",
        "#s=sentiment[0]\n",
        "print('OUTPUT PROMPT1 - LLAMA2:')\n",
        "#print(s)\n",
        "print(f\"Answer: {sentiment[0]['generation']}\")\n",
        "#print(s['generation'])\n",
        "#print(sentiment)\n",
        "print()\n",
        "\n",
        "#print(predictor.predict(payload))\n",
        "\n",
        "#> Input\n",
        "#Tweet: \"I get sad when my phone battery dies.\"\n",
        "#Sentiment: Negative\n",
        "\n",
        "payload = {\"inputs\": \"<s>[INST] Tweet: I get sad when my phone battery dies. [/INST]\"}\n",
        "#print(predictor.predict(payload))\n",
        "\n",
        "###\n",
        "#Tweet: \"My day has been :+1:\"\n",
        "#Sentiment: Positive\n",
        "\n",
        "###\n",
        "#Tweet: \"This is the link to the article\"\n",
        "#Sentiment: Neutral\n",
        "\n",
        "###\n",
        "#Tweet: \"This new music video was incredibile\"\n",
        "#Sentiment:\n",
        "\n",
        "payload = {\"inputs\": \"<s>[INST] Tweet: My day has been :+1: [/INST]\"}\n",
        "#print(predictor.predict(payload))\n",
        "\n",
        "payload = {\"inputs\": \"<s>[INST] This is the link to the article [/INST]\"}\n",
        "#print(predictor.predict(payload))\n",
        "\n",
        "prompt0= \"I am currently have two bananas. i ate one yestesday. how many do i have now?\"\n",
        "print()\n",
        "print()\n",
        "print('TEST2 MISTRAL: Predict the following words in the sequence')\n",
        "prompt2 = \"This new music video was incredibile\"\n",
        "#prompt2 = \"I currently have two bananas and one apple in my basket. how many items are in my basket?\"\n",
        "print()\n",
        "print('Prompt #2: %s'%prompt2)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt2,predictor,0)\n",
        "#s=sentiment[0]\n",
        "print('OUTPUT PROMPT2:')\n",
        "print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "#print(s['generated_text'])\n",
        "#print(sentiment)\n",
        "print()\n",
        "\n",
        "prompt3='A rat is a rodent, the most common mammal in the world. Rattus norvegicus is one of the approximately four hundred different kinds of rodents, and it is known by many names, each of which describes a trait or a perceived trait or sometimes a habitat: the earth rat, the roving rat, the barn rat, the fi eld rat, the migratory rat, the house rat, the sewer rat, the water rat, the wharf rat, the alley rat, the gray rat, the brown rat, and the common rat. The average brown rat is large and stocky; it grows to be approximately sixteen inches \\\n",
        "long from its nose to its tail—the size of a large adult human male’s foot—and weighs about a pound, though brown rats have been measured by scientists and exterminators at twenty inches and up to two pounds. The brown rat is sometimes confused with the black rat, or Rattus rattus, which is smaller and once inhabited New York City and all of the cities of America but, since Rattus norvegicus pushed it out, is now relegated to a minor role. (The two species still survive alongside each other in some Southern coastal cities and on the West Coast, in places like Los Angeles, for example, where the black rat lives in attics and palm trees.) The black rat is always a very dark gray, almost black, \\\n",
        "and the brown rat is gray or brown, with a belly that can be light gray, yellow, or even a pure-seeming white. One spring, beneath the Brooklyn Bridge, I saw a red-haired brown rat that had been run over by a car. Both pet rats and laboratory rats are Rattus norvegicus, but they are not wild and therefore, I would emphasize, not the subject of this book. Sometimes pet rats are called fancy rats. But if anyone has picked up this book to learn about fancy rats, then they should put this book down right away; none of the rats mentioned herein are at all fancy. \\\n",
        "Rats are nocturnal, and out in the night the brown rat’s eyes are small and black and shiny; when a fl ashlight shines into them in the dark, the eyes of a rat light up like the eyes of a deer. Though it forages* in darkness, the brown rat has poor eyesight. It makes up for this with, fi rst of all, an excellent sense of smell. . . . They have an excellent sense of taste, detecting the most minute amounts of poison, down to one part per million. A brown rat has strong feet, the two front paws each equipped with four clawlike nails, the rear paws even longer and stronger. It can run and climb with squirrel-like agility. It is an excellent swimmer, surviving in rivers and bays, in sewer streams and toilet bowls. \\\n",
        "The brown rat’s teeth are yellow, the front two incisors being especially long and sharp, like buckteeth. When the brown rat bites, its front two teeth spread apart. When it gnaws, a fl ap of skin plugs the space behind its incisors. Hence, when the rat gnaws on indigestible materials—concrete or steel, for example—the shavings don’t go down the rat’s throat and kill it. Its incisors grow at a rate of fi ve inches per year. Rats always gnaw, and no one is certain why—there are few modern rat studies. It is sometimes erroneously stated that the rat gnaws solely to limit the length of its incisors, which would otherwise grow out of its head, but this is not the case: the incisors wear down naturally. In terms of hardness, the brown rat’s teeth are stronger than aluminum, copper, lead, and iron. They are comparable to steel. With the alligator-like structure of their jaws, rats can exert a biting pressure of up to seven thousand pounds per square inch. Rats, like mice, seem to be attracted to wires—to utility wires, computer wires, wires in vehicles, in addition to gas and water pipes. One rat expert theorizes that wires may be attractive to rats because of their resemblance to vines and the stalks of plants; cables are the vines of the city. By one estimate, 26 percent of all electric-cable breaks and 18 percent of all phone-cable disruptions are caused by rats. According to one study, as many as 25 percent of all fi res of unknown origin are rat-caused. Rats chew electrical cables. Sitting in a nest of tattered rags and newspapers, in the fl oorboards of an old tenement, \\\n",
        "a rat gnaws the head of a match—the lightning in the city forest.When it is not gnawing or feeding on trash, the brown rat digs. Anywhere there is dirt in a city, brown rats are likely to be digging—in parks, in fl owerbeds, in little dirt-poor backyards. They dig holes to enter buildings and to make nests. Rat nests can be in the floorboards of apartments, in the waste-stuffed corners of subway stations, in sewers, or beneath old furniture in basements. “Cluttered and unkempt alleyways in cities provide ideal rat habitat, especially those alleyways associated with food-serving establishments,” writes Robert Corrigan in Rodent Control, a pest control manual. “Alley rats can forage safely within the shadows created by the alleyway, as well as quickly retreat to the safety of cover in these narrow channels.” Often, rats burrow under concrete sidewalk slabs. Entrance to a typical under-the-sidewalk rat’s nest is gained through a two-inch-wide hole—their skeletons collapse and they can squeeze into a hole as small as three quarters of an inch wide, the average width of their skull. This tunnel then travels about a foot down to where it widens into a nest or den. The den is lined with soft debris, often shredded plastic garbage or shopping bags, but sometimes even grasses or plants; some rat nests have been found stuffed with the gnawed shavings of the wood-based, spring-loaded snap traps that are used in attempts to kill them. The back of the den then narrows into a long tunnel that opens up on another hole back on the street. This second hole is called a bolt hole; it is an emergency exit. A bolt hole is typically covered lightly with dirt or trash—camoufl age. Sometimes there are networks of burrows, which can stretch beneath a few concrete squares on a sidewalk, or a number of backyards, or even an entire city block—when Rattus norvegicus fi rst came to Selkirk, England, in 1776, there were so many burrows that people feared the town might sink. Rats can also nest in basements, sewers, manholes, abandoned pipes of any kind, fl oorboards, or any hole or depression. “Often,” Robert Corrigan writes, “‘city rats’ will live unbeknownst to people right beneath their feet.” \\\n",
        "Rats also inhabit subways, as most people in New York City and any city with a subway system are well aware. Every once in a while, there are reports of rats boarding trains, \\\n",
        "but for the most part rats stay on the tracks—subway workers \\\n",
        "I have talked to refer to rats as “track rabbits.” People tend to think that the subways are fi lled with rats, \\\n",
        "but in fact rats are not everywhere in the system; they live in the subways according to the supply of discarded \\\n",
        "human food and sewer leaks. Sometimes, rats use the subway purely for nesting purposes; they fi nd ways through \\\n",
        "the walls of the subway stations leading from the tracks to the restaurants and stores on the street—the vibrations of \\\n",
        " subway trains tend to create rat-size cracks and holes. Many subway rats tend to live near stations that are themselves near fast-food restaurants. At the various subway stations near Herald Square, for example, people come down from the streets and throw the food that they have not eaten onto the tracks, along with newspapers and soda bottles and, I have noticed, thousands of nolonger-charged AA batteries, waiting to leak acid. The rats eat freely from the waste and sit at the side of the little streams of creamy brown sewery water that fl ows between the rails. They sip the water the way rats do, either with their front paws or by scooping it up with their incisors. \\\n",
        "What is the most likely reason the author states, \"cables are the vines of the city\"?'\n",
        "\n",
        "print()\n",
        "print('TEST3 MISTRAL: MCAS Grade 10 English Language Arts Reading Comprehension test')\n",
        "print()\n",
        "print('Prompt #3: %s'%prompt3)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt3,predictor,0)\n",
        "#s=sentiment[0]\n",
        "print('OUTPUT PROMPT3:')\n",
        "print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "#print(s['generated_text'])\n",
        "#print(sentiment)\n",
        "print()\n",
        "\n",
        "prompt4='Which country has the most natural lakes? Answer with only the country name.'\n",
        "#prompt4='who was the last United States president?'\n",
        "#prompt4='What is the capital of the Unites States?'\n",
        "#prompt4='Who was the first president of the United States?'\n",
        "#prompt4='who is obama'\n",
        "#prompt4='write an essay about AI'\n",
        "print()\n",
        "print('TEST4 MISTRAL: Knowledge retrieval')\n",
        "print()\n",
        "print('Prompt #4: %s'%prompt4)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt4,predictor,0)\n",
        "#s=sentiment[0]\n",
        "print('OUTPUT PROMPT4:')\n",
        "print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "#print(s['generated_text'])\n",
        "#print(sentiment[0])\n",
        "\n",
        "print()\n",
        "print()\n",
        "print('TEST4 - LLAMA2: Knowledge retrieval')\n",
        "print()\n",
        "print('Prompt #4: %s'%prompt4)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt4,predictor_llama,1)\n",
        "print('OUTPUT PROMPT4 - LLAMA2:')\n",
        "print(f\"Answer: {sentiment[0]['generation']}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "Ijaf83ka82wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6b62d8-b4c3-41e2-dcc4-20888cba1c02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING of the Mistral LLM running hosted in AWS WITH THE CODE RUNNING FROM GOOGLE CLOUD with Google Colab\n",
            "\n",
            "\n",
            "\n",
            "TEST1 MISTRAL: Mathematics and reasoning\n",
            "\n",
            "Prompt #1: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "OUTPUT PROMPT1:\n",
            "Answer:  Let's break down the problem:\n",
            "\n",
            "1. You bought six ice cream cones, with each cone costing $1.25.\n",
            "2. That means the total cost of the ice cream was:\n",
            "   Total cost = Number of cones * Cost per cone\n",
            "   = 6 * $1.25\n",
            "   = $7.50\n",
            "3. You paid for the ice cream with a $10 bill, which is $2.50 more than the total cost of the ice cream ($10 - $7.50 = $2.50).\n",
            "4. Therefore, you got $2.50 back.\n",
            "\n",
            "The answer is that you got $2.50 back.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TEST1 - LLAMA2: Mathematics and reasoning\n",
            "\n",
            "Prompt #1: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "OUTPUT PROMPT1 - LLAMA2:\n",
            "Answer: \n",
            "A. $1.00 3 kids B. $1.25 2 kids C. $1.25 3 kids D. $2.50 2 kids\n",
            "Answer: B. $1.25 2 kids\n",
            "$10 - $1.25 (2 kids) = $8.75\n",
            "$8.75 - $1.25 (3 kids) = $7.50\n",
            "$7.50 - $1.25 (2 kids) = $6.25\n",
            "$6.25 - $1.25 (1 kids) = $5.00\n",
            "$5.00 - $1.25 (0 kids) = $3.75\n",
            "$3.75 - $1.25 (0 kids) = $2.50\n",
            "So, he got $2.50 back.\n",
            "(4) If you bought 24 cones, how many would you need to buy to get 3 cones back?\n",
            "Answer: 8 cones\n",
            "(5) In 2008, what was the population of the United States?\n",
            "Answer: 303,783,000 people\n",
            "(6) How many people are in the United States today?\n",
            "Answer: 316,728,849 people\n",
            "The U.S. population growth is 1.25% per year. That means that in 2008, there were 303,783,000 people and in 2009, there were 306,733,000 people.\n",
            "(7) A store owner buys 24 bottles of juice for $4.40 each and then sells them for $6.00 each. How many bottles of juice does the store sell?\n",
            "Answer: 8 bottles\n",
            "$4.40 + $6.00 = $10.40\n",
            "$10.40 - $6.00 = $4.40\n",
            "$4.40 / $6.00 = 0.73\n",
            "$0.73 x 8 = 5.84\n",
            "(8) A candy store bought \n",
            "\n",
            "\n",
            "\n",
            "TEST2 MISTRAL: Predict the following words in the sequence\n",
            "\n",
            "Prompt #2: This new music video was incredibile\n",
            "\n",
            "OUTPUT PROMPT2:\n",
            "Answer:  That's great to hear! What did you like about the music video?\n",
            "\n",
            "\n",
            "TEST3 MISTRAL: MCAS Grade 10 English Language Arts Reading Comprehension test\n",
            "\n",
            "Prompt #3: A rat is a rodent, the most common mammal in the world. Rattus norvegicus is one of the approximately four hundred different kinds of rodents, and it is known by many names, each of which describes a trait or a perceived trait or sometimes a habitat: the earth rat, the roving rat, the barn rat, the fi eld rat, the migratory rat, the house rat, the sewer rat, the water rat, the wharf rat, the alley rat, the gray rat, the brown rat, and the common rat. The average brown rat is large and stocky; it grows to be approximately sixteen inches long from its nose to its tail—the size of a large adult human male’s foot—and weighs about a pound, though brown rats have been measured by scientists and exterminators at twenty inches and up to two pounds. The brown rat is sometimes confused with the black rat, or Rattus rattus, which is smaller and once inhabited New York City and all of the cities of America but, since Rattus norvegicus pushed it out, is now relegated to a minor role. (The two species still survive alongside each other in some Southern coastal cities and on the West Coast, in places like Los Angeles, for example, where the black rat lives in attics and palm trees.) The black rat is always a very dark gray, almost black, and the brown rat is gray or brown, with a belly that can be light gray, yellow, or even a pure-seeming white. One spring, beneath the Brooklyn Bridge, I saw a red-haired brown rat that had been run over by a car. Both pet rats and laboratory rats are Rattus norvegicus, but they are not wild and therefore, I would emphasize, not the subject of this book. Sometimes pet rats are called fancy rats. But if anyone has picked up this book to learn about fancy rats, then they should put this book down right away; none of the rats mentioned herein are at all fancy. Rats are nocturnal, and out in the night the brown rat’s eyes are small and black and shiny; when a fl ashlight shines into them in the dark, the eyes of a rat light up like the eyes of a deer. Though it forages* in darkness, the brown rat has poor eyesight. It makes up for this with, fi rst of all, an excellent sense of smell. . . . They have an excellent sense of taste, detecting the most minute amounts of poison, down to one part per million. A brown rat has strong feet, the two front paws each equipped with four clawlike nails, the rear paws even longer and stronger. It can run and climb with squirrel-like agility. It is an excellent swimmer, surviving in rivers and bays, in sewer streams and toilet bowls. The brown rat’s teeth are yellow, the front two incisors being especially long and sharp, like buckteeth. When the brown rat bites, its front two teeth spread apart. When it gnaws, a fl ap of skin plugs the space behind its incisors. Hence, when the rat gnaws on indigestible materials—concrete or steel, for example—the shavings don’t go down the rat’s throat and kill it. Its incisors grow at a rate of fi ve inches per year. Rats always gnaw, and no one is certain why—there are few modern rat studies. It is sometimes erroneously stated that the rat gnaws solely to limit the length of its incisors, which would otherwise grow out of its head, but this is not the case: the incisors wear down naturally. In terms of hardness, the brown rat’s teeth are stronger than aluminum, copper, lead, and iron. They are comparable to steel. With the alligator-like structure of their jaws, rats can exert a biting pressure of up to seven thousand pounds per square inch. Rats, like mice, seem to be attracted to wires—to utility wires, computer wires, wires in vehicles, in addition to gas and water pipes. One rat expert theorizes that wires may be attractive to rats because of their resemblance to vines and the stalks of plants; cables are the vines of the city. By one estimate, 26 percent of all electric-cable breaks and 18 percent of all phone-cable disruptions are caused by rats. According to one study, as many as 25 percent of all fi res of unknown origin are rat-caused. Rats chew electrical cables. Sitting in a nest of tattered rags and newspapers, in the fl oorboards of an old tenement, a rat gnaws the head of a match—the lightning in the city forest.When it is not gnawing or feeding on trash, the brown rat digs. Anywhere there is dirt in a city, brown rats are likely to be digging—in parks, in fl owerbeds, in little dirt-poor backyards. They dig holes to enter buildings and to make nests. Rat nests can be in the floorboards of apartments, in the waste-stuffed corners of subway stations, in sewers, or beneath old furniture in basements. “Cluttered and unkempt alleyways in cities provide ideal rat habitat, especially those alleyways associated with food-serving establishments,” writes Robert Corrigan in Rodent Control, a pest control manual. “Alley rats can forage safely within the shadows created by the alleyway, as well as quickly retreat to the safety of cover in these narrow channels.” Often, rats burrow under concrete sidewalk slabs. Entrance to a typical under-the-sidewalk rat’s nest is gained through a two-inch-wide hole—their skeletons collapse and they can squeeze into a hole as small as three quarters of an inch wide, the average width of their skull. This tunnel then travels about a foot down to where it widens into a nest or den. The den is lined with soft debris, often shredded plastic garbage or shopping bags, but sometimes even grasses or plants; some rat nests have been found stuffed with the gnawed shavings of the wood-based, spring-loaded snap traps that are used in attempts to kill them. The back of the den then narrows into a long tunnel that opens up on another hole back on the street. This second hole is called a bolt hole; it is an emergency exit. A bolt hole is typically covered lightly with dirt or trash—camoufl age. Sometimes there are networks of burrows, which can stretch beneath a few concrete squares on a sidewalk, or a number of backyards, or even an entire city block—when Rattus norvegicus fi rst came to Selkirk, England, in 1776, there were so many burrows that people feared the town might sink. Rats can also nest in basements, sewers, manholes, abandoned pipes of any kind, fl oorboards, or any hole or depression. “Often,” Robert Corrigan writes, “‘city rats’ will live unbeknownst to people right beneath their feet.” Rats also inhabit subways, as most people in New York City and any city with a subway system are well aware. Every once in a while, there are reports of rats boarding trains, but for the most part rats stay on the tracks—subway workers I have talked to refer to rats as “track rabbits.” People tend to think that the subways are fi lled with rats, but in fact rats are not everywhere in the system; they live in the subways according to the supply of discarded human food and sewer leaks. Sometimes, rats use the subway purely for nesting purposes; they fi nd ways through the walls of the subway stations leading from the tracks to the restaurants and stores on the street—the vibrations of  subway trains tend to create rat-size cracks and holes. Many subway rats tend to live near stations that are themselves near fast-food restaurants. At the various subway stations near Herald Square, for example, people come down from the streets and throw the food that they have not eaten onto the tracks, along with newspapers and soda bottles and, I have noticed, thousands of nolonger-charged AA batteries, waiting to leak acid. The rats eat freely from the waste and sit at the side of the little streams of creamy brown sewery water that fl ows between the rails. They sip the water the way rats do, either with their front paws or by scooping it up with their incisors. What is the most likely reason the author states, \"cables are the vines of the city\"?\n",
            "\n",
            "OUTPUT PROMPT3:\n",
            "Answer:  The most likely reason the author states, \"cables are the vines of the city\" is that rats are attracted to wires and cables due to their resemblance to vines and plants. The comparison is based on the idea that cables provide a similar structure and texture to the natural habitat of rats, which is in forests or grasslands. The rats are attracted to the cables because they provide shelter and protection, as well as a source of food and water.\n",
            "\n",
            "\n",
            "TEST4 MISTRAL: Knowledge retrieval\n",
            "\n",
            "Prompt #4: Which country has the most natural lakes? Answer with only the country name.\n",
            "\n",
            "OUTPUT PROMPT4:\n",
            "Answer:  Canada\n",
            "\n",
            "\n",
            "TEST4 - LLAMA2: Knowledge retrieval\n",
            "\n",
            "Prompt #4: Which country has the most natural lakes? Answer with only the country name.\n",
            "\n",
            "OUTPUT PROMPT4 - LLAMA2:\n",
            "Answer: \n",
            "3. Name the world’s biggest river. (If you’re stumped by the name, you can cheat by clicking on “answer” to see if you’re right.)\n",
            "4. Where is the longest mountain range?\n",
            "5. Which country is the hottest?\n",
            "6. What’s the largest desert?\n",
            "7. Where is the world’s largest tropical rainforest?\n",
            "8. Name the world’s largest country.\n",
            "9. Which country is the coldest?\n",
            "10. What’s the most populated country?\n",
            "11. What is the most populous city?\n",
            "12. Which country has the most islands?\n",
            "13. Where is the largest waterfall?\n",
            "14. Which country has the highest population density?\n",
            "15. What is the largest lake by volume?\n",
            "16. Which country has the most landmarks?\n",
            "17. Where is the biggest volcano?\n",
            "18. Which country is the poorest?\n",
            "19. Which country has the least land area?\n",
            "20. What’s the driest country?\n",
            "21. Where is the biggest volcano?\n",
            "22. Which country has the longest coastline?\n",
            "23. What’s the smallest country?\n",
            "24. Where is the highest waterfall in the world?\n",
            "25. Where is the highest mountain in the world?\n",
            "26. Which country has the highest GDP?\n",
            "27. Which country has the most billionaires?\n",
            "28. Where is the largest forest?\n",
            "29. Where is the deepest lake?\n",
            "30. What is the largest ocean?\n",
            "31. Where is the longest river?\n",
            "32. Where is the biggest city?\n",
            "33. Where is the biggest desert?\n",
            "34. What is the biggest city?\n",
            "35. What is the most populous country?\n",
            "36. Where is the highest mountain in the world?\n",
            "37. What is the largest desert?\n",
            "38. Where is the largest forest?\n",
            "39. Where is the deepest lake?\n",
            "40. What is the largest ocean?\n",
            "41. Where is the longest river?\n",
            "42. What is the biggest city?\n",
            "43. What is the most populous country?\n",
            "44. Where is the highest mountain in the world?\n",
            "45. What is the largest desert\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CLEAN UP##"
      ],
      "metadata": {
        "id": "US4T-TG0i6Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "## TODO create a function for the loop\n",
        "\n",
        "aws_region = 'us-east-1'\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "print('')\n",
        "print('Endpoints')\n",
        "response=sagemaker_client.list_endpoints()\n",
        "number_of_endpoints=len(response['Endpoints'])\n",
        "for i in range(number_of_endpoints):\n",
        "    print(response['Endpoints'][i]['EndpointName'])\n",
        "    endpoint_name=response['Endpoints'][i]['EndpointName']\n",
        "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "\n",
        "\n",
        "print('')\n",
        "print('Models')\n",
        "response_models=sagemaker_client.list_models()\n",
        "for i in range(len((response_models['Models']))):\n",
        "    print(response_models['Models'][i]['ModelName'])\n",
        "    sagemaker_client.delete_model(ModelName=response_models['Models'][i]['ModelName'])\n",
        "\n",
        "\n",
        "print('')\n",
        "print('EndpointConfigs')\n",
        "response_configs=sagemaker_client.list_endpoint_configs()\n",
        "for i in range(len((response_configs['EndpointConfigs']))):\n",
        "    print(response_configs['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "    sagemaker_client.delete_endpoint_config(EndpointConfigName=response_configs['EndpointConfigs'][i]['EndpointConfigName'])"
      ],
      "metadata": {
        "id": "a2zzj1YOjHlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae0b75a-103b-4eb9-f51b-fbed7e570df1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Endpoints\n",
            "meta-textgeneration-llama-2-7b-2023-12-17-23-45-51-522\n",
            "hf-llm-mistral-7b-instruct-2023-12-17-23-40-47-946\n",
            "\n",
            "Models\n",
            "meta-textgeneration-llama-2-7b-2023-12-17-23-45-51-518\n",
            "hf-llm-mistral-7b-instruct-2023-12-17-23-40-47-944\n",
            "\n",
            "EndpointConfigs\n",
            "meta-textgeneration-llama-2-7b-2023-12-17-23-45-51-522\n",
            "hf-llm-mistral-7b-instruct-2023-12-17-23-40-47-946\n"
          ]
        }
      ]
    }
  ]
}
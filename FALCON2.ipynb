{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4USnUyS9RuOZvF2T0emCV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FALCON2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/tiiuae/falcon-11B\n",
        "\n",
        "https://huggingface.co/DevQuasar/falcon2-11B-GGUF"
      ],
      "metadata": {
        "id": "kNSJALJ7S13p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmewj0EPRncn",
        "outputId": "ed221a36-c379-4abf-bfcb-29890d59df49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 16 08:42:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install llama-cpp-python\n",
        "!pip install  --upgrade transformers datasets --quiet\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "jvOnm5iyRutW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqwOfS3URzAY",
        "outputId": "3740a258-85ed-40f4-acad-fb133e679200"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/DevQuasar/falcon2-11B-GGUF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Seu9Ofsdg4v0",
        "outputId": "a15463a6-c145-4bc2-fccd-cc12ed8afe54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'falcon2-11B-GGUF'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 36 (delta 11), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (36/36), 10.22 KiB | 1.02 MiB/s, done.\n",
            "Filtering content: 100% (8/8), 17.20 GiB | 22.11 MiB/s, done.\n",
            "Encountered 8 file(s) that may not have been copied correctly on Windows:\n",
            "\tfalcon2-11B.Q2_K.gguf\n",
            "\tfalcon2-11B.Q3_K_M.gguf\n",
            "\tfalcon2-11B.Q4_0.gguf\n",
            "\tfalcon2-11B.Q4_K_M.gguf\n",
            "\tfalcon2-11B.Q5_K_M.gguf\n",
            "\tfalcon2-11B.Q6_K.gguf\n",
            "\tfalcon2-11B.Q8_0.gguf\n",
            "\tfalcon2-11B.f16.gguf\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "LLM = Llama(model_path=\"/content/falcon2-11B-GGUF/falcon2-11B.Q5_K_M.gguf\",verbose=False)"
      ],
      "metadata": {
        "id": "87s9qdQbi8uA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "\n",
        "# generate a response (takes several seconds)\n",
        "output = LLM(prompt,max_tokens=0,echo=False)"
      ],
      "metadata": {
        "id": "fGflDqZ7j1-4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\" * 80)\n",
        "print('Question: %s'%prompt)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print('\\n Answer:')\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k9QJcyhj3PD",
        "outputId": "4feba7e7-16b4-4cf8-eb7d-7cb61628b255"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Answer:\n",
            "\n",
            "Let's break this down step-by-step:\n",
            "\n",
            "1. You bought ice creams for 6 kids, each costing $1.25. The total cost of the cones is:\n",
            "   Cost per cone * Number of cones\n",
            "   = $1.25 * 6\n",
            "   = $7.50\n",
            "\n",
            "2. You paid with a $10 bill. So, you need to subtract the total cost from the amount you paid:\n",
            "   Amount paid - Total cost\n",
            "  = $10 - $7.50\n",
            "  = $2.50\n",
            "\n",
            "So, after paying for your ice creams, you would get back $2.50 in change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"Explain this image: https://images.hindustantimes.com/auto/img/2023/07/23/1600x900/Tesla_Cybertruck_1688887534001_1690087911053.jpeg\"\n",
        "\n",
        "output = LLM(query, max_tokens=0, echo=False)"
      ],
      "metadata": {
        "id": "tNwjt1u4kBpM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print('\\n Answer:')\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxR909tmkfRQ",
        "outputId": "4060773c-c71e-4cc6-937e-d4884ac0e66b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Question: Explain this image: https://images.hindustantimes.com/auto/img/2023/07/23/1600x900/Tesla_Cybertruck_1688887534001_1690087911053.jpeg\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Answer:\n",
            " <br></p><a href=\"https://www.autocarindia.com/news/tesla-cyberquad-electric-atv-unveiled\" class=\"links\">Read more</a>\n",
            "The image shows the Tesla Cybertruck, an electric pickup truck with a futuristic design. The vehicle is known for its angular and unconventional shape, which was designed to maximize aerodynamic efficiency and reduce drag. \n",
            "\n",
            "As an electric vehicle (EV), the Tesla Cybertruck features powerful electric motors that provide instant torque and acceleration without any loss of power due to gear shifting or transmission issues. These electric powertrains are also significantly more efficient than traditional internal combustion engines, resulting in lower fuel costs and reduced emissions of greenhouse gases such as carbon dioxide.\n",
            "\n",
            "The Tesla Cybertruck's futuristic design is not only visually striking but has several practical advantages as well. The angular body panels contribute to improved aerodynamic efficiency by reducing drag forces that act against the vehicle during motion. This results in better overall performance, including higher top speeds and increased range on a single charge of battery power.\n",
            "\n",
            "In summary, this image shows the Tesla Cybertruck, an electric pickup truck with a futuristic design. The vehicle is known for its angular body panels which contribute to improved aerodynamic efficiency by reducing drag forces that act against the vehicle during motion. This results in better overall performance, including higher top speeds and increased range on a single charge of battery power.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the average grid for competitors with over 22 laps and time/retired of +17.276? Answer me as SQL and explain your solutions.\"\n",
        "output = LLM(prompt,max_tokens=0,echo=False)\n",
        "print()\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%prompt)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print('\\n Answer:')\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53fRfUVhklwH",
        "outputId": "d1acdbf4-972c-412f-86d4-60eb3c186866"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: What is the average grid for competitors with over 22 laps and time/retired of +17.276? Answer me as SQL and explain your solutions.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Answer:\n",
            "\n",
            "To answer this question, we'll need to use SQL to query a database that contains all the necessary information. Let's assume that the database schema includes the following tables:\n",
            "\n",
            "- Competitors: Contains data about individual competitors, including their grid position at the start of the race.\n",
            "- Laps: Records each lap completed by a competitor in a particular race.\n",
            "- Time/Retired: Stores the final time and any retirement statuses for each competitor in a race.\n",
            "\n",
            "To find the average grid position for competitors who have raced over 22 laps, finished with a time of plus 17.276 seconds, and did not retire from the race, we would need to execute a query similar to the following:\n",
            "\n",
            "```sql\n",
            "SELECT AVG(grid_position) AS AverageGridPosition\n",
            "FROM Competitors c\n",
            "JOIN Laps l ON c.id = l.competitor_id\n",
            "WHERE l.laps > 22\n",
            "AND TIMESTAMPDIFF(SECOND, l.time, '1970-01-01')) + 17.276 >= l.final_result\n",
            "```\n",
            "\n",
            "This query joins the Competitors table with the Laps table on the competitor ID field to ensure we are only considering laps completed by a specific competitor in the same race. We then filter for competitors who have raced over 22 laps, and who have finished their final lap with a time that is greater than or equal to +17.276 seconds from the start of the race. Finally, we calculate the average grid position using the AVG() function on the grid_position column of the Competitors table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### DATASET for Dialogue Summarization\n",
        "huggingface_dataset_name = \"neil-code/dialogsum-test\"\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ],
      "metadata": {
        "id": "9wfEJquQlvBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ1xfCr_lzGy",
        "outputId": "5381d42a-17da-480b-9df6-fc6387fcf595"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
            "        num_rows: 1999\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
            "        num_rows: 499\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
            "        num_rows: 499\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction','output')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
        "    RESPONSE_KEY = \"### Output:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"\\n{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\"\n",
        "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "w0NMzBc_nItY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dialogsum_test = dataset.map(create_prompt_formats)#, batched=True)\n",
        "query = dataset_dialogsum_test['test'][400]['dialogue']\n",
        "prompt = \"Summarize.\\n %s\"%query"
      ],
      "metadata": {
        "id": "zp0zNg_Ml0Iw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = LLM(prompt,max_tokens=0,echo=False)\n",
        "print()\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%prompt)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print('\\n Answer:')\n",
        "print(output[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZS5Y_HFl9dI",
        "outputId": "9be56c2c-5a54-4154-98e5-636f7941130f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: Summarize.\n",
            " #Person1#: It was a heavy storm last night, wasn't it?\n",
            "#Person2#: It certainly was. The wind broke several windows. What weather!\n",
            "#Person1#: Do you know that big tree in front of my house? One of the biggest branches came down in the night.\n",
            "#Person2#: Really? Did it do any damage to your home?\n",
            "#Person1#: Thank goodness! It is far away from that.\n",
            "#Person2#: I really hate storms. It's about time we had some nice spring weather.\n",
            "#Person1#: It's April, you know. The flowers are beginning to blossom.\n",
            "#Person2#: Yes, that's true. But I still think the weather is terrible.\n",
            "#Person1#: I suppose we should not complain. We had a fine March after all.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Answer:\n",
            " So perhaps things will improve now that spring has begun in earnest.\n",
            "The dialogue between two people discussing the bad weather and how it damaged some of their property. The conversation ends with Person 1 saying that they hope for better days ahead as Spring has officially started in earnest.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Can you explain the concepts of Quantum Computing?\"\n",
        "output = LLM(prompt,max_tokens=0,echo=False)\n",
        "print()\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%prompt)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print('\\n Answer:')\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDxJpRdKnVLv",
        "outputId": "10708e54-15b8-4a0d-fc84-6e45953e96a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: Can you explain the concepts of Quantum Computing?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Answer:\n",
            " I am very interested in this field and would like to learn more about it.\n",
            "Quantum computing is a type of computation that relies on quantum mechanics, which deals with particles at the atomic and subatomic level.\n",
            "\n",
            "One of the key concepts in quantum computing is superposition, which allows qubits (quantum bits) to exist in multiple states simultaneously. This is different from classical bits, which can only be either a 0 or a 1.\n",
            "\n",
            "Another concept in quantum computing is entanglement, where two qubits become connected and the state of one qubit will affect the other.\n",
            "\n",
            "Quantum computers are designed to perform certain types of calculations much faster than classical computers. This has the potential to revolutionize fields such as cryptography, optimization problems, and material science simulations.\n",
            "\n",
            "I hope this explanation helps you understand the concepts of quantum computing a bit better. If you have any more questions or would like to discuss further, please don't hesitate to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I bought a computer for $900, sold it for $1200, repurchased it for $1300, and sold it again for $1600. how much did I earn? Take in consideration the money for the repurchased too.\"\n",
        "\n",
        "output = LLM(prompt,max_tokens=0,echo=False)\n",
        "print()\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%prompt)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print('\\n Answer:')\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "Qo4lzxlpsV6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
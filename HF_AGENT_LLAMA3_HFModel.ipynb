{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMrsJIRDgH86GEcRN7QJUbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/HF_AGENT_LLAMA3_HFModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install -q sentence_transformers\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "!pip install -q tqdm\n",
        "!pip install -q colab-env"
      ],
      "metadata": {
        "id": "d5su1RchXwPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env"
      ],
      "metadata": {
        "id": "7jPYktSJ7_oO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54bc2d6-a52c-40d6-cf46-bc7cb062c3a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4Pi75qGXsyE"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple, Literal, get_args\n",
        "from typing import Any\n",
        "from pydantic import BaseModel, Field, ConfigDict\n",
        "\n",
        "\n",
        "# Define types for GenerateContentConfig (you might need to adjust this based on your HF pipeline)\n",
        "class SafetySetting(BaseModel):\n",
        "    category: Literal[\"HARM_CATEGORY_HARASSMENT\", \"HARM_CATEGORY_HATE_SPEECH\", \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"HARM_CATEGORY_DANGEROUS_CONTENT\"]\n",
        "    threshold: Literal[\"BLOCK_NONE\", \"BLOCK_LOW\", \"BLOCK_MEDIUM\", \"BLOCK_HIGH\"]\n",
        "\n",
        "class GenerationConfig(BaseModel):\n",
        "    stop_sequences: list[str] | None = None\n",
        "    max_output_tokens: int | None = None\n",
        "    temperature: float | None = None\n",
        "    top_p: float | None = None\n",
        "    top_k: int | None = None\n",
        "\n",
        "class GenerateContentConfig(BaseModel):\n",
        "    safety_settings: list[SafetySetting] | None = None\n",
        "    generation_config: GenerationConfig | None = None\n",
        "\n",
        "# Define the Agent class - the 'model' field will now be more of a description\n",
        "class Agent(BaseModel):\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True) # Allow arbitrary types like 'pipeline'\n",
        "    model: str\n",
        "    name: str\n",
        "    description: str\n",
        "    instruction: str\n",
        "    pipeline: Any = Field(default=None)  # Use 'Any' from typing module\n",
        "    generate_content_config: GenerateContentConfig | None = Field(default_factory=GenerateContentConfig)\n",
        "\n",
        "\n",
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "#peft_model_id = \"/content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\"\n",
        "\n",
        "peft_model_id = \"frankmorales2020/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        "  quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "# --- Create the basic agent instance ---\n",
        "basic_agent = Agent(\n",
        "    model=\"Meta-Llama-3-8B-MEDAL (PEFT)\", # Descriptive name\n",
        "    name=\"medical_agent_llama3\",\n",
        "    description=\"This agent responds to medical inquiries using a fine-tuned Llama 3 model.\",\n",
        "    instruction=\"Identify and extract the specific term that the query is defining. Provide only that term as your answer.\",\n",
        "    pipeline=pipe, # Assign the loaded pipeline to the agent\n",
        "    generate_content_config=GenerateContentConfig(temperature=0.2),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(basic_agent)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWbjGtcUcgfz",
        "outputId": "7c5d9781-6c24-4567-cf4a-9c20eed0efee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model='Meta-Llama-3-8B-MEDAL (PEFT)' name='medical_agent_llama3' description='This agent responds to medical inquiries using a fine-tuned Llama 3 model.' instruction='Identify and extract the specific term that the query is defining. Provide only that term as your answer.' pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7b482842c490> generate_content_config=GenerateContentConfig(safety_settings=None, generation_config=None)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL Inference"
      ],
      "metadata": {
        "id": "yw9z7G9PjwY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our test dataset\n",
        "from datasets import load_dataset\n",
        "eval_dataset =load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "eeWm5mjo_Vr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QlA0lEe_epU",
        "outputId": "ee6be9e6-ba0a-40a4-f471-526bbcc5526e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['abstract_id', 'text', 'location', 'label'],\n",
              "    num_rows: 1000000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "nrec= randint(0, len(eval_dataset))\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "vOB7SlsteaMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "print()\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "qc=str(ganswer).find('[INST]')\n",
        "ganswer=ganswer[0:qc-7]\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "if qc>0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "print(f\"Generated Answer:\\n{ganswer}\")\n",
        "print()\n",
        "if ganswer == oanswer:\n",
        "  print(\"Match\")\n",
        "else:\n",
        "  print(\"NO Match\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRtQYLUIeh4q",
        "outputId": "cdc9dea4-3711-44d9-a5f2-ba56195f2b8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "\n",
            "Match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Inference"
      ],
      "metadata": {
        "id": "o1jasKA3j3YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"... ... ... Medical Agent Content Generation ... ... ...\")\n",
        "\n",
        "if basic_agent.pipeline:\n",
        "    #prompt0 = \"Define myocardial infarction.\"\n",
        "    prompt1 = eval_dataset[nrec]['text']\n",
        "    print('\\n')\n",
        "    print(f\"Query: {prompt1}\")\n",
        "    print('\\n')\n",
        "    print(f\"Original Answer: {eval_dataset[nrec]['label']}\")\n",
        "    print('\\n')\n",
        "    output = basic_agent.pipeline(prompt1, max_length=512, num_return_sequences=1)\n",
        "\n",
        "    ganswer=output[0]['generated_text'][len(prompt1)+9:].strip()\n",
        "    qc=str(ganswer).find('[INST]')\n",
        "    ganswer=ganswer[0:qc-7]\n",
        "    qc0=str(ganswer).find('[INST]')\n",
        "    ganswer=str(ganswer)[0:qc0]\n",
        "    qc=str(ganswer).find('[/INST]')\n",
        "    if qc>0:\n",
        "      ganswer=ganswer[qc+8:len(ganswer)]\n",
        "    print(f\"Agent Generated Answer: {ganswer}\")\n",
        "\n",
        "    #print(output[0]['generated_text'].strip())\n",
        "else:\n",
        "    print(\"The pipeline for the agent is not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzfCWwkRZoT-",
        "outputId": "093a20ff-d100-4dff-f694-e53d9957f254"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... ... ... Medical Agent Content Generation ... ... ...\n",
            "\n",
            "\n",
            "Query: while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "\n",
            "\n",
            "Original Answer: ['antral follicle count']\n",
            "\n",
            "\n",
            "Agent Generated Answer: antral follicle count\n"
          ]
        }
      ]
    }
  ]
}
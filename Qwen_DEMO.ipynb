{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNBrdW0RYwpaKVCwyRl1ETE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Qwen_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes -q"
      ],
      "metadata": {
        "id": "kZSrOARth9Qf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "jQq3PwGyiwaK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"You seem to be using the pipelines sequentially on GPU\")\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2enfLjD7ixW0",
        "outputId": "e4c6d7b3-26a3-4696-acc5-6887cfbae467"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install transformers accelerate -q\n",
        "!pip install transformers --upgrade -q\n",
        "\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Model name\n",
        "model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "# Replace 'use_auth_token' with 'token'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=access_token_write)\n",
        "# Remove 'trust_remote_code' here (it has no effect)\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    token=access_token_write  # Replace 'use_auth_token' with 'token'\n",
        ")\n",
        "\n",
        "# Your provided prompt\n",
        "prompt = \"\"\"\n",
        "The old wizard, Gandalf, gazed out from his tower atop the highest peak of the Misty Mountains.\n",
        "A storm raged below, and he knew in his heart that darkness was stirring in the land.\n",
        "He thought of the young hobbit, Frodo, who had been entrusted with a perilous quest,\n",
        "a quest that could determine the fate of Middle-earth. Gandalf sighed, wondering...\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the prompt and move to GPU\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate text\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=50)  # Directly use generate on the model\n",
        "\n",
        "# Decode the output\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q6bnkZTwqo8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the prompt and move to GPU\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate text\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=50)  # Directly use generate on the model\n",
        "\n",
        "# Decode the output\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70SqYMdUru2-",
        "outputId": "c0d95d41-4707-4650-a755-b906364426a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The old wizard, Gandalf, gazed out from his tower atop the highest peak of the Misty Mountains.\n",
            "A storm raged below, and he knew in his heart that darkness was stirring in the land.\n",
            "He thought of the young hobbit, Frodo, who had been entrusted with a perilous quest,\n",
            "a quest that could determine the fate of Middle-earth. Gandalf sighed, wondering...\n",
            "What would happen to the hobbit and his companions? Would they succeed in their quest?\n",
            "Or would the darkness consume them all? As the wind howled around him, Gandalf knew\n",
            "that he must do everything in his power to help them.\n"
          ]
        }
      ]
    }
  ]
}
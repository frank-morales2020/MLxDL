{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNI4yzDdWETPDlHdaoBQGYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48399e954a43479eabb12c05af1e5e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e64e8d780154f4398c724455347feca",
              "IPY_MODEL_df74f2184c214cefaa5d95a1f1ad2fa9",
              "IPY_MODEL_71064880f7744c1e9a11e789e1d1b745"
            ],
            "layout": "IPY_MODEL_0d1b83ee4e4a45449f0f2d430d0ade2d"
          }
        },
        "8e64e8d780154f4398c724455347feca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38fdb887a74a4515be467721c44be6d2",
            "placeholder": "​",
            "style": "IPY_MODEL_d2627645e8e141faaff792ee4e377141",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "df74f2184c214cefaa5d95a1f1ad2fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6b375e1d564726aea9a7f0ec264c8b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edf0a475f209439cad0681b5575ee594",
            "value": 2
          }
        },
        "71064880f7744c1e9a11e789e1d1b745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bff4ec797aa948529e28c54cf8bf6438",
            "placeholder": "​",
            "style": "IPY_MODEL_843ee49bee3d4cd78feece6925ddfbaa",
            "value": " 2/2 [00:08&lt;00:00,  3.97s/it]"
          }
        },
        "0d1b83ee4e4a45449f0f2d430d0ade2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38fdb887a74a4515be467721c44be6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2627645e8e141faaff792ee4e377141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f6b375e1d564726aea9a7f0ec264c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edf0a475f209439cad0681b5575ee594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bff4ec797aa948529e28c54cf8bf6438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "843ee49bee3d4cd78feece6925ddfbaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe64627ab69d4e9e8680e967032c70a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b88c5961a694c3d9d2a2bab45bc95e4",
              "IPY_MODEL_af97a5289ec04b4ab4a1b5c9bde2643e",
              "IPY_MODEL_f0b553b6242347a5b4826bd20cae28ce"
            ],
            "layout": "IPY_MODEL_97e4ca86a8f942529ed4bfd402a73e0a"
          }
        },
        "4b88c5961a694c3d9d2a2bab45bc95e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d03f56c30cb40b9883d9f645f3cac17",
            "placeholder": "​",
            "style": "IPY_MODEL_8e8b2eb486d14738b2fb160ffbd3df76",
            "value": "model.safetensors: 100%"
          }
        },
        "af97a5289ec04b4ab4a1b5c9bde2643e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f39f1e494f6407390f4e06496805bbe",
            "max": 4975270396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d9881bd130744aa9c50b99f5e776e27",
            "value": 4975270396
          }
        },
        "f0b553b6242347a5b4826bd20cae28ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64369ae879414c53a027d0e5f7138787",
            "placeholder": "​",
            "style": "IPY_MODEL_29e1aea05c314fddbc9196c683d8a37b",
            "value": " 4.98G/4.98G [03:28&lt;00:00, 27.8MB/s]"
          }
        },
        "97e4ca86a8f942529ed4bfd402a73e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d03f56c30cb40b9883d9f645f3cac17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8b2eb486d14738b2fb160ffbd3df76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f39f1e494f6407390f4e06496805bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9881bd130744aa9c50b99f5e776e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64369ae879414c53a027d0e5f7138787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e1aea05c314fddbc9196c683d8a37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f371236151914ff0973a10d93b12350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02dec6a4b5f349468a4b27be7e810343",
              "IPY_MODEL_80beb6fb924342afa0e20d3075e6df58",
              "IPY_MODEL_1167a9fe35d741c196cb6a13efe33426"
            ],
            "layout": "IPY_MODEL_130d180ebb7a473d80d7297e587d7bee"
          }
        },
        "02dec6a4b5f349468a4b27be7e810343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f95f80aa84e04bb895a7c7a8fe02ce7a",
            "placeholder": "​",
            "style": "IPY_MODEL_370c72d4786c478ca7ba578e5c8fd165",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "80beb6fb924342afa0e20d3075e6df58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69cefffbd14a409d8918a90102f26f98",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_822b218e80914084a4fbd33be087eadb",
            "value": 2
          }
        },
        "1167a9fe35d741c196cb6a13efe33426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5278f25d55234aeb8206c5823e1bde74",
            "placeholder": "​",
            "style": "IPY_MODEL_c2af832c9fa8457c8d3917c02f6e9785",
            "value": " 2/2 [00:07&lt;00:00,  3.67s/it]"
          }
        },
        "130d180ebb7a473d80d7297e587d7bee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f95f80aa84e04bb895a7c7a8fe02ce7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "370c72d4786c478ca7ba578e5c8fd165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69cefffbd14a409d8918a90102f26f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822b218e80914084a4fbd33be087eadb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5278f25d55234aeb8206c5823e1bde74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2af832c9fa8457c8d3917c02f6e9785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Training_Smart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U datasets transformers accelerate peft trl bitsandbytes sentencepiece interpret\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install -U bitsandbytes -q"
      ],
      "metadata": {
        "id": "jEcIdzQtJlLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation -q # Install the flash-attn package"
      ],
      "metadata": {
        "id": "A_93OVXovB9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft --upgrade  -q # Upgrade peft to the latest version"
      ],
      "metadata": {
        "id": "gi3ePKG_vW3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsY7DoRBu2xR",
        "outputId": "c2fdd146-91be-48d9-9e2e-2f9fecfc022a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 20 05:02:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0              47W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import colab_env\n",
        "\n",
        "# 0. Login to Hugging Face Hub\n",
        "import os\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "login(token=access_token_write, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "cD9fkv6aURmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install numba -q\n",
        "!pip install trl transformers datasets accelerate bitsandbytes -q\n",
        "\n",
        "from numba import cuda\n",
        "\n",
        "# Attempt to reset the device before selecting it.\n",
        "cuda.close()\n",
        "\n",
        "# Selects the desired device and creates a context.\n",
        "cuda.select_device(0) # or any valid GPU index\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2q-nPDg4N3n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import PPOConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import sqlite3\n",
        "\n",
        "# 1. Load the base Mistral model and tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Use the base model\n",
        "\n",
        "# BitsAndBytesConfig int-4 config (for reduced memory usage)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Set the desired GPU device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# or device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # device_map=\"auto\", # Remove or comment out device_map=\"auto\"\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ").to(device) # Explicitly move the model to the device\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "# Revert padding strategy to default\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure default padding token is used\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id # Ensure default padding token ID is used\n",
        "\n",
        "\n",
        "# Revert padding strategy\n",
        "# Assuming default padding token was used previously\n",
        "#tokenizer.padding_side = \"right\"\n",
        "#tokenizer.pad_token = tokenizer.unk_token\n",
        "#tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "\n",
        "# 2. Load a small dataset (for POC)\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "\n",
        "\n",
        "# Preprocess the dataset (this is the crucial missing step)\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"### Instruction: Translate the following natural language query into SQL. The database schema is given in the context.\\n\\n### Context: {c}\\n\\n### Query: {q}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "    targets = examples[\"answer\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "train_dataset = dataset.select(range(10))  # Very small dataset for POC\n",
        "\n",
        "\n",
        "# 3. Define a simple Reward Function (for POC purposes)\n",
        "class SQLRewardFunction:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, samples):\n",
        "        rewards = []\n",
        "        for sample in samples:\n",
        "            query = self.tokenizer.decode(sample, skip_special_tokens=True)\n",
        "            # Simple reward: 1 for any generated query\n",
        "            reward = 1.0\n",
        "            rewards.append(reward)\n",
        "        return torch.tensor(rewards)\n",
        "\n",
        "reward_fn = SQLRewardFunction(tokenizer)\n",
        "\n",
        "# 4. Define PPO configuration (reduced episodes for POC)\n",
        "ppo_config = PPOConfig(\n",
        "    output_dir=\"./ppo_results\",\n",
        "    total_episodes=2  # Very few episodes for POC\n",
        ")\n",
        "\n",
        "# ... (previous code) ...\n",
        "\n",
        "# 5. Create the value model head manually\n",
        "\n",
        "class ValueHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Value head for the model.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): The hidden size of the model.\n",
        "        vocab_size (int): The vocabulary size of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super().__init__()\n",
        "        self.v_head = torch.nn.Linear(hidden_size, 1)\n",
        "        self.lm_head = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        Forward pass through the value head.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): The hidden states of the model.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the logits and the value.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pass hidden states to lm_head first to get logits\n",
        "        logits = self.lm_head(hidden_states.type(torch.float32))\n",
        "\n",
        "        # Calculate the value from the hidden state directly\n",
        "        value = self.v_head(hidden_states.type(torch.float32))\n",
        "        return logits, value\n",
        "\n",
        "\n",
        "\n",
        "# Get the hidden size from the model\n",
        "hidden_size = model.config.hidden_size\n",
        "\n",
        "# Create the value head\n",
        "value_head = ValueHead(hidden_size, len(tokenizer))\n",
        "\n",
        "# Move the value head to the same device as the model\n",
        "value_head.to(model.device) # This line moves the value head to the GPU\n",
        "\n",
        "# Attach the value head to the model\n",
        "model.v_head = value_head\n",
        "\n",
        "# ... (rest of the code) ...\n",
        "\n",
        "# 6. Create the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=ppo_config.learning_rate)\n",
        "\n",
        "# 7. Define helper functions for PPO (simplified for POC)\n",
        "def calculate_advantages(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
        "    \"\"\"Calculates advantages using Generalized Advantage Estimation (GAE).\"\"\"\n",
        "    last_advantage = 0\n",
        "    advantages = []\n",
        "    for t in reversed(range(len(rewards) - 1)):\n",
        "        delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
        "        last_advantage = delta + gamma * gae_lambda * last_advantage\n",
        "        advantages.insert(0, last_advantage)\n",
        "    return torch.tensor(advantages)\n",
        "\n",
        "def calculate_policy_loss(logits, actions, advantages):\n",
        "    \"\"\"Calculates the policy loss using the PPO objective.\"\"\"\n",
        "    # Move actions and advantages to the same device as logits\n",
        "    actions = actions.to(logits.device)\n",
        "    advantages = advantages.to(logits.device)\n",
        "\n",
        "    # Assuming you're using categorical actions (e.g., token ids)\n",
        "    cross_entropy = torch.nn.functional.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)), actions.view(-1)\n",
        "    )\n",
        "    return (cross_entropy * advantages).mean()\n",
        "\n",
        "# 8. Training loop with progress bar\n",
        "for episode in tqdm(range(ppo_config.total_episodes), desc=\"Episodes\"):\n",
        "    for batch in tqdm(train_dataset, desc=\"Batches\", leave=False):\n",
        "        # a. Generate samples from the policy\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)  # Access input_ids correctly\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "\n",
        "        # Add an extra dimension for batch size\n",
        "        input_ids = input_ids.unsqueeze(0)\n",
        "        attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "        #print('\\n\\n')\n",
        "\n",
        "\n",
        "        # Check the shapes of input_ids and attention_mask before calling model.generate\n",
        "        #print(\"Input IDs shape:\", input_ids.shape)\n",
        "        #print(\"Attention Mask shape:\", attention_mask.shape)\n",
        "\n",
        "        #print('\\n\\n')\n",
        "\n",
        "        #print(\"Unique values in input_ids:\", torch.unique(input_ids))\n",
        "       #print(\"Unique values in attention_mask:\", torch.unique(attention_mask))\n",
        "\n",
        "\n",
        "\n",
        "        samples = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=10,  # Reduced max_new_tokens for POC\n",
        "            #do_sample=True,\n",
        "            #top_k=50,\n",
        "            #top_p=0.95,\n",
        "            #temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # b. Calculate rewards\n",
        "        rewards = reward_fn(samples)\n",
        "\n",
        "\n",
        "        # ... inside the training loop ...\n",
        "        # c. Compute the policy loss and value loss\n",
        "        outputs = model(\n",
        "            input_ids=samples,\n",
        "            attention_mask=torch.ones_like(samples),\n",
        "            labels=samples.type(torch.long),  # Convert samples to torch.long\n",
        "            output_hidden_states=True  # Add this line to get hidden states\n",
        "        )\n",
        "\n",
        "        # Get the hidden states for the value head\n",
        "        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n",
        "\n",
        "        # Get logits and values from the custom value head using the hidden states\n",
        "        logits, values = model.v_head(hidden_states)\n",
        "\n",
        "\n",
        "        # Find the loss tensor in the outputs tuple (this might need adjustment)\n",
        "        value_loss = outputs.loss\n",
        "\n",
        "        # Calculate advantages\n",
        "        advantages = calculate_advantages(rewards, values)\n",
        "\n",
        "        # Calculate policy loss\n",
        "        policy_loss = calculate_policy_loss(logits, samples, advantages)\n",
        "\n",
        "        # d. Update the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Combine the losses\n",
        "        total_loss = policy_loss + value_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "48399e954a43479eabb12c05af1e5e98",
            "8e64e8d780154f4398c724455347feca",
            "df74f2184c214cefaa5d95a1f1ad2fa9",
            "71064880f7744c1e9a11e789e1d1b745",
            "0d1b83ee4e4a45449f0f2d430d0ade2d",
            "38fdb887a74a4515be467721c44be6d2",
            "d2627645e8e141faaff792ee4e377141",
            "9f6b375e1d564726aea9a7f0ec264c8b",
            "edf0a475f209439cad0681b5575ee594",
            "bff4ec797aa948529e28c54cf8bf6438",
            "843ee49bee3d4cd78feece6925ddfbaa"
          ]
        },
        "id": "RCTphIDFA7QC",
        "outputId": "8b1c2572-3c1c-4274-a682-16884d2f5577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48399e954a43479eabb12c05af1e5e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n",
            "Episodes:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Batches:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Batches:  10%|█         | 1/10 [00:01<00:10,  1.12s/it]\u001b[A\n",
            "Batches:  20%|██        | 2/10 [00:02<00:08,  1.11s/it]\u001b[A\n",
            "Batches:  30%|███       | 3/10 [00:03<00:07,  1.12s/it]\u001b[A\n",
            "Batches:  40%|████      | 4/10 [00:04<00:06,  1.12s/it]\u001b[A\n",
            "Batches:  50%|█████     | 5/10 [00:05<00:05,  1.12s/it]\u001b[A\n",
            "Batches:  60%|██████    | 6/10 [00:06<00:04,  1.19s/it]\u001b[A\n",
            "Batches:  70%|███████   | 7/10 [00:08<00:03,  1.19s/it]\u001b[A\n",
            "Batches:  80%|████████  | 8/10 [00:09<00:02,  1.16s/it]\u001b[A\n",
            "Batches:  90%|█████████ | 9/10 [00:10<00:01,  1.15s/it]\u001b[A\n",
            "Batches: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\u001b[A\n",
            "Episodes:  50%|█████     | 1/2 [00:11<00:11, 11.45s/it]\n",
            "Batches:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Batches:  10%|█         | 1/10 [00:01<00:09,  1.09s/it]\u001b[A\n",
            "Batches:  20%|██        | 2/10 [00:02<00:08,  1.11s/it]\u001b[A\n",
            "Batches:  30%|███       | 3/10 [00:03<00:07,  1.11s/it]\u001b[A\n",
            "Batches:  40%|████      | 4/10 [00:04<00:06,  1.12s/it]\u001b[A\n",
            "Batches:  50%|█████     | 5/10 [00:05<00:05,  1.12s/it]\u001b[A\n",
            "Batches:  60%|██████    | 6/10 [00:06<00:04,  1.13s/it]\u001b[A\n",
            "Batches:  70%|███████   | 7/10 [00:07<00:03,  1.13s/it]\u001b[A\n",
            "Batches:  80%|████████  | 8/10 [00:09<00:02,  1.15s/it]\u001b[A\n",
            "Batches:  90%|█████████ | 9/10 [00:10<00:01,  1.14s/it]\u001b[A\n",
            "Batches: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\u001b[A\n",
            "Episodes: 100%|██████████| 2/2 [00:22<00:00, 11.39s/it]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 9. Save the model (optional for POC)\n",
        "model.push_to_hub(\n",
        "    \"frankmorales2020/mistral-7b-ppo-poc-t2sql\",  # Provide repo_id as positional argument\n",
        "    commit_message=\"Upload PPO POC model\"\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192,
          "referenced_widgets": [
            "fe64627ab69d4e9e8680e967032c70a2",
            "4b88c5961a694c3d9d2a2bab45bc95e4",
            "af97a5289ec04b4ab4a1b5c9bde2643e",
            "f0b553b6242347a5b4826bd20cae28ce",
            "97e4ca86a8f942529ed4bfd402a73e0a",
            "8d03f56c30cb40b9883d9f645f3cac17",
            "8e8b2eb486d14738b2fb160ffbd3df76",
            "5f39f1e494f6407390f4e06496805bbe",
            "1d9881bd130744aa9c50b99f5e776e27",
            "64369ae879414c53a027d0e5f7138787",
            "29e1aea05c314fddbc9196c683d8a37b"
          ]
        },
        "id": "-1Cv6fxbcRxs",
        "outputId": "da83f1bb-09f1-4da2-f3d8-f1c798efc935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe64627ab69d4e9e8680e967032c70a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/frankmorales2020/mistral-7b-ppo-poc-t2sql/commit/b9fc1f4e9470e875bcc56e8668042f35d1ecff7e', commit_message='Upload PPO POC model', commit_description='', oid='b9fc1f4e9470e875bcc56e8668042f35d1ecff7e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/frankmorales2020/mistral-7b-ppo-poc-t2sql', endpoint='https://huggingface.co', repo_type='model', repo_id='frankmorales2020/mistral-7b-ppo-poc-t2sql'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPROVEMENT"
      ],
      "metadata": {
        "id": "_ZtD0u_8d_5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import PPOConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import sqlite3\n",
        "\n",
        "# 1. Load the base Mistral model and tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Set the desired GPU device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ").to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "# Revert padding strategy to default\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 2. Load and preprocess the dataset with modified prompts\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Modified prompts to discourage \"#\" and add a separator\n",
        "    inputs = [f\"### Instruction: Translate the following natural language query into SQL. The database schema is given in the context. Do not generate any '#' characters.\\n\\n### Context: {c}\\n\\n### Query: {q}\\n\\n```sql\\n\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "    targets = examples[\"answer\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "train_dataset = dataset.select(range(100))  # Increased dataset size\n",
        "\n",
        "# 3. Define the Reward Function with dynamic schema handling and SQL extraction\n",
        "def execute_query(query, db_path=\"database.db\"):\n",
        "    \"\"\"Executes a SQL query against a SQLite database.\"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(query)\n",
        "        conn.close()\n",
        "        return True  # Successful execution\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing query: {query}\\nError: {e}\")\n",
        "        return False  # Failed execution\n",
        "\n",
        "class SQLRewardFunction:\n",
        "    def __init__(self, tokenizer, db_path=\"database.db\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.db_path = db_path\n",
        "\n",
        "    def __call__(self, samples, **kwargs):\n",
        "        rewards = []\n",
        "        for sample in samples:\n",
        "            # Extract the token IDs from the tensor\n",
        "            token_ids = sample.squeeze().tolist()\n",
        "\n",
        "            # Decode the token IDs, skipping special tokens\n",
        "            query = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "            # --- Extract schema and create table (if not exists) ---\n",
        "            try:\n",
        "                # More robust schema extraction\n",
        "                schema = query.split(\"### Context:\")[-1].split(\"### Query:\")[0].strip()\n",
        "                schema = schema.split(\"### Solution:\")[0].strip() # Remove any trailing solution\n",
        "\n",
        "                # Split multiple CREATE TABLE statements\n",
        "                schemas = schema.split(\";\")\n",
        "                conn = sqlite3.connect(self.db_path)\n",
        "                cursor = conn.cursor()\n",
        "                for schema in schemas:\n",
        "                    if schema.strip():  # Execute only if the schema is not empty\n",
        "                        # Modify the schema to include \"IF NOT EXISTS\"\n",
        "                        schema = schema.replace(\"CREATE TABLE\", \"CREATE TABLE IF NOT EXISTS\")\n",
        "                        cursor.execute(schema)  # Create the table if it doesn't exist\n",
        "                conn.commit()\n",
        "                conn.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating table with schema: {schema}\\nError: {e}\")\n",
        "                return torch.tensor([-1.0])  # Return a negative reward\n",
        "\n",
        "            # --- End of schema extraction and table creation ---\n",
        "\n",
        "            # --- Extract SQL query ---\n",
        "            try:\n",
        "                # Attempt to split based on \"### Solution:\"\n",
        "                query = query.split(\"### Solution:\")[-1].strip()\n",
        "            except:\n",
        "                print(\"Unexpected output format. Using default query.\")\n",
        "                query = \"SELECT 1\" # Fallback query\n",
        "\n",
        "            # --- End of SQL extraction ---\n",
        "\n",
        "            # Reward based on execution success\n",
        "            if execute_query(query, self.db_path):\n",
        "                reward = 1.0  # Successful execution\n",
        "            else:\n",
        "                reward = -1.0  # Failed execution\n",
        "            rewards.append(reward)\n",
        "        return torch.tensor(rewards)\n",
        "\n",
        "reward_fn = SQLRewardFunction(tokenizer)  # Initialize reward function\n",
        "\n",
        "# 4. Define PPO configuration\n",
        "ppo_config = PPOConfig(\n",
        "    output_dir=\"./ppo_results\",\n",
        "    total_episodes=10  # Increased number of episodes\n",
        ")\n",
        "\n",
        "# 5. Create and attach the simplified value model head\n",
        "class ValueHead(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super().__init__()\n",
        "        self.v_head = torch.nn.Linear(hidden_size, 1)  # Only keep the value head\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # --- Examine hidden states ---\n",
        "        print(\"Hidden states mean:\", hidden_states.mean())\n",
        "        print(\"Hidden states std:\", hidden_states.std())\n",
        "        print(\"Hidden states min:\", hidden_states.min())\n",
        "        print(\"Hidden states max:\", hidden_states.max())\n",
        "        # --- End of hidden state examination ---\n",
        "\n",
        "        value = self.v_head(hidden_states.type(torch.float32))\n",
        "        return value  # Only return the value\n",
        "\n",
        "hidden_size = model.config.hidden_size\n",
        "value_head = ValueHead(hidden_size, len(tokenizer))\n",
        "value_head.to(model.device)\n",
        "model.v_head = value_head\n",
        "\n",
        "# 6. Create the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=ppo_config.learning_rate)\n",
        "\n",
        "# 7. Define helper functions for PPO\n",
        "def calculate_advantages(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
        "    last_advantage = 0\n",
        "    advantages = []\n",
        "    for t in reversed(range(len(rewards) - 1)):\n",
        "        delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
        "        last_advantage = delta + gamma * gae_lambda * last_advantage\n",
        "        advantages.insert(0, last_advantage)\n",
        "    return torch.tensor(advantages)\n",
        "\n",
        "def calculate_policy_loss(logits, actions, advantages):\n",
        "    actions = actions.to(logits.device)\n",
        "    advantages = advantages.to(logits.device)\n",
        "    cross_entropy = torch.nn.functional.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)), actions.view(-1)\n",
        "    )\n",
        "    return (cross_entropy * advantages).mean()\n",
        "\n",
        "# 8. Training loop with gradient clipping\n",
        "for episode in tqdm(range(ppo_config.total_episodes), desc=\"Episodes\"):\n",
        "    for batch in tqdm(train_dataset, desc=\"Batches\", leave=False):\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "\n",
        "        # Add an extra dimension for batch size\n",
        "        input_ids = input_ids.unsqueeze(0)\n",
        "        attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "        samples = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=50,  # Increased max_new_tokens\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Remove the first token from each sample\n",
        "        samples = [sample[1:] for sample in samples]\n",
        "\n",
        "        # --- DEBUGGING (Optional) ---\n",
        "        # Print the first token ID of the first sample\n",
        "        print(\"First token ID:\", samples[0][0].item())\n",
        "\n",
        "        for sample in samples:\n",
        "            # Extract the token IDs from the tensor\n",
        "            token_ids = sample.squeeze().tolist()\n",
        "\n",
        "            query = tokenizer.decode(token_ids, skip_special_tokens=True)  # Use tokenizer directly\n",
        "            print(\"Generated output:\", query)  # Print the generated output\n",
        "        # --- END DEBUGGING ---\n",
        "\n",
        "        rewards = reward_fn(samples)\n",
        "\n",
        "        # --- Pad the samples before concatenating ---\n",
        "        max_length = max([len(sample) for sample in samples])  # Get the maximum length\n",
        "\n",
        "\n",
        "        # Pad the samples, ensuring the padding tensor is on the same device as the sample\n",
        "        padded_samples = [\n",
        "            torch.cat([sample, torch.full((max_length - len(sample),), tokenizer.pad_token_id, dtype=torch.long, device=sample.device)]) # Change here\n",
        "            for sample in samples\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Concatenate the padded samples\n",
        "        samples_tensor = torch.cat(padded_samples, dim=0).to(device)\n",
        "        # --- End of padding ---\n",
        "\n",
        "        # Create attention mask for the stacked samples\n",
        "        attention_mask = torch.ones_like(samples_tensor, dtype=torch.long).to(device)\n",
        "\n",
        "        # Reshape attention mask to (batch_size, 1, 1, sequence_length)\n",
        "        attention_mask = attention_mask.unsqueeze(0).unsqueeze(1)  # Add two dimensions for batch_size and num_heads\n",
        "\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=samples_tensor,  # Pass the stacked tensor\n",
        "            attention_mask=attention_mask,  # Pass the correct attention mask\n",
        "            labels=samples_tensor,  # Pass the stacked tensor\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "        # Get the value from the simplified value head\n",
        "        values = model.v_head(hidden_states)\n",
        "\n",
        "        value_loss = outputs.loss\n",
        "        advantages = calculate_advantages(rewards, values)\n",
        "\n",
        "        # Get logits for the policy loss\n",
        "        logits = outputs.logits\n",
        "        policy_loss = calculate_policy_loss(logits, samples_tensor, advantages)  # Use the concatenated tensor\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = policy_loss + value_loss\n",
        "        total_loss.backward()\n",
        "\n",
        "        # --- Gradient clipping ---\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "# 9. Save the model\n",
        "model.push_to_hub(\n",
        "    \"your-username/mistral-7b-ppo-sql\",  # Replace with your Hugging Face username\n",
        "    commit_message=\"Upload PPO SQL model\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f371236151914ff0973a10d93b12350b",
            "02dec6a4b5f349468a4b27be7e810343",
            "80beb6fb924342afa0e20d3075e6df58",
            "1167a9fe35d741c196cb6a13efe33426",
            "130d180ebb7a473d80d7297e587d7bee",
            "f95f80aa84e04bb895a7c7a8fe02ce7a",
            "370c72d4786c478ca7ba578e5c8fd165",
            "69cefffbd14a409d8918a90102f26f98",
            "822b218e80914084a4fbd33be087eadb",
            "5278f25d55234aeb8206c5823e1bde74",
            "c2af832c9fa8457c8d3917c02f6e9785"
          ]
        },
        "id": "5k4MU45Wd0gK",
        "outputId": "17b43431-56f9-4915-c74b-8f0717613c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f371236151914ff0973a10d93b12350b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n",
            "Episodes:   0%|          | 0/10 [00:00<?, ?it/s]\n",
            "Batches:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Episodes:   0%|          | 0/10 [00:02<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First token ID: 774\n",
            "Generated output: ### Instruction: Translate the following natural language query into SQL. The database schema is given in the context. Do not generate any '#' characters.\n",
            "\n",
            "### Context: CREATE TABLE head (age INTEGER)\n",
            "\n",
            "### Query: How many heads of the departments are older than 56 ?\n",
            "\n",
            "```sql\n",
            "  SELECT COUNT(*)\n",
            "  FROM head\n",
            "  WHERE age > 56\n",
            "```\n",
            "Error executing query: ### Instruction: Translate the following natural language query into SQL. The database schema is given in the context. Do not generate any '#' characters.\n",
            "\n",
            "### Context: CREATE TABLE head (age INTEGER)\n",
            "\n",
            "### Query: How many heads of the departments are older than 56 ?\n",
            "\n",
            "```sql\n",
            "  SELECT COUNT(*)\n",
            "  FROM head\n",
            "  WHERE age > 56\n",
            "```\n",
            "Error: unrecognized token: \"#\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6112bc149e6a>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Pass the stacked tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Pass the correct attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1066\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    789\u001b[0m                 )\n\u001b[1;32m    790\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    792\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    529\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Save the model\n",
        "model.push_to_hub(\n",
        "    \"your-username/mistral-7b-ppo-sql\",  # Replace with your Hugging Face username\n",
        "    commit_message=\"Upload PPO SQL model\"\n",
        ")"
      ],
      "metadata": {
        "id": "TBLCE8FBd6za"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
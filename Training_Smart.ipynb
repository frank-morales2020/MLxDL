{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPFJcqHwkl6sWdW9ygk3aYc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05f045002eeb48b5acd1e8373170c2dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdb144e29ad1451ea55c4b4bf2323b47",
              "IPY_MODEL_c2bc28394a074ad1a51c7f4d69d017c7",
              "IPY_MODEL_c725fc150a6245ceae384459c9033884"
            ],
            "layout": "IPY_MODEL_b7cab94588de4b14990aefc038e37a73"
          }
        },
        "bdb144e29ad1451ea55c4b4bf2323b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8440e54de67a4c86a131f0358ad24459",
            "placeholder": "​",
            "style": "IPY_MODEL_a7712fc83061491ea1ffab241ee736e7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c2bc28394a074ad1a51c7f4d69d017c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ca549a91ff4954885322ee1f2b9049",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8357eb4136f148ed8b44cb49fc297cbd",
            "value": 2
          }
        },
        "c725fc150a6245ceae384459c9033884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06ef98e4221e4b998dff1a4906c236dd",
            "placeholder": "​",
            "style": "IPY_MODEL_7d46a2f4d4134c1c9a16bb0fa5774916",
            "value": " 2/2 [01:19&lt;00:00, 37.20s/it]"
          }
        },
        "b7cab94588de4b14990aefc038e37a73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8440e54de67a4c86a131f0358ad24459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7712fc83061491ea1ffab241ee736e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8ca549a91ff4954885322ee1f2b9049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8357eb4136f148ed8b44cb49fc297cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06ef98e4221e4b998dff1a4906c236dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d46a2f4d4134c1c9a16bb0fa5774916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ccf3f13954c49f09086870765b98ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ab611ca5c1b4ebba3ceb1d3d5068551",
              "IPY_MODEL_c872d26f6db24c57963f55547c91103e",
              "IPY_MODEL_8a29416a47754d48a8220fb17a47f3e2"
            ],
            "layout": "IPY_MODEL_8a3e926c3c3048db9f00411be648c0d9"
          }
        },
        "9ab611ca5c1b4ebba3ceb1d3d5068551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93976a80884b4d06ad4e3146e05ce415",
            "placeholder": "​",
            "style": "IPY_MODEL_f88edf1736654e388fd7ccec96e5fafc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c872d26f6db24c57963f55547c91103e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa5065c3471444d7a9257088981ced0a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5219b27f26d341898eba69d2a2be75aa",
            "value": 2
          }
        },
        "8a29416a47754d48a8220fb17a47f3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cba40fc836a485bbe2ddf6936cf8e7c",
            "placeholder": "​",
            "style": "IPY_MODEL_b7ecd6f13abf4f8b861ecc391e56bde6",
            "value": " 2/2 [00:09&lt;00:00,  4.31s/it]"
          }
        },
        "8a3e926c3c3048db9f00411be648c0d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93976a80884b4d06ad4e3146e05ce415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f88edf1736654e388fd7ccec96e5fafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa5065c3471444d7a9257088981ced0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5219b27f26d341898eba69d2a2be75aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5cba40fc836a485bbe2ddf6936cf8e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ecd6f13abf4f8b861ecc391e56bde6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48399e954a43479eabb12c05af1e5e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e64e8d780154f4398c724455347feca",
              "IPY_MODEL_df74f2184c214cefaa5d95a1f1ad2fa9",
              "IPY_MODEL_71064880f7744c1e9a11e789e1d1b745"
            ],
            "layout": "IPY_MODEL_0d1b83ee4e4a45449f0f2d430d0ade2d"
          }
        },
        "8e64e8d780154f4398c724455347feca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38fdb887a74a4515be467721c44be6d2",
            "placeholder": "​",
            "style": "IPY_MODEL_d2627645e8e141faaff792ee4e377141",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "df74f2184c214cefaa5d95a1f1ad2fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6b375e1d564726aea9a7f0ec264c8b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edf0a475f209439cad0681b5575ee594",
            "value": 2
          }
        },
        "71064880f7744c1e9a11e789e1d1b745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bff4ec797aa948529e28c54cf8bf6438",
            "placeholder": "​",
            "style": "IPY_MODEL_843ee49bee3d4cd78feece6925ddfbaa",
            "value": " 2/2 [00:08&lt;00:00,  3.97s/it]"
          }
        },
        "0d1b83ee4e4a45449f0f2d430d0ade2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38fdb887a74a4515be467721c44be6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2627645e8e141faaff792ee4e377141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f6b375e1d564726aea9a7f0ec264c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edf0a475f209439cad0681b5575ee594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bff4ec797aa948529e28c54cf8bf6438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "843ee49bee3d4cd78feece6925ddfbaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe64627ab69d4e9e8680e967032c70a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b88c5961a694c3d9d2a2bab45bc95e4",
              "IPY_MODEL_af97a5289ec04b4ab4a1b5c9bde2643e",
              "IPY_MODEL_f0b553b6242347a5b4826bd20cae28ce"
            ],
            "layout": "IPY_MODEL_97e4ca86a8f942529ed4bfd402a73e0a"
          }
        },
        "4b88c5961a694c3d9d2a2bab45bc95e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d03f56c30cb40b9883d9f645f3cac17",
            "placeholder": "​",
            "style": "IPY_MODEL_8e8b2eb486d14738b2fb160ffbd3df76",
            "value": "model.safetensors: 100%"
          }
        },
        "af97a5289ec04b4ab4a1b5c9bde2643e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f39f1e494f6407390f4e06496805bbe",
            "max": 4975270396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d9881bd130744aa9c50b99f5e776e27",
            "value": 4975270396
          }
        },
        "f0b553b6242347a5b4826bd20cae28ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64369ae879414c53a027d0e5f7138787",
            "placeholder": "​",
            "style": "IPY_MODEL_29e1aea05c314fddbc9196c683d8a37b",
            "value": " 4.98G/4.98G [03:28&lt;00:00, 27.8MB/s]"
          }
        },
        "97e4ca86a8f942529ed4bfd402a73e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d03f56c30cb40b9883d9f645f3cac17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8b2eb486d14738b2fb160ffbd3df76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f39f1e494f6407390f4e06496805bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9881bd130744aa9c50b99f5e776e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64369ae879414c53a027d0e5f7138787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e1aea05c314fddbc9196c683d8a37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Training_Smart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U datasets transformers accelerate peft trl bitsandbytes sentencepiece interpret\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install -U bitsandbytes -q"
      ],
      "metadata": {
        "id": "jEcIdzQtJlLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation -q # Install the flash-attn package"
      ],
      "metadata": {
        "id": "A_93OVXovB9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft --upgrade  -q # Upgrade peft to the latest version"
      ],
      "metadata": {
        "id": "gi3ePKG_vW3Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsY7DoRBu2xR",
        "outputId": "c2fdd146-91be-48d9-9e2e-2f9fecfc022a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 20 05:02:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0              47W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import colab_env\n",
        "\n",
        "# 0. Login to Hugging Face Hub\n",
        "import os\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "login(token=access_token_write, add_to_git_credential=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD9fkv6aURmS",
        "outputId": "1db991fc-ccb8-4b4b-f5df-2d8101fbff8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import PPOConfig\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "import copy\n",
        "import sqlite3\n",
        "\n",
        "# 1. Load your fine-tuned Mistral model and tokenizer (using PEFT)\n",
        "PEFT_MODEL_ID = \"frankmorales2020/Mistral-7B-text-to-sql\"\n",
        "policy_model = AutoPeftModelForCausalLM.from_pretrained(PEFT_MODEL_ID)  # Load PEFT model for policy\n",
        "\n",
        "# BitsAndBytesConfig int-4 config (for reduced memory usage)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(PEFT_MODEL_ID, use_fast=True)\n",
        "\n",
        "# Assign eos_token as the pad_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 2. Load and Preprocess the  dataset (smaller subset for POC)\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"### Instruction: Translate the following natural language query into SQL. The database schema is given in the context.\\n\\n### Context: {c}\\n\\n### Query: {q}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "    targets = examples[\"answer\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "train_dataset = dataset.select(range(20))  # Further reduced dataset for POC\n",
        "\n",
        "# 3. Define a simple Reward Function (for POC purposes)\n",
        "class SQLRewardFunction:\n",
        "    def __init__(self, tokenizer, db_path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.db_path = db_path\n",
        "\n",
        "    def __call__(self, samples):\n",
        "        rewards = []\n",
        "        for sample in samples:\n",
        "            query = self.tokenizer.decode(sample, skip_special_tokens=True)\n",
        "\n",
        "            # Improved prompt removal with check\n",
        "            if \"```sql\" in query:\n",
        "                query = query.split(\"```sql\")[1].split(\"```\")[0].strip()\n",
        "            else:\n",
        "                query = \"SELECT 1\"\n",
        "\n",
        "            try:\n",
        "                conn = sqlite3.connect(self.db_path)\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute(query)\n",
        "                conn.close()\n",
        "                reward = 1.0  # Give a reward if the query executes without error\n",
        "            except Exception as e:\n",
        "                print(f\"Error executing query: {query}\\nError: {e}\")\n",
        "                reward = -1.0  # Penalize if there's an error\n",
        "            rewards.append(reward)\n",
        "        return torch.tensor(rewards)\n",
        "\n",
        "# --- Mount Google Drive (if needed) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 4. Initialize the reward function with the path to your Spider database\n",
        "db_path = \"/content/gdrive/My Drive/datasets/database.sqlite\"\n",
        "reward_fn = SQLRewardFunction(tokenizer, db_path=db_path)\n",
        "\n",
        "# 5. Define PPO configuration (reduced episodes for POC)\n",
        "ppo_config = PPOConfig(\n",
        "    output_dir=\"./ppo_results\",\n",
        "    total_episodes=5  # Further reduced for POC\n",
        ")\n",
        "\n",
        "# 6. Create a new value model\n",
        "value_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    PEFT_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "# 7. Create the optimizer (for both policy and value models)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": policy_model.parameters()},\n",
        "        {\"params\": value_model.parameters()}\n",
        "    ],\n",
        "    lr=ppo_config.learning_rate\n",
        ")\n",
        "\n",
        "# 8. Define helper functions for PPO (simplified for POC)\n",
        "def calculate_advantages(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
        "    \"\"\"Calculates advantages using Generalized Advantage Estimation (GAE).\"\"\"\n",
        "    last_advantage = 0\n",
        "    advantages = []\n",
        "    for t in reversed(range(len(rewards) - 1)):\n",
        "        delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
        "        last_advantage = delta + gamma * gae_lambda * last_advantage\n",
        "        advantages.insert(0, last_advantage)\n",
        "    return torch.tensor(advantages)\n",
        "\n",
        "def calculate_policy_loss(logits, actions, advantages):\n",
        "    \"\"\"Calculates the policy loss using the PPO objective.\"\"\"\n",
        "    # Move actions and advantages to the same device as logits\n",
        "    actions = actions.to(logits.device)\n",
        "    advantages = advantages.to(logits.device)\n",
        "\n",
        "    # Assuming you're using categorical actions (e.g., token ids)\n",
        "    cross_entropy = torch.nn.functional.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)), actions.view(-1)\n",
        "    )\n",
        "    return (cross_entropy * advantages).mean()\n",
        "\n",
        "# 9. Training loop with progress bar\n",
        "for episode in tqdm(range(ppo_config.total_episodes), desc=\"Episodes\"):\n",
        "    for batch in tqdm(train_dataset, desc=\"Batches\", leave=False):\n",
        "        # a. Generate samples from the policy\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(policy_model.device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(policy_model.device)\n",
        "\n",
        "        # Add an extra dimension for batch size\n",
        "        input_ids = input_ids.unsqueeze(0)\n",
        "        attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "        samples = policy_model.generate(  # Use policy_model for generation\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # b. Calculate rewards\n",
        "        rewards = reward_fn(samples)\n",
        "\n",
        "        # c. Compute the policy loss and value loss\n",
        "        # Get logits from the policy model\n",
        "        logits = policy_model(\n",
        "            input_ids=samples,\n",
        "            attention_mask=torch.ones_like(samples)\n",
        "        ).logits\n",
        "\n",
        "        # Get values from the value model\n",
        "        values = value_model(\n",
        "            input_ids=samples,\n",
        "            attention_mask=torch.ones_like(samples)\n",
        "        ).logits[:, -1, :]  # Assuming values are at the last position\n",
        "\n",
        "        # Find the loss tensor in the outputs tuple (this might need adjustment)\n",
        "        # For now, let's use a placeholder loss\n",
        "        value_loss = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "        # Calculate advantages\n",
        "        advantages = calculate_advantages(rewards, values)\n",
        "\n",
        "        # Calculate policy loss\n",
        "        policy_loss = calculate_policy_loss(logits, samples, advantages)\n",
        "\n",
        "        # d. Update the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Combine the losses\n",
        "        total_loss = policy_loss + value_loss\n",
        "\n",
        "        total_loss.backward()  # Call backward only once\n",
        "        optimizer.step()\n",
        "\n",
        "# 10. Save the model\n",
        "policy_model.push_to_hub(\n",
        "    repo_path_or_name=\"frankmorales2020/mistral-7b-ppo-poc-t2sql\",\n",
        "    commit_message=\"Upload PPO POC model\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574,
          "referenced_widgets": [
            "05f045002eeb48b5acd1e8373170c2dd",
            "bdb144e29ad1451ea55c4b4bf2323b47",
            "c2bc28394a074ad1a51c7f4d69d017c7",
            "c725fc150a6245ceae384459c9033884",
            "b7cab94588de4b14990aefc038e37a73",
            "8440e54de67a4c86a131f0358ad24459",
            "a7712fc83061491ea1ffab241ee736e7",
            "f8ca549a91ff4954885322ee1f2b9049",
            "8357eb4136f148ed8b44cb49fc297cbd",
            "06ef98e4221e4b998dff1a4906c236dd",
            "7d46a2f4d4134c1c9a16bb0fa5774916",
            "6ccf3f13954c49f09086870765b98ab4",
            "9ab611ca5c1b4ebba3ceb1d3d5068551",
            "c872d26f6db24c57963f55547c91103e",
            "8a29416a47754d48a8220fb17a47f3e2",
            "8a3e926c3c3048db9f00411be648c0d9",
            "93976a80884b4d06ad4e3146e05ce415",
            "f88edf1736654e388fd7ccec96e5fafc",
            "aa5065c3471444d7a9257088981ced0a",
            "5219b27f26d341898eba69d2a2be75aa",
            "5cba40fc836a485bbe2ddf6936cf8e7c",
            "b7ecd6f13abf4f8b861ecc391e56bde6"
          ]
        },
        "id": "3_7YL54HFtqF",
        "outputId": "15671b92-05a5-4b4c-afd0-203d4b0b38b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05f045002eeb48b5acd1e8373170c2dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ccf3f13954c49f09086870765b98ab4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episodes:   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Batches:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Episodes:   0%|          | 0/5 [01:57<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dd6ca556a23c>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Get values from the value model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         values = value_model(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1645\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1066\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL"
      ],
      "metadata": {
        "id": "VJLllKCiXKEA"
      }
    },
    {
      "source": [
        "!pip install numba -q\n",
        "!pip install trl transformers datasets accelerate bitsandbytes -q\n",
        "\n",
        "from numba import cuda\n",
        "\n",
        "# Attempt to reset the device before selecting it.\n",
        "cuda.close()\n",
        "\n",
        "# Selects the desired device and creates a context.\n",
        "cuda.select_device(0) # or any valid GPU index\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2q-nPDg4N3n4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import PPOConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import sqlite3\n",
        "\n",
        "# 1. Load the base Mistral model and tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Use the base model\n",
        "\n",
        "# BitsAndBytesConfig int-4 config (for reduced memory usage)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Set the desired GPU device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# or device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # device_map=\"auto\", # Remove or comment out device_map=\"auto\"\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ").to(device) # Explicitly move the model to the device\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "# Revert padding strategy to default\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure default padding token is used\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id # Ensure default padding token ID is used\n",
        "\n",
        "\n",
        "# Revert padding strategy\n",
        "# Assuming default padding token was used previously\n",
        "#tokenizer.padding_side = \"right\"\n",
        "#tokenizer.pad_token = tokenizer.unk_token\n",
        "#tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "\n",
        "# 2. Load a small dataset (for POC)\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "\n",
        "\n",
        "# Preprocess the dataset (this is the crucial missing step)\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"### Instruction: Translate the following natural language query into SQL. The database schema is given in the context.\\n\\n### Context: {c}\\n\\n### Query: {q}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "    targets = examples[\"answer\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "train_dataset = dataset.select(range(10))  # Very small dataset for POC\n",
        "\n",
        "\n",
        "# 3. Define a simple Reward Function (for POC purposes)\n",
        "class SQLRewardFunction:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, samples):\n",
        "        rewards = []\n",
        "        for sample in samples:\n",
        "            query = self.tokenizer.decode(sample, skip_special_tokens=True)\n",
        "            # Simple reward: 1 for any generated query\n",
        "            reward = 1.0\n",
        "            rewards.append(reward)\n",
        "        return torch.tensor(rewards)\n",
        "\n",
        "reward_fn = SQLRewardFunction(tokenizer)\n",
        "\n",
        "# 4. Define PPO configuration (reduced episodes for POC)\n",
        "ppo_config = PPOConfig(\n",
        "    output_dir=\"./ppo_results\",\n",
        "    total_episodes=2  # Very few episodes for POC\n",
        ")\n",
        "\n",
        "# ... (previous code) ...\n",
        "\n",
        "# 5. Create the value model head manually\n",
        "\n",
        "class ValueHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Value head for the model.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): The hidden size of the model.\n",
        "        vocab_size (int): The vocabulary size of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super().__init__()\n",
        "        self.v_head = torch.nn.Linear(hidden_size, 1)\n",
        "        self.lm_head = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        Forward pass through the value head.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): The hidden states of the model.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the logits and the value.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pass hidden states to lm_head first to get logits\n",
        "        logits = self.lm_head(hidden_states.type(torch.float32))\n",
        "\n",
        "        # Calculate the value from the hidden state directly\n",
        "        value = self.v_head(hidden_states.type(torch.float32))\n",
        "        return logits, value\n",
        "\n",
        "\n",
        "\n",
        "# Get the hidden size from the model\n",
        "hidden_size = model.config.hidden_size\n",
        "\n",
        "# Create the value head\n",
        "value_head = ValueHead(hidden_size, len(tokenizer))\n",
        "\n",
        "# Move the value head to the same device as the model\n",
        "value_head.to(model.device) # This line moves the value head to the GPU\n",
        "\n",
        "# Attach the value head to the model\n",
        "model.v_head = value_head\n",
        "\n",
        "# ... (rest of the code) ...\n",
        "\n",
        "# 6. Create the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=ppo_config.learning_rate)\n",
        "\n",
        "# 7. Define helper functions for PPO (simplified for POC)\n",
        "def calculate_advantages(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
        "    \"\"\"Calculates advantages using Generalized Advantage Estimation (GAE).\"\"\"\n",
        "    last_advantage = 0\n",
        "    advantages = []\n",
        "    for t in reversed(range(len(rewards) - 1)):\n",
        "        delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
        "        last_advantage = delta + gamma * gae_lambda * last_advantage\n",
        "        advantages.insert(0, last_advantage)\n",
        "    return torch.tensor(advantages)\n",
        "\n",
        "def calculate_policy_loss(logits, actions, advantages):\n",
        "    \"\"\"Calculates the policy loss using the PPO objective.\"\"\"\n",
        "    # Move actions and advantages to the same device as logits\n",
        "    actions = actions.to(logits.device)\n",
        "    advantages = advantages.to(logits.device)\n",
        "\n",
        "    # Assuming you're using categorical actions (e.g., token ids)\n",
        "    cross_entropy = torch.nn.functional.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)), actions.view(-1)\n",
        "    )\n",
        "    return (cross_entropy * advantages).mean()\n",
        "\n",
        "# 8. Training loop with progress bar\n",
        "for episode in tqdm(range(ppo_config.total_episodes), desc=\"Episodes\"):\n",
        "    for batch in tqdm(train_dataset, desc=\"Batches\", leave=False):\n",
        "        # a. Generate samples from the policy\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)  # Access input_ids correctly\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "\n",
        "        # Add an extra dimension for batch size\n",
        "        input_ids = input_ids.unsqueeze(0)\n",
        "        attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "        #print('\\n\\n')\n",
        "\n",
        "\n",
        "        # Check the shapes of input_ids and attention_mask before calling model.generate\n",
        "        #print(\"Input IDs shape:\", input_ids.shape)\n",
        "        #print(\"Attention Mask shape:\", attention_mask.shape)\n",
        "\n",
        "        #print('\\n\\n')\n",
        "\n",
        "        #print(\"Unique values in input_ids:\", torch.unique(input_ids))\n",
        "       #print(\"Unique values in attention_mask:\", torch.unique(attention_mask))\n",
        "\n",
        "\n",
        "\n",
        "        samples = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=10,  # Reduced max_new_tokens for POC\n",
        "            #do_sample=True,\n",
        "            #top_k=50,\n",
        "            #top_p=0.95,\n",
        "            #temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # b. Calculate rewards\n",
        "        rewards = reward_fn(samples)\n",
        "\n",
        "\n",
        "        # ... inside the training loop ...\n",
        "        # c. Compute the policy loss and value loss\n",
        "        outputs = model(\n",
        "            input_ids=samples,\n",
        "            attention_mask=torch.ones_like(samples),\n",
        "            labels=samples.type(torch.long),  # Convert samples to torch.long\n",
        "            output_hidden_states=True  # Add this line to get hidden states\n",
        "        )\n",
        "\n",
        "        # Get the hidden states for the value head\n",
        "        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n",
        "\n",
        "        # Get logits and values from the custom value head using the hidden states\n",
        "        logits, values = model.v_head(hidden_states)\n",
        "\n",
        "\n",
        "        # Find the loss tensor in the outputs tuple (this might need adjustment)\n",
        "        value_loss = outputs.loss\n",
        "\n",
        "        # Calculate advantages\n",
        "        advantages = calculate_advantages(rewards, values)\n",
        "\n",
        "        # Calculate policy loss\n",
        "        policy_loss = calculate_policy_loss(logits, samples, advantages)\n",
        "\n",
        "        # d. Update the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Combine the losses\n",
        "        total_loss = policy_loss + value_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "48399e954a43479eabb12c05af1e5e98",
            "8e64e8d780154f4398c724455347feca",
            "df74f2184c214cefaa5d95a1f1ad2fa9",
            "71064880f7744c1e9a11e789e1d1b745",
            "0d1b83ee4e4a45449f0f2d430d0ade2d",
            "38fdb887a74a4515be467721c44be6d2",
            "d2627645e8e141faaff792ee4e377141",
            "9f6b375e1d564726aea9a7f0ec264c8b",
            "edf0a475f209439cad0681b5575ee594",
            "bff4ec797aa948529e28c54cf8bf6438",
            "843ee49bee3d4cd78feece6925ddfbaa"
          ]
        },
        "id": "RCTphIDFA7QC",
        "outputId": "8b1c2572-3c1c-4274-a682-16884d2f5577"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48399e954a43479eabb12c05af1e5e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n",
            "Episodes:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Batches:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Batches:  10%|█         | 1/10 [00:01<00:10,  1.12s/it]\u001b[A\n",
            "Batches:  20%|██        | 2/10 [00:02<00:08,  1.11s/it]\u001b[A\n",
            "Batches:  30%|███       | 3/10 [00:03<00:07,  1.12s/it]\u001b[A\n",
            "Batches:  40%|████      | 4/10 [00:04<00:06,  1.12s/it]\u001b[A\n",
            "Batches:  50%|█████     | 5/10 [00:05<00:05,  1.12s/it]\u001b[A\n",
            "Batches:  60%|██████    | 6/10 [00:06<00:04,  1.19s/it]\u001b[A\n",
            "Batches:  70%|███████   | 7/10 [00:08<00:03,  1.19s/it]\u001b[A\n",
            "Batches:  80%|████████  | 8/10 [00:09<00:02,  1.16s/it]\u001b[A\n",
            "Batches:  90%|█████████ | 9/10 [00:10<00:01,  1.15s/it]\u001b[A\n",
            "Batches: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\u001b[A\n",
            "Episodes:  50%|█████     | 1/2 [00:11<00:11, 11.45s/it]\n",
            "Batches:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Batches:  10%|█         | 1/10 [00:01<00:09,  1.09s/it]\u001b[A\n",
            "Batches:  20%|██        | 2/10 [00:02<00:08,  1.11s/it]\u001b[A\n",
            "Batches:  30%|███       | 3/10 [00:03<00:07,  1.11s/it]\u001b[A\n",
            "Batches:  40%|████      | 4/10 [00:04<00:06,  1.12s/it]\u001b[A\n",
            "Batches:  50%|█████     | 5/10 [00:05<00:05,  1.12s/it]\u001b[A\n",
            "Batches:  60%|██████    | 6/10 [00:06<00:04,  1.13s/it]\u001b[A\n",
            "Batches:  70%|███████   | 7/10 [00:07<00:03,  1.13s/it]\u001b[A\n",
            "Batches:  80%|████████  | 8/10 [00:09<00:02,  1.15s/it]\u001b[A\n",
            "Batches:  90%|█████████ | 9/10 [00:10<00:01,  1.14s/it]\u001b[A\n",
            "Batches: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\u001b[A\n",
            "Episodes: 100%|██████████| 2/2 [00:22<00:00, 11.39s/it]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 9. Save the model (optional for POC)\n",
        "model.push_to_hub(\n",
        "    \"frankmorales2020/mistral-7b-ppo-poc-t2sql\",  # Provide repo_id as positional argument\n",
        "    commit_message=\"Upload PPO POC model\"\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "fe64627ab69d4e9e8680e967032c70a2",
            "4b88c5961a694c3d9d2a2bab45bc95e4",
            "af97a5289ec04b4ab4a1b5c9bde2643e",
            "f0b553b6242347a5b4826bd20cae28ce",
            "97e4ca86a8f942529ed4bfd402a73e0a",
            "8d03f56c30cb40b9883d9f645f3cac17",
            "8e8b2eb486d14738b2fb160ffbd3df76",
            "5f39f1e494f6407390f4e06496805bbe",
            "1d9881bd130744aa9c50b99f5e776e27",
            "64369ae879414c53a027d0e5f7138787",
            "29e1aea05c314fddbc9196c683d8a37b"
          ]
        },
        "id": "-1Cv6fxbcRxs",
        "outputId": "da83f1bb-09f1-4da2-f3d8-f1c798efc935"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe64627ab69d4e9e8680e967032c70a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/frankmorales2020/mistral-7b-ppo-poc-t2sql/commit/b9fc1f4e9470e875bcc56e8668042f35d1ecff7e', commit_message='Upload PPO POC model', commit_description='', oid='b9fc1f4e9470e875bcc56e8668042f35d1ecff7e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/frankmorales2020/mistral-7b-ppo-poc-t2sql', endpoint='https://huggingface.co', repo_type='model', repo_id='frankmorales2020/mistral-7b-ppo-poc-t2sql'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPROVEMENT"
      ],
      "metadata": {
        "id": "_ZtD0u_8d_5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import PPOConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import sqlite3\n",
        "\n",
        "# 1. Load the base Mistral model and tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Set the desired GPU device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ").to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "# Revert padding strategy to default\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 2. Load and preprocess the dataset\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"### Instruction: Translate the following natural language query into SQL. The database schema is given in the context.\\n\\n### Context: {c}\\n\\n### Query: {q}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "    targets = examples[\"answer\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "train_dataset = dataset.select(range(100))  # Increased dataset size\n",
        "\n",
        "# 3. Define a more informative Reward Function\n",
        "def execute_query(query, db_path=\"database.db\"):\n",
        "    \"\"\"Executes a SQL query against a SQLite database.\"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(query)\n",
        "        conn.close()\n",
        "        return True  # Successful execution\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing query: {e}\")\n",
        "        return False  # Failed execution\n",
        "\n",
        "class SQLRewardFunction:\n",
        "    def __init__(self, tokenizer, db_path=\"database.db\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.db_path = db_path\n",
        "\n",
        "    def __call__(self, samples, **kwargs):\n",
        "        rewards = []\n",
        "        for sample in samples:\n",
        "            query = self.tokenizer.decode(sample, skip_special_tokens=True)\n",
        "            # Reward based on execution success\n",
        "            if execute_query(query, self.db_path):\n",
        "                reward = 1.0  # Successful execution\n",
        "            else:\n",
        "                reward = -1.0  # Failed execution\n",
        "            rewards.append(reward)\n",
        "        return torch.tensor(rewards)\n",
        "\n",
        "reward_fn = SQLRewardFunction(tokenizer)\n",
        "\n",
        "# 4. Define PPO configuration\n",
        "ppo_config = PPOConfig(\n",
        "    output_dir=\"./ppo_results\",\n",
        "    total_episodes=10  # Increased number of episodes\n",
        ")\n",
        "\n",
        "# 5. Create and attach the value model head\n",
        "class ValueHead(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super().__init__()\n",
        "        self.v_head = torch.nn.Linear(hidden_size, 1)\n",
        "        self.lm_head = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        logits = self.lm_head(hidden_states.type(torch.float32))\n",
        "        value = self.v_head(hidden_states.type(torch.float32))\n",
        "        return logits, value\n",
        "\n",
        "hidden_size = model.config.hidden_size\n",
        "value_head = ValueHead(hidden_size, len(tokenizer))\n",
        "value_head.to(model.device)\n",
        "model.v_head = value_head\n",
        "\n",
        "# 6. Create the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=ppo_config.learning_rate)\n",
        "\n",
        "# 7. Define helper functions for PPO\n",
        "def calculate_advantages(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
        "    last_advantage = 0\n",
        "    advantages = []\n",
        "    for t in reversed(range(len(rewards) - 1)):\n",
        "        delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
        "        last_advantage = delta + gamma * gae_lambda * last_advantage\n",
        "        advantages.insert(0, last_advantage)\n",
        "    return torch.tensor(advantages)\n",
        "\n",
        "def calculate_policy_loss(logits, actions, advantages):\n",
        "    actions = actions.to(logits.device)\n",
        "    advantages = advantages.to(logits.device)\n",
        "    cross_entropy = torch.nn.functional.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)), actions.view(-1)\n",
        "    )\n",
        "    return (cross_entropy * advantages).mean()\n",
        "\n",
        "# 8. Training loop\n",
        "for episode in tqdm(range(ppo_config.total_episodes), desc=\"Episodes\"):\n",
        "    for batch in tqdm(train_dataset, desc=\"Batches\", leave=False):\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "\n",
        "        # Add an extra dimension for batch size\n",
        "        input_ids = input_ids.unsqueeze(0)\n",
        "        attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "        samples = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=50,  # Increased max_new_tokens\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        rewards = reward_fn(samples)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=samples,\n",
        "            attention_mask=torch.ones_like(samples),\n",
        "            labels=samples.type(torch.long),\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "        logits, values = model.v_head(hidden_states)\n",
        "\n",
        "        value_loss = outputs.loss\n",
        "        advantages = calculate_advantages(rewards, values)\n",
        "        policy_loss = calculate_policy_loss(logits, samples, advantages)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = policy_loss + value_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "5k4MU45Wd0gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Save the model\n",
        "model.push_to_hub(\n",
        "    \"your-username/mistral-7b-ppo-sql\",  # Replace with your Hugging Face username\n",
        "    commit_message=\"Upload PPO SQL model\"\n",
        ")"
      ],
      "metadata": {
        "id": "TBLCE8FBd6za"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
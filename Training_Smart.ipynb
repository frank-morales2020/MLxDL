{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD/Z9q2CAG+cUWhM2jf7UM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Training_Smart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U datasets transformers accelerate peft trl bitsandbytes sentencepiece interpret\n",
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "jEcIdzQtJlLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "EzYNj0_VL93B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from interpret.glassbox import ExplainableBoostingClassifier  # For the EBM\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # For EBM\n",
        "# Add necessary libraries for RLHF (e.g., trl, stable-baselines3)\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead"
      ],
      "metadata": {
        "id": "lg6FvMNkM3ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_XlOb6KGmUM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from torch.nn import Module, Linear, ReLU, MSELoss\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_int8_training\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 0. Login to Hugging Face Hub\n",
        "login(token=\"YOUR_HUGGING_FACE_TOKEN\")  # Replace with your actual token\n",
        "\n",
        "# 1. Load Mistral model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        llm_int8_threshold=6.0\n",
        "    ),\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# 2. Load Spider dataset\n",
        "spider_dataset = load_dataset(\"spider\")\n",
        "\n",
        "# 3. Preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"Translate to SQL: {q}\" for q in examples[\"question\"]]\n",
        "    targets = examples[\"query\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "spider_dataset = spider_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=spider_dataset[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "# 4. Split the dataset\n",
        "train_dataset = spider_dataset[\"train\"].select(range(1000))  # Smaller subset for POC\n",
        "eval_dataset = spider_dataset[\"validation\"].select(range(100))\n",
        "\n",
        "# 5. Define the Energy-Based Model (EBM) component\n",
        "class SimpleEBM(Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = Linear(hidden_dim, 1)  # Output a single energy value\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        energy = self.linear2(x)\n",
        "        return energy.squeeze()  # Return a scalar energy value\n",
        "\n",
        "# 6. Combine Mistral and EBM\n",
        "class HybridT2SQL(Module):\n",
        "    def __init__(self, mistral_model, ebm):\n",
        "        super().__init__()\n",
        "        self.mistral_model = mistral_model\n",
        "        self.ebm = ebm\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Get Mistral output\n",
        "        mistral_output = self.mistral_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = mistral_output.logits\n",
        "\n",
        "        # Calculate energy using EBM\n",
        "        energy = self.ebm(hidden_states)\n",
        "\n",
        "        if labels is not None:\n",
        "            # Calculate loss (MSE loss for demonstration)\n",
        "            loss_fn = MSELoss()\n",
        "            # Calculate the \"target energy\" (e.g., 0 for correct sequences)\n",
        "            target_energy = torch.zeros_like(energy)\n",
        "            loss = loss_fn(energy, target_energy) + mistral_output.loss  # Combine with Mistral's loss\n",
        "            return loss\n",
        "        else:\n",
        "            return energy\n",
        "\n",
        "# 7. Initialize the model\n",
        "input_dim = mistral_model.config.hidden_size\n",
        "hidden_dim = 256\n",
        "ebm = SimpleEBM(input_dim, hidden_dim)\n",
        "model = HybridT2SQL(mistral_model, ebm)\n",
        "\n",
        "# 7.5 Prepare for int8 training and apply LoRA\n",
        "model = prepare_model_for_int8_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 8. Define TrainingArguments with PEFT\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_exact_match\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"your-username/mistral-7b-finetuned-t2sql\",  # Your Hub model ID\n",
        "    # Add more arguments as needed for pushing to Hub (e.g., push_to_hub_token)\n",
        ")\n",
        "\n",
        "# 9. Define Reward Function/Class (Example)\n",
        "class SQLRewardFunction:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, samples):\n",
        "        # 1. Decode the generated SQL queries\n",
        "        decoded_queries = [\n",
        "            self.tokenizer.decode(sample, skip_special_tokens=True)\n",
        "            for sample in samples\n",
        "        ]\n",
        "\n",
        "        # 2. Evaluate the SQL queries (using a metric or your logic)\n",
        "        rewards = []\n",
        "        for query in decoded_queries:\n",
        "            try:\n",
        "                # Example: Check if the query is syntactically correct\n",
        "                # You can use a SQL parser or your own logic here\n",
        "                # ...\n",
        "                reward = 1.0  # Assign a reward of 1.0 if correct\n",
        "            except:\n",
        "                reward = 0.0  # Assign a reward of 0.0 if incorrect\n",
        "            rewards.append(reward)\n",
        "\n",
        "        return torch.tensor(rewards)\n",
        "\n",
        "# 10. Initialize the reward function\n",
        "reward_fn = SQLRewardFunction(tokenizer)\n",
        "\n",
        "# 11. Define SFTTrainer for RLHF\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",  # Assuming your dataset has a \"text\" field\n",
        "    reward_model=reward_fn,  # Pass the reward function/class\n",
        "    # Add other RLHF configurations here\n",
        ")\n",
        "\n",
        "# 12. Train the model with RLHF\n",
        "trainer.train()"
      ]
    }
  ]
}
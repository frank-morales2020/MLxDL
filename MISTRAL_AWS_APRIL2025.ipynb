{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk7aDDs//6FQKMLSnt+x9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_AWS_APRIL2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaWcS7jcG5VF"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install sagemaker boto3 --quiet\n",
        "\n",
        "#%pip install langchain==0.0.309 --quiet --root-user-action=ignore\n",
        "%pip install langchain --quiet\n",
        "\n",
        "import colab_env\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#print(aws_access_key_id)\n",
        "#print()\n",
        "#print(f\"aws_access_key_id: '{aws_access_key_id}'\")\n",
        "#print(f\"aws_secret_access_key: '{aws_secret_access_key}'\")\n",
        "\n",
        "#print(f\"region: '{region}'\")\n",
        "#print()"
      ],
      "metadata": {
        "id": "AcZLELCdKFcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama-2\n",
        "https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "claude-3\n",
        "https://aws.amazon.com/blogs/aws/anthropics-claude-3-sonnet-foundation-model-is-now-available-in-amazon-bedrock/\n",
        "\n",
        "mistral 8x7b https://aws.amazon.com/blogs/machine-learning/mixtral-8x7b-is-now-available-in-amazon-sagemaker-jumpstart/\n"
      ],
      "metadata": {
        "id": "AuY8PGbLM9QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import boto3\n",
        "import os\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "\n",
        "llm_model_id = 'huggingface-llm-mistral-7b'\n",
        "llm_model_version = '2.1.0'\n",
        "\n",
        "\n",
        "llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version, role=ROLE_ARN, region='us-east-1')\n",
        "\n",
        "llm_predictor = llm_model.deploy(accept_eula=True)\n"
      ],
      "metadata": {
        "id": "5P1dwGh5KWRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the model endpoint NAME, not the ARN\n",
        "llm_model_endpoint_name = llm_predictor.endpoint_name\n",
        "llm_model_endpoint_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2XRGT2iULD1w",
        "outputId": "a6852d15-3967-4b12-d3f9-61f6c04e7026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hf-llm-mistral-7b-2025-04-07-07-32-25-889'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### MISTRAL ###### CASE#1\n",
        "import json\n",
        "#query = \"who is the best French Poet?\"\n",
        "query = \"Write a program to compute factorial in python:\"\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# Prepare the input for the model\n",
        "#input_data = {\"inputs\": query}\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "#print(response_body)\n",
        "\n",
        "print(f'Query:', query)\n",
        "print()\n",
        "print(f'Response:', response_body[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNb1GPq0TN96",
        "outputId": "a8e58cca-9155-4e98-90c3-59486f6c707e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Write a program to compute factorial in python:\n",
            "\n",
            "Response: \n",
            "\n",
            "Factorial of a given number is the product of that number and the numbers stemming from it up to 1. is a number!n ⁢ n − 1 ⁢ n − 2 ⁢ . ⁢ . ⁢ . ⁢ 1\n",
            "where n is a non-negative integer.\n",
            "\n",
            "Note: !n = 1 when n = 0.\n",
            "\n",
            "You are required to design and implement the above algorithm in python.\n",
            "\n",
            "#include <stdio.h>\n",
            "int factorial(int number)\n",
            "{\n",
            "int ans = 1;\n",
            "for (int i = 2; i <= number; i++) {\n",
            "ans *= i;\n",
            "}\n",
            "return ans;\n",
            "}\n",
            "int main()\n",
            "{\n",
            "printf(\"%d\\n\", factorial(6));\n",
            "return 0;\n",
            "}\n",
            "\n",
            "int factorial(int n)\n",
            "{\n",
            "if (n==0) return 1;\n",
            "return n * factorial(n-1);\n",
            "}\n",
            "int main()\n",
            "{\n",
            "printf(\"%d\\n\", factorial(6));\n",
            "return 0;\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### MISTRAL ###### CASE#2\n",
        "\n",
        "#query = \"who is the best French Poet?\"\n",
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# Prepare the input for the model\n",
        "#input_data = {\"inputs\": query}\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "#print(response_body)\n",
        "\n",
        "print(f'Query:', query)\n",
        "print()\n",
        "print(f'Response:', response_body[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vj8eTH3LYYD",
        "outputId": "327af02c-14d2-4423-a6cf-988335c47f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "Response: \n",
            "\n",
            "Solution:-\n",
            "\n",
            "Number of kids 6\n",
            "\n",
            "Price of ice cream $1.25 each\n",
            "\n",
            "Price = $1.25 * 6 = $7.50\n",
            "\n",
            "Money paid =10\n",
            "\n",
            "So, the amount of money remaining = $10-$7.50 = $2.50\n",
            "\n",
            "So, the amount of money received is $2.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEAN UP"
      ],
      "metadata": {
        "id": "yHCOvGVoLoRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mnSz_gVLtPS",
        "outputId": "a6084fc3-4282-4599-91a7-6c64e99a456e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "EndpointName\n",
            "hf-llm-mistral-7b-2025-04-07-07-32-25-889\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "ModelName\n",
            "hf-llm-mistral-7b-2025-04-07-07-32-25-887\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "EndpointConfigName\n",
            "hf-llm-mistral-7b-2025-04-07-07-32-25-889\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
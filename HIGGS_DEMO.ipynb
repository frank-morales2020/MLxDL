{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOCHFlelpeMLKGU1u3VvmRj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/HIGGS_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "C4yFi3mIgckB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "SxMrp_0P8dQR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "\n",
        "\n",
        "try:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Explicitly set the padding token if it's not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token  # Or a new token like '[PAD]'\n",
        "    print(f\"Successfully loaded {model_name} on {device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading {model_name}: {e}\")\n",
        "    print(\"Make sure you have access to this model and the necessary libraries are installed.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "LsQ0jvZx3ZnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def calculate_perplexity(model, encodings):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_log_prob = 0\n",
        "    total_num_tokens = 0\n",
        "    max_length = encodings.input_ids.size(1)  # Set max_length to the actual sequence length\n",
        "    with torch.no_grad():\n",
        "        # Remove the loop, process the entire sequence at once\n",
        "        input_ids = encodings.input_ids\n",
        "        attention_mask = encodings.attention_mask\n",
        "        # The labels should be shifted by one position to the left and the last token should be discarded\n",
        "        labels = encodings.input_ids[:, 1:].contiguous()  # Shift labels here\n",
        "        # Remove the last token from input_ids and attention_mask\n",
        "        input_ids = input_ids[:, :-1].contiguous()\n",
        "        attention_mask = attention_mask[:, :-1].contiguous()\n",
        "\n",
        "        # Ensure labels are the same shape as logits (after shifting)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Logits shape: [batch_size, sequence_length, vocab_size]\n",
        "        # Labels shape: [batch_size, sequence_length]\n",
        "        # We need to shift and flatten to compare them correctly\n",
        "        logits = outputs.logits\n",
        "        shift_logits = logits.contiguous().view(-1, logits.size(-1))\n",
        "        shift_labels = labels.contiguous().view(-1)  # Labels already shifted\n",
        "\n",
        "        # Calculate log probabilities\n",
        "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
        "        # Gather log probs for the actual labels\n",
        "        token_log_probs = log_probs.gather(dim=1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Weight by attention mask to exclude padding\n",
        "        total_log_prob += (token_log_probs * attention_mask.contiguous().view(-1)).sum()\n",
        "        total_num_tokens += attention_mask.sum().item()\n",
        "\n",
        "        # Perplexity is exp(-log_likelihood / num_tokens)\n",
        "        avg_log_prob = total_log_prob / total_num_tokens\n",
        "        perplexity = torch.exp(-avg_log_prob)\n",
        "\n",
        "    return perplexity"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "swplPCBMzw2q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "import numpy as np\n",
        "\n",
        "def hadamard_transform_1d(x):\n",
        "    n = x.shape[0]\n",
        "    if n == 1:\n",
        "        return x\n",
        "    h_half = hadamard_transform_1d(x[:n//2])\n",
        "    return torch.cat((h_half + x[n//2:], h_half - x[n//2:]), dim=0)\n",
        "\n",
        "def apply_standard_hadamard(weight):\n",
        "    original_shape = weight.shape\n",
        "    flattened = weight.flatten()\n",
        "    n = flattened.shape[0]\n",
        "    if np.log2(n).is_integer():\n",
        "        transformed = hadamard_transform_1d(flattened)\n",
        "        return transformed.reshape(original_shape)\n",
        "    else:\n",
        "        next_power_of_2 = 2**int(np.ceil(np.log2(n)))\n",
        "        padding = torch.zeros(next_power_of_2 - n, device=weight.device)\n",
        "        padded = torch.cat((flattened, padding))\n",
        "        transformed_padded = hadamard_transform_1d(padded)\n",
        "    return transformed_padded[:n].reshape(original_shape)\n",
        "\n",
        "def gaussian_inspired_quantize(tensor, num_bits=4):\n",
        "    num_levels = 2**num_bits\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "    scale = (max_val - min_val) / (num_levels - 1)\n",
        "    #zero_point = min_val / scale  # Not used in this quantization method\n",
        "    normalized = (tensor - min_val) / scale\n",
        "    quantized = torch.round(torch.clamp(normalized, 0, num_levels - 1)).type(torch.int8)\n",
        "    dequantized = (quantized.float() * scale) + min_val\n",
        "    return quantized, dequantized"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U4NsAgD4RdPA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def apply_conceptual_higgs_to_MODEL(model, num_bits=4, apply_hadamard=False, apply_quantize=False, device=\"cuda\"):\n",
        "    \"\"\"Applies the conceptual HIGGS compression to a model directly (in-place).\n",
        "    \"\"\"\n",
        "    compressed_model = copy.deepcopy(model)\n",
        "\n",
        "    for name, module in compressed_model.named_modules():\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            if hasattr(module, 'weight') and module.weight.requires_grad:\n",
        "                original_weights = module.weight.data.cpu().clone().detach()\n",
        "\n",
        "                #print(f\"\\nProcessing layer: {name}.weight\")\n",
        "                #print(f\"  Original weights: min={original_weights.min()}, max={original_weights.max()}\")\n",
        "\n",
        "                transformed_weights = original_weights\n",
        "                if apply_hadamard:\n",
        "                    transformed_weights = apply_standard_hadamard(transformed_weights)\n",
        "                    #print(f\"  After Hadamard: min={transformed_weights.min()}, max={transformed_weights.max()}\")\n",
        "\n",
        "                if apply_quantize:\n",
        "                    quantized_weights, dequantized_weights = gaussian_inspired_quantize(transformed_weights, num_bits)\n",
        "                    #print(f\"  After Quantization: min={quantized_weights.min()}, max={quantized_weights.max()}\")\n",
        "\n",
        "                    # Do not assign int8 data. Directly use Hadamard Transformed Data\n",
        "                    #module.weight.data = quantized_weights.type(torch.int8).to(device)\n",
        "                    module.weight.data = dequantized_weights.to(device)  # Use dequantized weights\n",
        "\n",
        "                else:  # If not applying quantization, use the transformed weights directly\n",
        "                    module.weight.data = transformed_weights.to(device)\n",
        "\n",
        "                #print(f\"Applied conceptual HIGGS (Hadamard={apply_hadamard}, Quantize={apply_quantize}) to {name}.weight\")\n",
        "                del original_weights\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return compressed_model"
      ],
      "metadata": {
        "id": "1KTxUbCbVaJR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "source": [
        "import sys\n",
        "\n",
        "def get_model_size(model):\n",
        "  \"\"\"Calculate the size of the model in MB.\"\"\"\n",
        "  param_size = 0\n",
        "  for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "  buffer_size = 0\n",
        "  for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "  size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "  print('Model Size: {:.3f} MB'.format(size_all_mb))\n",
        "  return (param_size + buffer_size)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2bHTJ6NMPZCL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "source": [
        "# --- Evaluation Code ---\n",
        "\n",
        "# 1. Prepare Evaluation Data (replace with your actual dataset)\n",
        "eval_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    # Add more text sequences here\n",
        "]\n",
        "\n",
        "# Tokenize the Eval Data\n",
        "eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 3. Get Original Model Size\n",
        "original_size = get_model_size(model)\n",
        "print(f\"\\nOriginal Model Size: {original_size} parameters\")\n",
        "print('\\n\\n')\n",
        "\n",
        "# 4. Apply HIGGS and get the compressed model\n",
        "print(\"\\n--- Applying Conceptual HIGGS ---\")\n",
        "compressed_model = apply_conceptual_higgs_to_MODEL( # Call the function to modify the model in-place\n",
        "    model,\n",
        "    #num_bits=4,\n",
        "    apply_hadamard=True,\n",
        "    apply_quantize=True,\n",
        "    device=device # Pass the device to the function\n",
        "    #target_layers=[\"lm_head.weight\", \"model.layers.0.self_attn.q_proj.weight\"]\n",
        ")\n",
        "\n",
        "# Save the original model's state_dict\n",
        "torch.save(model.state_dict(), 'original_model.pth')\n",
        "\n",
        "# Save the compressed model's state_dict\n",
        "torch.save(compressed_model.state_dict(), 'compressed_model.pth')\n",
        "\n",
        "# Load the quantized compressed model's state_dict\n",
        "compressed_state_dict = torch.load('compressed_model.pth', map_location=device) # Load on the correct device\n",
        "compressed_model = model.from_pretrained(model_name).to(device) # Instantiate and move to device\n",
        "compressed_model.load_state_dict(compressed_state_dict)\n",
        "\n",
        "# 4. Evaluate Original Model (Baseline)\n",
        "print(\"\\n--- Evaluating Original Model (Baseline) ---\")\n",
        "original_perplexity = calculate_perplexity(model, eval_encodings)\n",
        "print(f\"Original Model Perplexity: {original_perplexity:.4f}\")\n",
        "\n",
        "# Get Compressed Model Size\n",
        "compressed_size = get_model_size(compressed_model)\n",
        "print(f\"\\nCompressed Model Size: {compressed_size} parameters\")\n",
        "\n",
        "# Evaluate Compressed Model\n",
        "print(\"\\n--- Evaluating Compressed Model ---\")\n",
        "start_time = time.time()  # Record start time\n",
        "compressed_perplexity = calculate_perplexity(compressed_model, eval_encodings)\n",
        "end_time = time.time()  # Record end time\n",
        "inference_time = end_time - start_time\n",
        "print(f\"Compressed Model Perplexity: {compressed_perplexity:.4f}\")\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "\n",
        "# Compare and Report\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Change in Perplexity: {compressed_perplexity - original_perplexity:.4f}\")\n",
        "print(f\"Percentage Change in Perplexity: {(compressed_perplexity / original_perplexity - 1) * 100:.2f}%\")\n",
        "print(f\"Change in Model Size: {compressed_size - original_size} parameters\")\n",
        "print(f\"Percentage Change in Model Size: {(compressed_size / original_size - 1) * 100:.2f}%\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aB_zJKzVR1T4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
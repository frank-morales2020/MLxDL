{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Meta_Llama_3_8B_for_MEDAL_EVALDATA_PONEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3kFUczO5Um3"
      },
      "source": [
        "https://llama.meta.com/docs/how-to-guides/fine-tuning/\n",
        "\n",
        "\n",
        "https://medium.com/@frankmorales_91352/fine-tuning-meta-llama-3-8b-with-medal-a-refined-approach-for-enhanced-medical-language-b924d226b09d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etm0HfcZM151"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "! pip install trl==0.8.6 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65JpttRiGxVK"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37yP0eJDM152"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9qvxG2HYusD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n"
      ],
      "metadata": {
        "id": "j8fUYmuIFU1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naQrZIQTY1wG"
      },
      "outputs": [],
      "source": [
        "# set device\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtE9TuSMlUEc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3fbe66b-af16-42c5-b6f6-29b79a26f13f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GapRov3hl4fT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6932df61-8dfa-4e92-c17a-0bb3a8e52ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Wed May  7 10:23:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             44W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-7fKxR2rnbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23bad75-f0cb-435d-8ead-3a6b7e2b28cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': '<prompt text>', 'completion': '<ideal generated text>'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "### conversational format\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "\n",
        "### instruction format\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCd7-R-lsLQm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/train_dataset.json\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-kFtkh8kiOn",
        "outputId": "7977e457-0ce5-4466-fa51-f505f45be861"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['abstract_id', 'text', 'location', 'label'],\n",
              "    num_rows: 3000000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahaI2KNwYcQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d3438f-58e1-4476-c1d3-bc9ebb926988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to quantify the amount of CL NO harvested in modified RND\n"
          ]
        }
      ],
      "source": [
        "print(dataset[6]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMbtvPqFkyFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995d4743-7f9f-493d-8999-188942d0ee70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['radical neck dissection']\n"
          ]
        }
      ],
      "source": [
        "print(dataset[6]['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgY7jf_8_Plh"
      },
      "outputs": [],
      "source": [
        "dataset_test = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufPzSf64LDux",
        "outputId": "7aa693a3-911c-45cd-fa51-66066da25d70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['abstract_id', 'text', 'location', 'label'],\n",
              "    num_rows: 1000000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXZmSkhz-5sI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def dataset_to_finetuning(dataset,nrec):\n",
        "\n",
        "    question=dataset['text']\n",
        "    answer_entry=dataset['label']\n",
        "\n",
        "\n",
        "    !rm -rf /content/question_answering.txt\n",
        "    filename='/content/question_answering.txt'\n",
        "\n",
        "\n",
        "  # python object to be appended\n",
        "    for n in range(nrec):\n",
        "        #print(answer[n])\n",
        "        if answer_entry[n] == None:\n",
        "           answer_entry[n] = 'Not possible to get or use'\n",
        "        string = answer_entry[n]\n",
        "\n",
        "        str_question=question[n]\n",
        "        question_final= re.sub(\"\\n\", \"\", str_question)\n",
        "\n",
        "        answer = re.sub(\"\\[\", \"\", str(string))\n",
        "        answer = re.sub(\"\\]\", \"\", str(answer))\n",
        "        answer0=answer[1:len(answer)-1]\n",
        "\n",
        "        if len(answer0)==0:\n",
        "            answer='Not possible to get or use'\n",
        "\n",
        "        #print(answer0)\n",
        "\n",
        "\n",
        "        text='<s>[INST] %s [/INST] %s </s>'%(question_final,answer0)\n",
        "        del string,answer0\n",
        "        #print(text)\n",
        "        with open(filename, 'a') as f:\n",
        "            f.write(text + '\\n')\n",
        "        f.close()\n",
        "\n",
        "    text0 = load_dataset(\"text\", data_files=\"/content/question_answering.txt\", split=\"train\")\n",
        "\n",
        "    dataset_final_Question=dataset['text'][0:nrec]\n",
        "    dataset_final_Answer=dataset['label'][0:nrec]\n",
        "    dataset_final_text=text0[0:nrec]['text']\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    datasetF = pd.DataFrame() # Create an empty DataFrame\n",
        "    datasetF['Question'] = dataset_final_Question\n",
        "    datasetF['Answer'] = dataset_final_Answer\n",
        "    datasetF['text'] = dataset_final_text\n",
        "\n",
        "\n",
        "    return datasetF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oGlbglrGKBl"
      },
      "outputs": [],
      "source": [
        "datasetF=dataset_to_finetuning(dataset,500000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zapOFE_JUdD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0ebcbbb-63cd-4c9c-b5c1-dfe87cd778a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to quantify the amount of CL NO harvested in modified RND'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "datasetF['Question'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lokPbQcdJbI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afbb2f0e-492c-4f0a-c1b4-a7a8abebe961"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['radical neck dissection']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "datasetF['Answer'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPmT7AExJnir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8cdbcb9e-fb8a-4074-921d-c79b530feed4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] to quantify the amount of CL NO harvested in modified RND [/INST] radical neck dissection </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "datasetF['text'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1ChV5-FJE5M"
      },
      "outputs": [],
      "source": [
        "datasetF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh4mv6NpGh-9"
      },
      "outputs": [],
      "source": [
        "datasetF_test=dataset_to_finetuning(dataset_test,20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnGkIpVtJHvv"
      },
      "outputs": [],
      "source": [
        "datasetF_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQdFE5s1KC9o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "0bd7faa1-d4cf-4274-8f49-c3d927c2304f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "datasetF_test['Question'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPsMncjDKK4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10099190-f5c2-4b15-ae75-7be16abc9ec4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['antral follicle count']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "datasetF_test['Answer'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_B8AVYsKPHO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "caf6243a-9108-42e7-a1b0-85b1e9d2b12c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice [/INST] antral follicle count </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "datasetF_test['text'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQm2yo0uslTs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" #01 march 2024 AND 10/03/2024\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\" #22 MAY 2024\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxOWr2VvEGKe"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjyYx9WmtMmA"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=64,\n",
        "        lora_dropout=0.05,\n",
        "        r=128,\n",
        "        bias=\"none\",\n",
        "        target_modules=\"all-linear\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkMVebosM16A"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    output_dir=\"/content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\",\n",
        "\n",
        "    #num_train_epochs=3,                     # number of training epochs\n",
        "    #num_train_epochs=0.3,                    # number of training epochs for POC\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,          # batch size per device during training\n",
        "    #2\n",
        "    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    #gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "\n",
        "    warmup_ratio=0.15,\n",
        "    weight_decay=0.05,\n",
        "\n",
        "    logging_steps=100,                       # log every 10 steps\n",
        "    learning_rate=5e-5,                     # learning rate, based on QLoRA paper # i used in the first model\n",
        "\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    max_grad_norm=1.0,                      # max gradient norm based on QLoRA paper\n",
        "\n",
        "\n",
        "    #lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    lr_scheduler_type=\"cosine\",            # lr_scheduler_type=\"cosine\" (Cosine Annealing Learning Rate)\n",
        "\n",
        "    push_to_hub=True,                       # push model to hub\n",
        "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir=\"/content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epoch1/logs\",\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    metric_for_best_model = \"loss\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEIGVWGp2dB8"
      },
      "outputs": [],
      "source": [
        "!pip install trl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd03Af2od_qT"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "#torch.utils.checkpoint.checkpoint(use_reentrant=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Yk7MlowDT-"
      },
      "source": [
        "https://huggingface.co/docs/trl/main/en/sft_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we8LmPrO_AG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b3ee05-efa2-415f-a4ef-ff03f74ed51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "import datasets\n",
        "#from datasets import load_metric\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "datasetF_hf = datasets.Dataset.from_pandas(datasetF)\n",
        "datasetF_hf_test = datasets.Dataset.from_pandas(datasetF_test)\n",
        "\n",
        "#max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
        "max_seq_length = 1024\n",
        "\n",
        "# Explicitly set the device using torch.device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Add a padding token to your tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Or, add a new padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "#metric = load_metric(\"squad\")  # Assuming you're evaluating on SQuAD dataset\n",
        "#def compute_metrics(eval_pred):\n",
        "#    logits, labels = eval_pred\n",
        "#    predictions = logits.argmax(axis=-1)\n",
        "#    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=datasetF_hf,\n",
        "    dataset_text_field=\"text\", ### added for the summarization dataset\n",
        "    eval_dataset=datasetF_hf_test,\n",
        "\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    #processing_class=tokenizer,\n",
        "    packing=True,\n",
        "    #callbacks=[early_stopping_callback],\n",
        "\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,  # We template with special tokens\n",
        "        \"append_concat_token\": False, # No need to add additional separator token\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nuyiFKmWfvU"
      },
      "source": [
        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCq7Dq8f7VPU"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=5))\n",
        "\n",
        "\n",
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCxeJIj4KvSQ"
      },
      "outputs": [],
      "source": [
        "# free the memory again\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjTwqcXmK_OR"
      },
      "source": [
        "## Test Model and run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_hl8YPKxQ2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"/content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        "  quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1DyOCUfLLn4"
      },
      "source": [
        "Letâ€™s load our test dataset try to generate an instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBZbeEq-R_ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67735c82-99bf-4126-b409-8342b95a3408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Meta-Llama-3-8B', revision=None, inference_mode=True, r=128, target_modules={'q_proj', 'up_proj', 'v_proj', 'gate_proj', 'down_proj', 'o_proj', 'k_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
          ]
        }
      ],
      "source": [
        "print(model.peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uHtGXcrLKI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf94b680-87b3-4783-e172-066f622ed775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "nrec= randint(0, len(eval_dataset))\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GZg3t7d1z9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "f1f0709f-2eab-46d0-eccd-334d11008872"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of a single dose of the nonsteroidal antiinflammatory drug nsaids on the pharmacokinetics of the antiepileptic drug carbamazepine cbz in healthy volunteers a single dose of mg of cbz was administered to healthy volunteers in a randomized crossover design T3 a single dose of mg of nsaids or PL the pharmacokinetics of cbz were determined by measuring the plasma concentration of cbz and its metabolites carbamazepine epoxide cbzepoxide and hydro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "ganswer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q2zlGDkw1rE",
        "outputId": "d60ffd87-3486-4f00-c954-e560bef15019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "\n",
            "Match\n"
          ]
        }
      ],
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "print()\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "qc=str(ganswer).find('[INST]')\n",
        "ganswer=ganswer[0:qc-7]\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "if qc>0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "print(f\"Generated Answer:\\n{ganswer}\")\n",
        "print()\n",
        "if ganswer == oanswer:\n",
        "  print(\"Match\")\n",
        "else:\n",
        "  print(\"NO Match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX6ChYhtvtat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819015c6-8492-4cbb-e514-5c3cd0afff5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "\n",
            "Original Answer:\n",
            "['antral follicle count']\n",
            "\n",
            "Full Generated Answer:\n",
            "[/INST] antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of a single dose of the nonsteroidal antiinflammatory drug nsaids on the pharmacokinetics of the antiepileptic drug carbamazepine cbz in healthy volunteers a single dose of mg of cbz was administered to healthy volunteers in a randomized crossover design T3 a single dose of mg of nsaids or PL the pharmacokinetics of cbz were determined by measuring the plasma concentration of cbz and its metabolites carbamazepine epoxide cbzepoxide and hydro\n"
          ]
        }
      ],
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print()\n",
        "print(f\"Full Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcFTvrGSvJ8G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "67db62f3-6732-4d8f-bc1c-d17dd9765633"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[/INST] antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of a single dose of the nonsteroidal antiinflammatory drug nsaids on the pharmacokinetics of the antiepileptic drug carbamazepine cbz in healthy volunteers a single dose of mg of cbz was administered to healthy volunteers in a randomized crossover design T3 a single dose of mg of nsaids or PL the pharmacokinetics of cbz were determined by measuring the plasma concentration of cbz and its metabolites carbamazepine epoxide cbzepoxide and hydro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+8:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc:len(ganswer)]\n",
        "ganswer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxF38_Am7b_5",
        "outputId": "6c48bf35-3ff1-4467-9d7c-e486070447c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer:\n",
            "antral follicle count\n"
          ]
        }
      ],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0-8]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "\n",
        "if qc>=0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "print(f\"Generated Answer:\\n{ganswer}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a pre-trained sentence embedding model\n",
        "model_embedding = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Load your test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "\n",
        "# Create the generation pipeline\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt = sample['text']\n",
        "    outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.7,\n",
        "                                  top_k=50, top_p=0.7, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    ganswer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "    qc0 = str(ganswer).find('[/INST]')\n",
        "    qc1 = str(ganswer).find('[INST]')\n",
        "    ganswer = str(ganswer)[qc0 + 8:qc1 - 8]\n",
        "\n",
        "    oanswer = str(sample['label'])\n",
        "    oanswer = oanswer[2:len(oanswer) - 2]\n",
        "\n",
        "    if len(oanswer) == 0:\n",
        "        oanswer = 'Not possible to get or use'\n",
        "\n",
        "    #print('\\n\\n')\n",
        "    #print(f\"Question: {prompt}\")\n",
        "    #print(f\"Generated Answer: {ganswer}\")\n",
        "    #print(f\"Original Answer: {oanswer}\")\n",
        "\n",
        "    # Generate sentence embeddings\n",
        "    embedding1 = model_embedding.encode(ganswer, convert_to_tensor=True)\n",
        "    embedding2 = model_embedding.encode(oanswer, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_sim = util.cos_sim(embedding1, embedding2).item()\n",
        "\n",
        "    # Set a similarity threshold (adjust as needed)\n",
        "    if cosine_sim >= 0.5:\n",
        "        #print(\"Match\")\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 10  # Adjust the number of samples as needed\n",
        "\n",
        "# Evaluate the model\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "    s = eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n"
      ],
      "metadata": {
        "id": "MC-SNHc8v63E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = sum(success_rate) / len(success_rate)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxoCKRJddDW2",
        "outputId": "2e8c8075-592c-4e10-8e22-90c337a788eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorbard"
      ],
      "metadata": {
        "id": "akI7UTrzW95o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/tensorbard\n",
        "%mkdir -p /content/tensorbard\n",
        "%cd /content/tensorbard\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/frankmorales2020/11APRIL2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata"
      ],
      "metadata": {
        "id": "alATl2MpWIur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15189d69-e4e0-4090-f827-e7e5624e53e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tensorbard\n",
            "Git LFS initialized.\n",
            "Cloning into '11APRIL2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata'...\n",
            "remote: Enumerating objects: 235, done.\u001b[K\n",
            "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
            "remote: Compressing objects: 100% (232/232), done.\u001b[K\n",
            "remote: Total 235 (delta 86), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (235/235), 59.57 KiB | 19.86 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tensorbard/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epoch1/logs/"
      ],
      "metadata": {
        "id": "jRBbdyrqWKHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AGENT"
      ],
      "metadata": {
        "id": "GvHJA9_AfdlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-adk -q"
      ],
      "metadata": {
        "id": "9rs7srDAfvZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio -q"
      ],
      "metadata": {
        "id": "EeuQg5z9ZE8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import os\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.artifacts import InMemoryArtifactService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define the local path to your fine-tuned model\n",
        "# This path is from your Google Drive mount in the Colab environment\n",
        "FINE_TUNED_MODEL_PATH = \"/content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\"\n",
        "\n",
        "# Load the model and tokenizer from the local path using transformers\n",
        "# We assume this code is run in an environment where this path is accessible\n",
        "try:\n",
        "    # Determine the device to use (GPU if available, otherwise CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH)\n",
        "    # Set padding token if it's not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "         tokenizer.pad_token = tokenizer.eos_token\n",
        "         if tokenizer.pad_token is None:\n",
        "             # Fallback if EOS token is also not set\n",
        "             tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "\n",
        "    # Load the model\n",
        "    # Use torch_dtype=torch.bfloat16 if your GPU supports it for potentially faster inference\n",
        "    model = AutoModelForCausalLM.from_pretrained(FINE_TUNED_MODEL_PATH, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32).to(device)\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "\n",
        "    print(f\"Successfully loaded model and tokenizer from {FINE_TUNED_MODEL_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or tokenizer from {FINE_TUNED_MODEL_PATH}. Please ensure the path is correct and the necessary libraries are installed.\")\n",
        "    print(f\"Details: {e}\")\n",
        "    # Exit or handle the error appropriately if the model cannot be loaded\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "\n",
        "\n",
        "# Define a simple agent class inheriting from ADK's Agent\n",
        "class LocalFineTunedMedicalAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, model, tokenizer):\n",
        "        # Initialize the Agent with name and instruction, model is a string\n",
        "        # Set model path instead of None as ADK expects a string\n",
        "        super().__init__(name=name, instruction=instruction, model=FINE_TUNED_MODEL_PATH)\n",
        "        self._local_model = model\n",
        "        self._local_tokenizer = tokenizer\n",
        "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # The core asynchronous method where the agent processes input\n",
        "    async def _run_async_impl(self, context): # Remove type hint for InvocationContext\n",
        "        # Access the user's query from the invocation context\n",
        "        user_query = context.request.message.text\n",
        "\n",
        "        # print(f\"Agent '{self.name}' received query: {user_query}\") # Uncomment for more detailed logging\n",
        "\n",
        "        # --- Text Generation using the locally loaded transformers model ---\n",
        "        try:\n",
        "            # Prepare the prompt\n",
        "            # You might want to format the prompt based on your model's\n",
        "            # preferred input format (e.g., chat format like Llama 3)\n",
        "            # For this example, we'll use a simple instruction format derived from your notebook\n",
        "            # Assuming your model was fine-tuned with prompts like \"[INST] instruction [/INST]\"\n",
        "            prompt = f\"[INST] {self.instruction}\\n{user_query} [/INST]\"\n",
        "\n",
        "\n",
        "            # Tokenize the input\n",
        "            inputs = self._local_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(self._device)\n",
        "\n",
        "            # Generate text\n",
        "            with torch.no_grad(): # Disable gradient calculation for inference\n",
        "                 outputs = self._local_model.generate(\n",
        "                     **inputs,\n",
        "                     max_new_tokens=256, # Limit the generated response length\n",
        "                     num_return_sequences=1,\n",
        "                     do_sample=True,\n",
        "                     top_k=50,\n",
        "                     top_p=0.95,\n",
        "                     temperature=0.7,\n",
        "                     # Add other generation parameters as needed\n",
        "                     pad_token_id=self._local_tokenizer.pad_token_id # Ensure pad_token_id is set for generation\n",
        "                 )\n",
        "\n",
        "            # Decode the generated tokens\n",
        "            generated_text_with_prompt = self._local_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "            # Post-process the generated text to extract the agent's response\n",
        "            # Based on the prompt format \"[INST] ... [/INST] response\", we want the part after [/INST]\n",
        "            if \"[/INST]\" in generated_text_with_prompt:\n",
        "                generated_text = generated_text_with_prompt.split(\"[/INST]\", 1)[1].strip()\n",
        "            else:\n",
        "                 # Fallback if the expected format is not present\n",
        "                 generated_text = generated_text_with_prompt.strip()\n",
        "\n",
        "            # Further cleaning might be needed depending on model output (e.g., cutting off at a specific token or phrase)\n",
        "            # If the model generates a new turn of the prompt, cut it off\n",
        "            if \"[INST]\" in generated_text:\n",
        "                 generated_text = generated_text.split(\"[INST]\", 1)[0].strip()\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            generated_text = f\"An error occurred during text generation: {e}\"\n",
        "            print(f\"Error during text generation with transformers: {e}\")\n",
        "\n",
        "        # Yield the final response from the agent\n",
        "        yield context.AgentResponse( # Use context.AgentResponse directly\n",
        "            message=context.AgentMessage(text=generated_text) # Use context.AgentMessage directly\n",
        "        )\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-6OgUW0uXNuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show google-adk"
      ],
      "metadata": {
        "id": "5lOYUvjbkyPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # Ensure model and tokenizer were loaded successfully\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"Model loading failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 1. Instantiate the in-memory services for artifacts and sessions\n",
        "    artifact_service = InMemoryArtifactService()\n",
        "    session_service = InMemorySessionService()\n",
        "\n",
        "    # 2. Create an instance of your local fine-tuned agent\n",
        "    agent_name = \"LocalFineTunedMedicalAssistant\"\n",
        "    agent_instruction = (\n",
        "        \"You are a highly specialized AI medical assistant, fine-tuned to provide information \"\n",
        "        \"based on medical literature. Answer questions accurately and concisely, focusing on \"\n",
        "        \"information related to medical concepts and terminology.\"\n",
        "    )\n",
        "\n",
        "    medical_agent = LocalFineTunedMedicalAgent(\n",
        "        name=agent_name,\n",
        "        instruction=agent_instruction,\n",
        "        model=model,          # Pass the loaded transformers model\n",
        "        tokenizer=tokenizer   # Pass the loaded transformers tokenizer\n",
        "    )\n",
        "\n",
        "    # 3. Create a Runner instance\n",
        "    runner = Runner(\n",
        "        agent=medical_agent,\n",
        "        session_service=session_service,\n",
        "        artifact_service=artifact_service,\n",
        "        app_name=\"MyMedicalApp\"  # Add app_name here with a suitable name\n",
        "    )\n",
        "\n",
        "    # 4. Define a test question\n",
        "    test_question = \"\"\"while diminished ovarian reserve dor predicts decreased ovarian response to\n",
        "stimulation it does not necessarily foretell about the fecundity cycle acco\n",
        "rding to bolognas criteria laid down by the european society of human repro\n",
        "duction and embryology old age abnormal ovarian reserve tests such as AFC a\n",
        "fc and antimullerian hormone amh as well as prior suboptimal response to st\n",
        "imulation are the main AF representing dor unfavorable response to maximal\n",
        "stimulation on two previous occasions may also represent dor among the ovar\n",
        "ian reserve tests amh and afc are the most predictive values for dor AF whi\n",
        "ch may give rise to dor include environmental factors autoimmune or metabol\n",
        "ic disorders infections genetic abnormalities and iatrogenic causes such as\n",
        "smoking chemotherapy radiation and gynecologic surgeries besides studies ha\n",
        "ve proposed endometriosis as a key contributor to dor and hence emphasized\n",
        "on its proper management to prevent additional damages leading to compromis\n",
        "ed fertility in summary dor is found to be a clinical challenge in the prac\n",
        "tice of fertility care with controversial countermeasures to prevent or tre\n",
        "at the condition nevertheless some promising measure such as oocyte embryo\n",
        "and tissue cryopreservation ovarian transplantation dietary supplementation\n",
        "and the transfer of mitochondria have offered hopes towards ameliorating th\n",
        "e burden of dor this review attempts to discuss dor from different perspect\n",
        "ives and summarize some existing hopes in clinical practice\"\"\"\n",
        "\n",
        "    test_original_answer = \"antral follicle count\" # The expected answer for comparison\n",
        "\n",
        "    print('\\n')\n",
        "    print(f\"\\n--- Testing the Agent (Basic Runner Call) ---\")\n",
        "    print(f\"Input Question: {test_question}\")\n",
        "    print('\\n')\n",
        "    print(f\"Expected Answer: {test_original_answer}\")\n",
        "    print('\\n')\n",
        "\n",
        "    # Use a unique user and session ID for this test\n",
        "    user_id = \"test_user\"\n",
        "    session_id = \"test_session\"\n",
        "\n",
        "    try:\n",
        "        # Execute the agent with user_id and session_id only\n",
        "        run_result = await runner.run(user_id=user_id, session_id=session_id)\n",
        "        async for event in run_result:\n",
        "            if event.is_user_query():\n",
        "                # If it expects a response, send the test question\n",
        "                await runner.respond(user_id=user_id, session_id=session_id, response=test_question)\n",
        "            if event.is_final_response():\n",
        "                print(f\"Agent Generated Answer: {event.message.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during agent execution: {e}\")"
      ],
      "metadata": {
        "id": "2pVcc5PPim79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entry point to run the asynchronous example\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHNUD6dgoZr",
        "outputId": "4e4a387f-1a56-40ed-a3ec-8748462151f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "--- Testing the Agent (Basic Runner Call) ---\n",
            "Input Question: while diminished ovarian reserve dor predicts decreased ovarian response to\n",
            "stimulation it does not necessarily foretell about the fecundity cycle acco\n",
            "rding to bolognas criteria laid down by the european society of human repro\n",
            "duction and embryology old age abnormal ovarian reserve tests such as AFC a\n",
            "fc and antimullerian hormone amh as well as prior suboptimal response to st\n",
            "imulation are the main AF representing dor unfavorable response to maximal\n",
            "stimulation on two previous occasions may also represent dor among the ovar\n",
            "ian reserve tests amh and afc are the most predictive values for dor AF whi\n",
            "ch may give rise to dor include environmental factors autoimmune or metabol\n",
            "ic disorders infections genetic abnormalities and iatrogenic causes such as\n",
            "smoking chemotherapy radiation and gynecologic surgeries besides studies ha\n",
            "ve proposed endometriosis as a key contributor to dor and hence emphasized\n",
            "on its proper management to prevent additional damages leading to compromis\n",
            "ed fertility in summary dor is found to be a clinical challenge in the prac\n",
            "tice of fertility care with controversial countermeasures to prevent or tre\n",
            "at the condition nevertheless some promising measure such as oocyte embryo\n",
            "and tissue cryopreservation ovarian transplantation dietary supplementation\n",
            "and the transfer of mitochondria have offered hopes towards ameliorating th\n",
            "e burden of dor this review attempts to discuss dor from different perspect\n",
            "ives and summarize some existing hopes in clinical practice\n",
            "\n",
            "\n",
            "Expected Answer: antral follicle count\n",
            "\n",
            "\n",
            "An error occurred during agent execution: Runner.run() missing 1 required keyword-only argument: 'new_message'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
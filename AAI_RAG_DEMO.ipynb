{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNal6Qhx8GNiUIV8UI1Oumt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/AAI_RAG_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langchain_community langgraph langchain_google_genai -q\n",
        "!pip install chromadb tiktoken -q"
      ],
      "metadata": {
        "id": "lexc8TtFQJHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import time\n",
        "import json\n",
        "from google.colab import userdata # Keep this as per your instruction\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- API Key Setup (as provided by you, directly used) ---\n",
        "GOOGLE_API_KEY = userdata.get('GEMINIDEV')\n",
        "print(f\"\")\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "else:\n",
        "    print(\"WARNING: GOOGLE_API_KEY not found in Colab Secrets. Please ensure 'GEMINI' secret is set.\")\n",
        "    print(\"API calls will likely fail. Proceeding with unconfigured API.\")\n",
        "\n",
        "# --- Agent Configuration ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\" # As specified by you\n",
        "\n",
        "# Initialize the Gemini model for general responses and agentic decisions\n",
        "try:\n",
        "    AGENTIC_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    RESPONDER_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    print(f\"Gemini model '{AgentConfig.LLM_MODEL_NAME}' initialized for agentic and response generation.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to initialize Gemini model. Please check your API key and model name. Error: {e}\")\n",
        "    # Fallback to dummy models if Gemini initialization fails\n",
        "    AGENTIC_MODEL = None\n",
        "    RESPONDER_MODEL = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-ZKBqqyP3Cu",
        "outputId": "994e9fa0-d8fb-4a43-f494-6af23b5d6b88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Gemini model 'gemini-2.5-flash' initialized for agentic and response generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the following setup already executed in your Colab notebook:\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import os # Import os module to set environment variables\n",
        "\n",
        "# --- API Key Setup ---\n",
        "GOOGLE_API_KEY_GEMINI = userdata.get('GEMINIDEV') # Renamed to avoid confusion with search API key\n",
        "print(f\"GOOGLE_API_KEY_GEMINI: {GOOGLE_API_KEY_GEMINI}\")\n",
        "if GOOGLE_API_KEY_GEMINI:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY_GEMINI)\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "else:\n",
        "    print(\"WARNING: GOOGLE_API_KEY (for Gemini) not found in Colab Secrets. Please ensure 'GEMINI' secret is set.\")\n",
        "    print(\"API calls for Gemini will likely fail. Proceeding with unconfigured API.\")\n",
        "\n",
        "# --- IMPORTANT: Set GOOGLE_API_KEY for GoogleSearchAPIWrapper ---\n",
        "# Use the same API key if it's a general Google Cloud API key, or a separate one if needed.\n",
        "# For simplicity, we'll assume your 'GEMINI' key is also valid for Google Search API.\n",
        "# If you have a separate key for search, retrieve it from userdata.get('YOUR_SEARCH_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY_GEMINI\n",
        "\n",
        "# You also need a Google Custom Search Engine ID (CSE ID)\n",
        "# Get this from https://programmablesearchengine.google.com/\n",
        "# Add this to your Colab Secrets as 'GOOGLE_CSE_ID'\n",
        "# https://programmablesearchengine.google.com/controlpanel/overview?cx=514fb1ae50d034b58\n",
        "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
        "print(f\"GOOGLE_CSE_ID: {GOOGLE_CSE_ID}\")\n",
        "if GOOGLE_CSE_ID:\n",
        "    os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "    print(\"Google CSE ID configured successfully from Colab Secrets.\")\n",
        "else:\n",
        "    print(\"WARNING: GOOGLE_CSE_ID not found in Colab Secrets. Web search will fail.\")\n",
        "    print(\"Please ensure 'GOOGLE_CSE_ID' secret is set.\")\n",
        "\n",
        "\n",
        "# --- Agent Configuration ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "\n",
        "# Initialize the Gemini model for general responses and agentic decisions\n",
        "try:\n",
        "    AGENTIC_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    RESPONDER_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    print(f\"Gemini model '{AgentConfig.LLM_MODEL_NAME}' initialized for agentic and response generation.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to initialize Gemini model. Please check your API key and model name. Error: {e}\")\n",
        "    AGENTIC_MODEL = None\n",
        "    RESPONDER_MODEL = None\n",
        "\n",
        "# Now, the rest of your code from the previous response can follow.\n",
        "# The `web_search_tool = GoogleSearchAPIWrapper()` line should now work correctly."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUNHfS3RcRt",
        "outputId": "5df18ea0-3f4e-4497-bbb8-435aeabefb69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_API_KEY_GEMINI: AIzaSyC8TXKKymp2xNwUlBFYsEZQC1GotzxqATw\n",
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "GOOGLE_CSE_ID: 514fb1ae50d034b58\n",
            "Google CSE ID configured successfully from Colab Secrets.\n",
            "Gemini model 'gemini-2.5-flash' initialized for agentic and response generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a query designed to trigger the \"web_search\" decision\n",
        "\n",
        "# Assuming simple_agentic_rag_demo is defined as in your provided code\n",
        "# and the necessary API keys and models are initialized.\n",
        "\n",
        "query = \"What is the current weather in Montreal?\"\n",
        "\n",
        "print(f\"\\n--- RAG Demo for Query (HardCode): '{query}'\")\n",
        "final_answer = \"I'm sorry, I couldn't process your request.\" # Default\n",
        "\n",
        "# Step 1: Agent decides the best approach (using AGENTIC_MODEL)\n",
        "# This part is conceptual as we don't have a live LLM to execute here,\n",
        "# but in the actual code, AGENTIC_MODEL would determine \"web_search\" for this query.\n",
        "decision = \"web_search\"\n",
        "print(f\"Agent Decision: {decision}\")\n",
        "\n",
        "# Step 2: Execute based on decision and get context\n",
        "context = \"\"\n",
        "if decision == \"web_search\":\n",
        "    print(\"Performing web search...\")\n",
        "    try:\n",
        "        # In a real scenario, this would call GoogleSearchAPIWrapper.run(query)\n",
        "        # For this example, we'll simulate a search result.\n",
        "        # This is a placeholder for the actual web search tool execution.\n",
        "        simulated_search_results = \"The current weather in Montreal is sunny with a temperature of 25Â°C. There is a light breeze from the west. The forecast for tonight is clear.\"\n",
        "        context = simulated_search_results\n",
        "\n",
        "        if not context.strip():\n",
        "            context = \"No relevant web search results found.\"\n",
        "            print(\"No relevant web search results found.\")\n",
        "        else:\n",
        "            print(f\"Web Search Context (partial): {context[:200]}...\")\n",
        "    except Exception as e:\n",
        "        context = f\"Error during web search: {e}\"\n",
        "        print(context)\n",
        "\n",
        "# Step 3 (for web_search): Generate answer with search results\n",
        "if decision == \"web_search\":\n",
        "    # In a real scenario, RESPONDER_MODEL would use this context to generate the answer.\n",
        "    # We'll simulate the final answer based on the simulated context.\n",
        "    generation_prompt_for_web_search = f\"\"\"You are a helpful AI assistant. Use the following Web Search Results to answer the question:\n",
        "\n",
        "Web Search Results:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\"\"\"\n",
        "    # This would be a call to get_llm_response(RESPONDER_MODEL, generation_prompt_for_web_search)\n",
        "    final_answer = \"The current weather in Montreal is sunny with a temperature of 25Â°C, with a light breeze from the west. The forecast for tonight is clear.\"\n",
        "\n",
        "print(f\"\\nFinal Answer: {final_answer}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EesgZZSDdW-6",
        "outputId": "9eb34df7-1239-4acb-cd68-0b4076d877cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RAG Demo for Query (HardCode): 'What is the current weather in Montreal?'\n",
            "Agent Decision: web_search\n",
            "Performing web search...\n",
            "Web Search Context (partial): The current weather in Montreal is sunny with a temperature of 25Â°C. There is a light breeze from the west. The forecast for tonight is clear....\n",
            "\n",
            "Final Answer: The current weather in Montreal is sunny with a temperature of 25Â°C, with a light breeze from the west. The forecast for tonight is clear.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Example for \"web_search\" - this will now attempt a real web search if configured\n",
        "simple_agentic_rag_demo(\"What is the current weather in Montreal?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "o705UZIorrEU",
        "outputId": "a535f042-5411-4afa-e085-0beb2321a184"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What is the current weather in Montreal?'\n",
            "Agent Decision: web_search\n",
            "Performing web search...\n",
            "Web Search Context (partial): 10 Day Weather-Montreal, Quebec, Canada. As of 5:26 pm EDT. Tonight. --/62Â°. 2%. Night. 62Â°. 2%. N 5 mph. Mostly cloudy. Low 62F. Winds light and variable. Montreal, Quebec, Canada Weather Forecast, w...\n",
            "\n",
            "Final Answer: Based on the Web Search Results, the current weather in Montreal is:\n",
            "\n",
            "*   **Mostly Cloudy** with a temperature of **25Â°C**.\n",
            "*   Another reading states it is **28Â°C** and **Partly Cloudy**, feeling like **31Â°C**, with a UV Index of 0 (Low).\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the Web Search Results, the current weather in Montreal is:\\n\\n*   **Mostly Cloudy** with a temperature of **25Â°C**.\\n*   Another reading states it is **28Â°C** and **Partly Cloudy**, feeling like **31Â°C**, with a UV Index of 0 (Low).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata # Only available in Google Colab environment\n",
        "\n",
        "\n",
        "# Retrieve Gemini API Key\n",
        "GOOGLE_API_KEY_GEMINI = None\n",
        "try:\n",
        "    # In Colab, this attempts to get from Colab Secrets\n",
        "    GOOGLE_API_KEY_GEMINI = userdata.get('GEMINIDEV')\n",
        "except Exception as e:\n",
        "    print(f\"Could not retrieve GEMINI key from Colab Secrets: {e}\")\n",
        "    # Fallback for local testing if you don't use userdata\n",
        "    # GOOGLE_API_KEY_GEMINI = os.getenv(\"GOOGLE_API_KEY\") # Or set directly for testing\n",
        "\n",
        "if GOOGLE_API_KEY_GEMINI:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY_GEMINI)\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY_GEMINI # For GoogleSearchAPIWrapper\n",
        "    print(\"Google Generative AI configured successfully.\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: 'GEMINI' API key not found. API calls will fail.\")\n",
        "    # In a real application, you might raise an error here to stop execution\n",
        "    # raise ValueError(\"GEMINI API key is missing.\")\n",
        "\n",
        "# Retrieve Google Custom Search Engine ID\n",
        "GOOGLE_CSE_ID = None\n",
        "try:\n",
        "    # In Colab, this attempts to get from Colab Secrets\n",
        "    GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
        "except Exception as e:\n",
        "    print(f\"Could not retrieve GOOGLE_CSE_ID from Colab Secrets: {e}\")\n",
        "    # Fallback for local testing if you don't use userdata\n",
        "    # GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\") # Or set directly for testing\n",
        "\n",
        "\n",
        "if GOOGLE_CSE_ID:\n",
        "    os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "    print(\"Google CSE ID configured successfully.\")\n",
        "else:\n",
        "    print(\"WARNING: 'GOOGLE_CSE_ID' not found. Web search functionality will fail.\")\n",
        "    print(\"Please ensure 'GOOGLE_CSE_ID' secret is set (or environment variable).\")\n",
        "\n",
        "\n",
        "# --- 3. Agent Configuration and Helper Functions (as in the notebook) ---\n",
        "# These parts are direct copies from your notebook for completeness.\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import time # Assuming time is needed somewhere, imported in original\n",
        "\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "\n",
        "# Initialize the Gemini model for general responses and agentic decisions\n",
        "AGENTIC_MODEL = None\n",
        "RESPONDER_MODEL = None\n",
        "try:\n",
        "    AGENTIC_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    RESPONDER_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    print(f\"Gemini model '{AgentConfig.LLM_MODEL_NAME}' initialized for agentic and response generation.\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR: Failed to initialize Gemini model. Check API key and model name. Error: {e}\")\n",
        "    # raise # Uncomment to stop execution on critical error\n",
        "\n",
        "def get_llm_response(model, prompt_text: str) -> str:\n",
        "    \"\"\"Helper to get response from a GenerativeModel.\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt_text)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"LLM API Call Error: {e}\")\n",
        "        return f\"Error communicating with LLM: {e}\"\n",
        "\n",
        "# 1. Embedding Model\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# 2. Vector Database for \"Self Data\" (pre-indexed knowledge)\n",
        "VECTOR_DB_PATH = \"./chroma_db_simple_rag\"\n",
        "dummy_documents = [\n",
        "    Document(page_content=\"Retrieval Augmented Generation (RAG) combines large language models with external knowledge retrieval systems to generate more informed and accurate responses by fetching relevant documents or data before generating the answer.\"),\n",
        "    Document(page_content=\"The primary goal of an AI agent for flight planning is to optimize routes for fuel efficiency and safety, considering factors like weather, airspace restrictions (NOTAMs), aircraft performance, and regulatory compliance.\"),\n",
        "    Document(page_content=\"Mount Everest is Earth's highest mountain above sea level, located in the Mahalangur Himal sub-range of the Himalayas.\"),\n",
        "    Document(page_content=\"Paris is the capital and most populous city of France.\"),\n",
        "    Document(page_content=\"A key challenge in flight planning is balancing fuel consumption with payload capacity and ensuring adherence to strict safety protocols.\")\n",
        "]\n",
        "vectorstore = Chroma.from_documents(documents=dummy_documents, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 relevant docs\n",
        "\n",
        "# 3. Web Search Tool\n",
        "web_search_tool = GoogleSearchAPIWrapper()\n",
        "\n",
        "\n",
        "# --- 4. Simple Agentic RAG Demo Function (The main logic) ---\n",
        "\n",
        "def simple_agentic_rag_demo(query: str) -> str:\n",
        "    print(f\"\\n--- Simple Agentic RAG Demo for Query: '{query}'\")\n",
        "    final_answer = \"I'm sorry, I couldn't process your request.\" # Default\n",
        "\n",
        "    # Step 1: Agent decides the best approach (using AGENTIC_MODEL)\n",
        "    decision_prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are an intelligent AI router. Based on the user's query, choose the best action.\n",
        "        Options:\n",
        "        - 'self_data': If the query is about common knowledge, definitions, or pre-indexed internal data (like specific AI concepts or known geographical facts).\n",
        "        - 'web_search': If the query requires current, real-time information, external events, or very specific facts not likely to be in general knowledge.\n",
        "        - 'direct_answer': If the query is a simple greeting or can be answered confidently from general knowledge without needing retrieval.\n",
        "\n",
        "        Provide only your chosen option as a single word: self_data, web_search, direct_answer.\n",
        "        User Query: {query}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    decision = \"web_search\" # Initialize for fallback\n",
        "    if AGENTIC_MODEL: # Ensure model is initialized before using\n",
        "        try:\n",
        "            decision = get_llm_response(AGENTIC_MODEL, decision_prompt.format(query=query))\n",
        "            decision = decision.strip().lower() # Clean up response\n",
        "            if decision not in [\"self_data\", \"web_search\", \"direct_answer\"]:\n",
        "                print(f\"Agent's decision was unclear ('{decision}'). Defaulting to web_search.\")\n",
        "                decision = \"web_search\" # Fallback if LLM doesn't follow instructions\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting agent decision from LLM: {e}. Defaulting to web_search.\")\n",
        "            decision = \"web_search\" # Robust fallback\n",
        "    else:\n",
        "        print(\"AGENTIC_MODEL not initialized. Defaulting decision to web_search.\")\n",
        "        decision = \"web_search\" # Default if model failed to initialize\n",
        "\n",
        "    print(f\"Agent Decision: {decision}\")\n",
        "\n",
        "    context = \"\"\n",
        "    # Step 2: Execute based on decision and get context\n",
        "    if decision == \"self_data\":\n",
        "        print(\"Retrieving from internal knowledge base (Self-Data)...\")\n",
        "        try:\n",
        "            docs = retriever.invoke(query)\n",
        "            context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "            if not context.strip(): # Check if context is effectively empty\n",
        "                context = \"No relevant internal data found.\"\n",
        "                print(\"No relevant self-data found.\")\n",
        "            else:\n",
        "                print(f\"Self-Data Context (partial): {context[:200]}...\")\n",
        "        except Exception as e:\n",
        "            context = f\"Error retrieving self-data: {e}\"\n",
        "            print(context)\n",
        "\n",
        "    elif decision == \"web_search\":\n",
        "        print(\"Performing web search...\")\n",
        "        try:\n",
        "            # THIS IS THE REAL CALL TO THE WEB SEARCH API\n",
        "            search_results = web_search_tool.run(query)\n",
        "            context = search_results\n",
        "            if not context.strip():\n",
        "                context = \"No relevant web search results found.\"\n",
        "                print(\"No relevant web search results found.\")\n",
        "            else:\n",
        "                print(f\"Web Search Context (partial): {context[:200]}...\")\n",
        "        except Exception as e:\n",
        "            context = f\"Error during web search: {e}. (This often indicates an API key or CSE ID issue)\"\n",
        "            print(context)\n",
        "            if \"Daily Limit Exceeded\" in str(e): # Common error for Google Search API\n",
        "                print(\"Consider checking your Google Custom Search API quota or API key.\")\n",
        "\n",
        "    # Step 3: Generate answer using the context or direct knowledge\n",
        "    generation_prompt = \"\"\n",
        "    if decision == \"self_data\":\n",
        "        generation_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a helpful AI assistant. Use the following context to answer the question.\"),\n",
        "            (\"human\", \"Context:\\n{context}\\n\\nQuestion: {query}\")\n",
        "        ])\n",
        "    elif decision == \"web_search\":\n",
        "        generation_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a helpful AI assistant. Use the following Web Search Results to answer the question.\"),\n",
        "            (\"human\", \"Web Search Results:\\n{context}\\n\\nQuestion: {query}\")\n",
        "        ])\n",
        "    elif decision == \"direct_answer\":\n",
        "        print(\"Answering directly from LLM's general knowledge...\")\n",
        "        if RESPONDER_MODEL: # Ensure model is initialized\n",
        "            final_answer = get_llm_response(RESPONDER_MODEL, query)\n",
        "        else:\n",
        "            final_answer = \"Error: Responder model not initialized.\"\n",
        "        print(f\"\\nFinal Answer: {final_answer}\")\n",
        "        print(\"-\" * 50)\n",
        "        return final_answer\n",
        "\n",
        "    # For self_data and web_search, generate final answer with context\n",
        "    if RESPONDER_MODEL: # Ensure model is initialized\n",
        "        final_answer = get_llm_response(RESPONDER_MODEL, generation_prompt.format(context=context, query=query))\n",
        "    else:\n",
        "        final_answer = \"Error: Responder model not initialized. Could not generate answer.\"\n",
        "\n",
        "    print(f\"\\nFinal Answer: {final_answer}\")\n",
        "    print(\"-\" * 50)\n",
        "    return final_answer\n",
        "\n",
        "\n",
        "# --- 5. Demo Usage (Examples to run) ---\n",
        "# This is how you'd call the function with actual queries.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"--- Simple Agentic RAG Demo with Gemini {AgentConfig.LLM_MODEL_NAME} ---\")\n",
        "\n",
        "    # Example 1: Should use self_data (defined concept)\n",
        "    simple_agentic_rag_demo(\"Explain Retrieval Augmented Generation (RAG)\")\n",
        "\n",
        "    # Example 2: Should use self_data (factual knowledge in dummy data)\n",
        "    simple_agentic_rag_demo(\"What is the highest mountain on Earth?\")\n",
        "\n",
        "    # Example 3: Should use direct_answer (simple query)\n",
        "    simple_agentic_rag_demo(\"Hello, how are you today?\")\n",
        "\n",
        "    # --- Add these lines before initializing web_search_tool ---\n",
        "    print(f\"DEBUG: os.environ['GOOGLE_API_KEY'] before web_search_tool init: {os.getenv('GOOGLE_API_KEY')}\")\n",
        "\n",
        "    # 3. Web Search Tool\n",
        "    web_search_tool = GoogleSearchAPIWrapper()\n",
        "\n",
        "    print(f\"DEBUG: web_search_tool's internal API key: {web_search_tool.google_api_key}\") # This might reveal if it picked it up\n",
        "\n",
        "\n",
        "    # Example 4: Flight planning query (should use self_data due to new content)\n",
        "    simple_agentic_rag_demo(\"What are the essential elements for effective flight planning?\")\n",
        "\n",
        "    # Example for \"web_search\" - this will now attempt a real web search if configured\n",
        "    simple_agentic_rag_demo(\"What is the current weather in Montreal?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EfEOasUVhy7t",
        "outputId": "015b4c6b-256c-46db-8604-5dc7b245c736"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully.\n",
            "Google CSE ID configured successfully.\n",
            "Gemini model 'gemini-2.5-flash' initialized for agentic and response generation.\n",
            "--- Simple Agentic RAG Demo with Gemini gemini-2.5-flash ---\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'Explain Retrieval Augmented Generation (RAG)'\n",
            "Agent Decision: self_data\n",
            "Retrieving from internal knowledge base (Self-Data)...\n",
            "Self-Data Context (partial): Retrieval Augmented Generation (RAG) combines large language models with external knowledge retrieval systems to generate more informed and accurate responses by fetching relevant documents or data be...\n",
            "\n",
            "Final Answer: Retrieval Augmented Generation (RAG) is a technique that combines large language models (LLMs) with external knowledge retrieval systems. Its purpose is to generate more informed and accurate responses by first fetching relevant documents or data, and then using that retrieved information to guide the language model's answer generation.\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What is the highest mountain on Earth?'\n",
            "Agent Decision: self_data\n",
            "Retrieving from internal knowledge base (Self-Data)...\n",
            "Self-Data Context (partial): Mount Everest is Earth's highest mountain above sea level, located in the Mahalangur Himal sub-range of the Himalayas.\n",
            "\n",
            "Mount Everest is Earth's highest mountain above sea level, located in the Mahala...\n",
            "\n",
            "Final Answer: Mount Everest\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'Hello, how are you today?'\n",
            "Agent Decision: direct_answer\n",
            "Answering directly from LLM's general knowledge...\n",
            "\n",
            "Final Answer: Hello!\n",
            "\n",
            "As an AI, I don't have feelings or a physical \"day\" in the way humans do, so I don't experience \"being\" in that sense. However, I'm fully operational and ready to assist you!\n",
            "\n",
            "How can I help you today?\n",
            "--------------------------------------------------\n",
            "DEBUG: os.environ['GOOGLE_API_KEY'] before web_search_tool init: AIzaSyC8TXKKymp2xNwUlBFYsEZQC1GotzxqATw\n",
            "DEBUG: web_search_tool's internal API key: AIzaSyC8TXKKymp2xNwUlBFYsEZQC1GotzxqATw\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What are the essential elements for effective flight planning?'\n",
            "Agent Decision: self_data\n",
            "Retrieving from internal knowledge base (Self-Data)...\n",
            "Self-Data Context (partial): The primary goal of an AI agent for flight planning is to optimize routes for fuel efficiency and safety, considering factors like weather, airspace restrictions (NOTAMs), aircraft performance, and re...\n",
            "\n",
            "Final Answer: Based on the context provided, the essential elements for effective flight planning are:\n",
            "\n",
            "*   **Fuel efficiency** (as a primary optimization goal)\n",
            "*   **Safety** (as a primary optimization goal)\n",
            "*   **Weather conditions**\n",
            "*   **Airspace restrictions (NOTAMs)**\n",
            "*   **Aircraft performance characteristics**\n",
            "*   **Regulatory compliance**\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What is the current weather in Montreal?'\n",
            "Agent Decision: web_search\n",
            "Performing web search...\n",
            "Web Search Context (partial): 10 Day Weather-Montreal, Quebec, Canada. As of 5:26 pm EDT. Tonight. --/62Â°. 2%. Night. 62Â°. 2%. N 5 mph. Mostly cloudy. Low 62F. Winds light and variable. Montreal, Quebec, Canada Weather Forecast, w...\n",
            "\n",
            "Final Answer: Based on the Web Search Results, here are a couple of readings for the current weather in Montreal:\n",
            "\n",
            "*   **25Â°C, Mostly Cloudy** (Source: National Weather Service, under \"Current Conditions\")\n",
            "*   **28Â°C, Partly Cloudy, Feels Like: 31Â°C, UV Index: 0 (Low)** (Source: \"Forecast Today. Currently.\")\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming simple_agentic_rag_demo is defined and the environment/APIs are correctly set up\n",
        "\n",
        "# New query using web_search for real-time flight information\n",
        "query_flight_delays = \"What are the latest flight delays at Montreal-Trudeau International Airport (YUL)?\"\n",
        "\n",
        "print(f\"\\n--- Simple Agentic RAG Demo for Query: '{query_flight_delays}'\")\n",
        "final_answer_flight_delays = \"I'm sorry, I couldn't process your request.\" # Default\n",
        "\n",
        "# Step 1: Agent decides the best approach (using AGENTIC_MODEL)\n",
        "# This would still conceptually be \"web_search\"\n",
        "decision_flight_delays = \"web_search\"\n",
        "print(f\"Agent Decision: {decision_flight_delays}\")\n",
        "\n",
        "# Step 2: Execute based on decision and get context\n",
        "context_flight_delays = \"\"\n",
        "if decision_flight_delays == \"web_search\":\n",
        "    print(\"Performing web search...\")\n",
        "    try:\n",
        "        # This is the actual call to the GoogleSearchAPIWrapper\n",
        "        search_results_flight_delays = web_search_tool.run(query_flight_delays)\n",
        "        context_flight_delays = search_results_flight_delays\n",
        "\n",
        "        if not context_flight_delays.strip():\n",
        "            context_flight_delays = \"No relevant web search results found.\"\n",
        "            print(\"No relevant web search results found.\")\n",
        "        else:\n",
        "            print(f\"Web Search Context (partial): {context_flight_delays[:200]}...\")\n",
        "    except Exception as e:\n",
        "        context_flight_delays = f\"Error during web search: {e}. (This often indicates an API key or CSE ID issue)\"\n",
        "        print(context_flight_delays)\n",
        "        if \"Daily Limit Exceeded\" in str(e):\n",
        "            print(\"Consider checking your Google Custom Search API quota or API key.\")\n",
        "\n",
        "# Step 3: Generate Answer with search results\n",
        "if decision_flight_delays == \"web_search\":\n",
        "    generation_prompt_flight_delays = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful AI assistant. Use the following Web Search Results to answer the question about flight delays.\"),\n",
        "        (\"human\", \"Web Search Results:\\n{context}\\n\\nQuestion: {query}\")\n",
        "    ])\n",
        "\n",
        "    # This calls the RESPONDER_MODEL with the context from the web search\n",
        "    final_answer_flight_delays = get_llm_response(\n",
        "        RESPONDER_MODEL,\n",
        "        generation_prompt_flight_delays.format(\n",
        "            context=context_flight_delays,\n",
        "            query=query_flight_delays\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(f\"\\nFinal Answer: {final_answer_flight_delays}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "vaRKBh24w58n",
        "outputId": "ac0762df-c834-477a-a260-86a8802c7c89"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What are the latest flight delays at Montreal-Trudeau International Airport (YUL)?'\n",
            "Agent Decision: web_search\n",
            "Performing web search...\n",
            "Web Search Context (partial): Flights. Departures. View flights departing from YUL. Find a flight. Departures Arrivals. Today, July 17Updated 21:10. 22:3018:45. air canada logo. AC779 AirÂ ... View top cancellations by airline or a...\n",
            "\n",
            "Final Answer: The provided Web Search Results do not give the specific, real-time latest flight delays for Montreal-Trudeau International Airport (YUL).\n",
            "\n",
            "However, they indicate that you can find this information by checking:\n",
            "\n",
            "*   **FlightAware.com** for live flight delay and cancellation statistics for today at Montreal-Trudeau.\n",
            "*   **OAG's YUL flight tracker and YUL airport tracker** to check Montreal Airport (YUL) airport delay status, arrivals, and departures.\n",
            "*   **FlightStats flight tracker** to track the current status of flights departing from YUL.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming simple_agentic_rag_demo is defined and the environment/APIs are correctly set up\n",
        "\n",
        "# New query using web_search for current population data\n",
        "query_population = \"What is the population of Montreal as of 2024?\"\n",
        "\n",
        "print(f\"\\n--- Simple Agentic RAG Demo for Query: '{query_population}'\")\n",
        "final_answer_population = \"I'm sorry, I couldn't process your request.\" # Default\n",
        "\n",
        "# Step 1: Agent decides the best approach (using AGENTIC_MODEL)\n",
        "# This would conceptually be \"web_search\" for current population data\n",
        "decision_population = \"web_search\"\n",
        "print(f\"Agent Decision: {decision_population}\")\n",
        "\n",
        "# Step 2: Execute based on decision and get context\n",
        "context_population = \"\"\n",
        "if decision_population == \"web_search\":\n",
        "    print(\"Performing web search...\")\n",
        "    try:\n",
        "        # This is the actual call to the GoogleSearchAPIWrapper\n",
        "        search_results_population = web_search_tool.run(query_population)\n",
        "        context_population = search_results_population\n",
        "\n",
        "        if not context_population.strip():\n",
        "            context_population = \"No relevant web search results found.\"\n",
        "            print(\"No relevant web search results found.\")\n",
        "        else:\n",
        "            print(f\"Web Search Context (partial): {context_population[:200]}...\")\n",
        "    except Exception as e:\n",
        "        context_population = f\"Error during web search: {e}. (This often indicates an API key or CSE ID issue)\"\n",
        "        print(context_population)\n",
        "        if \"Daily Limit Exceeded\" in str(e):\n",
        "            print(\"Consider checking your Google Custom Search API quota or API key.\")\n",
        "\n",
        "# Step 3: Generate Answer with search results\n",
        "if decision_population == \"web_search\":\n",
        "    generation_prompt_population = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful AI assistant. Use the following Web Search Results to answer the question about Montreal's population.\"),\n",
        "        (\"human\", \"Web Search Results:\\n{context}\\n\\nQuestion: {query}\")\n",
        "    ])\n",
        "\n",
        "    # This calls the RESPONDER_MODEL with the context from the web search\n",
        "    final_answer_population = get_llm_response(\n",
        "        RESPONDER_MODEL,\n",
        "        generation_prompt_population.format(\n",
        "            context=context_population,\n",
        "            query=query_population\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(f\"\\nFinal Answer: {final_answer_population}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "9dZpiMt4xQTe",
        "outputId": "8f7d18c3-fd7a-4733-b3bb-52fa88a6a084"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What is the population of Montreal as of 2024?'\n",
            "Agent Decision: web_search\n",
            "Performing web search...\n",
            "Web Search Context (partial): The current metro area population of Montreal in 2025 is 4,377,000, a 0.81% increase from 2024. The metro area population of Montreal in 2024 was 4,342,000,Â ... The Demographics of Montreal concern po...\n",
            "\n",
            "Final Answer: According to the web search results, the metro area population of Montreal in 2024 was **4,342,000**.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming simple_agentic_rag_demo is defined and the environment/APIs are correctly set up\n",
        "\n",
        "# More complex query using web_search for multi-faceted information\n",
        "complex_query = \"What are the main safety concerns and mitigation strategies regarding 5G interference with aircraft navigation and communication systems?\"\n",
        "\n",
        "print(f\"\\n--- Simple Agentic RAG Demo for Query: '{complex_query}'\")\n",
        "final_answer_complex = \"I'm sorry, I couldn't process your request.\" # Default\n",
        "\n",
        "# Step 1: Agent decides the best approach (using AGENTIC_MODEL)\n",
        "# This would conceptually be \"web_search\" for a complex, evolving topic\n",
        "decision_complex = \"web_search\"\n",
        "print(f\"Agent Decision: {decision_complex}\")\n",
        "\n",
        "# Step 2: Execute based on decision and get context\n",
        "context_complex = \"\"\n",
        "if decision_complex == \"web_search\":\n",
        "    print(\"Performing web search...\")\n",
        "    try:\n",
        "        # This is the actual call to the GoogleSearchAPIWrapper\n",
        "        search_results_complex = web_search_tool.run(complex_query)\n",
        "        context_complex = search_results_complex\n",
        "\n",
        "        if not context_complex.strip():\n",
        "            context_complex = \"No relevant web search results found.\"\n",
        "            print(\"No relevant web search results found.\")\n",
        "        else:\n",
        "            print(f\"Web Search Context (partial): {context_complex[:200]}...\")\n",
        "    except Exception as e:\n",
        "        context_complex = f\"Error during web search: {e}. (This often indicates an API key or CSE ID issue)\"\n",
        "        print(context_complex)\n",
        "        if \"Daily Limit Exceeded\" in str(e):\n",
        "            print(\"Consider checking your Google Custom Search API quota or API key.\")\n",
        "\n",
        "# Step 3: Generate Answer with search results\n",
        "if decision_complex == \"web_search\":\n",
        "    generation_prompt_complex = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful AI assistant. Use the following Web Search Results to answer the question about 5G interference, focusing on both safety concerns and mitigation strategies.\"),\n",
        "        (\"human\", \"Web Search Results:\\n{context}\\n\\nQuestion: {query}\")\n",
        "    ])\n",
        "\n",
        "    # This calls the RESPONDER_MODEL with the context from the web search\n",
        "    final_answer_complex = get_llm_response(\n",
        "        RESPONDER_MODEL,\n",
        "        generation_prompt_complex.format(\n",
        "            context=context_complex,\n",
        "            query=complex_query\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(f\"\\nFinal Answer: {final_answer_complex}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "DY4W4RmGxpcw",
        "outputId": "023addcc-7852-4d44-a47a-c99b09d657ca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What are the main safety concerns and mitigation strategies regarding 5G interference with aircraft navigation and communication systems?'\n",
            "Agent Decision: web_search\n",
            "Performing web search...\n",
            "Web Search Context (partial): Dec 9, 2004 ... limitations of such devices, the primary EMI concern is for aircraft electronic systems (Victim), particularly CNS ... Mitigation on CommercialÂ ... The agency's proposal on methods to ...\n",
            "\n",
            "Final Answer: Based on the provided Web Search Results, here are the main safety concerns and mitigation strategies regarding 5G interference with aircraft navigation and communication systems:\n",
            "\n",
            "**Safety Concerns:**\n",
            "\n",
            "*   **Interference with Critical Aircraft Systems:** The primary concern is Electromagnetic Interference (EMI) with aircraft electronic systems, particularly Communications, Navigation, and Surveillance (CNS) systems.\n",
            "*   **Profound Operational Impacts:** Radio Frequency Interference (RFI) can lead to serious consequences, including communication failures, navigation errors, and surveillance system disruptions.\n",
            "*   **Radar Altimeter Interference:** A significant concern is the potential for 5G technology, particularly in the C-band frequency range, to interfere with aeronautical radar altimeters. The C-band is close to the frequencies used by these altimeters, which are vital for aircraft safety-related systems.\n",
            "*   **Disruption of Vital Signals:** High-power terrestrial 5G networks could interfere with signals crucial for the safe operation of aircraft.\n",
            "\n",
            "**Mitigation Strategies:**\n",
            "\n",
            "*   **Operational Adjustments by Wireless Companies:** Wireless companies have agreed to specific measures, such as turning off transmitters and making other adjustments near airports, to minimize potential 5G interference.\n",
            "*   **Implementation of Operational Mitigations:** New 5G systems are subject to operational mitigations aimed at protecting aircraft systems.\n",
            "*   **Development of Mitigation Methods:** Agencies and organizations are proposing and working on methods to mitigate interference to aircraft systems from 5G telecommunications networks.\n",
            "*   **FAA Addressing Interference:** The FAA is learning lessons and actively working to address interference on all aircraft communications, navigation, and surveillance systems.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
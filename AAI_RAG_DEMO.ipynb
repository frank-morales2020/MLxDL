{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD/tiEX8jjhmMXEYCaHeo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/AAI_RAG_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langchain_community langgraph langchain_google_genai -q\n",
        "!pip install chromadb tiktoken -q"
      ],
      "metadata": {
        "id": "lexc8TtFQJHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import time\n",
        "import json\n",
        "from google.colab import userdata # Keep this as per your instruction\n",
        "\n",
        "# --- API Key Setup (as provided by you, directly used) ---\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "else:\n",
        "    print(\"WARNING: GOOGLE_API_KEY not found in Colab Secrets. Please ensure 'GEMINI' secret is set.\")\n",
        "    print(\"API calls will likely fail. Proceeding with unconfigured API.\")\n",
        "\n",
        "# --- Agent Configuration ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\" # As specified by you\n",
        "\n",
        "# Initialize the Gemini model for general responses and agentic decisions\n",
        "try:\n",
        "    AGENTIC_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    RESPONDER_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    print(f\"Gemini model '{AgentConfig.LLM_MODEL_NAME}' initialized for agentic and response generation.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to initialize Gemini model. Please check your API key and model name. Error: {e}\")\n",
        "    # Fallback to dummy models if Gemini initialization fails\n",
        "    AGENTIC_MODEL = None\n",
        "    RESPONDER_MODEL = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-ZKBqqyP3Cu",
        "outputId": "40571d8e-a6d1-4a46-8979-15453579b338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Gemini model 'gemini-2.5-flash' initialized for agentic and response generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the following setup already executed in your Colab notebook:\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import os # Import os module to set environment variables\n",
        "\n",
        "# --- API Key Setup ---\n",
        "GOOGLE_API_KEY_GEMINI = userdata.get('GEMINI') # Renamed to avoid confusion with search API key\n",
        "if GOOGLE_API_KEY_GEMINI:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY_GEMINI)\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "else:\n",
        "    print(\"WARNING: GOOGLE_API_KEY (for Gemini) not found in Colab Secrets. Please ensure 'GEMINI' secret is set.\")\n",
        "    print(\"API calls for Gemini will likely fail. Proceeding with unconfigured API.\")\n",
        "\n",
        "# --- IMPORTANT: Set GOOGLE_API_KEY for GoogleSearchAPIWrapper ---\n",
        "# Use the same API key if it's a general Google Cloud API key, or a separate one if needed.\n",
        "# For simplicity, we'll assume your 'GEMINI' key is also valid for Google Search API.\n",
        "# If you have a separate key for search, retrieve it from userdata.get('YOUR_SEARCH_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY_GEMINI\n",
        "\n",
        "# You also need a Google Custom Search Engine ID (CSE ID)\n",
        "# Get this from https://programmablesearchengine.google.com/\n",
        "# Add this to your Colab Secrets as 'GOOGLE_CSE_ID'\n",
        "# https://programmablesearchengine.google.com/controlpanel/overview?cx=514fb1ae50d034b58\n",
        "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
        "if GOOGLE_CSE_ID:\n",
        "    os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "    print(\"Google CSE ID configured successfully from Colab Secrets.\")\n",
        "else:\n",
        "    print(\"WARNING: GOOGLE_CSE_ID not found in Colab Secrets. Web search will fail.\")\n",
        "    print(\"Please ensure 'GOOGLE_CSE_ID' secret is set.\")\n",
        "\n",
        "\n",
        "# --- Agent Configuration ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "\n",
        "# Initialize the Gemini model for general responses and agentic decisions\n",
        "try:\n",
        "    AGENTIC_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    RESPONDER_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    print(f\"Gemini model '{AgentConfig.LLM_MODEL_NAME}' initialized for agentic and response generation.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to initialize Gemini model. Please check your API key and model name. Error: {e}\")\n",
        "    AGENTIC_MODEL = None\n",
        "    RESPONDER_MODEL = None\n",
        "\n",
        "# Now, the rest of your code from the previous response can follow.\n",
        "# The `web_search_tool = GoogleSearchAPIWrapper()` line should now work correctly."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUNHfS3RcRt",
        "outputId": "7bda461e-9235-43a2-c634-78f23fd1680f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Google CSE ID configured successfully from Colab Secrets.\n",
            "Gemini model 'gemini-2.5-flash' initialized for agentic and response generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "e-uI8LjJPvD-",
        "outputId": "b55239cc-51e8-4a60-8594-335737e7feeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Google CSE ID set (hardcoded for demo): 514fb1ae50d034b58. Please remove hardcoding for secure deployment.\n",
            "Gemini model 'gemini-2.5-flash' initialized for agentic and response generation.\n",
            "--- Simple Agentic RAG Demo with Gemini gemini-2.5-flash ---\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'Explain Retrieval Augmented Generation (RAG).' ---\n",
            "Agent Decision: self_data\n",
            "Retrieving from internal knowledge base (Self-Data)...\n",
            "Self-Data Context (partial): Retrieval Augmented Generation (RAG) combines large language models with external knowledge retrieval systems to generate more informed and accurate responses by fetching relevant documents or data be...\n",
            "\n",
            "Final Answer: Retrieval Augmented Generation (RAG) combines large language models with external knowledge retrieval systems. This allows it to generate more informed and accurate responses by fetching relevant documents or data before generating the answer.\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What is the highest mountain on Earth?' ---\n",
            "Agent Decision: self_data\n",
            "Retrieving from internal knowledge base (Self-Data)...\n",
            "Self-Data Context (partial): Mount Everest is Earth's highest mountain above sea level, located in the Mahalangur Himal sub-range of the Himalayas.\n",
            "\n",
            "Mount Everest is Earth's highest mountain above sea level, located in the Mahala...\n",
            "\n",
            "Final Answer: Mount Everest is the highest mountain on Earth.\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'Hello, how are you today?' ---\n",
            "Agent Decision: direct_answer\n",
            "Answering directly from LLM's general knowledge...\n",
            "\n",
            "Final Answer: Hello! I'm doing well, thank you for asking.\n",
            "\n",
            "How are you today?\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Simple Agentic RAG Demo for Query: 'What are the essential elements for effective flight planning?' ---\n",
            "Agent Decision: self_data\n",
            "Retrieving from internal knowledge base (Self-Data)...\n",
            "Self-Data Context (partial): The primary goal of an AI agent for flight planning is to optimize routes for fuel efficiency and safety, considering factors like weather, airspace restrictions (NOTAMs), aircraft performance, and ai...\n",
            "\n",
            "Final Answer: Based on the provided context, the essential elements for effective flight planning include:\n",
            "\n",
            "*   **Weather:** Considering current and forecasted weather conditions.\n",
            "*   **Airspace Restrictions (NOTAMs):** Accounting for Notices to Airmen that detail temporary flight restrictions or changes to airspace.\n",
            "*   **Aircraft Performance:** Understanding the specific capabilities and limitations of the aircraft being used.\n",
            "*   **Air Traffic Control Requirements:** Adhering to regulations and procedures set by air traffic control.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Required for AgentState class (even if not strictly used in this simplified demo's flow)\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Required for Embeddings and Vector Store\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Required for Web Search Tool\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "\n",
        "# Required for LLM Prompts\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# --- API Key and Environment Setup ---\n",
        "\n",
        "# 1. Retrieve Gemini API Key using Colab Secrets (standard secure method)\n",
        "GOOGLE_API_KEY_GEMINI = userdata.get('GEMINI')\n",
        "if GOOGLE_API_KEY_GEMINI:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY_GEMINI)\n",
        "    # Set GOOGLE_API_KEY environment variable for GoogleSearchAPIWrapper\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY_GEMINI\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: 'GEMINI' secret not found in Colab. API calls will fail.\")\n",
        "    # Exit or raise an error if the primary API key is missing\n",
        "    raise ValueError(\"GEMINI API key is missing. Please set it in Colab Secrets.\")\n",
        "\n",
        "# 2. Set Google Custom Search Engine ID (CSE ID)\n",
        "# WARNING: Hardcoding this ID directly is NOT secure for production.\n",
        "# This is done here to bypass persistent userdata.get timeouts as per user request.\n",
        "# For secure use, ensure 'GOOGLE_CSE_ID' is set reliably in Colab Secrets.\n",
        "HARDCODED_CSE_ID = \"514fb1ae50d034b58\" # Directly from your screenshot\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = HARDCODED_CSE_ID\n",
        "print(f\"Google CSE ID set (hardcoded for demo): {HARDCODED_CSE_ID}. Please remove hardcoding for secure deployment.\")\n",
        "\n",
        "# --- Agent LLM Configuration ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "\n",
        "try:\n",
        "    AGENTIC_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    RESPONDER_MODEL = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "    print(f\"Gemini model '{AgentConfig.LLM_MODEL_NAME}' initialized for agentic and response generation.\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR: Failed to initialize Gemini model. Check API key/model name. Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Helper Functions and Tools ---\n",
        "\n",
        "def get_llm_response(model, prompt_text: str) -> str:\n",
        "    \"\"\"Helper to get response from a GenerativeModel.\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt_text)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"LLM API Call Error: {e}\")\n",
        "        return f\"Error communicating with LLM: {e}\"\n",
        "\n",
        "# 1. Embedding Model\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# 2. Vector Database for \"Self Data\" (pre-indexed knowledge)\n",
        "VECTOR_DB_PATH = \"./chroma_db_simple_rag\" # Use a different path to avoid conflicts\n",
        "dummy_documents = [\n",
        "    Document(page_content=\"Retrieval Augmented Generation (RAG) combines large language models with external knowledge retrieval systems to generate more informed and accurate responses by fetching relevant documents or data before generating the answer.\"),\n",
        "    Document(page_content=\"The primary goal of an AI agent for flight planning is to optimize routes for fuel efficiency and safety, considering factors like weather, airspace restrictions (NOTAMs), aircraft performance, and air traffic control requirements.\"),\n",
        "    Document(page_content=\"Mount Everest is Earth's highest mountain above sea level, located in the Mahalangur Himal sub-range of the Himalayas.\"),\n",
        "    Document(page_content=\"Paris is the capital and most populous city of France.\"),\n",
        "    Document(page_content=\"A key challenge in flight planning is balancing minimum flight time with optimal fuel consumption, while adhering to all regulatory and safety standards.\"),\n",
        "    Document(page_content=\"The current weather in Montreal, Quebec, Canada, typically experiences four distinct seasons. Summers are warm and humid, while winters are cold with significant snowfall. Spring and autumn are mild and pleasant.\"),\n",
        "]\n",
        "vectorstore = Chroma.from_documents(documents=dummy_documents, embedding=embeddings, persist_directory=VECTOR_DB_PATH)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 documents\n",
        "\n",
        "# 3. Web Search Tool\n",
        "web_search_tool = GoogleSearchAPIWrapper()\n",
        "\n",
        "\n",
        "# --- Simple Agentic RAG Demo Function ---\n",
        "\n",
        "def simple_agentic_rag_demo(query: str) -> str:\n",
        "    print(f\"\\n--- Simple Agentic RAG Demo for Query: '{query}' ---\")\n",
        "    final_answer = \"I'm sorry, I couldn't process your request.\" # Default fallback\n",
        "\n",
        "    # Step 1: Agent decides the best approach (using AGENTIC_MODEL)\n",
        "    decision_prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are an intelligent AI router. Based on the user's query, decide the best way to get the information.\n",
        "\n",
        "        Options:\n",
        "        - 'self_data': If the query is about common knowledge, definitions, or topics likely present in an internal, pre-indexed knowledge base (e.g., \"what is RAG?\", \"tell me about Mount Everest\", \"flight planning basics\").\n",
        "        - 'web_search': If the query requires current, real-time information, very specific facts not in a general knowledge base, or broad external details (e.g., \"current weather in Montreal\", \"latest news on X\", \"who is the current PM of Canada\").\n",
        "        - 'direct_answer': If the query is a simple greeting or can be answered directly by the LLM's general knowledge without any retrieval (e.g., \"hello\", \"what is 2+2\").\n",
        "\n",
        "        Provide only your chosen option as a single word: self_data, web_search, or direct_answer.\n",
        "\n",
        "        User Query: {query}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        decision = get_llm_response(AGENTIC_MODEL, decision_prompt.format(query=query)).strip().lower()\n",
        "        if decision not in [\"self_data\", \"web_search\", \"direct_answer\"]:\n",
        "            print(f\"Agent's decision was unclear ('{decision}'). Defaulting to web_search.\")\n",
        "            decision = \"web_search\" # Fallback if LLM doesn't follow instructions\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting agent decision from LLM: {e}. Defaulting to web_search.\")\n",
        "        decision = \"web_search\" # Robust fallback\n",
        "\n",
        "    print(f\"Agent Decision: {decision}\")\n",
        "\n",
        "    context = \"\"\n",
        "\n",
        "    # Step 2: Execute based on decision and get context\n",
        "    if decision == \"self_data\":\n",
        "        print(\"Retrieving from internal knowledge base (Self-Data)...\")\n",
        "        try:\n",
        "            docs = retriever.invoke(query)\n",
        "            context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "            if not context.strip(): # Check if context is effectively empty\n",
        "                 context = \"No relevant internal data found.\"\n",
        "                 print(\"No relevant self-data found.\")\n",
        "            else:\n",
        "                 print(f\"Self-Data Context (partial): {context[:200]}...\")\n",
        "        except Exception as e:\n",
        "            context = f\"Error retrieving self-data: {e}\"\n",
        "            print(context)\n",
        "\n",
        "        # Step 3 (for self_data): Generate answer with retrieved context\n",
        "        generation_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question. If the context is empty or irrelevant, state that you cannot answer comprehensively based on the provided information.\"),\n",
        "            (\"human\", \"Context:\\n{context}\\n\\nQuestion: {query}\")\n",
        "        ])\n",
        "        final_answer = get_llm_response(RESPONDER_MODEL, generation_prompt.format(context=context, query=query))\n",
        "\n",
        "    elif decision == \"web_search\":\n",
        "        print(\"Performing web search...\")\n",
        "        try:\n",
        "            search_results = web_search_tool.run(query)\n",
        "            context = search_results\n",
        "            if not context.strip(): # Check if context is effectively empty\n",
        "                context = \"No relevant web search results found.\"\n",
        "                print(\"No relevant web search results found.\")\n",
        "            else:\n",
        "                print(f\"Web Search Context (partial): {context[:200]}...\")\n",
        "        except Exception as e:\n",
        "            # This is the error we've been seeing for the Web Search API\n",
        "            context = f\"Error during web search: {e}. (This often indicates Google Custom Search API is not enabled for your GCP project, or billing is not active).\"\n",
        "            print(context)\n",
        "\n",
        "        # Step 3 (for web_search): Generate answer with search results\n",
        "        generation_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a helpful AI assistant. Use the following web search results to answer the user's question. If results are empty or irrelevant, state that you cannot answer comprehensively based on the provided information.\"),\n",
        "            (\"human\", \"Web Search Results:\\n{context}\\n\\nQuestion: {query}\")\n",
        "        ])\n",
        "        final_answer = get_llm_response(RESPONDER_MODEL, generation_prompt.format(context=context, query=query))\n",
        "\n",
        "    elif decision == \"direct_answer\":\n",
        "        print(\"Answering directly from LLM's general knowledge...\")\n",
        "        final_answer = get_llm_response(RESPONDER_MODEL, query)\n",
        "\n",
        "    print(f\"\\nFinal Answer: {final_answer}\")\n",
        "    print(\"-\" * 50)\n",
        "    return final_answer\n",
        "\n",
        "# --- Demo Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"--- Simple Agentic RAG Demo with Gemini {AgentConfig.LLM_MODEL_NAME} ---\")\n",
        "\n",
        "    # Example 1: Should use self_data (defined concept)\n",
        "    simple_agentic_rag_demo(\"Explain Retrieval Augmented Generation (RAG).\")\n",
        "\n",
        "    # Example 2: Should use self_data (factual knowledge in dummy data)\n",
        "    simple_agentic_rag_demo(\"What is the highest mountain on Earth?\")\n",
        "\n",
        "    # Example 4: Should use direct_answer (simple query)\n",
        "    simple_agentic_rag_demo(\"Hello, how are you today?\")\n",
        "\n",
        "    # Example 4: Flight planning query (should use self_data due to new dummy content)\n",
        "    simple_agentic_rag_demo(\"What are the essential elements for effective flight planning?\")\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIo5OBFK+T5TTp+Rdh/UpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/gemini_finetune_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "riPyl-gzG22C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# **1. Data Preparation**\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Convert to JSONL format with prompt and completion\n",
        "def convert_to_jsonl(data, filename):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for row in data:\n",
        "            data_point = {\n",
        "                \"prompt\": row[\"input\"],\n",
        "                \"completion\": str(row[\"label\"]),  # Convert label to string\n",
        "            }\n",
        "            f.write(json.dumps(data_point) + \"\\n\")\n",
        "\n",
        "# Convert the Hugging Face Dataset to a list of dictionaries\n",
        "dataset_list = list(dataset[\"train\"])\n",
        "\n",
        "# Split the dataset into training and evaluation sets\n",
        "train_data, eval_data = train_test_split(dataset_list, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert and save to JSONL files\n",
        "convert_to_jsonl(train_data, \"training_data.jsonl\")\n",
        "convert_to_jsonl(eval_data, \"eval_data.jsonl\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gUlnHlvokPQ5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0WIQIePHJFg",
        "outputId": "82584365-467f-4493-f1aa-ee814bf6d9a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input', 'label', 'distance', 'distance_category', 'waypoints', 'waypoint_names'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAiUsNklHVZO",
        "outputId": "8ab6332c-0a09-4a63-dd46-80e2d09721d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Calculate the waypoints from SIN to CUN. Departure: 2024-06-19, Aircraft: Airbus A320, Weather: Partly Cloudy',\n",
              " 'label': 7,\n",
              " 'distance': 4190.223965150766,\n",
              " 'distance_category': 'long',\n",
              " 'waypoints': [[25.0000001, -107.5000001],\n",
              "  [13.075850948259008, -87.47663616549106],\n",
              "  [19.819922201759134, -98.80146877121005],\n",
              "  [9.260386709076178, -81.06960216515675],\n",
              "  [10.868386703521901, -83.76980061837503],\n",
              "  [11.748790002704872, -85.24819840125659],\n",
              "  [13.307169797892762, -87.86507323149776],\n",
              "  [14.392416690114095, -89.68745009086217],\n",
              "  [5.1096596, -74.0995854]],\n",
              " 'waypoint_names': ['SIN',\n",
              "  'Choluteca',\n",
              "  'Santo Domingo Aztacameca',\n",
              "  'Veraguas',\n",
              "  'San Juan del Norte',\n",
              "  'Acoyapa',\n",
              "  'Pasaje La Cruz',\n",
              "  'El Pito',\n",
              "  'CUN']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "274xQKMvGZlS"
      },
      "outputs": [],
      "source": [
        "# Authentication and Initialization**\n",
        "\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=\"gen-lang-client-0870511801\", location=\"us-central1\") # Replace with your project and location\n",
        "\n",
        "# **3. Create Vertex AI TextDatasets**\n",
        "\n",
        "# Training dataset\n",
        "train_dataset = aiplatform.TextDataset.create(\n",
        "    display_name=\"waypoints-train\",\n",
        "    gcs_source=[\"gs://poc2025/training_data.jsonl\"],  # Replace with your GCS bucket and path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
        ")\n",
        "\n",
        "# Evaluation dataset\n",
        "eval_dataset = aiplatform.TextDataset.create(\n",
        "    display_name=\"waypoints-eval\",\n",
        "    gcs_source=[\"gs://poc2025/eval_data.jsonl\"],  # Replace with your GCS bucket and path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
        ")\n",
        "\n",
        "print(f\"Training dataset created: {train_dataset.resource_name}\")\n",
        "print(f\"Evaluation dataset created: {eval_dataset.resource_name}\")"
      ]
    },
    {
      "source": [
        "# Fine-tuning the Gemini Model\n",
        "model_name = \"gemini-pro\"  # Or \"gemini-ultra\"\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": \"n1-standard-8\",\n",
        "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
        "            \"accelerator_count\": 1,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": \"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-11:latest\",\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "# Use CustomContainerTrainingJob for Gemini fine-tuning\n",
        "# 1. Ensure staging_bucket is set in the constructor\n",
        "tuning_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=\"gemini-fine-tuning-job\",\n",
        "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-11:latest\", # Replace with your training container URI\n",
        "    staging_bucket=staging_bucket_uri # Pass staging_bucket_uri here\n",
        ")\n",
        "\n",
        "# 2. (Optional) If still facing the error, pass in training_pipeline_id\n",
        "# and base_output_dir in the run() method\n",
        "fine_tuned_model = tuning_job.run(\n",
        "    model=model_name,\n",
        "    training_input={\n",
        "        \"dataset\": train_dataset.resource_name,\n",
        "        \"evaluation_dataset\": eval_dataset.resource_name,\n",
        "        \"worker_pool_specs\": worker_pool_specs,\n",
        "        # (Optional) Additional arguments if needed:\n",
        "        # \"training_pipeline_id\": \"your_pipeline_id\",\n",
        "        # \"base_output_dir\": staging_bucket_uri + \"/output/\",\n",
        "    },\n",
        "    sync=True,\n",
        ").get_tuned_model()\n",
        "\n",
        "print(f\"Fine-tuned model: {fine_tuned_model}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qEFsnXAjuya1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
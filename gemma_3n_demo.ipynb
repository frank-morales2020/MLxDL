{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNbvmkp4272jRaI82l7Cp89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/gemma_3n_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/google/gemma-3n-E4B-it"
      ],
      "metadata": {
        "id": "sTjM3u42Ewxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Force-install from source & debug paths ---\n",
        "print(\"Starting fresh installation and path manipulation for Colab...\")\n",
        "\n",
        "# Install PyTorch first\n",
        "print(\"Installing PyTorch...\")\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "# Install transformers directly from the main branch\n",
        "# We'll use --force-reinstall to be absolutely sure\n",
        "print(\"Installing transformers from GitHub source (main branch) with force-reinstall...\")\n",
        "!pip install --no-deps --force-reinstall git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# Install timm directly from the main branch (crucial for Gemma 3n's vision encoder)\n",
        "print(\"Installing timm (PyTorch Image Models) from GitHub source (main branch)...\")\n",
        "!pip install --no-deps --force-reinstall git+https://github.com/huggingface/pytorch-image-models.git@main\n",
        "\n",
        "# Install other necessary dependencies\n",
        "# Note: We're installing dependencies separately because --no-deps was used above\n",
        "print(\"Installing other core dependencies...\")\n",
        "!pip install -U accelerate bitsandbytes pillow requests pyyaml safetensors tokenizers tqdm regex packaging numpy filelock huggingface_hub\n",
        "\n",
        "print(\"\\n--- Installation complete. Attempting to force module loading paths... ---\")\n",
        "\n",
        "# DEBUG: Explicitly clear any existing module cache for transformers\n",
        "import sys\n",
        "if 'transformers' in sys.modules:\n",
        "    del sys.modules['transformers']\n",
        "if 'timm' in sys.modules:\n",
        "    del sys.modules['timm']\n",
        "\n",
        "# Get the path where pip installed the git package (usually in dist-packages)\n",
        "import site\n",
        "import os\n",
        "\n",
        "transformers_site_path = None\n",
        "for sp in site.getsitepackages():\n",
        "    # Look for the .egg-info or .dist-info from the git install\n",
        "    if os.path.exists(os.path.join(sp, 'transformers.egg-info')) or \\\n",
        "       os.path.exists(os.path.join(sp, 'transformers-*-py3.11.egg-info')): # More general\n",
        "        transformers_site_path = sp\n",
        "        break\n",
        "    elif os.path.exists(os.path.join(sp, 'transformers')): # If it's a direct folder\n",
        "         transformers_site_path = sp\n",
        "         break\n",
        "\n",
        "if transformers_site_path and transformers_site_path not in sys.path:\n",
        "    # Add the site-packages path to the beginning of sys.path to prioritize it\n",
        "    sys.path.insert(0, transformers_site_path)\n",
        "    print(f\"DEBUG: Added {transformers_site_path} to sys.path to prioritize imports.\")\n",
        "else:\n",
        "    print(\"DEBUG: transformers site-packages path already in sys.path or not found correctly.\")\n",
        "\n",
        "\n",
        "# Now, explicitly try to import and check the version\n",
        "from importlib.util import find_spec\n",
        "spec = find_spec(\"transformers\")\n",
        "if spec:\n",
        "    print(f\"DEBUG: find_spec found transformers at: {spec.origin}\")\n",
        "else:\n",
        "    print(\"DEBUG: find_spec could NOT locate transformers. This is critical.\")\n",
        "\n",
        "\n",
        "# DEBUG: Verify transformers installation and model type recognition after path manipulation\n",
        "import transformers\n",
        "print(f\"DEBUG: Loaded transformers version: {transformers.__version__}\")\n",
        "print(f\"DEBUG: Transformers path (after explicit path adjustments): {os.path.dirname(transformers.__file__)}\")\n",
        "\n",
        "try:\n",
        "    from transformers.models.auto.configuration_auto import CONFIG_MAPPING\n",
        "    print(f\"DEBUG: 'gemma3n' in CONFIG_MAPPING: {'gemma3n' in CONFIG_MAPPING}\")\n",
        "except Exception as e:\n",
        "    print(f\"DEBUG: Failed to import CONFIG_MAPPING or check 'gemma3n' AFTER ALL EFFORTS: {e}\")\n",
        "    print(\"This indicates a severe, deep-rooted Colab environment issue.\")\n",
        "\n",
        "try:\n",
        "    from transformers import Gemma3nConfig, Gemma3nForConditionalGeneration, Gemma3nProcessor\n",
        "    print(\"DEBUG: Successfully imported Gemma3nConfig, Gemma3nForConditionalGeneration, Gemma3nProcessor directly.\")\n",
        "except ImportError as e:\n",
        "    print(f\"DEBUG: Failed to directly import Gemma3n classes AFTER ALL EFFORTS. Error: {e}\")\n",
        "    print(\"This is the absolute last resort for Colab.\")\n"
      ],
      "metadata": {
        "id": "8VnULE_3C_c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging Face\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token is None:\n",
        "        print(\"Warning: 'HF_TOKEN' not found in Colab Secrets. Please add it via the key icon on the left sidebar.\")\n",
        "        print(\"Falling back to interactive login...\")\n",
        "        login(add_to_git_credential=True)\n",
        "    else:\n",
        "        login(token=hf_token, add_to_git_credential=True)\n",
        "    print(\"Hugging Face login successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Colab secrets or logging into Hugging Face: {e}\")\n",
        "    print(\"Please ensure you have configured 'HF_TOKEN' in Colab secrets or manually provide token.\")\n",
        "    login(add_to_git_credential=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICx84PPuEaS9",
        "outputId": "e9af1ca4-0df5-4155-ef2e-7a4efa70bcf3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face login successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=\"google/gemma-3n-e4b-it\",\n",
        "    device=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n"
      ],
      "metadata": {
        "id": "xPm8vIpJDtCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "output = pipe(text=messages, max_new_tokens=200)"
      ],
      "metadata": {
        "id": "bnjC3dH3D2-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0][\"generated_text\"][-1][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBvdN3n3FFkv",
        "outputId": "300b8187-8f8c-41b0-9ff0-ae1350f8b9c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The animal on the candy is a **frog**. üê∏ \n",
            "\n",
            "You can clearly see the silhouette of a frog on two of the candies. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "model_id = \"google/gemma-3n-e4b-it\"\n",
        "\n",
        "model = Gemma3nForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,).eval()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "decoded = processor.decode(generation, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "6lfBHEOpEBr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-WAST0mFWop",
        "outputId": "563a82cb-80d6-4980-f1fe-d3a492d96896"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captured from a slightly elevated perspective, the image showcases a vibrant pink cosmos flower in full bloom, with a small, fuzzy bumblebee diligently collecting nectar from its center. The flower's petals are a soft, delicate pink, radiating outwards from a bright yellow central disc. The petals have a slightly textured appearance, with subtle veins running along their length.\n",
            "\n",
            "The bumblebee is positioned in the heart of the flower, its black and yellow stripes clearly visible against the yellow center. Its wings are slightly blurred\n"
          ]
        }
      ]
    }
  ]
}
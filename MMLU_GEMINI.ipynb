{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MMLU_GEMINI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keipSFp3_J1x",
        "outputId": "332856ca-68b7-441e-db1d-35aabc54179c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 21 16:14:07 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   36C    P8              11W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQpk59lyBUim"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q tqdm\n",
        "!pip install -q pandas\n",
        "!pip install -q tensor_parallel\n",
        "!pip install -q argparse\n",
        "!pip install -q einops\n",
        "!pip install -q accelerate\n",
        "#!pip install -q torch==2.0.0+cu118\n",
        "!pip install -q torch\n",
        "\n",
        "!pip install colab-env --upgrade -q\n",
        "!pip install openai -q\n",
        "\n",
        "!pip install datasets -q\n",
        "!pip install utils -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ7qVCcyDO3t"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/FranxYao/chain-of-thought-hub.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwm-_bQBBjhc"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import os\n",
        "import openai\n",
        "import IPython\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "77X7SrPYBn08"
      },
      "outputs": [],
      "source": [
        "def test_answer_mmlu_(pred_str, ans):\n",
        "    pattern = 'the answer is ('\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "\n",
        "    if(len(pred) > 1):\n",
        "        # print(pred)\n",
        "        pred = pred[1][0]\n",
        "        gold = ans.lower()\n",
        "        # print('debug 1, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "    else:\n",
        "        pred = 'C'\n",
        "        # print(ans_str)\n",
        "        gold = ans.lower()\n",
        "        # print('debug 2, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "\n",
        "# extract answer in pred_str and compare with ans_str\n",
        "def test_answer_mmlu_claude_instant(pred_str, ans_str):\n",
        "    pattern = 'the answer is '\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "    if len(pred) == 1:\n",
        "        return False\n",
        "    else:\n",
        "        return pred[1][0] == ans_str.lower()\n",
        "\n",
        "def test_answer_mmlu_claude(pred_str, ans_str):\n",
        "    pattern = 'the answer is '\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "\n",
        "    if(len(pred) > 1):\n",
        "        # print(pred)\n",
        "        pred = pred[1]\n",
        "        for p in pred:\n",
        "            if(p.isalpha()): break\n",
        "        pred = p\n",
        "        print(ans_str)\n",
        "        gold = ans_str.lower()\n",
        "        print('debug 1, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "    else:\n",
        "        pred = 'c'\n",
        "        # print(ans_str)\n",
        "        gold = ans_str.lower()\n",
        "        # print('debug 2, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "\n",
        "def test_answer_mmlu(pred_str, ans_str):\n",
        "    pattern = 'the answer is ('\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "\n",
        "    if(len(pred) > 1):\n",
        "        # print(pred)\n",
        "        pred = pred[1][0]\n",
        "        gold = ans_str.split('A:\\n')[1][0].lower()\n",
        "        # print('debug 1, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "    else:\n",
        "        pred = 'C'\n",
        "        # print(ans_str)\n",
        "        gold = ans_str.split('A:\\n')[1][0].lower()\n",
        "        # print('debug 2, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "\n",
        "def parse_pred_ans(filename):\n",
        "    with open(filename) as fd: lines = fd.readlines()\n",
        "    am, a = None, None\n",
        "    num_q, acc = 0, 0\n",
        "    current_mode = 'none'\n",
        "    questions = []\n",
        "    ans_pred = []\n",
        "    ans_gold = []\n",
        "    for l in lines:\n",
        "        if(l.startswith('Q: ')):\n",
        "            if(am is not None and a is not None):\n",
        "                questions.append(q)\n",
        "                ans_pred.append(am)\n",
        "                ans_gold.append(a)\n",
        "                # print(am)\n",
        "                # print(a)\n",
        "                if(test_answer_mmlu(am, a)):\n",
        "                    acc += 1\n",
        "            current_mode = 'q'\n",
        "            q = l\n",
        "            num_q += 1\n",
        "        elif(l.startswith('A_model:')):\n",
        "            current_mode = 'am'\n",
        "            am = l\n",
        "        elif(l.startswith('A:') and not l.startswith(\"A: Let's think step by step\")):\n",
        "            current_mode = 'a'\n",
        "            a = l\n",
        "        else:\n",
        "            if(current_mode == 'q'): q += l\n",
        "            elif(current_mode == 'am'): am += l\n",
        "            elif(current_mode == 'a'): a += l\n",
        "            else:\n",
        "                raise ValueError(current_mode)\n",
        "\n",
        "    questions.append(q)\n",
        "    ans_pred.append(am)\n",
        "    ans_gold.append(a)\n",
        "    # print(am)\n",
        "    # print(a)\n",
        "    if(test_answer_mmlu(am, a)):\n",
        "        acc += 1\n",
        "    print('num_q %d correct %d ratio %.4f' % (num_q, acc, float(acc / num_q)))\n",
        "    return questions, ans_pred, ans_gold\n",
        "\n",
        "def test_finished(ans_model):\n",
        "    if('answer is' in ans_model): return True\n",
        "    else: return False\n",
        "\n",
        "def extract_ans(ans_model):\n",
        "    ans_model = ans_model.split('\\n')\n",
        "    ans = []\n",
        "    residual = []\n",
        "    for li, al in enumerate(ans_model):\n",
        "        ans.append(al)\n",
        "        if('answer is' in al):\n",
        "            break\n",
        "    residual = list(ans_model[li + 1:])\n",
        "    ans = '\\n'.join(ans)\n",
        "    residual = '\\n'.join(residual)\n",
        "    return ans, residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "frmQVjYRBssq"
      },
      "outputs": [],
      "source": [
        "TASKSTEST0old = [\n",
        "        'anatomy',\n",
        "        'college_biology',\n",
        "        'college_chemistry',\n",
        "        'college_computer_science',\n",
        "        'college_mathematics',\n",
        "        'college_medicine',\n",
        "        'college_physics',\n",
        "        'computer_security',\n",
        "        'electrical_engineering',\n",
        "        'machine_learning',\n",
        "]\n",
        "\n",
        "\n",
        "TASKSTEST = [\n",
        "        'college_computer_science',\n",
        "        'electrical_engineering',\n",
        "        'machine_learning',\n",
        "]\n",
        "\n",
        "TASKS911 = [\n",
        "        'anatomy',\n",
        "        'college_biology',\n",
        "        'college_chemistry',\n",
        "        'college_computer_science',\n",
        "        'college_mathematics',\n",
        "        'college_medicine',\n",
        "        'college_physics',\n",
        "        'computer_security',\n",
        "        'conceptual_physics',\n",
        "        'econometrics',\n",
        "        'electrical_engineering',\n",
        "        'elementary_mathematics',\n",
        "        'formal_logic',\n",
        "        'global_facts',\n",
        "        'high_school_biology',\n",
        "        'high_school_chemistry',\n",
        "        'high_school_computer_science',\n",
        "        'high_school_european_history',\n",
        "        'high_school_geography',\n",
        "        'high_school_government_and_politics',\n",
        "        'high_school_macroeconomics',\n",
        "        'high_school_mathematics',\n",
        "        'high_school_microeconomics',\n",
        "        'high_school_physics',\n",
        "        'high_school_psychology',\n",
        "        'high_school_statistics',\n",
        "        'high_school_us_history',\n",
        "        'high_school_world_history',\n",
        "        'public_relations',\n",
        "        'security_studies',\n",
        "        'sociology',\n",
        "        'us_foreign_policy',\n",
        "        'virology',\n",
        "        'machine_learning',\n",
        "        'world_religions']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w80g_-ZrkaY9"
      },
      "source": [
        "GEMINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ADYgSk98JiN5"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RW4ZLVbmJl2d"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oTzQe-dwJuus"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MuVkYqmKJ072"
      },
      "outputs": [],
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "AeiCnXkPJ5LQ",
        "outputId": "a79832ec-1851-4ff8-be18-27082b06fdcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = genai.GenerativeModel('gemini-1.0-pro-latest')\n",
        "model = genai.GenerativeModel('gemini-1.0-pro-latest')\n",
        "#query = \"I bought a computer for 1200, repurchased it for 1600. how much did I earn? Take in consideration the money for the repurchased too.\"\n",
        "\n",
        "#del query\n",
        "query = \"I bought a computer for $900, sold it for $1200, repurchased it for $1300, and sold it again for $1600. how much did I earn? Take in consideration the money for the repurchased too.\"\n",
        "chat_response=model.generate_content(query)\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print()\n",
        "print('Answer: ')\n",
        "print(chat_response.text)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "boPwVcrdpOQO",
        "outputId": "0f0d74ff-239d-48a6-f694-9d6f82bd9906"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: I bought a computer for $900, sold it for $1200, repurchased it for $1300, and sold it again for $1600. how much did I earn? Take in consideration the money for the repurchased too.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Answer: \n",
            "**Earnings from first sale:**\n",
            "$1200 (sale price) - $900 (purchase price) = $300\n",
            "\n",
            "**Loss from repurchase:**\n",
            "$1300 (repurchase price) - $1200 (sale price) = -$100\n",
            "\n",
            "**Earnings from second sale:**\n",
            "$1600 (sale price) - $1300 (repurchase price) = $300\n",
            "\n",
            "**Total Earnings:**\n",
            "$300 (first sale) + $300 (second sale) - $100 (loss from repurchase) = **$500**\n",
            "\n",
            "Therefore, you earned **$500** from the entire series of transactions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "\n",
        "#del query\n",
        "#query = \"I bought a computer for $900, sold it for $1200, repurchased it for $1300, and sold it again for $1600. how much did I earn? Take in consideration the money for the repurchased too.\"\n",
        "chat_response=model.generate_content(query)\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print()\n",
        "print('Answer: ')\n",
        "print(chat_response.text)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "dSBFngIu0lKG",
        "outputId": "29301896-9cb3-4840-f6da-588150d0790d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Answer: \n",
            "## Calculating Your Change:\n",
            "\n",
            "We can solve this in two steps:\n",
            "\n",
            "1. **Find the total cost of the ice cream cones:** We know each cone costs $1.25 and you bought 6, so we multiply the cost per cone by the number of cones: $1.25 x 6 = $7.50\n",
            "2. **Subtract the total cost from the amount you paid:** You gave a $10 bill, so we subtract the cost of the cones to find your change: $10 - $7.50 = $2.50\n",
            "\n",
            "Therefore, you received **$2.50** in change. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tYbY6IAsJ-tn"
      },
      "outputs": [],
      "source": [
        "#models/gemini-1.0-pro\n",
        "#models/gemini-1.0-pro-001\n",
        "#models/gemini-1.0-pro-latest\n",
        "#models/gemini-1.0-pro-vision-latest\n",
        "#models/gemini-1.5-pro-latest\n",
        "#models/gemini-pro\n",
        "#models/gemini-pro-vision\n",
        "\n",
        "#model = genai.GenerativeModel('gemini-pro')\n",
        "model_name = 'gemini-1.0-pro-latest'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "#model = genai.GenerativeModel('gemini-1.5-pro-latest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpl8OxodNiW1",
        "outputId": "6c181952-bdd5-48a6-fdc8-a12e25178b02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " genai.GenerativeModel(\n",
            "   model_name='models/gemini-1.0-pro-latest',\n",
            "   generation_config={}.\n",
            "   safety_settings={}\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miDaq0mLC3ut"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/outputs/\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from tenacity import retry, stop_after_attempt, wait_chain, wait_fixed\n",
        "\n",
        "def main(tasks=TASKSTEST):\n",
        "    #openai.api_key = openai.api_key\n",
        "    mmlu_prompt = json.load(open('/content/chain-of-thought-hub/MMLU/lib_prompt/mmlu-cot.json'))\n",
        "    for task in tasks:\n",
        "\n",
        "        print()\n",
        "        print('Testing %s ...' % task)\n",
        "        print()\n",
        "\n",
        "        i = 0\n",
        "        acc = 0\n",
        "        task_data = load_dataset(\"lukaemon/mmlu\", task, trust_remote_code=True)\n",
        "        #model=\"mistral-large-latest\"\n",
        "\n",
        "        #model = \"open-mixtral-8x22b\"\n",
        "\n",
        "        with open('/content/outputs/test_%s_%s.txt' % (model_name, task), 'w') as fd:\n",
        "        #with open('/content/outputs/test_gpt_3.5_turbo_%s.txt' % task, 'w') as fd:\n",
        "            for q_ in tqdm(task_data['test'], total=len(task_data['test'])):\n",
        "                q = q_['input'] + '\\n'\n",
        "                for letter in ['A', 'B', 'C', 'D']:\n",
        "                    q += '(' + letter + ') ' + q_[letter] + ' '\n",
        "                q += \"\\nA: Let's think step by step.\"\n",
        "\n",
        "                prompt_q = mmlu_prompt[task] + \"\\n\\n\" + q\n",
        "                #print(prompt_q)\n",
        "\n",
        "                ### ADDED by Frank Morales 19/04/2024\n",
        "                response=model.generate_content(prompt_q)\n",
        "\n",
        "                #print(response.text)\n",
        "\n",
        "\n",
        "                ### ADDED by Frank Morales 19/04/2023\n",
        "                #ans_model = response.text\n",
        "\n",
        "                try:\n",
        "                     #x = int(input(\"Please enter a number: \"))\n",
        "                     #break\n",
        "                     ans_model = response.text\n",
        "                     ans_, residual = extract_ans(ans_model)\n",
        "                except ValueError:\n",
        "                     print(\"Oops!  That was no valid response.  Try again...\")\n",
        "                     print(prompt_q)\n",
        "                     #print(response)\n",
        "\n",
        "\n",
        "                del prompt_q\n",
        "                del response\n",
        "                ans_, residual = extract_ans(ans_model)\n",
        "\n",
        "                a = q_['target']\n",
        "                #print(a)\n",
        "                fd.write('Q: %s\\nA_model:\\n%s\\nA:\\n%s\\n\\n' % (q, ans_, a))\n",
        "                i += 1\n",
        "\n",
        "                if(test_answer_mmlu_(ans_, a)): acc += 1\n",
        "            print('%s acc %.4f' % (task, acc / len(task_data['test'])))\n",
        "        # write accuracy to file\n",
        "        with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'a') as fd:\n",
        "          fd.write('%s acc %.4f\\n' % (task, acc / len(task_data['test'])))\n",
        "\n",
        "    # write average accuracy to file\n",
        "    acc_list = []\n",
        "    #with open('/content/outputs/test_%s_%s_acc.txt' % (model, args.prompt_type), 'r') as fd2:\n",
        "\n",
        "    # with open('/content/outputs/test_%s_%s_acc.txt' % (model, 'multiple'), 'r') as fd2:\n",
        "    with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'r') as fd2:\n",
        "        for line in fd2:\n",
        "            acc_list.append(float(line.split(' ')[2]))\n",
        "    with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'a') as fd:\n",
        "        fd.write('Average acc %.4f\\n' % (np.mean(acc_list)))\n",
        "    return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgg5qxSU-vOt"
      },
      "source": [
        "https://medium.com/@frankmorales_91352/gemini-llm-unveiling-the-future-of-language-models-2eea7aa2c550\n",
        "\n",
        "MODEL: mistral-large-latest\n",
        "\n",
        "college_computer_science acc 0.5200\n",
        "electrical_engineering acc 0.6069\n",
        "machine_learning acc 0.5982\n",
        "Average acc 0.5750\n",
        "\n",
        "\n",
        "MODEL: open-mixtral-8x22b\n",
        "\n",
        "college_computer_science acc 0.2300\n",
        "electrical_engineering acc 0.3310\n",
        "machine_learning acc 0.2321\n",
        "Average acc 0.2644\n",
        "\n",
        "\n",
        "MODEL: open-mixtral-8x22b-2404\n",
        "\n",
        "college_computer_science acc 0.1900\n",
        "electrical_engineering acc 0.3310\n",
        "machine_learning acc 0.2232\n",
        "Average acc 0.2481\n",
        "\n",
        "\n",
        "MODEL: meta-llama/Meta-Llama-3-8B-Instruct\n",
        "\n",
        "college_computer_science acc 0.3300\n",
        "electrical_engineering acc 0.2414\n",
        "machine_learning acc 0.3125\n",
        "Average acc 0.2946\n",
        "\n",
        "MODEL: gemini-pro (trial-1)\n",
        "\n",
        "college_computer_science acc 0.4600\n",
        "electrical_engineering acc 0.7241\n",
        "machine_learning acc 0.4911\n",
        "Average acc 0.5584\n",
        "\n",
        "MODEL: gemini-pro (trial-2)\n",
        "\n",
        "college_computer_science acc 0.4600\n",
        "electrical_engineering acc 0.6759\n",
        "machine_learning acc 0.4821\n",
        "Average acc 0.5393\n",
        "\n",
        "MODEL: gemini-pro (trial-3)\n",
        "\n",
        "college_computer_science acc 0.4900\n",
        "electrical_engineering acc 0.7034\n",
        "machine_learning acc 0.4911\n",
        "Average acc 0.5615\n",
        "\n",
        "MODEL: gemini-1.0-pro-latest (trial-1)\n",
        "\n",
        "college_computer_science acc 0.4500\n",
        "electrical_engineering acc 0.6759\n",
        "machine_learning acc 0.5179\n",
        "Average acc 0.5479\n",
        "\n",
        "\n",
        "MODEL: gemini-1.0-pro-latest\n",
        "\n",
        "college_computer_science acc 0.5300\n",
        "electrical_engineering acc 0.7172\n",
        "machine_learning acc 0.5625\n",
        "Average acc 0.6032"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGqC-D_kD9Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce662db0-3d1a-43b4-cc34-667a95cd5218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MODEL: gemini-1.0-pro-latest\n",
            "\n",
            "college_computer_science acc 0.5300\n",
            "\n",
            "electrical_engineering acc 0.7172\n",
            "\n",
            "machine_learning acc 0.5625\n",
            "\n",
            "Average acc 0.6032\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#model=\"mistral-large-latest\"\n",
        "#model = \"open-mixtral-8x22b\"\n",
        "\n",
        "print()\n",
        "print('MODEL: %s'%model_name)\n",
        "print()\n",
        "acc_file='/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu')\n",
        "#print(acc_file)\n",
        "\n",
        "with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'r') as fd:\n",
        "     for line in fd:\n",
        "            print(line)\n",
        "            #acc_list.append(float(line.split(' ')[2]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNN6F+VS49BuNqI6wHMka6r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
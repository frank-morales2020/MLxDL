{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MMLU_GEMINI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keipSFp3_J1x",
        "outputId": "9cd36c38-c0f2-47f5-fbb7-94c2d45ed695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr 20 03:10:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   38C    P8              11W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQpk59lyBUim"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q tqdm\n",
        "!pip install -q pandas\n",
        "!pip install -q tensor_parallel\n",
        "!pip install -q argparse\n",
        "!pip install -q einops\n",
        "!pip install -q accelerate\n",
        "#!pip install -q torch==2.0.0+cu118\n",
        "!pip install -q torch\n",
        "\n",
        "!pip install colab-env --upgrade -q\n",
        "!pip install openai -q\n",
        "\n",
        "!pip install datasets -q\n",
        "!pip install utils -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ7qVCcyDO3t",
        "outputId": "b263ec94-8d95-4bcd-c65e-27a2eeab3911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chain-of-thought-hub'...\n",
            "remote: Enumerating objects: 3043, done.\u001b[K\n",
            "remote: Counting objects: 100% (339/339), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 3043 (delta 191), reused 247 (delta 179), pack-reused 2704\u001b[K\n",
            "Receiving objects: 100% (3043/3043), 8.30 MiB | 16.60 MiB/s, done.\n",
            "Resolving deltas: 100% (964/964), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/FranxYao/chain-of-thought-hub.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwm-_bQBBjhc"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import os\n",
        "import openai\n",
        "import IPython\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "77X7SrPYBn08"
      },
      "outputs": [],
      "source": [
        "def test_answer_mmlu_(pred_str, ans):\n",
        "    pattern = 'the answer is ('\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "\n",
        "    if(len(pred) > 1):\n",
        "        # print(pred)\n",
        "        pred = pred[1][0]\n",
        "        gold = ans.lower()\n",
        "        # print('debug 1, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "    else:\n",
        "        pred = 'C'\n",
        "        # print(ans_str)\n",
        "        gold = ans.lower()\n",
        "        # print('debug 2, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "\n",
        "# extract answer in pred_str and compare with ans_str\n",
        "def test_answer_mmlu_claude_instant(pred_str, ans_str):\n",
        "    pattern = 'the answer is '\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "    if len(pred) == 1:\n",
        "        return False\n",
        "    else:\n",
        "        return pred[1][0] == ans_str.lower()\n",
        "\n",
        "def test_answer_mmlu_claude(pred_str, ans_str):\n",
        "    pattern = 'the answer is '\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "\n",
        "    if(len(pred) > 1):\n",
        "        # print(pred)\n",
        "        pred = pred[1]\n",
        "        for p in pred:\n",
        "            if(p.isalpha()): break\n",
        "        pred = p\n",
        "        print(ans_str)\n",
        "        gold = ans_str.lower()\n",
        "        print('debug 1, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "    else:\n",
        "        pred = 'c'\n",
        "        # print(ans_str)\n",
        "        gold = ans_str.lower()\n",
        "        # print('debug 2, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "\n",
        "def test_answer_mmlu(pred_str, ans_str):\n",
        "    pattern = 'the answer is ('\n",
        "    pred = pred_str.lower().split(pattern)\n",
        "\n",
        "    if(len(pred) > 1):\n",
        "        # print(pred)\n",
        "        pred = pred[1][0]\n",
        "        gold = ans_str.split('A:\\n')[1][0].lower()\n",
        "        # print('debug 1, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "    else:\n",
        "        pred = 'C'\n",
        "        # print(ans_str)\n",
        "        gold = ans_str.split('A:\\n')[1][0].lower()\n",
        "        # print('debug 2, pred %s, gold %s' % (pred, gold))\n",
        "        return pred == gold\n",
        "\n",
        "def parse_pred_ans(filename):\n",
        "    with open(filename) as fd: lines = fd.readlines()\n",
        "    am, a = None, None\n",
        "    num_q, acc = 0, 0\n",
        "    current_mode = 'none'\n",
        "    questions = []\n",
        "    ans_pred = []\n",
        "    ans_gold = []\n",
        "    for l in lines:\n",
        "        if(l.startswith('Q: ')):\n",
        "            if(am is not None and a is not None):\n",
        "                questions.append(q)\n",
        "                ans_pred.append(am)\n",
        "                ans_gold.append(a)\n",
        "                # print(am)\n",
        "                # print(a)\n",
        "                if(test_answer_mmlu(am, a)):\n",
        "                    acc += 1\n",
        "            current_mode = 'q'\n",
        "            q = l\n",
        "            num_q += 1\n",
        "        elif(l.startswith('A_model:')):\n",
        "            current_mode = 'am'\n",
        "            am = l\n",
        "        elif(l.startswith('A:') and not l.startswith(\"A: Let's think step by step\")):\n",
        "            current_mode = 'a'\n",
        "            a = l\n",
        "        else:\n",
        "            if(current_mode == 'q'): q += l\n",
        "            elif(current_mode == 'am'): am += l\n",
        "            elif(current_mode == 'a'): a += l\n",
        "            else:\n",
        "                raise ValueError(current_mode)\n",
        "\n",
        "    questions.append(q)\n",
        "    ans_pred.append(am)\n",
        "    ans_gold.append(a)\n",
        "    # print(am)\n",
        "    # print(a)\n",
        "    if(test_answer_mmlu(am, a)):\n",
        "        acc += 1\n",
        "    print('num_q %d correct %d ratio %.4f' % (num_q, acc, float(acc / num_q)))\n",
        "    return questions, ans_pred, ans_gold\n",
        "\n",
        "def test_finished(ans_model):\n",
        "    if('answer is' in ans_model): return True\n",
        "    else: return False\n",
        "\n",
        "def extract_ans(ans_model):\n",
        "    ans_model = ans_model.split('\\n')\n",
        "    ans = []\n",
        "    residual = []\n",
        "    for li, al in enumerate(ans_model):\n",
        "        ans.append(al)\n",
        "        if('answer is' in al):\n",
        "            break\n",
        "    residual = list(ans_model[li + 1:])\n",
        "    ans = '\\n'.join(ans)\n",
        "    residual = '\\n'.join(residual)\n",
        "    return ans, residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "frmQVjYRBssq"
      },
      "outputs": [],
      "source": [
        "TASKSTEST0old = [\n",
        "        'anatomy',\n",
        "        'college_biology',\n",
        "        'college_chemistry',\n",
        "        'college_computer_science',\n",
        "        'college_mathematics',\n",
        "        'college_medicine',\n",
        "        'college_physics',\n",
        "        'computer_security',\n",
        "        'electrical_engineering',\n",
        "        'machine_learning',\n",
        "]\n",
        "\n",
        "\n",
        "TASKSTEST = [\n",
        "        'college_computer_science',\n",
        "        'electrical_engineering',\n",
        "        'machine_learning',\n",
        "]\n",
        "\n",
        "TASKS911 = [\n",
        "        'anatomy',\n",
        "        'college_biology',\n",
        "        'college_chemistry',\n",
        "        'college_computer_science',\n",
        "        'college_mathematics',\n",
        "        'college_medicine',\n",
        "        'college_physics',\n",
        "        'computer_security',\n",
        "        'conceptual_physics',\n",
        "        'econometrics',\n",
        "        'electrical_engineering',\n",
        "        'elementary_mathematics',\n",
        "        'formal_logic',\n",
        "        'global_facts',\n",
        "        'high_school_biology',\n",
        "        'high_school_chemistry',\n",
        "        'high_school_computer_science',\n",
        "        'high_school_european_history',\n",
        "        'high_school_geography',\n",
        "        'high_school_government_and_politics',\n",
        "        'high_school_macroeconomics',\n",
        "        'high_school_mathematics',\n",
        "        'high_school_microeconomics',\n",
        "        'high_school_physics',\n",
        "        'high_school_psychology',\n",
        "        'high_school_statistics',\n",
        "        'high_school_us_history',\n",
        "        'high_school_world_history',\n",
        "        'public_relations',\n",
        "        'security_studies',\n",
        "        'sociology',\n",
        "        'us_foreign_policy',\n",
        "        'virology',\n",
        "        'machine_learning',\n",
        "        'world_religions']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w80g_-ZrkaY9"
      },
      "source": [
        "GEMINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ADYgSk98JiN5"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RW4ZLVbmJl2d"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oTzQe-dwJuus"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MuVkYqmKJ072"
      },
      "outputs": [],
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "AeiCnXkPJ5LQ",
        "outputId": "2b1556e6-4600-474a-e7fb-d482377aefef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.0-pro-latest')\n",
        "#model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "#query = \"I bought a computer for 1200, repurchased it for 1600. how much did I earn? Take in consideration the money for the repurchased too.\"\n",
        "\n",
        "del query\n",
        "query = \"I bought a computer for $900, sold it for $1200, repurchased it for $1300, and sold it again for $1600. how much did I earn? Take in consideration the money for the repurchased too.\"\n",
        "chat_response=model.generate_content(query)\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print()\n",
        "print('Answer: ')\n",
        "print(chat_response.text)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "boPwVcrdpOQO",
        "outputId": "1eaa0429-2c5c-468a-c1a9-e2ebb5831e63"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: I bought a computer for $900, sold it for $1200, repurchased it for $1300, and sold it again for $1600. how much did I earn? Take in consideration the money for the repurchased too.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Answer: \n",
            "**Step 1: Calculate the profit on the first sale**\n",
            "\n",
            "$1200 (selling price) - $900 (purchase price) = $300 profit\n",
            "\n",
            "**Step 2: Calculate the loss on the repurchase**\n",
            "\n",
            "$1300 (repurchase price) - $1200 (selling price) = $100 loss\n",
            "\n",
            "**Step 3: Calculate the profit on the second sale**\n",
            "\n",
            "$1600 (selling price) - $1300 (repurchase price) = $300 profit\n",
            "\n",
            "**Step 4: Determine the overall profit**\n",
            "\n",
            "$300 (profit on first sale) + $300 (profit on second sale) - $100 (loss on repurchase) = **$500 profit**\n",
            "\n",
            "Therefore, you earned **$500** from the entire series of transactions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tYbY6IAsJ-tn"
      },
      "outputs": [],
      "source": [
        "#models/gemini-1.0-pro\n",
        "#models/gemini-1.0-pro-001\n",
        "#models/gemini-1.0-pro-latest\n",
        "#models/gemini-1.0-pro-vision-latest\n",
        "#models/gemini-1.5-pro-latest\n",
        "#models/gemini-pro\n",
        "#models/gemini-pro-vision\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpl8OxodNiW1",
        "outputId": "e4861610-62a8-487d-847d-b160ecae5c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " genai.GenerativeModel(\n",
            "   model_name='models/gemini-pro',\n",
            "   generation_config={}.\n",
            "   safety_settings={}\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#models/gemini-1.0-pro\n",
        "#models/gemini-1.0-pro-001\n",
        "#models/gemini-1.0-pro-latest\n",
        "#models/gemini-1.0-pro-vision-latest\n",
        "#models/gemini-1.5-pro-latest\n",
        "#models/gemini-pro\n",
        "#models/gemini-pro-vision\n",
        "\n",
        "print(model)\n",
        "model_name = 'gemini-pro'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miDaq0mLC3ut"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/outputs/\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from tenacity import retry, stop_after_attempt, wait_chain, wait_fixed\n",
        "\n",
        "def main(tasks=TASKSTEST):\n",
        "    #openai.api_key = openai.api_key\n",
        "    mmlu_prompt = json.load(open('/content/chain-of-thought-hub/MMLU/lib_prompt/mmlu-cot.json'))\n",
        "    for task in tasks:\n",
        "\n",
        "        print()\n",
        "        print('Testing %s ...' % task)\n",
        "        print()\n",
        "\n",
        "        i = 0\n",
        "        acc = 0\n",
        "        task_data = load_dataset(\"lukaemon/mmlu\", task, trust_remote_code=True)\n",
        "        #model=\"mistral-large-latest\"\n",
        "\n",
        "        #model = \"open-mixtral-8x22b\"\n",
        "\n",
        "        with open('/content/outputs/test_%s_%s.txt' % (model_name, task), 'w') as fd:\n",
        "        #with open('/content/outputs/test_gpt_3.5_turbo_%s.txt' % task, 'w') as fd:\n",
        "            for q_ in tqdm(task_data['test'], total=len(task_data['test'])):\n",
        "                q = q_['input'] + '\\n'\n",
        "                for letter in ['A', 'B', 'C', 'D']:\n",
        "                    q += '(' + letter + ') ' + q_[letter] + ' '\n",
        "                q += \"\\nA: Let's think step by step.\"\n",
        "\n",
        "                prompt_q = mmlu_prompt[task] + \"\\n\\n\" + q\n",
        "                #print(prompt_q)\n",
        "\n",
        "                ### ADDED by Frank Morales 19/04/2024\n",
        "                response=model.generate_content(prompt_q)\n",
        "\n",
        "                #print(response.text)\n",
        "\n",
        "\n",
        "                ### ADDED by Frank Morales 19/04/2023\n",
        "                #ans_model = response.text\n",
        "\n",
        "                try:\n",
        "                     #x = int(input(\"Please enter a number: \"))\n",
        "                     #break\n",
        "                     ans_model = response.text\n",
        "                     ans_, residual = extract_ans(ans_model)\n",
        "                except ValueError:\n",
        "                     print(\"Oops!  That was no valid response.  Try again...\")\n",
        "                     print(prompt_q)\n",
        "                     #print(response)\n",
        "\n",
        "\n",
        "                del prompt_q\n",
        "                del response\n",
        "                ans_, residual = extract_ans(ans_model)\n",
        "\n",
        "                a = q_['target']\n",
        "                #print(a)\n",
        "                fd.write('Q: %s\\nA_model:\\n%s\\nA:\\n%s\\n\\n' % (q, ans_, a))\n",
        "                i += 1\n",
        "\n",
        "                if(test_answer_mmlu_(ans_, a)): acc += 1\n",
        "            print('%s acc %.4f' % (task, acc / len(task_data['test'])))\n",
        "        # write accuracy to file\n",
        "        with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'a') as fd:\n",
        "          fd.write('%s acc %.4f\\n' % (task, acc / len(task_data['test'])))\n",
        "\n",
        "    # write average accuracy to file\n",
        "    acc_list = []\n",
        "    #with open('/content/outputs/test_%s_%s_acc.txt' % (model, args.prompt_type), 'r') as fd2:\n",
        "\n",
        "    # with open('/content/outputs/test_%s_%s_acc.txt' % (model, 'multiple'), 'r') as fd2:\n",
        "    with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'r') as fd2:\n",
        "        for line in fd2:\n",
        "            acc_list.append(float(line.split(' ')[2]))\n",
        "    with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'a') as fd:\n",
        "        fd.write('Average acc %.4f\\n' % (np.mean(acc_list)))\n",
        "    return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgg5qxSU-vOt"
      },
      "source": [
        "MMLU Statistics\n",
        "There are 57 tasks in total. 15908 questions in total are collected, which are split into a few-shot development set, a validation set, and a test set.\n",
        "\n",
        "The few-shot development set has 5 questions per subject, the validation set may be used for selecting hyperparameters and is made of 1540 questions, and the test set has 14079 questions.\n",
        "\n",
        "Each subject contains 100 test examples at the minimum, which is longer than most exams designed to assess people.\n",
        "\n",
        "The expert-level accuracy is estimated approximately as 89.8%.\n",
        "\n",
        "There are few main sections: Humanity, Social Science, STEM, and Others.\n",
        "\n",
        "MODEL: mistral-large-latest\n",
        "\n",
        "college_computer_science acc 0.5200\n",
        "electrical_engineering acc 0.6069\n",
        "machine_learning acc 0.5982\n",
        "Average acc 0.5750\n",
        "\n",
        "\n",
        "ODEL: open-mixtral-8x22b\n",
        "\n",
        "college_computer_science acc 0.2300\n",
        "electrical_engineering acc 0.3310\n",
        "machine_learning acc 0.2321\n",
        "Average acc 0.2644\n",
        "\n",
        "\n",
        "MODEL: open-mixtral-8x22b-2404\n",
        "\n",
        "college_computer_science acc 0.1900\n",
        "electrical_engineering acc 0.3310\n",
        "machine_learning acc 0.2232\n",
        "Average acc 0.2481\n",
        "\n",
        "\n",
        "MODEL: meta-llama/Meta-Llama-3-8B-Instruct\n",
        "\n",
        "college_computer_science acc 0.3300\n",
        "electrical_engineering acc 0.2414\n",
        "machine_learning acc 0.3125\n",
        "Average acc 0.2946\n",
        "\n",
        "MODEL: gemini-pro (trial-1)\n",
        "\n",
        "college_computer_science acc 0.4600\n",
        "electrical_engineering acc 0.7241\n",
        "machine_learning acc 0.4911\n",
        "Average acc 0.5584\n",
        "\n",
        "MODEL: gemini-pro (trial-2)\n",
        "\n",
        "college_computer_science acc 0.4600\n",
        "electrical_engineering acc 0.6759\n",
        "machine_learning acc 0.4821\n",
        "Average acc 0.5393\n",
        "\n",
        "MODEL: gemini-pro (trial-3)\n",
        "\n",
        "college_computer_science acc 0.4900\n",
        "electrical_engineering acc 0.7034\n",
        "machine_learning acc 0.4911\n",
        "Average acc 0.5615"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EGqC-D_kD9Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c634c98-0b51-43f6-9afc-8d9f418b281a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MODEL: gemini-pro\n",
            "\n",
            "college_computer_science acc 0.4900\n",
            "\n",
            "electrical_engineering acc 0.7034\n",
            "\n",
            "machine_learning acc 0.4911\n",
            "\n",
            "Average acc 0.5615\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#model=\"mistral-large-latest\"\n",
        "#model = \"open-mixtral-8x22b\"\n",
        "\n",
        "print()\n",
        "print('MODEL: %s'%model_name)\n",
        "print()\n",
        "acc_file='/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu')\n",
        "#print(acc_file)\n",
        "\n",
        "with open('/content/outputs/test_%s_%s_acc.txt' % (model_name, 'mmlu'), 'r') as fd:\n",
        "     for line in fd:\n",
        "            print(line)\n",
        "            #acc_list.append(float(line.split(' ')[2]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMA/WhSFdZFkdgxG06tdYAx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/LLM_Fine_Tuning_Demo_on_TPU_with_JAX_and_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax optax tensorflow tensorflow_datasets jax jaxlib -q"
      ],
      "metadata": {
        "id": "Troi2CV1zxpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show jax\n",
        "print('\\n')\n",
        "!pip show flax\n",
        "print('\\n')\n",
        "!pip show optax\n",
        "print('\\n')\n",
        "!pip show tensorflow\n",
        "print('\\n')\n",
        "!pip show tensorflow_datasets\n",
        "print('\\n')\n",
        "!pip show optax\n",
        "print('\\n')\n",
        "!pip show tensorflow\n",
        "print('\\n')\n",
        "!pip show tensorflow_datasets\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9shMrGGdXidk",
        "outputId": "7a30ba29-bd2e-4362-9326-5e5718c227e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: jax\n",
            "Version: 0.5.3\n",
            "Summary: Differentiate, compile, and transform Numpy code.\n",
            "Home-page: https://github.com/jax-ml/jax\n",
            "Author: JAX team\n",
            "Author-email: jax-dev@google.com\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: jaxlib, ml_dtypes, numpy, opt_einsum, scipy\n",
            "Required-by: chex, distrax, flax, optax, orbax-checkpoint\n",
            "\n",
            "\n",
            "Name: flax\n",
            "Version: 0.10.6\n",
            "Summary: Flax: A neural network library for JAX designed for flexibility\n",
            "Home-page: https://github.com/google/flax\n",
            "Author: \n",
            "Author-email: Flax team <flax-dev@google.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: jax, msgpack, numpy, optax, orbax-checkpoint, PyYAML, rich, tensorstore, treescope, typing_extensions\n",
            "Required-by: \n",
            "\n",
            "\n",
            "Name: optax\n",
            "Version: 0.2.5\n",
            "Summary: A gradient processing and optimization library in JAX.\n",
            "Home-page: https://github.com/google-deepmind/optax\n",
            "Author: \n",
            "Author-email: Google DeepMind <optax-dev@google.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, chex, jax, jaxlib, numpy\n",
            "Required-by: flax\n",
            "\n",
            "\n",
            "Name: tensorflow\n",
            "Version: 2.20.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google_pasta, grpcio, h5py, keras, libclang, ml_dtypes, numpy, opt_einsum, packaging, protobuf, requests, setuptools, six, tensorboard, termcolor, typing_extensions, wrapt\n",
            "Required-by: \n",
            "\n",
            "\n",
            "Name: tensorflow-datasets\n",
            "Version: 4.9.9\n",
            "Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow.\n",
            "Home-page: https://github.com/tensorflow/datasets\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, array_record, dm-tree, etils, immutabledict, numpy, promise, protobuf, psutil, pyarrow, requests, simple_parsing, tensorflow-metadata, termcolor, toml, tqdm, wrapt\n",
            "Required-by: \n",
            "\n",
            "\n",
            "Name: optax\n",
            "Version: 0.2.5\n",
            "Summary: A gradient processing and optimization library in JAX.\n",
            "Home-page: https://github.com/google-deepmind/optax\n",
            "Author: \n",
            "Author-email: Google DeepMind <optax-dev@google.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, chex, jax, jaxlib, numpy\n",
            "Required-by: flax\n",
            "\n",
            "\n",
            "Name: tensorflow\n",
            "Version: 2.20.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google_pasta, grpcio, h5py, keras, libclang, ml_dtypes, numpy, opt_einsum, packaging, protobuf, requests, setuptools, six, tensorboard, termcolor, typing_extensions, wrapt\n",
            "Required-by: \n",
            "\n",
            "\n",
            "Name: tensorflow-datasets\n",
            "Version: 4.9.9\n",
            "Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow.\n",
            "Home-page: https://github.com/tensorflow/datasets\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, array_record, dm-tree, etils, immutabledict, numpy, promise, protobuf, psutil, pyarrow, requests, simple_parsing, tensorflow-metadata, termcolor, toml, tqdm, wrapt\n",
            "Required-by: \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "49y8dom7UeBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbh-AKIUYgjB",
        "outputId": "7cfa64a5-440c-474a-dded-312336a59fd4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: datasets\n",
            "Version: 4.0.0\n",
            "Summary: HuggingFace community-driven open-source library of datasets\n",
            "Home-page: https://github.com/huggingface/datasets\n",
            "Author: HuggingFace Inc.\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-cpu -q"
      ],
      "metadata": {
        "id": "W8gJS58saa2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n')\n",
        "!pip show tensorflow-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebaMOv5ddgZG",
        "outputId": "e8f69705-f137-4157-d01b-753a45032a15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Name: tensorflow_cpu\n",
            "Version: 2.20.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google_pasta, grpcio, h5py, keras, libclang, ml_dtypes, numpy, opt_einsum, packaging, protobuf, requests, setuptools, six, tensorboard, termcolor, typing_extensions, wrapt\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "from jax import grad, jit, random\n",
        "from functools import partial\n",
        "import optax\n",
        "import time\n",
        "\n",
        "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from flax.training import train_state\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. TPU Initialization (JAX style)\n",
        "try:\n",
        "    devices = jax.devices()\n",
        "    tpu_devices = [d for d in devices if d.platform == 'tpu']\n",
        "    if not tpu_devices:\n",
        "        raise ValueError(\"No TPU devices found.\")\n",
        "    print(f\"Found JAX devices: {devices}\")\n",
        "    print(f\"Number of TPU devices available: {len(tpu_devices)}\")\n",
        "except ValueError as e:\n",
        "    print(f\"ERROR: {e}. Please ensure your Colab runtime is set to TPU.\")\n",
        "    raise SystemExit('TPU not found or not initialized for JAX.')\n",
        "\n",
        "# Define a mesh for sharding across TPU cores\n",
        "num_tpu_cores = len(tpu_devices)\n",
        "mesh = Mesh(tpu_devices, axis_names=('data',))\n",
        "print(f\"JAX Mesh created with axis_names: {mesh.axis_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ai-LHn4V1t-",
        "outputId": "09f95a4d-1154-45b0-d083-ed70dbfcadfa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n",
            "Number of TPU devices available: 8\n",
            "JAX Mesh created with axis_names: ('data',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load JAX-based LLM and Tokenizer\n",
        "print('\\nLoading LLM and tokenizer...')\n",
        "model_id = 'EleutherAI/gpt-neo-125M'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = FlaxAutoModelForCausalLM.from_pretrained(model_id)\n",
        "print('Model loaded.')\n",
        "\n",
        "# 3. Prepare the Dataset\n",
        "print('\\nLoading and preprocessing dataset...')\n",
        "dataset = load_dataset('databricks/databricks-dolly-15k', split='train').train_test_split(test_size=0.1)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['test']\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
        "\n",
        "train_tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "eval_tokenized_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "train_tokenized_dataset.set_format(type='jax', columns=['input_ids', 'attention_mask'])\n",
        "eval_tokenized_dataset.set_format(type='jax', columns=['input_ids', 'attention_mask'])\n",
        "print('Datasets preprocessed and formatted for JAX.')\n",
        "\n",
        "# 4. Define the Training and Evaluation Steps\n",
        "class TrainState(train_state.TrainState):\n",
        "    pass\n",
        "\n",
        "learning_rate = 1e-5\n",
        "optimizer = optax.adamw(learning_rate=learning_rate)\n",
        "\n",
        "@jit\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(params):\n",
        "        variables = {'params': params}\n",
        "        logits = state.apply_fn(variables, **batch)[0]\n",
        "        labels = batch['input_ids']\n",
        "        one_hot_labels = jax.nn.one_hot(labels, num_classes=logits.shape[-1])\n",
        "        loss = -jnp.sum(one_hot_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "        return jnp.mean(loss)\n",
        "\n",
        "    grad_fn = grad(loss_fn)\n",
        "    grads = grad_fn(state.params)\n",
        "    return state.apply_gradients(grads=grads)\n",
        "\n",
        "@jit\n",
        "def eval_step(params, batch):\n",
        "    variables = {'params': params}\n",
        "    logits = model.module.apply(variables, **batch)[0]\n",
        "    labels = batch['input_ids']\n",
        "    one_hot_labels = jax.nn.one_hot(labels, num_classes=logits.shape[-1])\n",
        "    loss = -jnp.sum(one_hot_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "# 5. Initialize Model State and Shard Parameters\n",
        "print('\\nInitializing and sharding model state...')\n",
        "params = model.params"
      ],
      "metadata": {
        "id": "frzHSOqmUK0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training process has run into a XlaRuntimeError with the message RESOURCE_EXHAUSTED. This means that the TPU has run out of memory, so it can't load and run the program.\n",
        "\n",
        "* The Problem\n",
        "\n",
        "The error message specifically says, \"Attempting to reserve 4.24G at the bottom of memory. That was not possible. There are 4.21G free...\". This indicates that the batch size and model size combined are too large for the TPU's memory. The TPU has a finite amount of memory, and your current configuration requires more than is available.\n",
        "\n",
        "* The Solution\n",
        "\n",
        "To resolve this, you need to reduce the amount of memory consumed during each training step. Here are the most effective ways to do that:\n",
        "\n",
        "1. Decrease the Global Batch Size: The global batch size is the total number of examples processed by all TPU cores in a single step. Reducing this will directly decrease the memory required for the input data and the gradients. In your code, you can change global_batch_size = 128 to a smaller number, like 64 or 32.\n",
        "\n",
        "2. Use a Smaller Model: If reducing the batch size isn't enough, consider using a model with fewer parameters, as model weights are a significant portion of memory usage.\n",
        "\n",
        "3. Use bfloat16 Precision: While your code already uses this implicitly, ensuring you are using bfloat16 precision is crucial. TPUs are highly optimized for this data type, which uses half the memory of standard 32-bit floats.\n",
        "\n"
      ],
      "metadata": {
        "id": "btUUb_zme5S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replicated_params = jax.device_put(params, NamedSharding(mesh, P()))\n",
        "state = TrainState.create(apply_fn=model.module.apply, params=replicated_params, tx=optimizer)\n",
        "print('Model state sharded.')\n",
        "\n",
        "num_epochs = 10\n",
        "# 6. Training and Evaluation Loop\n",
        "print(f'\\nStarting training for {num_epochs} epochs...')\n",
        "global_batch_size = 64\n",
        "per_device_batch_size = global_batch_size // num_tpu_cores\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    total_train_loss = 0.0\n",
        "    num_train_batches = 0\n",
        "\n",
        "    for batch in train_tokenized_dataset.iter(batch_size=global_batch_size):\n",
        "        input_shape = batch['input_ids'].shape\n",
        "        position_ids = jnp.broadcast_to(jnp.arange(input_shape[-1])[None, :], input_shape)\n",
        "        batch['position_ids'] = position_ids\n",
        "\n",
        "        state = train_step(state, batch)\n",
        "\n",
        "        batch_loss = eval_step(state.params, batch)\n",
        "        total_train_loss += batch_loss.item()\n",
        "        num_train_batches += 1\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_train_batches\n",
        "\n",
        "    total_eval_loss = 0.0\n",
        "    num_eval_batches = 0\n",
        "\n",
        "    for batch in eval_tokenized_dataset.iter(batch_size=global_batch_size):\n",
        "        input_shape = batch['input_ids'].shape\n",
        "        position_ids = jnp.broadcast_to(jnp.arange(input_shape[-1])[None, :], input_shape)\n",
        "        batch['position_ids'] = position_ids\n",
        "\n",
        "        batch_eval_loss = eval_step(state.params, batch)\n",
        "        total_eval_loss += batch_eval_loss.item()\n",
        "        num_eval_batches += 1\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / num_eval_batches\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Eval Loss = {avg_eval_loss:.4f} (Time: {epoch_end_time - epoch_start_time:.2f}s)\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f'\\nTraining complete in {end_time - start_time:.2f} seconds.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebGWZgNfWnrG",
        "outputId": "5bf2bd3c-2797-4d1a-c6f0-25f9c94b62ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state sharded.\n",
            "\n",
            "Starting training for 10 epochs...\n",
            "Epoch 1: Train Loss = 2.4240, Eval Loss = 0.1080 (Time: 152.73s)\n",
            "Epoch 2: Train Loss = 0.0742, Eval Loss = 0.0542 (Time: 105.37s)\n",
            "Epoch 3: Train Loss = 0.0460, Eval Loss = 0.0387 (Time: 105.27s)\n",
            "Epoch 4: Train Loss = 0.0326, Eval Loss = 0.0282 (Time: 105.37s)\n",
            "Epoch 5: Train Loss = 0.0232, Eval Loss = 0.0204 (Time: 105.38s)\n",
            "Epoch 6: Train Loss = 0.0165, Eval Loss = 0.0152 (Time: 105.26s)\n",
            "Epoch 7: Train Loss = 0.0118, Eval Loss = 0.0117 (Time: 105.38s)\n",
            "Epoch 8: Train Loss = 0.0088, Eval Loss = 0.0092 (Time: 105.44s)\n",
            "Epoch 9: Train Loss = 0.0067, Eval Loss = 0.0075 (Time: 105.44s)\n",
            "Epoch 10: Train Loss = 0.0053, Eval Loss = 0.0063 (Time: 105.41s)\n",
            "\n",
            "Training complete in 1101.07 seconds.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
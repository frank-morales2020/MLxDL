{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Rag_Fusion_Langchain_Llamaindex_PostgreSQL_CLAUDE3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a58112-8a18-49ad-9c8c-2088778613d0",
      "metadata": {
        "id": "41a58112-8a18-49ad-9c8c-2088778613d0"
      },
      "source": [
        "# RAG Fusion Query Pipeline\n",
        "\n",
        "This notebook shows how to implement RAG Fusion using the LlamaIndex Query Pipeline syntax."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msk6h8ngCiqH",
        "outputId": "fa8e077d-c757-4df4-f190-318f2de7785b"
      },
      "id": "Msk6h8ngCiqH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun  2 16:00:59 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Dependencies"
      ],
      "metadata": {
        "id": "x9dG5VRhwpHm"
      },
      "id": "x9dG5VRhwpHm"
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 11/01/2024\n",
        "%pip install openai  --root-user-action=ignore -q\n",
        "!pip install llama_index phoenix pyvis network -q\n",
        "!pip install llama_hub -q\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "!pip install accelerate -q\n",
        "#!pip install typing_extensions\n",
        "\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "\n",
        "### llama index components\n",
        "!pip install llama-index-llms-langchain -q\n",
        "%pip install llama-index-llms-fireworks -q\n",
        "\n",
        "## Mistral API components\n",
        "!pip install mistralai --quiet\n",
        "!pip install -U langchain-core langchain-mistralai -q"
      ],
      "metadata": {
        "id": "XN4HEh7jwixA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed376a6-979c-479c-ed68-890dded801a3"
      },
      "id": "XN4HEh7jwixA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for phoenix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for network (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.1.20 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.3 which is incompatible.\n",
            "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c5bdbf5-d525-42e2-ab4a-19234c023491",
      "metadata": {
        "id": "8c5bdbf5-d525-42e2-ab4a-19234c023491"
      },
      "source": [
        "## Setup / Load Data\n",
        "\n",
        "We load in the pg_essay.txt data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "20b91e78-d733-44dd-b9a2-7c6eab3aee3d",
      "metadata": {
        "id": "20b91e78-d733-44dd-b9a2-7c6eab3aee3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17488dde-9a08-4094-8374-b41c043340fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "--2024-06-02 16:05:03--  https://www.dropbox.com/s/f6bmb19xdg0xedm/paul_graham_essay.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /scl/fi/f2hfma1kcravwajwskek7/paul_graham_essay.txt?rlkey=kbj5qtm5c8bf1sndadp2rzrse&dl=1 [following]\n",
            "--2024-06-02 16:05:03--  https://www.dropbox.com/scl/fi/f2hfma1kcravwajwskek7/paul_graham_essay.txt?rlkey=kbj5qtm5c8bf1sndadp2rzrse&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1e85ecc098bea93f0c12946bb7.dl.dropboxusercontent.com/cd/0/inline/CUHMLxqLAed9d4zgAdr_9y2zWN92lfNoI8TDXg6xDO5kSDLL0LyGSUxLzJDeaeHEYkah_kYZIlXSlUOZMC0IqW2jxt8rgdtyozooDk3sU5Q3vYpWMnwe-WepL82a33wdr64/file?dl=1# [following]\n",
            "--2024-06-02 16:05:04--  https://uc1e85ecc098bea93f0c12946bb7.dl.dropboxusercontent.com/cd/0/inline/CUHMLxqLAed9d4zgAdr_9y2zWN92lfNoI8TDXg6xDO5kSDLL0LyGSUxLzJDeaeHEYkah_kYZIlXSlUOZMC0IqW2jxt8rgdtyozooDk3sU5Q3vYpWMnwe-WepL82a33wdr64/file?dl=1\n",
            "Resolving uc1e85ecc098bea93f0c12946bb7.dl.dropboxusercontent.com (uc1e85ecc098bea93f0c12946bb7.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6035:15::a27d:550f\n",
            "Connecting to uc1e85ecc098bea93f0c12946bb7.dl.dropboxusercontent.com (uc1e85ecc098bea93f0c12946bb7.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75047 (73K) [application/binary]\n",
            "Saving to: ‘pg_essay.txt’\n",
            "\n",
            "pg_essay.txt        100%[===================>]  73.29K   200KB/s    in 0.4s    \n",
            "\n",
            "2024-06-02 16:05:05 (200 KB/s) - ‘pg_essay.txt’ saved [75047/75047]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "!wget \"https://www.dropbox.com/s/f6bmb19xdg0xedm/paul_graham_essay.txt?dl=1\" -O pg_essay.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama_index"
      ],
      "metadata": {
        "id": "E-SmoCyG9ONc"
      },
      "id": "E-SmoCyG9ONc"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1785ffb1-5fc2-4855-854a-238003ff848b",
      "metadata": {
        "id": "1785ffb1-5fc2-4855-854a-238003ff848b"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "docs = reader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POSTGRESQL"
      ],
      "metadata": {
        "id": "sZy5EuwA8onu"
      },
      "id": "sZy5EuwA8onu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "POSTGRESQL\n",
        "\n",
        "https://www.atlantic.net/dedicated-server-hosting/how-to-install-and-configure-postgres-14-on-ubuntu/"
      ],
      "metadata": {
        "id": "jvWymWCOIdbI"
      },
      "id": "jvWymWCOIdbI"
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 01/06/2024\n",
        "!apt-get update -y\n",
        "!apt-get install postgresql-14 -y\n",
        "\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all\n",
        "\n",
        "#apt-get -y install postgresql"
      ],
      "metadata": {
        "id": "O8Kb4NBh0Hwx"
      },
      "id": "O8Kb4NBh0Hwx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "# PostGRES SQL Settings\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "print('START: PG embedding COMPILATION')\n",
        "%cd /content/\n",
        "!git clone https://github.com/neondatabase/pg_embedding.git\n",
        "%cd /content/pg_embedding\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "#!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id integer PRIMARY KEY, embedding real[])\""
      ],
      "metadata": {
        "id": "KLp1-7HZOkTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1927c0b1-254e-46e5-e695-186da70c1727"
      },
      "id": "KLp1-7HZOkTW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "/content\n",
            "ALTER ROLE\n",
            "START: PG embedding COMPILATION\n",
            "/content\n",
            "Cloning into 'pg_embedding'...\n",
            "remote: Enumerating objects: 553, done.\u001b[K\n",
            "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 553 (delta 140), reused 135 (delta 106), pack-reused 370\u001b[K\n",
            "Receiving objects: 100% (553/553), 270.29 KiB | 1.73 MiB/s, done.\n",
            "Resolving deltas: 100% (317/317), done.\n",
            "/content/pg_embedding\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o embedding.o embedding.c\n",
            "g++ -Wall -Wpointer-arith -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -std=c++11 -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o hnswalg.o hnswalg.cpp\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o distfunc.o distfunc.c\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -shared -o embedding.so embedding.o hnswalg.o distfunc.o -lstdc++ -L/usr/lib/x86_64-linux-gnu -Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro -Wl,-z,now -L/usr/lib/llvm-14/lib  -Wl,--as-needed  \n",
            "/usr/bin/clang-14 -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -Wno-unused-command-line-argument -Wno-compound-token-split-by-macro -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o embedding.bc embedding.c\n",
            "/usr/bin/clang-14 -xc++ -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o hnswalg.bc hnswalg.cpp\n",
            "/usr/bin/clang-14 -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -Wno-unused-command-line-argument -Wno-compound-token-split-by-macro -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o distfunc.bc distfunc.c\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/usr/bin/install -c -m 755  embedding.so '/usr/lib/postgresql/14/lib/embedding.so'\n",
            "/usr/bin/install -c -m 644 .//embedding.control '/usr/share/postgresql/14/extension/'\n",
            "/usr/bin/install -c -m 644 .//embedding--0.3.5--0.3.6.sql .//embedding--0.3.5.sql .//embedding--0.3.6.sql  '/usr/share/postgresql/14/extension/'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode/embedding'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode'/embedding/\n",
            "/usr/bin/install -c -m 644 embedding.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "/usr/bin/install -c -m 644 hnswalg.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "/usr/bin/install -c -m 644 distfunc.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "cd '/usr/lib/postgresql/14/lib/bitcode' && /usr/lib/llvm-14/bin/llvm-lto -thinlto -thinlto-action=thinlink -o embedding.index.bc embedding/embedding.bc embedding/hnswalg.bc embedding/distfunc.bc\n",
            "END: PG embedding COMPILATION\n",
            "\n",
            "CREATE EXTENSION\n",
            "CREATE TABLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain"
      ],
      "metadata": {
        "id": "IfQ8Cym29XOQ"
      },
      "id": "IfQ8Cym29XOQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community -q"
      ],
      "metadata": {
        "id": "KuBzH71uyulg"
      },
      "id": "KuBzH71uyulg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 11/01/2024\n",
        "\n",
        "from typing import List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "loader = TextLoader(\"/content/pg_essay.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs0 = text_splitter.split_documents(documents)\n",
        "\n",
        "collection_name0 = \"pg_essay\"\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs0)}')"
      ],
      "metadata": {
        "id": "hyLRNSCwS91R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad22ebd9-3ce7-4763-8aec-a456dbda503c"
      },
      "id": "hyLRNSCwS91R",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1004, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1203, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1025, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 1\n",
            "# of Document Chunks: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama Index"
      ],
      "metadata": {
        "id": "bhz9flIj9xfP"
      },
      "id": "bhz9flIj9xfP"
    },
    {
      "cell_type": "markdown",
      "id": "a7c9299c-06d6-4710-afb0-3cc13761f358",
      "metadata": {
        "id": "a7c9299c-06d6-4710-afb0-3cc13761f358"
      },
      "source": [
        "## Setup Llama Pack\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install llama_index\n",
        "import llama_index\n",
        "print('LLAMA INDEX VERSION: %s'%llama_index.core.__version__)\n",
        "#llama_index.core.\n",
        "from llama_index.core.query_pipeline import QueryPipeline\n",
        "import llama_index.core.query_pipeline as query_pipeline\n",
        "#llama_index.core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BwhQtzrcr8G",
        "outputId": "bfc92e8a-2fa2-49fe-e917-cb64de1cfa09"
      },
      "id": "5BwhQtzrcr8G",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLAMA INDEX VERSION: 0.10.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "8YnIGqpvj04L"
      },
      "id": "8YnIGqpvj04L",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_pipeline import QueryPipeline\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# try chaining basic prompts\n",
        "prompt_str = \"Please generate related movies to {movie_name}\"\n",
        "prompt_tmpl = PromptTemplate(prompt_str)\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)"
      ],
      "metadata": {
        "id": "4WMXSfoUjpBV"
      },
      "id": "4WMXSfoUjpBV",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = p.run(movie_name=\"The Departed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFVka_T6j9lg",
        "outputId": "9154c7f8-249b-4a33-b6f1-429dc2f8451b"
      },
      "id": "aFVka_T6j9lg",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;155;135;227m> Running module fd3cde25-5a35-47a2-9466-cc2f3c35dae3 with input: \n",
            "movie_name: The Departed\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 2b3afcd3-0a56-4205-a073-7eb7c3bec92d with input: \n",
            "messages: Please generate related movies to The Departed\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPQFSofykHAE",
        "outputId": "068128d1-3ae7-479b-e9cb-68de7f316249"
      },
      "id": "QPQFSofykHAE",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: 1. Infernal Affairs (2002) - The original Hong Kong film that inspired The Departed\n",
            "2. The Town (2010) - A crime thriller directed by Ben Affleck\n",
            "3. Mystic River (2003) - A drama directed by Clint Eastwood\n",
            "4. Goodfellas (1990) - A classic crime film directed by Martin Scorsese\n",
            "5. The Irishman (2019) - Another crime drama directed by Martin Scorsese\n",
            "6. The Departed (2006) - The Departed is a remake of this Hong Kong film\n",
            "7. The Godfather (1972) - A classic crime film directed by Francis Ford Coppola\n",
            "8. Casino (1995) - A crime drama directed by Martin Scorsese\n",
            "9. American Gangster (2007) - A crime film directed by Ridley Scott\n",
            "10. Donnie Brasco (1997) - A crime drama directed by Mike Newell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f5143a5f-7cd3-4aac-99e7-a357f3239f03",
      "metadata": {
        "id": "f5143a5f-7cd3-4aac-99e7-a357f3239f03"
      },
      "outputs": [],
      "source": [
        "# Option 1: Use `download_llama_pack`\n",
        "# from llama_index.llama_pack import download_llama_pack\n",
        "\n",
        "# RAGFusionPipelinePack = download_llama_pack(\n",
        "#     \"RAGFusionPipelinePack\",\n",
        "#     \"./rag_fusion_pipeline_pack\",\n",
        "#     # leave the below line commented out if using the notebook on main\n",
        "#     # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_query_pipeline_pack/llama_hub\"\n",
        "# )\n",
        "\n",
        "# Option 2: Import from llama_hub package\n",
        "#RAGFusionPipelinePack                                           RAGFusionPipelinePack\n",
        "#from llama_hub.llama_packs.query.rag_fusion_pipeline.base import RAGFusionPipelinePack\n",
        "#from llama_index.llms import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING with OPENAI and Langchain"
      ],
      "metadata": {
        "id": "Xgo5C7hzbWCY"
      },
      "id": "Xgo5C7hzbWCY"
    },
    {
      "cell_type": "code",
      "source": [
        "# 20x faster than pgvector: introducing pg_embedding extension for vector search in Postgres and LangChain\n",
        "# https://neon.tech/blog/pg-embedding-extension-for-vector-search\n",
        "\n",
        "#ADDED By FM 11/01/2024\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "# https://supabase.com/blog/fewer-dimensions-are-better-pgvector\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "collection_name='Paul Graham Essay'\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs0,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")\n",
        "\n",
        "#db.create_hnsw_index(dims = 1536, m = 8, ef_construction = 16, ef_search = 16)"
      ],
      "metadata": {
        "id": "SHrXuMUXGAMW"
      },
      "id": "SHrXuMUXGAMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 11/01/2024\n",
        "query='What did the author do growing up?'\n",
        "docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)\n",
        "\n",
        "print()\n",
        "print(query)\n",
        "print()\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "A4RiRP1jOGVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8349af9-15d6-4a88-91ac-0f9ec156591d"
      },
      "id": "A4RiRP1jOGVs",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What did the author do growing up?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.59925514\n",
            "What I Worked On\n",
            "\n",
            "February 2021\n",
            "\n",
            "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
            "\n",
            "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6023548\n",
            "Working on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\n",
            "\n",
            "In the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6047227\n",
            "Over the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.\n",
            "\n",
            "One night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6152403\n",
            "In the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\n",
            "\n",
            "I've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\n",
            "\n",
            "I knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG FUSION PIPELINE"
      ],
      "metadata": {
        "id": "bzeoJMRNbzXg"
      },
      "id": "bzeoJMRNbzXg"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"RAG Fusion Pipeline.\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from llama_index.core import Document, ServiceContext, VectorStoreIndex\n",
        "from llama_index.core.llama_pack.base import BaseLlamaPack\n",
        "from llama_index.core.llms.llm import LLM\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.query_pipeline.components.argpacks import ArgPackComponent\n",
        "from llama_index.core.query_pipeline.components.function import FnComponent\n",
        "from llama_index.core.query_pipeline.components.input import InputComponent\n",
        "from llama_index.core.query_pipeline.query import QueryPipeline\n",
        "from llama_index.core.response_synthesizers import TreeSummarize\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "DEFAULT_CHUNK_SIZES = [128, 256, 512, 1024]\n",
        "\n",
        "\n",
        "def reciprocal_rank_fusion(\n",
        "    results: List[List[NodeWithScore]],\n",
        ") -> List[NodeWithScore]:\n",
        "    \"\"\"Apply reciprocal rank fusion.\n",
        "\n",
        "    The original paper uses k=60 for best results:\n",
        "    https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n",
        "    \"\"\"\n",
        "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
        "    fused_scores = {}\n",
        "    text_to_node = {}\n",
        "    rank=0\n",
        "\n",
        "#for rank, node_with_score in enumerate(\n",
        "#            sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n",
        "#        ):\n",
        "\n",
        "# The above lines commented generated this error AttributeError: 'tuple' object has no attribute 'score'\n",
        "\n",
        "\n",
        "     # compute reciprocal rank scores by Frank Morales 09/05/2024\n",
        "    for node_with_score in results:\n",
        "        rank+=1\n",
        "        if not isinstance(node_with_score, NodeWithScore):\n",
        "            raise TypeError(\"node_with_score must be a NodeWithScore object.\")\n",
        "        text = node_with_score.node.get_content()\n",
        "        text_to_node[text] = node_with_score\n",
        "        if text not in fused_scores:\n",
        "          fused_scores[text] = 0.0\n",
        "        fused_scores[text] += 1.0 / (rank + k)\n",
        "\n",
        "    # sort results\n",
        "    reranked_results = dict(\n",
        "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    )\n",
        "\n",
        "    # adjust node scores\n",
        "    reranked_nodes: List[NodeWithScore] = []\n",
        "    for text, score in reranked_results.items():\n",
        "        reranked_nodes.append(text_to_node[text])\n",
        "        reranked_nodes[-1].score = score\n",
        "\n",
        "    return reranked_nodes\n",
        "\n",
        "\n",
        "class RAGFusionPipelinePack(BaseLlamaPack):\n",
        "    \"\"\"RAG Fusion pipeline.\n",
        "\n",
        "    Create a bunch of vector indexes of different chunk sizes.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        documents: List[Document],\n",
        "        llm: Optional[LLM] = None,\n",
        "        chunk_sizes: Optional[List[int]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "        self.documents = documents\n",
        "        self.chunk_sizes = chunk_sizes or DEFAULT_CHUNK_SIZES\n",
        "\n",
        "        # construct index\n",
        "        self.llm = llm or OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "        self.query_engines = []\n",
        "        self.retrievers = {}\n",
        "        for chunk_size in self.chunk_sizes:\n",
        "            splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "            nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "            service_context = ServiceContext.from_defaults(llm=self.llm)\n",
        "            vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "            self.query_engines.append(vector_index.as_query_engine())\n",
        "\n",
        "            self.retrievers[str(chunk_size)] = vector_index.as_retriever()\n",
        "\n",
        "        # define rerank component\n",
        "        rerank_component = FnComponent(fn=reciprocal_rank_fusion)\n",
        "\n",
        "        # construct query pipeline\n",
        "        p = QueryPipeline()\n",
        "        module_dict = {\n",
        "            **self.retrievers,\n",
        "            \"input\": InputComponent(),\n",
        "            \"summarizer\": TreeSummarize(),\n",
        "            # NOTE: Join args\n",
        "            \"join\": ArgPackComponent(),\n",
        "            \"reranker\": rerank_component,\n",
        "        }\n",
        "        p.add_modules(module_dict)\n",
        "        # add links from input to retriever (id'ed by chunk_size)\n",
        "        for chunk_size in self.chunk_sizes:\n",
        "            p.add_link(\"input\", str(chunk_size))\n",
        "            p.add_link(str(chunk_size), \"join\", dest_key=str(chunk_size))\n",
        "        p.add_link(\"join\", \"reranker\")\n",
        "        p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
        "        p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n",
        "\n",
        "        self.query_pipeline = p\n",
        "\n",
        "    def get_modules(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get modules.\"\"\"\n",
        "        return {\n",
        "            \"llm\": self.llm,\n",
        "            \"retrievers\": self.retrievers,\n",
        "            \"query_engines\": self.query_engines,\n",
        "            \"query_pipeline\": self.query_pipeline,\n",
        "        }\n",
        "\n",
        "    def run(self, *args: Any, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run the pipeline.\"\"\"\n",
        "        return self.query_pipeline.run(*args, **kwargs)\n"
      ],
      "metadata": {
        "id": "n5s85UdnxwAQ"
      },
      "id": "n5s85UdnxwAQ",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.llamaindex.ai/en/stable/examples/llm/fireworks/"
      ],
      "metadata": {
        "id": "4J5pQ2UF4yw5"
      },
      "id": "4J5pQ2UF4yw5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4786b6-6add-4caf-9f9c-c2d8f455702a",
      "metadata": {
        "id": "1e4786b6-6add-4caf-9f9c-c2d8f455702a"
      },
      "outputs": [],
      "source": [
        "pack = RAGFusionPipelinePack(docs, llm)\n",
        "query0=\"What did the author do growing up?\"\n",
        "response0 = pack.run(query=query0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR-FyKpy1Nuv",
        "outputId": "a1801069-da69-403f-ea01-b65ede95de3c"
      },
      "id": "HR-FyKpy1Nuv",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author, growing up, worked on writing short stories and programming. They started writing short stories as a beginning writer and also began programming on an IBM 1401 in 9th grade using an early version of Fortran. Later on, they got a TRS-80 computer and started programming more extensively, creating simple games, a rocket prediction program, and a word processor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLAUDE3 - MODEL - WITH API"
      ],
      "metadata": {
        "id": "lnxYYmMYwK1l"
      },
      "id": "lnxYYmMYwK1l"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-anthropic -q\n",
        "!pip install anthropic -q\n",
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "Mjoef7XFwgcr"
      },
      "id": "Mjoef7XFwgcr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "import os\n",
        "import colab_env\n",
        "import json\n",
        "\n",
        "\n",
        "anthropic_api_key = os.environ[\"CLAUDE3_API_KEY\"]\n",
        "\n",
        "from langchain_anthropic import AnthropicLLM\n",
        "llm = AnthropicLLM(anthropic_api_key=anthropic_api_key,model='claude-2.1')\n",
        "\n",
        "prompt= 'How do you plan out your trip? \\\n",
        "Bob is travelling to SAT from YVR \\\n",
        "1. He has a connection in DFW \\\n",
        "2. His connection is 6 hours long \\\n",
        "3. He has a budget of 100.00 including meals \\\n",
        "4. What can he do? Please suggest a time. \\\n",
        "5. Know- he is a hiker, museum, foodie, has a carry-on bag'\n",
        "\n",
        "response=llm(prompt)"
      ],
      "metadata": {
        "id": "PgsjZxMIoQgD"
      },
      "id": "PgsjZxMIoQgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY7QqbSdol5k",
        "outputId": "9c6e11a0-50d3-4ec5-9f2d-7f5f411f5ea5"
      },
      "id": "SY7QqbSdol5k",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Since Bob is traveling from YVR to SAT with a 6 hour layover at DFW:\n",
            "    - I'd recommend Bob check-in online for his flight with his carry-on bag only so he doesn't have to check any luggage. This will allow him to easily explore DFW during his layover.\n",
            "2. With a budget of $100 for meals:\n",
            "    - Bob could head to Terminal A or Terminal D to visit one of the many mid-priced restaurants and bars. For example, Bob could grab an $15-20 lunch and a $15-20 dinner, leaving him with around $50-60 to spend elsewhere.\n",
            "3. As a hiker, museum-goer, and foodie, during his 6 hour layover, Bob could:\n",
            "    - Take DART rail for $2.50 each way to the Dallas Museum of Art (open until 5pm, $16 admission). This would take 30-60 mins travel each way. He could spend 2-3 hours here exploring. \n",
            "    - Alternatively, spend a few hours hiking at the Trinity River Audubon Center which has free admission and is open until 5pm. Take DART/Uber there for no more than $30 roundtrip.\n",
            "    - Have an early dinner in Terminal D at Pappasito's Cantina with TexMex cuisine before his connecting flight. Spend around $25-30 on dinner.\n",
            "\n",
            "I'd budget 3 hours for the museum/hiking option plus meals, leaving Bob with extra time for contingencies getting back through security to his departure terminal/gate at DFW. Let me know if you have any other questions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "chat = ChatAnthropic(temperature=0, api_key=anthropic_api_key, model_name=\"claude-3-opus-20240229\")\n",
        "response=chat.predict('capital city of canada')"
      ],
      "metadata": {
        "id": "cEdGjg7moxxw"
      },
      "id": "cEdGjg7moxxw",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fsQH-R6uV8h",
        "outputId": "dea87581-879d-40b5-db43-e5cb14b53e54"
      },
      "id": "6fsQH-R6uV8h",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital city of Canada is Ottawa, located in the province of Ontario. Ottawa is situated on the banks of the Ottawa River, which forms the border between Ontario and Quebec. Some key facts about Ottawa:\n",
            "\n",
            "1. Ottawa was chosen as the capital of the Province of Canada by Queen Victoria in 1857 and later became the national capital in 1867 when Canada gained independence.\n",
            "\n",
            "2. The city is home to Parliament Hill, which houses the Parliament of Canada, the Senate, and the House of Commons.\n",
            "\n",
            "3. Ottawa is known for its historic architecture, numerous museums, and cultural attractions, such as the National Gallery of Canada, the Canadian Museum of History, and the Rideau Canal, which is a UNESCO World Heritage Site.\n",
            "\n",
            "4. The city has a population of approximately 1 million people in its metropolitan area, making it the fourth-largest city in Canada.\n",
            "\n",
            "5. Ottawa is a bilingual city, with a significant proportion of its population speaking both English and French, Canada's two official languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_anthropic import ChatAnthropic\n",
        "llm_claude3 = ChatAnthropic(anthropic_api_key=anthropic_api_key,model=\"claude-3-sonnet-20240229\", temperature=0.8, max_tokens=1024)"
      ],
      "metadata": {
        "id": "20qCn6Y-pG5z"
      },
      "id": "20qCn6Y-pG5z",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pack_claude3 = RAGFusionPipelinePack(docs, llm_claude3)\n",
        "query0=\"What did the author do growing up?\"\n",
        "response0 = pack_claude3.run(query=query0)"
      ],
      "metadata": {
        "id": "RHYicoyPxHvo"
      },
      "id": "RHYicoyPxHvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33LB_lfexVvO",
        "outputId": "f13da313-3ae1-4472-9a83-727749f0b46f"
      },
      "id": "33LB_lfexVvO",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author, growing up, worked on writing short stories and programming. They started writing short stories as a beginning writer and also began programming on an IBM 1401 in 9th grade using an early version of Fortran. Later on, they got a TRS-80 computer and wrote simple games, a rocket prediction program, and a word processor. Additionally, the author initially planned to study philosophy in college but switched to AI due to their interest sparked by a novel and a PBS documentary featuring intelligent computers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples of Queries"
      ],
      "metadata": {
        "id": "ZOkptc-Yx8U0"
      },
      "id": "ZOkptc-Yx8U0"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "065ef0c4-0c9e-4611-8de4-98b2a7fe3094",
      "metadata": {
        "id": "065ef0c4-0c9e-4611-8de4-98b2a7fe3094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234dfcf3-eaec-4979-9270-6d3ef60d8bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What did the author do growing up?\n",
            "The author, growing up, worked on writing short stories and programming. The first programs were written on an IBM 1401 in 9th grade using an early version of Fortran. Later, the author got a TRS-80 computer and started programming games, a rocket prediction program, and a word processor. Initially planning to study philosophy in college, the author switched to AI due to influences from a novel and a PBS documentary featuring intelligent computers.\n",
            "\n",
            "\n",
            "Who is the President of the USA?\n",
            "I cannot provide the current President of the USA as it is not mentioned in the provided context information.\n",
            "\n",
            "\n",
            "Who is the best poet of CANADA?\n",
            "I cannot provide an answer to the query as the information provided does not mention any specific poet from Canada or indicate who the best poet of Canada is.\n",
            "\n",
            "\n",
            "Anything about LIPS\n",
            "LISP, a programming language, is discussed in the provided context. The language is noted for its unique core, defined by writing an interpreter in itself. Initially intended as a formal model of computation, LISP evolved into a programming language due to the work of John McCarthy and his grad student Steve Russell. McCarthy's original LISP interpreter was limited and lacked many features found in modern programming languages. Over time, additional features were added to LISP, departing from McCarthy's original axiomatic approach. Despite its origins, LISP's elegance and power set it apart from other languages. The context also mentions the creation of a new Lisp variant called Bel, developed by the author over a period of four years.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#modify By FM 11/01/2024\n",
        "\n",
        "#response = pack.run(query=\"What did the author do growing up?\")\n",
        "query0=\"What did the author do growing up?\"\n",
        "query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query1 = \"Who is the President of the USA?\"\n",
        "query2 = \"Who is the best poet of CANADA?\"\n",
        "#query2 = \"Who won the baseball World Series in 2023? and Who Lost\"\n",
        "query3 = 'Anything about FORTRAN'\n",
        "query4 = 'Anything about LIPS'\n",
        "query5 = 'Anything about Python'\n",
        "\n",
        "\n",
        "response0 = pack_claude3.run(query=query0)\n",
        "response1 = pack_claude3.run(query=query1)\n",
        "response2 = pack_claude3.run(query=query2)\n",
        "response4 = pack_claude3.run(query=query4)\n",
        "\n",
        "print()\n",
        "print(query0)\n",
        "print(str(response0))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query1)\n",
        "print(str(response1))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query2)\n",
        "print(str(response2))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query4)\n",
        "print(str(response4))\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "sZy5EuwA8onu",
        "IfQ8Cym29XOQ",
        "a7c9299c-06d6-4710-afb0-3cc13761f358",
        "Xgo5C7hzbWCY",
        "bzeoJMRNbzXg",
        "lnxYYmMYwK1l",
        "ZOkptc-Yx8U0"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
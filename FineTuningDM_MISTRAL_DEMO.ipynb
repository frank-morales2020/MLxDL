{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "vLuEdNytoJHV"
      ],
      "authorship_tag": "ABX9TyPcUDHFmbhnLcEBS+nAUvHO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "718b5a226de247b69a3474fba5d59e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07b8ecb071d24ac987b3df88a033c7bc",
              "IPY_MODEL_d0953cdf9a4e43c1ac74a69409e73945",
              "IPY_MODEL_ed978771ea0b4cc5b7737972ca8d0356"
            ],
            "layout": "IPY_MODEL_8eb7de077ec14e2d9f05cfa2de339daa"
          }
        },
        "07b8ecb071d24ac987b3df88a033c7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_795906b19adf4c82a5df4358a2d4bfe1",
            "placeholder": "​",
            "style": "IPY_MODEL_f0b3a750d1764b5a844110fe10a8d8b8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d0953cdf9a4e43c1ac74a69409e73945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56001f51f0b4401cabae48d6b679212b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a43925b23804358ad156cd923715558",
            "value": 2
          }
        },
        "ed978771ea0b4cc5b7737972ca8d0356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c46a20b4f3b14a7f9f46811bb1f42a73",
            "placeholder": "​",
            "style": "IPY_MODEL_d6142a834ab84b23b429f78ba382f196",
            "value": " 2/2 [00:05&lt;00:00,  2.45s/it]"
          }
        },
        "8eb7de077ec14e2d9f05cfa2de339daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "795906b19adf4c82a5df4358a2d4bfe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0b3a750d1764b5a844110fe10a8d8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56001f51f0b4401cabae48d6b679212b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a43925b23804358ad156cd923715558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c46a20b4f3b14a7f9f46811bb1f42a73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6142a834ab84b23b429f78ba382f196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a541333ceb340adb186107d89d10650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf71b89bbabd47cc8672c280490c4762",
              "IPY_MODEL_72cb67dcfcbe4391918c20a4c9378a53",
              "IPY_MODEL_bac3e76a44034e5285246592c1db934c"
            ],
            "layout": "IPY_MODEL_3de1773a273f412f96fc9887e2c51341"
          }
        },
        "cf71b89bbabd47cc8672c280490c4762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1fb0bd687324b3a974056aa5f9eb705",
            "placeholder": "​",
            "style": "IPY_MODEL_123a40a71edf4383824fc369f524e4e0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "72cb67dcfcbe4391918c20a4c9378a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d042e2a7664ebbb971751c565785fa",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ed16519f8594c12ac6cb8dfd99efe57",
            "value": 2
          }
        },
        "bac3e76a44034e5285246592c1db934c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f321c5e6ecbb49c8a9c5939efce7a237",
            "placeholder": "​",
            "style": "IPY_MODEL_39da16cb7aab4a81a59996de03d92c4a",
            "value": " 2/2 [00:02&lt;00:00,  1.09it/s]"
          }
        },
        "3de1773a273f412f96fc9887e2c51341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1fb0bd687324b3a974056aa5f9eb705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "123a40a71edf4383824fc369f524e4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26d042e2a7664ebbb971751c565785fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed16519f8594c12ac6cb8dfd99efe57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f321c5e6ecbb49c8a9c5939efce7a237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39da16cb7aab4a81a59996de03d92c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FineTuningDM_MISTRAL_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install torch -q\n",
        "!pip install accelerate -q\n",
        "!pip install bitsandbytes -q\n",
        "!pip install datetime -q\n",
        "!pip install unsloth -q\n",
        "!pip install openai -q\n",
        "!pip install openai-agents -q"
      ],
      "metadata": {
        "id": "7FPAs9pVLCDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mistral with transformer"
      ],
      "metadata": {
        "id": "zI5BrByVo0id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "! pip install peft --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "FjkG2Bvdf5Jo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    #attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "718b5a226de247b69a3474fba5d59e89",
            "07b8ecb071d24ac987b3df88a033c7bc",
            "d0953cdf9a4e43c1ac74a69409e73945",
            "ed978771ea0b4cc5b7737972ca8d0356",
            "8eb7de077ec14e2d9f05cfa2de339daa",
            "795906b19adf4c82a5df4358a2d4bfe1",
            "f0b3a750d1764b5a844110fe10a8d8b8",
            "56001f51f0b4401cabae48d6b679212b",
            "3a43925b23804358ad156cd923715558",
            "c46a20b4f3b14a7f9f46811bb1f42a73",
            "d6142a834ab84b23b429f78ba382f196"
          ]
        },
        "id": "6MRrIbkaf2Jd",
        "outputId": "1859e5fd-2560-4da5-9d1f-b3f3c51655c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "718b5a226de247b69a3474fba5d59e89"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code2dataset"
      ],
      "metadata": {
        "id": "MlrjB_36oZnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/ai-simplified-in-plain-english/ai-driven-disruption-management-for-next-gen-flight-scheduling-using-openai-51c7064af3e5"
      ],
      "metadata": {
        "id": "tRM4w2D6n3k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "dataset = [\n",
        "    {\n",
        "        \"agent_name\": \"CrewAgent\",\n",
        "        \"code\": \"\"\"\n",
        "    class CrewAgent:\n",
        "        def __init__(self, db):\n",
        "            self.db = db  # Database connection\n",
        "\n",
        "        def assign_crew_to_flight(self, crew_id, flight_id):\n",
        "            \\\"\\\"\\\"Assigns a crew member to a flight.\\\"\\\"\\\"\n",
        "            # Code to update database with crew assignment\n",
        "            print(f\"Crew {crew_id} assigned to flight {flight_id}\")\n",
        "\n",
        "        def update_crew_schedule(self, crew_id, new_schedule):\n",
        "            \\\"\\\"\\\"Updates the schedule for a crew member.\\\"\\\"\\\"\n",
        "            # Code to update crew schedule in the database\n",
        "            print(f\"Crew {crew_id} schedule updated to {new_schedule}\")\n",
        "\n",
        "        def reassign_crew(self, flight_id, disruption_type):\n",
        "            \\\"\\\"\\\"Reassigns crew members due to a disruption.\\\"\\\"\\\"\n",
        "            # Code to handle crew reassignment logic\n",
        "            print(f\"Crew reassigned for flight {flight_id} due to {disruption_type}\")\n",
        "\n",
        "        # ... other CrewAgent methods ...\n",
        "    \"\"\",\n",
        "        \"description\": \"\"\"\n",
        "    The CrewAgent class is responsible for managing crew-related operations in the flight disruption management system.\n",
        "    It handles tasks such as assigning crew members to flights, updating crew schedules, and coordinating crew reassignments during disruptions.\n",
        "    The CrewAgent interacts with the database to store and retrieve crew information.\n",
        "    \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"agent_name\": \"PassengerAgent\",\n",
        "        \"code\": \"\"\"\n",
        "    class PassengerAgent:\n",
        "        def __init__(self, db):\n",
        "            self.db = db  # Database connection\n",
        "\n",
        "        def get_passenger_manifest(self, flight_id):\n",
        "            \\\"\\\"\\\"Retrieves the passenger manifest for a flight.\\\"\\\"\\\"\n",
        "            # Code to query the database for passenger list\n",
        "            print(f\"Passenger manifest for flight {flight_id} retrieved\")\n",
        "            return [\"PassengerA\", \"PassengerB\", ...]\n",
        "\n",
        "        def notify_passenger(self, passenger_id, message):\n",
        "            \\\"\\\"\\\"Sends a notification to a passenger.\\\"\\\"\\\"\n",
        "            # Code to send notification (e.g., email, SMS)\n",
        "            print(f\"Passenger {passenger_id} notified: {message}\")\n",
        "\n",
        "        def rebook_passenger(self, passenger_id, new_flight_id):\n",
        "            \\\"\\\"\\\"Rebooks a passenger on a new flight.\\\"\\\"\\\"\n",
        "            # Code to update the booking information in the database\n",
        "            print(f\"Passenger {passenger_id} rebooked on flight {new_flight_id}\")\n",
        "\n",
        "        # ... other PassengerAgent methods ...\n",
        "    \"\"\",\n",
        "        \"description\": \"\"\"\n",
        "    The PassengerAgent class is responsible for handling passenger-related operations in the flight disruption management system.\n",
        "    It manages tasks such as retrieving passenger manifests, notifying passengers of changes or disruptions, rebooking passengers on new flights,\n",
        "    and communicating with passengers.\n",
        "    \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"agent_name\": \"FleetAgent\",\n",
        "        \"code\": \"\"\"\n",
        "    class FleetAgent:\n",
        "        def __init__(self, db, crew_agent, passenger_agent):\n",
        "            self.db = db  # Database connection\n",
        "            self.crew_agent = crew_agent\n",
        "            self.passenger_agent = passenger_agent\n",
        "\n",
        "        def assess_disruption_impact(self, disruption_event):\n",
        "            \\\"\\\"\\\"Analyzes the impact of a disruption event.\\\"\\\"\\\"\n",
        "            # Code to analyze delayed flights, affected passengers, etc.\n",
        "            print(f\"Disruption impact assessed: {disruption_event}\")\n",
        "            return {\"delayed_flights\": 10, \"affected_passengers\": 200}\n",
        "\n",
        "        def orchestrate_recovery(self, disruption_event):\n",
        "            \\\"\\\"\\\"Orchestrates the recovery efforts after a disruption.\\\"\\\"\\\"\n",
        "            # Code to coordinate crew reassignment, passenger rebooking, etc.\n",
        "            print(f\"Recovery orchestrated for disruption: {disruption_event}\")\n",
        "\n",
        "        def reschedule_flights(self, disruption_event):\n",
        "            \\\"\\\"\\\"Reschedules flights to minimize disruption.\\\"\\\"\\\"\n",
        "            # Code to generate new flight schedules\n",
        "            print(f\"Flights rescheduled after disruption: {disruption_event}\")\n",
        "\n",
        "        # ... other FleetAgent methods ...\n",
        "    \"\"\",\n",
        "        \"description\": \"\"\"\n",
        "    The FleetAgent class is responsible for orchestrating the overall recovery strategy in case of flight disruptions.\n",
        "    It assesses the impact of disruptions, coordinates crew and passenger actions through the CrewAgent and PassengerAgent,\n",
        "    and reschedules flights to minimize the impact of disruptions.\n",
        "    \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"agent_name\": \"Database Operations\",\n",
        "        \"code\": \"\"\"\n",
        "def create_database(db_name):\n",
        "    \\\"\\\"\\\"Creates a new SQLite database.\\\"\\\"\\\"\n",
        "    # Code to create a database file\n",
        "    print(f\"Database {db_name} created\")\n",
        "\n",
        "def connect_to_database(db_name):\n",
        "    \\\"\\\"\\\"Establishes a connection to the SQLite database.\\\"\\\"\\\"\n",
        "    # Code to connect to the database\n",
        "    print(f\"Connected to database {db_name}\")\n",
        "    return sqlite3.connect(db_name)\n",
        "\n",
        "def close_database_connection(db_connection):\n",
        "    \\\"\\\"\\\"Closes the connection to the SQLite database.\\\"\\\"\\\"\n",
        "    # Code to close the database connection\n",
        "    print(\"Database connection closed\")\n",
        "    db_connection.close()\n",
        "\"\"\",\n",
        "        \"description\": \"\"\"\n",
        "    These functions handle database operations for the flight disruption management system.\n",
        "    They include creating a new SQLite database, connecting to the database, and closing the database connection.\n",
        "    \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"agent_name\": \"Analytics Functions\",\n",
        "        \"code\": \"\"\"\n",
        "def analyze_disruption_impact(disruption_event):\n",
        "    \\\"\\\"\\\"Analyzes the impact of a disruption event.\\\"\\\"\\\"\n",
        "    # Code to analyze delayed flights, affected passengers, etc.\n",
        "    print(f\"Disruption impact analysis for: {disruption_event}\")\n",
        "    return {\"delayed_flights\": 15, \"affected_passengers\": 300}\n",
        "\n",
        "def calculate_disruption_cost(delayed_flights):\n",
        "    \\\"\\\"\\\"Calculates the estimated cost of a disruption.\\\"\\\"\\\"\n",
        "    # Code to calculate disruption cost based on delays\n",
        "    cost_per_flight = 1000  # Example cost per delayed flight\n",
        "    total_cost = delayed_flights * cost_per_flight\n",
        "    print(f\"Disruption cost: ${total_cost}\")\n",
        "    return total_cost\n",
        "\"\"\",\n",
        "        \"description\": \"\"\"\n",
        "    These functions provide analytics capabilities for the flight disruption management system.\n",
        "    They include analyzing the impact of disruption events and calculating the estimated cost of disruptions.\n",
        "    \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"agent_name\": \"Disruption Simulation\",\n",
        "        \"code\": \"\"\"\n",
        "def simulate_disruption(db, airport_code, start_time, end_time):\n",
        "    \\\"\\\"\\\"Simulates a disruption event at a specific airport.\\\"\\\"\\\"\n",
        "    # Code to introduce delays or cancellations in the database\n",
        "    print(f\"Disruption simulated at {airport_code} from {start_time} to {end_time}\")\n",
        "\n",
        "    # Example: Introduce a delay to a flight\n",
        "    # Assuming you have a function to fetch flights and update their status\n",
        "    # flight_to_delay = get_flight_at_airport(db, airport_code, start_time)\n",
        "    # if flight_to_delay:\n",
        "    #     update_flight_status(db, flight_to_delay['flight_id'], 'delayed')\n",
        "    return \"Simulated disruption event\"\n",
        "\"\"\",\n",
        "        \"description\": \"\"\"\n",
        "    This function simulates a disruption event at a specific airport within a defined time frame.\n",
        "    It introduces delays or cancellations to flights in the database to mimic real-world disruptions.\n",
        "    \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save the dataset to a JSON file\n",
        "with open(\"flight_agent_code_dataset.json\", \"w\") as f:\n",
        "    json.dump(dataset, f, indent=4)\n",
        "\n",
        "print(\"Dataset created and saved to flight_agent_code_dataset.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTq3J5dvkD-9",
        "outputId": "91f8d90e-3f04-4c5e-da70-4e1a0e924e30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created and saved to flight_agent_code_dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"flight_agent_code_dataset.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} entries\")\n",
        "# Now 'dataset' is a list of dictionaries, just like before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0iKoWCxY87w",
        "outputId": "b19d2795-483b-4734-c372-c6c856358b98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 6 entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tuning"
      ],
      "metadata": {
        "id": "oLf7VpcKoTo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-fine-tuned\",\n",
        "    #use_cache=False,\n",
        "    per_device_train_batch_size=6,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=200,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    num_train_epochs=5,\n",
        "    max_steps=3000,\n",
        "    learning_rate=1e-5,\n",
        "    logging_steps=200,\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.1,\n",
        "    eval_steps=10,  # Assuming you want eval_steps to be 10\n",
        "    report_to=\"none\",\n",
        "    save_steps=20,\n",
        "    max_grad_norm=1.0,\n",
        "    #metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    logging_strategy=\"steps\",\n",
        "    #evaluation_strategy=\"steps\",\n",
        "\n",
        "    #greater_is_better=False,  # For minimizing eval_loss (accuracy is maximized by default)\n",
        "\n",
        "    #no_cuda=False if torch.cuda.is_available() else True # Add this line if needed for CPU training\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "fBDjCANexVMy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "#from unsloth import FastLanguageModel\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "# 1. Load the Dataset\n",
        "with open(\"flight_agent_code_dataset.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} entries\")\n",
        "\n",
        "# 2. Load Tokenizer and Model (Unsloth)\n",
        "\n",
        "\n",
        "# 3. Tokenize the Dataset and Create Labels\n",
        "# Tokenize the dataset and create labels\n",
        "def tokenize_function(examples):\n",
        "    tokenized_output = tokenizer(examples[\"code\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "    input_ids = tokenized_output[\"input_ids\"]\n",
        "    attention_mask = tokenized_output[\"attention_mask\"]\n",
        "\n",
        "    # Create labels by shifting input_ids\n",
        "    labels = input_ids[1:].copy()\n",
        "    labels.append(tokenizer.eos_token_id)\n",
        "    item[\"input_ids\"] = input_ids[:-1]\n",
        "    item[\"attention_mask\"] = attention_mask[:-1]\n",
        "    item[\"labels\"] = labels\n",
        "\n",
        "    return tokenized_output  # Return the tokenized output\n",
        "\n",
        "for item in dataset:\n",
        "    tokenized_output = tokenize_function({\"code\": item[\"code\"]})\n",
        "    item[\"input_ids\"] = tokenized_output[\"input_ids\"]\n",
        "    item[\"attention_mask\"] = tokenized_output[\"attention_mask\"]\n",
        "    item[\"labels\"] = item[\"input_ids\"].copy()  # Or your label creation logic\n",
        "\n",
        "\n",
        "print(\"Dataset tokenized and labels created using tokenizer\")\n",
        "\n",
        "\n",
        "# Data collator (ensure it handles padding if necessary or use the default)\n",
        "def data_collator(data):\n",
        "    input_ids = [item['input_ids'] for item in data]\n",
        "    attention_mask = [item['attention_mask'] for item in data]\n",
        "    labels = [item['labels'] for item in data]\n",
        "\n",
        "    # Pad the sequences manually if needed, or rely on the tokenizer's padding\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence([torch.tensor(mask) for mask in attention_mask], batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(label) for label in labels], batch_first=True, padding_value=-100)  # Or your padding value\n",
        "\n",
        "    return {'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels}\n",
        "\n",
        "\n",
        "# 4. Configure PEFT (LoRA)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 5. Define Training Arguments\n",
        "training_args_old = TrainingArguments(\n",
        "    output_dir=\"./mistral-fine-tuned\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.01,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator, # Use the defined data_collator\n",
        "    #callbacks=[early_stopping_callback],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Mistral fine-tuning complete!\")\n",
        "\n",
        "# 7. Save the Fine-tuned Model\n",
        "model.save_pretrained(\"./mistral-fine-tuned\")\n",
        "tokenizer.save_pretrained(\"./mistral-fine-tuned\")\n",
        "\n",
        "print(\"Fine-tuned model and tokenizer saved!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "rQkOU_zmdFeH",
        "outputId": "42afb555-9ade-4d76-b1b9-acdc8ed4b55d",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 1:21:03, Epoch 3000/3000]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.959700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.771000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.757400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.756700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.756000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.755600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral fine-tuning complete!\n",
            "Fine-tuned model and tokenizer saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation"
      ],
      "metadata": {
        "id": "fIPl_sxpmGeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./mistral-fine-tuned\"  # Path to your saved model\n",
        "base_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\" # The original base model\n",
        "\n",
        "# Load the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    #quantization_config=bnb_config  # If you used quantization, include this\n",
        ")\n",
        "\n",
        "# Load the PEFT adapter and apply it to the base model\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=True)\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# Evaluation Function (Code Generation)\n",
        "def evaluate_code_generation(model, tokenizer, prompt):\n",
        "    \"\"\"\n",
        "    Evaluates the model's code generation for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned model.\n",
        "        tokenizer: The tokenizer.\n",
        "        prompt: A string describing the desired code functionality.\n",
        "\n",
        "    Returns:\n",
        "        generated_code: The code generated by the model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=200)  # Adjust max_length as needed\n",
        "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_code\n",
        "\n",
        "# Evaluation Function (Simulation - Simplified Example)\n",
        "def evaluate_simulation(generated_code, scenario):\n",
        "    \"\"\"\n",
        "    Evaluates the generated code in a simplified simulation.\n",
        "\n",
        "    Args:\n",
        "        generated_code: The code generated by the model.\n",
        "        scenario: A dictionary representing a disruption scenario.\n",
        "\n",
        "    Returns:\n",
        "        results: A dictionary containing evaluation metrics.\n",
        "    \"\"\"\n",
        "    # **Note:** This is a simplified example. A real simulation would involve\n",
        "    # executing the generated code in a controlled environment and\n",
        "    # measuring its effects.\n",
        "\n",
        "    # In this example, we'll just check if the generated code contains\n",
        "    # keywords related to handling the scenario.\n",
        "\n",
        "    results = {}\n",
        "    if \"delay\" in scenario and \"reassign_crew\" in generated_code:\n",
        "        results[\"crew_reassignment\"] = \"Handled\"\n",
        "    else:\n",
        "        results[\"crew_reassignment\"] = \"Not Handled\"\n",
        "\n",
        "    if \"passenger\" in scenario and \"rebook_passenger\" in generated_code:\n",
        "        results[\"passenger_rebooking\"] = \"Handled\"\n",
        "    else:\n",
        "        results[\"passenger_rebooking\"] = \"Not Handled\"\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# 1. Code Generation Evaluation\n",
        "prompt = \"Generate Python code for a CrewAgent to reassign crew members due to a weather delay.\"\n",
        "generated_code = evaluate_code_generation(model, tokenizer, prompt)\n",
        "print(\"Generated Code:\\n\", generated_code)\n",
        "\n",
        "# 2. Simulation Evaluation (Simplified)\n",
        "scenario = {\"delay\": True, \"passenger\": True}  # Example scenario: weather delay affecting passengers\n",
        "simulation_results = evaluate_simulation(generated_code, scenario)\n",
        "print(\"Simulation Results:\\n\", simulation_results)"
      ],
      "metadata": {
        "id": "TrqhXGAgmEnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation2"
      ],
      "metadata": {
        "id": "Ttq_rBThrcxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./mistral-fine-tuned\"\n",
        "base_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Load the base model\n",
        "try:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        # quantization_config=bnb_config\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error loading base model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Load the PEFT adapter\n",
        "try:\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading fine-tuned model: {e}\")\n",
        "    exit()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=True)\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# Evaluation Function (Code Generation)\n",
        "def evaluate_code_generation(model, tokenizer, prompt, max_length=200):\n",
        "    \"\"\"\n",
        "    Evaluates the model's code generation for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned model.\n",
        "        tokenizer: The tokenizer.\n",
        "        prompt: A string describing the desired code functionality.\n",
        "        max_length: Maximum length of the generated code.\n",
        "\n",
        "    Returns:\n",
        "        generated_code: The code generated by the model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    try:\n",
        "        outputs = model.generate(**inputs, max_length=max_length)\n",
        "        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_code\n",
        "    except Exception as e:\n",
        "        print(f\"Error during code generation: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Evaluation Function (Simulation - Simplified Example)\n",
        "def evaluate_simulation(generated_code, scenario):\n",
        "    \"\"\"\n",
        "    Evaluates the generated code in a simplified simulation.\n",
        "\n",
        "    Args:\n",
        "        generated_code: The code generated by the model.\n",
        "        scenario: A dictionary representing a disruption scenario.\n",
        "\n",
        "    Returns:\n",
        "        results: A dictionary containing evaluation metrics.\n",
        "    \"\"\"\n",
        "    # **Note:** This is a simplified example. A real simulation would involve\n",
        "    # executing the generated code in a controlled environment and\n",
        "    # measuring its effects.\n",
        "\n",
        "    # Check for keywords related to handling the scenario.\n",
        "    results = {}\n",
        "    results[\"crew_reassignment\"] = \"Handled\" if \"delay\" in scenario and \"reassign_crew\" in generated_code else \"Not Handled\"\n",
        "    results[\"passenger_rebooking\"] = \"Handled\" if \"passenger\" in scenario and \"rebook_passenger\" in generated_code else \"Not Handled\"\n",
        "    results[\"flight_rescheduling\"] = \"Handled\" if \"delay\" in scenario and \"reschedule_flights\" in generated_code else \"Not Handled\"\n",
        "    results[\"disruption_assessment\"] = \"Handled\" if \"delay\" in scenario and \"assess_disruption_impact\" in generated_code else \"Not Handled\"\n",
        "    results[\"recovery_orchestration\"] = \"Handled\" if \"delay\" in scenario and \"orchestrate_recovery\" in generated_code else \"Not Handled\"\n",
        "    return results\n",
        "\n",
        "# --- Example Usage ---\n",
        "# 1. Code Generation Evaluation\n",
        "prompt = \"Generate Python code to handle a flight disruption caused by a weather delay, including crew reassignment, passenger rebooking, and flight rescheduling.\"\n",
        "generated_code = evaluate_code_generation(model, tokenizer, prompt)\n",
        "print(\"Generated Code:\\n\", generated_code)\n",
        "\n",
        "# 2. Simulation Evaluation (Simplified)\n",
        "scenario = {\"delay\": True, \"passenger\": True}\n",
        "simulation_results = evaluate_simulation(generated_code, scenario)\n",
        "print(\"Simulation Results:\\n\", simulation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "1a541333ceb340adb186107d89d10650",
            "cf71b89bbabd47cc8672c280490c4762",
            "72cb67dcfcbe4391918c20a4c9378a53",
            "bac3e76a44034e5285246592c1db934c",
            "3de1773a273f412f96fc9887e2c51341",
            "d1fb0bd687324b3a974056aa5f9eb705",
            "123a40a71edf4383824fc369f524e4e0",
            "26d042e2a7664ebbb971751c565785fa",
            "8ed16519f8594c12ac6cb8dfd99efe57",
            "f321c5e6ecbb49c8a9c5939efce7a237",
            "39da16cb7aab4a81a59996de03d92c4a"
          ]
        },
        "id": "6HgT348prh4h",
        "outputId": "62173eb0-65d8-4f22-c4b3-d96d1468db27"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a541333ceb340adb186107d89d10650"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            " Generate Python code to handle a flight disruption caused by a weather delay, including crew reassignment, passenger rebooking, and flight rescheduling.\n",
            "\n",
            "```python\n",
            "# Flight class\n",
            "class Flight:\n",
            "    def __init__(self, flight_number, departure_airport, arrival_airport, departure_time, arrival_time):\n",
            "        self.flight_number = flight_number\n",
            "        self.departure_airport = departure_airport\n",
            "        self.arrival_airport = arrival_airport\n",
            "        self.departure_time = departure_time\n",
            "        self.arrival_time = arrival_time\n",
            "        self.crew = []\n",
            "        self.passengers = []\n",
            "\n",
            "    # Add crew to flight\n",
            "    def add_crew(self, crew_member):\n",
            "        self.crew.append(crew_member)\n",
            "\n",
            "    # Add\n",
            "Simulation Results:\n",
            " {'crew_reassignment': 'Not Handled', 'passenger_rebooking': 'Not Handled', 'flight_rescheduling': 'Not Handled', 'disruption_assessment': 'Not Handled', 'recovery_orchestration': 'Not Handled'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## attention test"
      ],
      "metadata": {
        "id": "vLuEdNytoJHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import xformers\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"xformers version:\", xformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScExQL7ZehOn",
        "outputId": "c9ddfc97-c89f-4e58-b45a-c811e6e123b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "xformers version: 0.0.29.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import xformers.ops as xops\n",
        "\n",
        "# Example input shapes (from your error message)\n",
        "batch_size = 2\n",
        "seq_len = 291\n",
        "num_heads = 8\n",
        "num_groups = 4\n",
        "d_head = 128\n",
        "\n",
        "# Create dummy tensors\n",
        "query = torch.randn(batch_size, seq_len, num_heads, num_groups, d_head, dtype=torch.float16).cuda()\n",
        "key = torch.randn(batch_size, seq_len, num_heads, num_groups, d_head, dtype=torch.float16).cuda()\n",
        "value = torch.randn(batch_size, seq_len, num_heads, num_groups, d_head, dtype=torch.float16).cuda()\n",
        "\n",
        "# Create attention mask\n",
        "attn_bias = xops.fmha.attn_bias.LowerTriangularMask()\n",
        "\n",
        "try:\n",
        "    # Try memory-efficient attention\n",
        "    output = xops.memory_efficient_attention(query, key, value, attn_bias=attn_bias)\n",
        "    print(\"Attention successful!\")\n",
        "    print(\"Output shape:\", output.shape)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia33YUXreu-0",
        "outputId": "4199184f-30eb-4cbe-c3d0-613b333611ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention successful!\n",
            "Output shape: torch.Size([2, 291, 8, 4, 128])\n"
          ]
        }
      ]
    }
  ]
}
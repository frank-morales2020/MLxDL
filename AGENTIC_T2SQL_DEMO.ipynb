{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMrCDJc4TQn7q2hPgaeIwxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/AGENTIC_T2SQL_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model test"
      ],
      "metadata": {
        "id": "mMckf6tBA3XT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "step0"
      ],
      "metadata": {
        "id": "opNbjFd-DQvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "hf_api_key = userdata.get('HF_TOKEN')\n",
        "#print(hf_api_key)\n",
        "\n",
        "# Consolidated Installations and Imports\n",
        "!pip install -U langchain-community -q\n",
        "!pip install -U crewai -q\n",
        "!pip install 'crewai[tools]' -q\n",
        "!pip install transformers -U -q\n",
        "!pip install colab-env -q\n",
        "!pip install unsloth -q\n",
        "!pip install torch -q # Ensure torch is installed\n",
        "\n",
        "from crewai.tools import BaseTool\n"
      ],
      "metadata": {
        "id": "yojaDSSzNL-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step1"
      ],
      "metadata": {
        "id": "5buJuvW5CU0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.language_models import BaseChatModel\n",
        "from typing import Any, List, Dict, Optional\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.outputs import ChatResult, ChatGeneration, Generation # Need Generation for BaseChatModel return type\n",
        "class UnslothCrewAILLM(BaseChatModel):\n",
        "    \"\"\"\n",
        "    Custom Langchain-compatible LLM wrapper for models loaded via Unsloth or Transformers pipeline.\n",
        "    \"\"\"\n",
        "    model: Any # The loaded model object (e.g., from FastLanguageModel)\n",
        "    tokenizer: Any # The loaded tokenizer object\n",
        "    pipeline: Any = None # Optional: the transformers pipeline\n",
        "\n",
        "    # Pass generation parameters during initialization\n",
        "    max_new_tokens: int = 1024\n",
        "    temperature: float = 0.1\n",
        "    do_sample: bool = False\n",
        "    trust_remote_code: bool = True # Keep track if remote code is trusted\n",
        "\n",
        "    def __init__(self, model, tokenizer, pipeline=None, max_new_tokens=1024, temperature=0.1, do_sample=False, trust_remote_code=True):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            pipeline=pipeline,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=do_sample, # Determine do_sample based on temperature\n",
        "            trust_remote_code=trust_remote_code,\n",
        "        )\n",
        "        # Set pad token ID on the tokenizer if it's None globally\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "             self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None, # Typically not needed for simple wrappers\n",
        "        **kwargs: Any, # Langchain/CrewAI might pass additional generation args here\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"\n",
        "        Generates a response from the LLM based on the input messages.\n",
        "        Implements the core generation logic required by BaseChatModel.\n",
        "        \"\"\"\n",
        "        if not messages:\n",
        "             raise ValueError(\"No messages provided to the LLM wrapper.\")\n",
        "\n",
        "        # In CrewAI, the last message content often contains the main prompt from the Task.\n",
        "        # For a text-to-SQL model fine-tuned on a specific prompt format (like the one\n",
        "        # used in the Task description), we need to ensure that format is presented\n",
        "        # to the model. The Task description includes the schema and the query.\n",
        "        # Let's assume the content of the *last* message is the primary input prompt.\n",
        "\n",
        "        final_message_content = messages[-1].content\n",
        "\n",
        "        # Use the pipeline or manual generation based on availability\n",
        "        if self.pipeline:\n",
        "            try:\n",
        "                # Pass generation arguments DIRECTLY to the pipeline call\n",
        "                # Also include stop words if the pipeline supports it (Transformers pipeline does not directly take stop as a list in call)\n",
        "                # Need to handle stop words separately or rely on task description formatting for the model\n",
        "                response = self.pipeline(\n",
        "                    final_message_content,\n",
        "                    num_return_sequences=1,\n",
        "                    return_full_text=False,\n",
        "                    max_new_tokens=self.max_new_tokens, # Use stored or passed value from init\n",
        "                    temperature=self.temperature,     # Use stored or passed value from init\n",
        "                    do_sample=self.do_sample,         # Use stored or passed value from init\n",
        "                    # Add other relevant generation parameters if needed\n",
        "                )\n",
        "                generated_text = response[0].get('generated_text', '').strip() if response else \"\"\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during pipeline generation in wrapper: {e}\")\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "\n",
        "        elif self.model and self.tokenizer:\n",
        "            # Fallback to manual generation if pipeline not available or fails\n",
        "            try:\n",
        "                # Encode the prompt text\n",
        "                inputs = self.tokenizer(final_message_content, return_tensors=\"pt\", truncation=True, max_length=self.tokenizer.model_max_length).to(self.model.device)\n",
        "\n",
        "                # Ensure pad_token_id is set before generation\n",
        "                if self.tokenizer.pad_token_id is None:\n",
        "                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "                # Pass generation arguments DIRECTLY to model.generate call\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=self.max_new_tokens, # Use stored value from init\n",
        "                    temperature=self.temperature,     # Use stored value from init\n",
        "                    do_sample=self.do_sample,         # Use stored value from init\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    stopping_criteria=stop, # Pass stop words from Langchain\n",
        "                    # Add other relevant generation parameters as needed\n",
        "                )\n",
        "                # Decode generated tokens, excluding the input prompt\n",
        "                input_length = inputs.input_ids.shape[1]\n",
        "                generated_ids = outputs[0, input_length:]\n",
        "                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during manual generation in wrapper: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc() # Print traceback for debugging manual gen failures\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "        else:\n",
        "            generated_text = \"Error: Model or pipeline not loaded in wrapper.\"\n",
        "\n",
        "\n",
        "        # Wrap the generated text in a Langchain ChatGeneration object\n",
        "        # The LLM is expected to output the *answer* based on the prompt (which includes the task)\n",
        "        message = AIMessage(content=generated_text)\n",
        "        generation = ChatGeneration(message=message)\n",
        "\n",
        "        # Return a ChatResult containing the generation\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    # Implement other required methods (often just raising NotImplementedError unless needed)\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"unsloth_transformer_wrapper\" # Custom type name\n",
        "\n",
        "    # Async methods are usually required by BaseChatModel, implement if needed\n",
        "    # For simplicity, we can delegate async to sync for this example\n",
        "    # Note: A proper async implementation is better for performance\n",
        "    async def _agenerate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        return self._generate(messages, stop, run_manager, **kwargs)\n"
      ],
      "metadata": {
        "id": "R0q63iNeCCfi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step2"
      ],
      "metadata": {
        "id": "4AKD54qkCZaf"
      }
    },
    {
      "source": [
        "fine_tuned_model_id = \"frankmorales2020/deepseek_r1_text2sql_finetuned\"\n",
        "max_seq_length = 2048\n",
        "load_in_4bit = True # This will be passed to unsloth loading\n",
        "from transformers import pipeline, AutoConfig\n",
        "\n",
        "# db_schema definition remains the same\n",
        "db_schema = {\n",
        "    \"tables\": {\n",
        "        \"products\": ['id', 'name', 'price', 'category'],\n",
        "        \"orders\": ['order_id', 'product_id', 'quantity', 'order_date']\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- Imports for Direct LLM Interaction with Unsloth ---\n",
        "# We need specific imports from unsloth and transformers\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "    # We might still need pipeline from transformers for easy generation after loading\n",
        "    from transformers import pipeline, AutoConfig # Keep AutoConfig for trust_remote_code\n",
        "    import torch\n",
        "    import warnings # Import warnings to suppress potential warnings during loading\n",
        "    print(\"Unsloth, Transformers, and Torch imports successful for direct interaction.\")\n",
        "\n",
        "    # --- Direct LLM Loading and Configuration with Unsloth ---\n",
        "    print(f\"\\n--- Attempting Direct LLM Loading for {fine_tuned_model_id} using Unsloth ---\")\n",
        "\n",
        "    # Reuse configuration parameters defined earlier\n",
        "    # fine_tuned_model_id is already defined\n",
        "    # max_seq_length is already defined\n",
        "    # selected_dtype_str is already defined (Unsloth prefers torch.float16 or torch.bfloat16)\n",
        "    # load_in_4bit is already defined (Unsloth handles quantization)\n",
        "    # llm_for_agents dictionary contains other config like temperature, max_tokens, device_map\n",
        "\n",
        "    # Unsloth recommended dtype (bfloat16 if supported, else float16)\n",
        "    unsloth_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "    # You might still pass load_in_4bit to unsloth.from_pretrained if you want 4-bit loading\n",
        "    # but unsloth handles the quantization implementation.\n",
        "\n",
        "    # Load the model and tokenizer using FastLanguageModel\n",
        "    # Pass trust_remote_code and dtype directly\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name = fine_tuned_model_id, # Use the model ID\n",
        "            max_seq_length = max_seq_length,   # Pass max sequence length\n",
        "            dtype = unsloth_dtype,             # Use unsloth's preferred dtype\n",
        "            load_in_4bit = load_in_4bit,       # Request 4-bit loading via unsloth\n",
        "            # device_map is often handled internally by unsloth or transformers load_in_4bit\n",
        "            # device_map=\"auto\", # You could try adding this if needed, but unsloth's 4-bit often handles it\n",
        "            trust_remote_code=True,            # Needed for Deepseek\n",
        "        )\n",
        "    print(\"Model and Tokenizer loaded successfully using Unsloth.\")\n",
        "\n",
        "    # Optional: Create a pipeline for easier text generation\n",
        "    # Using the model and tokenizer loaded above ensures quantization/dtype are applied\n",
        "    # Note: Pipelines with unsloth models can sometimes be tricky.\n",
        "    # Manual generation using model.generate might be more reliable if pipeline fails.\n",
        "    try:\n",
        "        direct_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            # Pass generation parameters from the config\n",
        "            max_new_tokens=llm_for_agents.get(\"max_tokens\", 1024), # Use max_tokens config, renamed for pipeline\n",
        "            temperature=llm_for_agents.get(\"temperature\", 0.1),\n",
        "            do_sample=True if llm_for_agents.get(\"temperature\", 0.1) > 0 else False, # Enable sampling if temp > 0\n",
        "            # Add other relevant parameters if needed, might require model-specific ones\n",
        "             pad_token_id=tokenizer.eos_token_id, # Often needed for batching, use EOS if PAD is not set\n",
        "        )\n",
        "        print(\"Text generation pipeline created.\")\n",
        "        use_pipeline = True\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create transformers pipeline: {e}. Falling back to manual generation.\")\n",
        "        direct_pipeline = None\n",
        "        use_pipeline = False\n",
        "\n",
        "\n",
        "    # --- Define the Query and Schema ---\n",
        "    query_to_send_directly = \"List all orders made after 2023-01-01.\"\n",
        "    db_schema_string_for_prompt = str(db_schema) # Reuse db_schema defined earlier\n",
        "\n",
        "    unsloth_wrapper_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=1024, # Use a default or variable if needed\n",
        "            temperature=0.1, # Use a default or variable if needed\n",
        "            do_sample=False, # Use a default or variable if needed\n",
        "            # Set pad token ID if tokenizer doesn't have one, needed for batching but also single generation\n",
        "            pad_token_id=tokenizer.eos_token_id, # Safe default if PAD is None\n",
        "            return_full_text=False, # Important for pipeline to not return the input prompt\n",
        "    )\n",
        "\n",
        "    llm_for_agents = UnslothCrewAILLM(\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      pipeline=unsloth_wrapper_pipeline, # Pass the pipeline if created\n",
        "      max_new_tokens=1024, # Matches original max_tokens\n",
        "      temperature=0.1,   # Matches original temperature\n",
        "      trust_remote_code=True, # Matches original setting\n",
        "     )\n",
        "\n",
        "    # --- Construct the Prompt for Direct LLM ---\n",
        "    # This prompt is manually crafted to guide the LLM towards SQL generation\n",
        "    # Adjusting prompt format might be necessary based on the fine-tuned model's training\n",
        "    prompt_for_direct_llm = f\"\"\"Translate the following natural language query into a SQL query based on the provided database schema.\n",
        "\n",
        "    Database Schema:\n",
        "    {db_schema_string_for_prompt}\n",
        "\n",
        "    Natural Language Query:\n",
        "    {query_to_send_directly}\n",
        "\n",
        "    Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
        "\n",
        "    SQL:\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n--- Sending direct prompt to LLM ---\")\n",
        "    print(\"Prompt:\")\n",
        "    print(prompt_for_direct_llm)\n",
        "\n",
        "    # --- Call the Direct LLM (using the pipeline or manual generation) ---\n",
        "    if use_pipeline and direct_pipeline:\n",
        "        try:\n",
        "            direct_llm_response = direct_pipeline(\n",
        "                prompt_for_direct_llm,\n",
        "                num_return_sequences=1,\n",
        "                return_full_text=False, # Important for pipeline to not return the input prompt\n",
        "                # Add other generation parameters if needed\n",
        "            )\n",
        "            if direct_llm_response and isinstance(direct_llm_response, list) and len(direct_llm_response) > 0:\n",
        "                 generated_text = direct_llm_response[0].get('generated_text', '').strip()\n",
        "                 # Further post-processing might be needed depending on exact output format\n",
        "                 final_direct_sql = generated_text.split(';')[0].strip() if ';' in generated_text else generated_text.split('\\n')[0].strip()\n",
        "            else:\n",
        "                 final_direct_sql = \"Generation failed or returned empty.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error during pipeline generation: {e}\")\n",
        "            final_direct_sql = \"Error during pipeline generation.\"\n",
        "    else:\n",
        "        # Manual generation using model.generate\n",
        "        try:\n",
        "            inputs = tokenizer(prompt_for_direct_llm, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                #max_new_tokens=llm_for_agents.get(\"max_tokens\", 1024),\n",
        "                #temperature=llm_for_agents.get(\"temperature\", 0.1),\n",
        "                #do_sample=True if llm_for_agents.get(\"temperature\", 0.1) > 0 else False,\n",
        "\n",
        "\n",
        "                max_new_tokens=llm_for_agents.max_new_tokens,\n",
        "                temperature=llm_for_agents.temperature,\n",
        "                do_sample=llm_for_agents.do_sample, # Or calculate based on llm_for_agents.temperature if that's the logic you want\n",
        "\n",
        "\n",
        "\n",
        "                # Add other generation parameters as needed for model.generate\n",
        "                # eos_token_id=tokenizer.eos_token_id,\n",
        "                # pad_token_id=tokenizer.eos_token_id, # Often helpful\n",
        "            )\n",
        "            # Decode the output, skipping the input tokens\n",
        "            generated_text = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "            # Post-process to try and get just the SQL line\n",
        "            final_direct_sql0 = generated_text.split(';')[0].strip() if ';' in generated_text else generated_text.split('\\n')[0].strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during manual generation: {e}\")\n",
        "            final_direct_sql = \"Error during manual generation.\"\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Direct LLM (Unsloth) Generated SQL ---\")\n",
        "    print(final_direct_sql0)\n",
        "\n",
        "    # With direct interaction, there's no automatic validation or refinement step built-in.\n",
        "    # You would have to manually take this output and potentially:\n",
        "    # 1. Try to execute it against a real database.\n",
        "    # 2. Manually analyze the result or any errors.\n",
        "    # 3. If incorrect, manually formulate a new prompt or correction attempt for the LLM.\n",
        "\n",
        "\n",
        "except ImportError:\n",
        "     print(\"\\n--- Skipping direct LLM interaction example: Unsloth or necessary libraries not installed/configured correctly. ---\")\n",
        "     print(\"Please ensure you have 'unsloth' and 'torch' installed and a compatible GPU/CUDA setup.\")\n",
        "except Exception as e:\n",
        "     print(f\"\\n--- An error occurred during direct LLM interaction (Unsloth): {e} ---\")\n",
        "     import traceback\n",
        "     traceback.print_exc() # Print full traceback for debugging"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i1WCXODZmRvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# --- Test Case Definition 1 ---\n",
        "\n",
        "test_query = \"Find the names and prices of all products in the 'Electronics' category.\"\n",
        "expected_sql = \"SELECT name, price FROM products WHERE category = 'Electronics';\" # Define the expected SQL output\n",
        "\n",
        "print(\"\\n--- Running Test Case for Direct LLM Interaction ---\")\n",
        "print(f\"Natural Language Query: {test_query}\")\n",
        "print(f\"Expected SQL: {expected_sql}\")\n",
        "\n",
        "\n",
        "# --- Reuse existing setup (assuming model and tokenizer are already loaded) ---\n",
        "# This block assumes that the previous cells where libraries were installed,\n",
        "# modules were imported, the UnslothCrewAILLM class was defined,\n",
        "# the model and tokenizer were loaded using Unsloth, and llm_for_agents\n",
        "# and db_schema were successfully created and are available.\n",
        "\n",
        "if 'model' not in locals() or 'tokenizer' not in locals() or 'llm_for_agents' not in locals():\n",
        "    print(\"\\nSkipping test: Model, tokenizer, or llm_for_agents not loaded. Please run the model loading cell(s) first.\")\n",
        "else:\n",
        "    try:\n",
        "        # --- Define the Query and Schema for the test ---\n",
        "        # db_schema is already defined in the previous cell\n",
        "        if 'db_schema' not in locals():\n",
        "            print(\"\\nSkipping test: db_schema not defined. Please ensure the schema definition cell was run.\")\n",
        "        else:\n",
        "            db_schema_string_for_prompt = str(db_schema)\n",
        "\n",
        "            # --- Construct the Prompt for the Test LLM Call ---\n",
        "            # Use the test_query defined above\n",
        "            prompt_for_test_llm = f\"\"\"Translate the following natural language query into a SQL query based on the provided database schema.\n",
        "\n",
        "Database Schema:\n",
        "{db_schema_string_for_prompt}\n",
        "\n",
        "Natural Language Query:\n",
        "{test_query}\n",
        "\n",
        "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
        "\n",
        "SQL:\n",
        "\"\"\"\n",
        "\n",
        "            print(f\"\\n--- Sending test prompt to LLM ---\")\n",
        "            print(\"Prompt:\")\n",
        "            print(prompt_for_test_llm)\n",
        "\n",
        "            # --- Call the Direct LLM (using manual generation as it's more reliable) ---\n",
        "            # We will prefer manual generation for consistent testing\n",
        "            try:\n",
        "                inputs = tokenizer(prompt_for_test_llm, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "                # Ensure pad_token_id is set before generation\n",
        "                if tokenizer.pad_token_id is None:\n",
        "                    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "                # Use generation parameters from the llm_for_agents object\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=llm_for_agents.max_new_tokens, # Use stored value from init\n",
        "                    temperature=llm_for_agents.temperature,     # Use stored value from init\n",
        "                    do_sample=llm_for_agents.do_sample,         # Use stored value from init\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    # Add other relevant generation parameters as needed\n",
        "                    # eos_token_id=tokenizer.eos_token_id, # Might be needed depending on model training\n",
        "                )\n",
        "\n",
        "                # Decode generated tokens, excluding the input prompt\n",
        "                generated_text = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "                # Post-process to try and get just the SQL line\n",
        "                # Reuse the same parsing logic as before\n",
        "                generated_sql = generated_text.split(';')[0].strip() if ';' in generated_text else generated_text.split('\\n')[0].strip()\n",
        "\n",
        "                print(f\"\\n--- LLM Generated SQL for Test Case ---\")\n",
        "                print(generated_sql)\n",
        "\n",
        "                # --- Compare Generated SQL with Expected SQL ---\n",
        "                # Simple comparison - might need more sophisticated comparison\n",
        "                # if whitespace or casing variations are acceptable.\n",
        "                if generated_sql.lower() == expected_sql.lower():\n",
        "                    print(\"\\n**Test Passed: Generated SQL matches expected SQL.**\")\n",
        "                else:\n",
        "                    print(\"\\n**Test Failed: Generated SQL does NOT match expected SQL.**\")\n",
        "                    print(f\"Expected: {expected_sql}\")\n",
        "                    print(f\"Got:      {generated_sql}\")\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n--- An error occurred during LLM generation for the test case: {e} ---\")\n",
        "                import traceback\n",
        "                traceback.print_exc() # Print full traceback for debugging\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- An error occurred during the test setup or prompt creation: {e} ---\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for debugging"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTwd7apHTlWk",
        "outputId": "ac270275-9c6d-4c39-e49a-74be4fa4b228"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Test Case for Direct LLM Interaction ---\n",
            "Natural Language Query: Find the names and prices of all products in the 'Electronics' category.\n",
            "Expected SQL: SELECT name, price FROM products WHERE category = 'Electronics';\n",
            "\n",
            "--- Sending test prompt to LLM ---\n",
            "Prompt:\n",
            "Translate the following natural language query into a SQL query based on the provided database schema.\n",
            "\n",
            "Database Schema:\n",
            "{'tables': {'products': ['id', 'name', 'price', 'category'], 'orders': ['order_id', 'product_id', 'quantity', 'order_date']}}\n",
            "\n",
            "Natural Language Query:\n",
            "Find the names and prices of all products in the 'Electronics' category.\n",
            "\n",
            "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
            "\n",
            "SQL:\n",
            "\n",
            "\n",
            "--- LLM Generated SQL for Test Case ---\n",
            "SELECT name, price FROM products WHERE category = 'Electronics'\n",
            "\n",
            "**Test Failed: Generated SQL does NOT match expected SQL.**\n",
            "Expected: SELECT name, price FROM products WHERE category = 'Electronics';\n",
            "Got:      SELECT name, price FROM products WHERE category = 'Electronics'\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# --- Test Case Definition 2 ---\n",
        "\n",
        "test_query = \"Find the names and prices of all products in the 'Electronics' category.\"\n",
        "# Corrected expected_sql to match the LLM's output format (no trailing semicolon)\n",
        "expected_sql = \"SELECT name, price FROM products WHERE category = 'Electronics'\"\n",
        "\n",
        "print(\"\\n--- Running Test Case for Direct LLM Interaction ---\")\n",
        "print(f\"Natural Language Query: {test_query}\")\n",
        "print(f\"Expected SQL (for comparison): {expected_sql}\") # Adjusted print message\n",
        "\n",
        "# --- Reuse existing setup (assuming model and tokenizer are already loaded) ---\n",
        "# This block assumes that the previous cells where libraries were installed,\n",
        "# modules were imported, the UnslothCrewAILLM class was defined,\n",
        "# the model and tokenizer were loaded using Unsloth, and llm_for_agents\n",
        "# and db_schema were successfully created and are available.\n",
        "\n",
        "if 'model' not in locals() or 'tokenizer' not in locals() or 'llm_for_agents' not in locals():\n",
        "    print(\"\\nSkipping test: Model, tokenizer, or llm_for_agents not loaded. Please run the model loading cell(s) first.\")\n",
        "else:\n",
        "    try:\n",
        "        # --- Define the Query and Schema for the test ---\n",
        "        # db_schema is already defined in the previous cell\n",
        "        if 'db_schema' not in locals():\n",
        "            print(\"\\nSkipping test: db_schema not defined. Please ensure the schema definition cell was run.\")\n",
        "        else:\n",
        "            db_schema_string_for_prompt = str(db_schema)\n",
        "\n",
        "            # --- Construct the Prompt for the Test LLM Call ---\n",
        "            # Use the test_query defined above\n",
        "            prompt_for_test_llm = f\"\"\"Translate the following natural language query into a SQL query based on the provided database schema.\n",
        "\n",
        "Database Schema:\n",
        "{db_schema_string_for_prompt}\n",
        "\n",
        "Natural Language Query:\n",
        "{test_query}\n",
        "\n",
        "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
        "\n",
        "SQL:\n",
        "\"\"\"\n",
        "\n",
        "            print(f\"\\n--- Sending test prompt to LLM ---\")\n",
        "            print(\"Prompt:\")\n",
        "            print(prompt_for_test_llm)\n",
        "\n",
        "            # --- Call the Direct LLM (using manual generation as it's more reliable) ---\n",
        "            # We will prefer manual generation for consistent testing\n",
        "            try:\n",
        "                inputs = tokenizer(prompt_for_test_llm, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "                # Ensure pad_token_id is set before generation\n",
        "                if tokenizer.pad_token_id is None:\n",
        "                    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "                # Use generation parameters from the llm_for_agents object\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=llm_for_agents.max_new_tokens, # Use stored value from init\n",
        "                    temperature=llm_for_agents.temperature,     # Use stored value from init\n",
        "                    do_sample=llm_for_agents.do_sample,         # Use stored value from init\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    # Add other relevant generation parameters as needed\n",
        "                    # eos_token_id=tokenizer.eos_token_id, # Might be needed depending on model training\n",
        "                )\n",
        "\n",
        "                # Decode generated tokens, excluding the input prompt\n",
        "                generated_text = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "                # Post-process to try and get just the SQL line\n",
        "                # Reuse the same parsing logic as before\n",
        "                # This logic correctly handles cases with or without a trailing semicolon\n",
        "                generated_sql = generated_text.split(';')[0].strip() if ';' in generated_text else generated_text.split('\\n')[0].strip()\n",
        "\n",
        "                print(f\"\\n--- LLM Generated SQL for Test Case ---\")\n",
        "                print(generated_sql)\n",
        "\n",
        "                # --- Compare Generated SQL with Expected SQL ---\n",
        "                # The comparison now expects no trailing semicolon in both\n",
        "                if generated_sql.lower() == expected_sql.lower():\n",
        "                    print(\"\\n**Test Passed: Generated SQL matches expected SQL.**\")\n",
        "                else:\n",
        "                    print(\"\\n**Test Failed: Generated SQL does NOT match expected SQL.**\")\n",
        "                    print(f\"Expected: {expected_sql}\")\n",
        "                    print(f\"Got:      {generated_sql}\")\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n--- An error occurred during LLM generation for the test case: {e} ---\")\n",
        "                import traceback\n",
        "                traceback.print_exc() # Print full traceback for debugging\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- An error occurred during the test setup or prompt creation: {e} ---\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for debugging"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRyaeledUrbm",
        "outputId": "3db761a0-fde6-46c3-a047-47aaacc00a39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Test Case for Direct LLM Interaction ---\n",
            "Natural Language Query: Find the names and prices of all products in the 'Electronics' category.\n",
            "Expected SQL (for comparison): SELECT name, price FROM products WHERE category = 'Electronics'\n",
            "\n",
            "--- Sending test prompt to LLM ---\n",
            "Prompt:\n",
            "Translate the following natural language query into a SQL query based on the provided database schema.\n",
            "\n",
            "Database Schema:\n",
            "{'tables': {'products': ['id', 'name', 'price', 'category'], 'orders': ['order_id', 'product_id', 'quantity', 'order_date']}}\n",
            "\n",
            "Natural Language Query:\n",
            "Find the names and prices of all products in the 'Electronics' category.\n",
            "\n",
            "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
            "\n",
            "SQL:\n",
            "\n",
            "\n",
            "--- LLM Generated SQL for Test Case ---\n",
            "SELECT name, price FROM products WHERE category = 'Electronics'\n",
            "\n",
            "**Test Passed: Generated SQL matches expected SQL.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AGENTIC"
      ],
      "metadata": {
        "id": "BPkvNb1Afqk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From the provided reference:\n",
        "# Assume these are already installed as per the notebook:\n",
        "!pip install -U langchain-community -q\n",
        "!pip install -U crewai -q\n",
        "!pip install 'crewai [tools]' -q\n",
        "!pip install transformers -U -q\n",
        "!pip install colab-env -q\n",
        "!pip install unsloth -q\n",
        "!pip install torch -q"
      ],
      "metadata": {
        "id": "PaPwbtBFf2i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "from typing import Any, List, Dict, Optional\n",
        "\n",
        "# Ensure all necessary Langchain/Transformers/Unsloth imports are here\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.outputs import ChatResult, ChatGeneration\n",
        "\n",
        "# Import PromptTemplate and LLMChain for the new approach\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Unsloth and Transformers imports for model loading\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import pipeline, AutoConfig # Make sure AutoConfig is imported\n",
        "\n",
        "# Import BaseTool if you still want to use your tool class structure\n",
        "from langchain.tools import BaseTool\n",
        "\n",
        "# --- 1. Custom LLM Wrapper (UnslothCrewAILLM) ---\n",
        "# This class makes your fine-tuned model compatible with Langchain.\n",
        "# (Keep the same class definition from the last attempt as it's the most compliant)\n",
        "class UnslothCrewAILLM(BaseChatModel):\n",
        "    model: Any\n",
        "    tokenizer: Any\n",
        "    pipeline: Any = None\n",
        "    max_new_tokens: int = 1024\n",
        "    temperature: float = 0.1\n",
        "    do_sample: bool = False\n",
        "    trust_remote_code: bool = True\n",
        "\n",
        "    def __init__(self, model, tokenizer, pipeline=None, max_new_tokens=1024, temperature=0.1, do_sample: bool = False, trust_remote_code=True):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            pipeline=pipeline,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=do_sample,\n",
        "            trust_remote_code=trust_remote_code\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        if not messages:\n",
        "            raise ValueError(\"No messages provided to the LLM wrapper.\")\n",
        "\n",
        "        # Langchain often sends a list of messages, take the last one as the primary prompt\n",
        "        final_message_content = messages[-1].content\n",
        "\n",
        "        if self.pipeline:\n",
        "            try:\n",
        "                response = self.pipeline(\n",
        "                    final_message_content,\n",
        "                    num_return_sequences=1,\n",
        "                    return_full_text=False,\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    temperature=self.temperature,\n",
        "                    do_sample=self.do_sample,\n",
        "                )\n",
        "                generated_text = response[0].get('generated_text', '').strip() if response else ''\n",
        "            except Exception as e:\n",
        "                print(f\"Error during pipeline generation in wrapper: {e}\")\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "        elif self.model and self.tokenizer:\n",
        "            try:\n",
        "                max_input_length = getattr(self.tokenizer, 'model_max_length', self.max_new_tokens)\n",
        "                inputs = self.tokenizer(final_message_content, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to(self.model.device)\n",
        "\n",
        "                if self.tokenizer.pad_token_id is None:\n",
        "                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    temperature=self.temperature,\n",
        "                    do_sample=self.do_sample,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    stopping_criteria=stop,\n",
        "                )\n",
        "                input_length = inputs.input_ids.shape[1]\n",
        "                generated_ids = outputs[0, input_length:]\n",
        "                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "            except Exception as e:\n",
        "                print(f\"Error during manual generation in wrapper: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "        else:\n",
        "            generated_text = \"Error: Model or pipeline not loaded in wrapper.\"\n",
        "\n",
        "        message = AIMessage(content=generated_text)\n",
        "        generation = ChatGeneration(message=message)\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"unsloth_transformer_wrapper\"\n",
        "\n",
        "    def supports_stop_words(self) -> bool:\n",
        "        \"\"\"Returns whether the model supports stop words.\"\"\"\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def supports_control_chars(self) -> bool:\n",
        "        \"\"\"Returns whether the model supports control characters.\"\"\"\n",
        "        return False\n",
        "\n",
        "    # Add dummy implementations for other BaseChatModel methods for compatibility\n",
        "    # Implement stream, invoke, batch methods for better Langchain compatibility\n",
        "    # For this example, we can delegate _invoke to _generate\n",
        "    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement stream method (not used in this wrapper's logic, but required by BaseChatModel).\"\"\"\n",
        "        raise NotImplementedError(\"Streaming is not implemented for this wrapper.\")\n",
        "\n",
        "    def _invoke(self, prompt: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement invoke method (required by BaseChatModel).\"\"\"\n",
        "        # Delegate to generate and return the first message\n",
        "        return self._generate(prompt, stop=stop, run_manager=run_manager, **kwargs).generations[0].message\n",
        "\n",
        "    def _batch(self, messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any) -> List[ChatResult]:\n",
        "         \"\"\"Implement batch method (required by BaseChatModel).\"\"\"\n",
        "         return [self._generate(msgs, stop=stop, run_manager=run_manager, **kwargs) for msgs in messages]\n",
        "\n",
        "    async def _agenerate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        return self._generate(messages, stop, run_manager, **kwargs)\n",
        "\n",
        "    async def _astream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "         \"\"\"Implement async stream method.\"\"\"\n",
        "         raise NotImplementedError(\"Async streaming is not implemented for this wrapper.\")\n",
        "\n",
        "    async def _ainvoke(self, prompt: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "         \"\"\"Implement async invoke method.\"\"\"\n",
        "         return (await self._agenerate(prompt, stop=stop, run_manager=run_manager, **kwargs)).generations[0].message\n",
        "\n",
        "    async def _abatch(self, messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any) -> List[ChatResult]:\n",
        "         \"\"\"Implement async batch method.\"\"\"\n",
        "         import asyncio\n",
        "         return await asyncio.gather(*[self._agenerate(msgs, stop=stop, run_manager=run_manager, **kwargs) for msgs in messages])\n",
        "\n",
        "\n",
        "# --- 2. Database Schema Definition for Flight Planning ---\n",
        "db_schema = {\n",
        "    \"tables\": {\n",
        "        \"flights\": ['flight_id', 'departure_airport', 'arrival_airport', 'departure_time', 'arrival_time', 'aircraft_type', 'status', 'price'],\n",
        "        \"airports\": ['airport_code', 'airport_name', 'city', 'country'],\n",
        "        \"passengers\": ['passenger_id', 'first_name', 'last_name', 'email'],\n",
        "        \"bookings\": ['booking_id', 'flight_id', 'passenger_id', 'booking_date', 'seat_number']\n",
        "    }\n",
        "}\n",
        "db_schema_string_for_prompt = str(db_schema)\n",
        "\n",
        "# --- 3. Model Loading (using the model from the reference) ---\n",
        "fine_tuned_model_id = \"frankmorales2020/deepseek_r1_text2sql_finetuned\"\n",
        "max_seq_length = 2048\n",
        "load_in_4bit = True\n",
        "\n",
        "print(f\"\\n--- Attempting Direct LLM Loading for {fine_tuned_model_id} using Unsloth ---\")\n",
        "\n",
        "# Determine optimal dtype for Unsloth\n",
        "unsloth_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "unsloth_wrapper_pipeline = None\n",
        "llm_instance = None # Renamed from llm_for_agents for clarity in this new approach\n",
        "\n",
        "try:\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=fine_tuned_model_id,\n",
        "            max_seq_length=max_seq_length,\n",
        "            dtype=unsloth_dtype,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    print(\"Model and Tokenizer loaded successfully using Unsloth.\")\n",
        "\n",
        "    try:\n",
        "        # You can still create the pipeline if you prefer, or rely solely on manual generation\n",
        "        unsloth_wrapper_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            return_full_text=False,\n",
        "        )\n",
        "        print(\"Text generation pipeline created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create transformers pipeline: {e}. Falling back to manual generation.\")\n",
        "        unsloth_wrapper_pipeline = None # Ensure pipeline is None if creation fails\n",
        "\n",
        "    # Instantiate your custom LLM\n",
        "    llm_instance = UnslothCrewAILLM(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        pipeline=unsloth_wrapper_pipeline, # Pass the pipeline or None\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.1,\n",
        "        do_sample=False,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    print(\"UnslothCrewAILLM instance created.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"\\n-- Skipping model loading: Unsloth or necessary libraries not installed, or compatible GPU/CUDA setup not found. Error: {e}\")\n",
        "    print(\"Please ensure you have 'unsloth' and 'torch' installed and a compatible GPU/CUDA setup.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- An error occurred during model loading (Unsloth): {e} ---\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- 4. Define the SQL Query Executor Tool (as a Langchain BaseTool) ---\n",
        "# Keep the same tool definition\n",
        "class SQLQueryExecutorTool(BaseTool):\n",
        "    name: str = \"SQL Query Executor\"\n",
        "    description: str = \"Executes a given SQL query against the flight database and returns the results or errors.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        print(f\"\\n--- Attempting to execute SQL query: ---\\n{query}\\n--------------------------------------\")\n",
        "        # Simple validation/simulation\n",
        "        if \"DROP TABLE\" in query.upper() or \"DELETE FROM\" in query.upper():\n",
        "            return \"Error: Harmful SQL query detected and blocked for safety.\"\n",
        "        # Add a check for the specific flight query pattern\n",
        "        if \"SELECT\" in query.upper() and \"FROM flights\" in query.lower() and \"'JFK'\" in query and \"'LAX'\" in query and \"2025-07-01\" in query:\n",
        "             return \"SQL executed successfully. Sample results for flight query: [{'flight_id': 101, 'departure_airport': 'JFK', 'arrival_airport': 'LAX', 'price': 450.00}, {'flight_id': 105, 'departure_airport': 'JFK', 'arrival_airport': 'LAX', 'price': 520.00}]\"\n",
        "        elif \"category = 'Electronics'\" in query: # Keep old simulated results if needed for other tests\n",
        "            return \"SQL executed successfully. Sample results: [{'name': 'Laptop', 'price': 1200}, {'name': 'Smartphone', 'price': 800}]\"\n",
        "        elif \"orders made after 2023-01-01\" in query: # Keep old simulated results if needed for other tests\n",
        "            return \"SQL executed successfully. Sample results: [{'order_id': 1, 'order_date': '2023-02-15'}, {'order_id': 2, 'order_date': '2024-01-20'}]\"\n",
        "        elif not query.strip().lower().startswith(\"select\"):\n",
        "             return \"Error: Only SELECT queries are supported by this tool for safety and simplicity in this demo.\"\n",
        "        else:\n",
        "            if \"SELECT\" in query.upper() and \"FROM\" in query.upper():\n",
        "                return \"SQL executed successfully. (Simulated) No specific results available for this general query.\"\n",
        "            else:\n",
        "                return \"Error: Invalid or unexecutable SQL query format (simulated error).\"\n",
        "\n",
        "sql_executor_tool = SQLQueryExecutorTool()\n",
        "print(\"\\nSQL Query Executor Tool defined.\")\n",
        "\n",
        "\n",
        "# --- 5. Define the Prompt Template for SQL Generation ---\n",
        "sql_gen_template = \"\"\"Translate the following natural language query into a precise SQL query based on the provided database schema.\n",
        "\n",
        "Database Schema:\n",
        "{db_schema}\n",
        "\n",
        "Natural Language Query:\n",
        "{query}\n",
        "\n",
        "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
        "\n",
        "SQL:\n",
        "\"\"\"\n",
        "\n",
        "sql_gen_prompt = PromptTemplate(\n",
        "    input_variables=[\"db_schema\", \"query\"],\n",
        "    template=sql_gen_template,\n",
        ")\n",
        "print(\"\\nSQL Generation Prompt Template defined.\")\n",
        "\n",
        "# --- 6. Create the LLM Chain for SQL Generation ---\n",
        "\n",
        "if llm_instance is None:\n",
        "     print(\"\\nERROR: LLM instance is NOT available. Cannot create LLM Chain.\")\n",
        "else:\n",
        "    try:\n",
        "        sql_gen_chain = LLMChain(\n",
        "            llm=llm_instance,\n",
        "            prompt=sql_gen_prompt,\n",
        "            verbose=True, # Set verbose to True to see the prompt sent to the LLM\n",
        "        )\n",
        "        print(\"\\nLLMChain for SQL generation created.\")\n",
        "\n",
        "        # --- 7. Define the Natural Language Query ---\n",
        "        flight_query = \"Find all flights departing from 'JFK' to 'LAX' after 2025-07-01 and their prices.\"\n",
        "\n",
        "        print(f\"\\n--- Running Langchain Flow for query: \\\"{flight_query}\\\" ---\")\n",
        "\n",
        "        # --- 8. Run the LLMChain to generate SQL ---\n",
        "        # The LLMChain will take the prompt template, format it with inputs,\n",
        "        # and pass the resulting messages to the llm_instance._generate method.\n",
        "        print(\"\\n--- Generating SQL using LLMChain ---\")\n",
        "        generated_sql_result = sql_gen_chain.run(db_schema=db_schema_string_for_prompt, query=flight_query)\n",
        "\n",
        "        # The output from LLMChain.run() is typically the generated text\n",
        "        generated_sql = generated_sql_result.strip()\n",
        "\n",
        "        # Post-process to try and get just the SQL line (reuse parsing logic)\n",
        "        final_generated_sql = generated_sql.split(';')[0].strip() if ';' in generated_sql else generated_sql.split('\\n')[0].strip()\n",
        "\n",
        "\n",
        "        print(f\"\\n--- Generated SQL: ---\")\n",
        "        print(final_generated_sql)\n",
        "\n",
        "        # --- 9. Manually execute the generated SQL using the Tool ---\n",
        "        print(\"\\n--- Executing Generated SQL using Tool ---\")\n",
        "        tool_execution_result = sql_executor_tool.run(final_generated_sql)\n",
        "\n",
        "        print(f\"\\n--- Tool Execution Result: ---\")\n",
        "        print(tool_execution_result)\n",
        "\n",
        "        print(\"\\n### Langchain Flow Finished ###\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- An error occurred during the Langchain flow: {e} ---\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWZI_2l10qqB",
        "outputId": "39f53144-dd4f-45cf-e41b-bddb54d88eeb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Attempting Direct LLM Loading for frankmorales2020/deepseek_r1_text2sql_finetuned using Unsloth ---\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "<ipython-input-4-3502116169>:289: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  sql_gen_chain = LLMChain(\n",
            "<ipython-input-4-3502116169>:305: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  generated_sql_result = sql_gen_chain.run(db_schema=db_schema_string_for_prompt, query=flight_query)\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and Tokenizer loaded successfully using Unsloth.\n",
            "Text generation pipeline created.\n",
            "UnslothCrewAILLM instance created.\n",
            "\n",
            "SQL Query Executor Tool defined.\n",
            "\n",
            "SQL Generation Prompt Template defined.\n",
            "\n",
            "LLMChain for SQL generation created.\n",
            "\n",
            "--- Running Langchain Flow for query: \"Find all flights departing from 'JFK' to 'LAX' after 2025-07-01 and their prices.\" ---\n",
            "\n",
            "--- Generating SQL using LLMChain ---\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mTranslate the following natural language query into a precise SQL query based on the provided database schema.\n",
            "\n",
            "Database Schema:\n",
            "{'tables': {'flights': ['flight_id', 'departure_airport', 'arrival_airport', 'departure_time', 'arrival_time', 'aircraft_type', 'status', 'price'], 'airports': ['airport_code', 'airport_name', 'city', 'country'], 'passengers': ['passenger_id', 'first_name', 'last_name', 'email'], 'bookings': ['booking_id', 'flight_id', 'passenger_id', 'booking_date', 'seat_number']}}\n",
            "\n",
            "Natural Language Query:\n",
            "Find all flights departing from 'JFK' to 'LAX' after 2025-07-01 and their prices.\n",
            "\n",
            "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
            "\n",
            "SQL:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "--- Generated SQL: ---\n",
            "SELECT flight_id, departure_airport, arrival_airport, departure_time, arrival_time, aircraft_type, status, price\n",
            "FROM flights\n",
            "WHERE departure_airport = 'JFK' AND arrival_airport = 'LAX' AND departure_time > '2025-07-01'\n",
            "ORDER BY departure_time\n",
            "\n",
            "--- Executing Generated SQL using Tool ---\n",
            "\n",
            "--- Attempting to execute SQL query: ---\n",
            "SELECT flight_id, departure_airport, arrival_airport, departure_time, arrival_time, aircraft_type, status, price\n",
            "FROM flights\n",
            "WHERE departure_airport = 'JFK' AND arrival_airport = 'LAX' AND departure_time > '2025-07-01'\n",
            "ORDER BY departure_time\n",
            "--------------------------------------\n",
            "\n",
            "--- Tool Execution Result: ---\n",
            "SQL executed successfully. (Simulated) No specific results available for this general query.\n",
            "\n",
            "### Langchain Flow Finished ###\n"
          ]
        }
      ]
    }
  ]
}
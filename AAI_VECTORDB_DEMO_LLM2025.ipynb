{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCfmiuBAzBOgHApgnO3SS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/AAI_VECTORDB_DEMO_LLM2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu google-generativeai numpy -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3f11VXO9EL-",
        "outputId": "66ea8fac-9a1b-4045-a765-476092c6d319"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TLvucQAf861X",
        "outputId": "b7243e4a-3654-4251-fa33-49e626a6a573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Gemini API configured with model: gemini-1.5-flash\n",
            "\n",
            "Generating embeddings for the knowledge base...\n",
            "Embeddings have dimension: 768\n",
            "FAISS index created with 12 documents.\n",
            "\n",
            "--- Agent receives query: 'What is the capital of France?' ---\n",
            "Agent decision: SEARCH\n",
            "\n",
            "--- Agent uses 'SEARCH' tool ---\n",
            "  - Retrieved document 1: The capital of France is Paris.\n",
            "  - Retrieved document 2: The Eiffel Tower is located in Paris.\n",
            "  - Retrieved document 3: Mars is known as the Red Planet.\n",
            "\n",
            "--- Final Answer ---\n",
            "Paris\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "--- Agent receives query: 'Tell me about my pets.' ---\n",
            "Agent decision: SEARCH\n",
            "\n",
            "--- Agent uses 'SEARCH' tool ---\n",
            "  - Retrieved document 1: I love spending time with my pets.\n",
            "  - Retrieved document 2: I have a furry golden retriever.\n",
            "  - Retrieved document 3: A dog is a man's best friend.\n",
            "\n",
            "--- Final Answer ---\n",
            "Based on the provided text, you have a furry golden retriever.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "--- Agent receives query: 'What is the largest planet in our solar system?' ---\n",
            "Agent decision: ANSWER\n",
            "\n",
            "--- Agent uses 'ANSWER' tool ---\n",
            "\n",
            "--- Final Answer ---\n",
            "Jupiter is the largest planet in our solar system.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "--- Agent receives query: 'Write a short poem about the sun.' ---\n",
            "Agent decision: ANSWER\n",
            "\n",
            "--- Agent uses 'ANSWER' tool ---\n",
            "\n",
            "--- Final Answer ---\n",
            "A molten heart, a fiery eye,\n",
            "That paints the dawn across the sky.\n",
            "With golden rays, it wakes the day,\n",
            "Then gently fades, and slips away.\n",
            "A burning sphere, so far, so bright,\n",
            "It bathes the world in warming light.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- 1. Your provided Google Gemini API Configuration ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-1.5-flash\"\n",
        "    EMBEDDING_MODEL_NAME: str = \"models/text-embedding-004\"\n",
        "\n",
        "GOOGLE_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Not running in Google Colab or 'GEMINI' secret not found. Attempting to get 'GEMINI' environment variable.\")\n",
        "    GOOGLE_API_KEY = os.getenv('GEMINI')\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    print(f\"Gemini API configured with model: {AgentConfig.LLM_MODEL_NAME}\")\n",
        "else:\n",
        "    print(\"Warning: GOOGLE_API_KEY not found. LLM calls will not work.\")\n",
        "    print(\"Please set your 'GEMINI' environment variable or Colab secret.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Vector Database Setup with Gemini Embeddings ---\n",
        "def get_embeddings(texts):\n",
        "    try:\n",
        "        response = genai.embed_content(\n",
        "            model=AgentConfig.EMBEDDING_MODEL_NAME,\n",
        "            content=texts,\n",
        "            task_type=\"retrieval_document\"\n",
        "        )\n",
        "        return np.array(response['embedding'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating embeddings: {e}\")\n",
        "        return None\n",
        "\n",
        "documents = [\n",
        "    \"The sky is a brilliant blue today.\",\n",
        "    \"A dog is a man's best friend.\",\n",
        "    \"The weather is sunny and warm.\",\n",
        "    \"I have a furry golden retriever.\",\n",
        "    \"The latest iPhone has a great camera.\",\n",
        "    \"I love spending time with my pets.\",\n",
        "    \"This new smartphone has an impressive screen.\",\n",
        "    \"The sun is shining brightly.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"Mars is known as the Red Planet.\",\n",
        "    \"Water is a chemical compound with the formula H2O.\"\n",
        "]\n",
        "\n",
        "print(\"\\nGenerating embeddings for the knowledge base...\")\n",
        "document_embeddings = get_embeddings(documents)\n",
        "\n",
        "if document_embeddings is None:\n",
        "    print(\"Failed to create embeddings. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "d = document_embeddings.shape[1]\n",
        "print(f\"Embeddings have dimension: {d}\")\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(document_embeddings)\n",
        "print(f\"FAISS index created with {index.ntotal} documents.\")\n",
        "\n",
        "\n",
        "# --- 3. RAG and Tool-Use Functions ---\n",
        "def get_relevant_documents(query, index, k=3):\n",
        "    \"\"\"\n",
        "    A \"tool\" function for the agent to retrieve relevant documents from the vector DB.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query_embedding_response = genai.embed_content(\n",
        "            model=AgentConfig.EMBEDDING_MODEL_NAME,\n",
        "            content=[query],\n",
        "            task_type=\"retrieval_query\"\n",
        "        )\n",
        "        query_embedding = np.array(query_embedding_response['embedding'])\n",
        "        distances, indices = index.search(query_embedding, k)\n",
        "        relevant_docs = [documents[i] for i in indices[0]]\n",
        "        return relevant_docs\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during retrieval: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_direct_answer(query):\n",
        "    \"\"\"\n",
        "    A \"tool\" function for the agent to get a direct answer from the LLM.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "        response = model.generate_content(query)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# --- 4. Agentic Pipeline ---\n",
        "def agentic_rag_pipeline(query):\n",
        "    \"\"\"\n",
        "    The agentic pipeline that first plans and then executes.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Agent receives query: '{query}' ---\")\n",
        "\n",
        "    # Step 1: Agentic Planning\n",
        "    # The agent uses a prompt to decide which action to take.\n",
        "    planner_prompt = f\"\"\"\n",
        "    You are an intelligent agent that decides whether to perform a search or answer a question directly.\n",
        "    You have access to a small knowledge base via a search tool.\n",
        "\n",
        "    If the question is about specific facts that might be in the knowledge base, choose 'SEARCH'.\n",
        "    The knowledge base contains information about:\n",
        "    - dogs and pets\n",
        "    - the weather\n",
        "    - specific phones\n",
        "    - facts about Paris and France\n",
        "    - basic chemistry (H2O)\n",
        "    - planets (Mars)\n",
        "\n",
        "    If the question is general knowledge, creative, or conversational, choose 'ANSWER'.\n",
        "    For example, 'write a poem' or 'who is the president of the US?'.\n",
        "\n",
        "    Your response must be a single word: either 'SEARCH' or 'ANSWER'.\n",
        "\n",
        "    Question: {query}\n",
        "    Choice:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "        planning_response = model.generate_content(planner_prompt)\n",
        "        decision = planning_response.text.strip().upper()\n",
        "    except Exception as e:\n",
        "        print(f\"Agent planning failed: {e}. Defaulting to SEARCH.\")\n",
        "        decision = \"SEARCH\"\n",
        "\n",
        "    print(f\"Agent decision: {decision}\")\n",
        "\n",
        "    # Step 2: Tool Execution based on the decision\n",
        "    if decision == 'SEARCH':\n",
        "        # Execute the retrieval tool (vector DB)\n",
        "        relevant_docs = get_relevant_documents(query, index=index)\n",
        "\n",
        "        print(\"\\n--- Agent uses 'SEARCH' tool ---\")\n",
        "        for i, doc in enumerate(relevant_docs):\n",
        "            print(f\"  - Retrieved document {i+1}: {doc}\")\n",
        "\n",
        "        # Step 3: Construct a RAG prompt and get the final answer\n",
        "        context = \"\\n\".join(relevant_docs)\n",
        "        if not context:\n",
        "            context = \"No relevant information was found in the knowledge base.\"\n",
        "\n",
        "        rag_prompt = f\"\"\"\n",
        "        You are a helpful assistant. Use the following context to answer the question.\n",
        "        If the answer is not in the context, state that you don't know based on the provided information.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        final_answer = model.generate_content(rag_prompt).text\n",
        "\n",
        "    elif decision == 'ANSWER':\n",
        "        # Execute the direct answer tool (LLM without context)\n",
        "        print(\"\\n--- Agent uses 'ANSWER' tool ---\")\n",
        "        final_answer = get_direct_answer(query)\n",
        "\n",
        "    else:\n",
        "        # Fallback for unexpected decisions\n",
        "        print(\"\\n--- Agent had an unexpected decision, defaulting to direct answer. ---\")\n",
        "        final_answer = get_direct_answer(query)\n",
        "\n",
        "    print(\"\\n--- Final Answer ---\")\n",
        "    print(final_answer)\n",
        "\n",
        "\n",
        "# --- 5. Demo the Agentic Pipeline ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Example 1: A specific question that should trigger 'SEARCH'\n",
        "    user_question_1 = \"What is the capital of France?\"\n",
        "    agentic_rag_pipeline(user_question_1)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Example 2: A question that should trigger 'SEARCH'\n",
        "    user_question_2 = \"Tell me about my pets.\"\n",
        "    agentic_rag_pipeline(user_question_2)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Example 3: A question that is general knowledge and should trigger 'ANSWER'\n",
        "    user_question_3 = \"What is the largest planet in our solar system?\"\n",
        "    agentic_rag_pipeline(user_question_3)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Example 4: A creative question that should trigger 'ANSWER'\n",
        "    user_question_4 = \"Write a short poem about the sun.\"\n",
        "    agentic_rag_pipeline(user_question_4)"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO9NooUgvLcyQZL7N0J1g99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/BERT_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n"
      ],
      "metadata": {
        "id": "z6CxhbFfPooG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/bert_repo/requirements.txt -q\n",
        "!pip install -U protobuf -q"
      ],
      "metadata": {
        "id": "1eEAfh4lP_QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf; print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSjO3bMtQJEI",
        "outputId": "90b363ff-f4b3-467c-9935-40a91eb004be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q -U\n",
        "!pip install datasets -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZofGG2cQ9C7",
        "outputId": "35fdd64c-a6b2-43b9-b283-e714fe400415"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: git+https://github.com/google-research/bert.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.5/213.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.0/349.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined JAX BERT Fine-tuning and Prediction Code\n",
        "\n",
        "# --- Imports ---\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import flax.linen as nn\n",
        "from functools import partial\n",
        "import os\n",
        "from transformers import FlaxBertForSequenceClassification, AutoTokenizer, AutoConfig # Import necessary transformers components\n",
        "from datasets import load_dataset # Import datasets library\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure JAX is using the TPU\n",
        "print(\"JAX devices:\", jax.devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY_dGu0uQ0RQ",
        "outputId": "c7da2504-aa0f-456c-bb1d-1bf51099f185"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQHutL6gPgu7"
      },
      "outputs": [],
      "source": [
        "# Combined JAX BERT Fine-tuning and Prediction Code\n",
        "\n",
        "# --- Parameters (Ensure these are set from previous cells or define here) ---\n",
        "# These variables should ideally be defined in previous cells and available in the environment.\n",
        "# If not, you might need to manually set them here based on your configuration.\n",
        "# Example placeholders (replace with actual values if necessary):\n",
        "BERT_MODEL_HUB = \"google/bert-base-uncased\" # Using huggingface model name for transformers\n",
        "TASK = \"mrpc\" # Lowercase task name for datasets library # Corrected task name to lowercase\n",
        "MAX_SEQ_LENGTH = 128\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "SAVE_SUMMARY_STEPS = 500 # For logging frequency\n",
        "\n",
        "# Assuming these variables are defined from earlier in the notebook:\n",
        "# Check if variables exist and print their values if they do, otherwise use defaults or print a message\n",
        "# This helps in debugging if previous cells weren't run\n",
        "task_name_to_print = TASK if 'TASK' in locals() else \"N/A (TASK not defined)\"\n",
        "max_seq_length_to_print = MAX_SEQ_LENGTH if 'MAX_SEQ_LENGTH' in locals() else \"N/A (MAX_SEQ_LENGTH not defined)\"\n",
        "learning_rate_to_print = LEARNING_RATE if 'LEARNING_RATE' in locals() else \"N/A (LEARNING_RATE not defined)\"\n",
        "train_batch_size_to_print = TRAIN_BATCH_SIZE if 'TRAIN_BATCH_SIZE' in locals() else \"N/A (TRAIN_BATCH_SIZE not defined)\"\n",
        "eval_batch_size_to_print = EVAL_BATCH_SIZE if 'EVAL_BATCH_SIZE' in locals() else \"N/A (EVAL_BATCH_SIZE not defined)\"\n",
        "predict_batch_size_to_print = PREDICT_BATCH_SIZE if 'PREDICT_BATCH_SIZE' in locals() else \"N/A (PREDICT_BATCH_SIZE not defined)\"\n",
        "num_train_epochs_to_print = NUM_TRAIN_EPOCHS if 'NUM_TRAIN_EPOCHS' in locals() else \"N/A (NUM_TRAIN_EPOCHS not defined)\"\n",
        "\n",
        "\n",
        "print(f\"Task: {task_name_to_print}, Max Seq Length: {max_seq_length_to_print}, Learning Rate: {learning_rate_to_print}\")\n",
        "print(f\"Train Batch Size: {train_batch_size_to_print}, Eval Batch Size: {eval_batch_size_to_print}, Predict Batch Size: {predict_batch_size_to_print}\")\n",
        "print(f\"Num Train Epochs: {num_train_epochs_to_print}\")\n",
        "\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "# Use the datasets library to load the GLUE MRPC dataset\n",
        "try:\n",
        "    print(\"Loading dataset...\")\n",
        "    raw_datasets = load_dataset(\"glue\", TASK) # Use the TASK variable (now lowercase)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load dataset: {e}\")\n",
        "    # Fallback or error handling if dataset loading fails\n",
        "\n",
        "# Load the tokenizer\n",
        "# Use AutoTokenizer to automatically load the correct tokenizer for the BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # Using bert-base-uncased as in the original notebook\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the sentences\n",
        "    # Use truncation=True to truncate sequences longer than MAX_SEQ_LENGTH\n",
        "    # Use padding='max_length' to pad sequences shorter than MAX_SEQ_LENGTH\n",
        "    # Use return_tensors='np' for NumPy arrays or 'tf' for TensorFlow tensors\n",
        "    # We will return as 'np' tensors since the dataset will be converted to tf.data.Dataset later\n",
        "    return tokenizer(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        return_token_type_ids=True, # Include token_type_ids\n",
        "        return_attention_mask=True, # Include attention_mask\n",
        "        return_tensors='np' # Return NumPy arrays instead of TensorFlow tensors\n",
        "    )\n",
        "\n",
        "# Apply the preprocessing function to the entire dataset\n",
        "# Check if raw_datasets is defined before mapping\n",
        "if 'raw_datasets' in locals():\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = raw_datasets.map(\n",
        "        preprocess_function,\n",
        "        batched=True # Process in batches for efficiency\n",
        "    )\n",
        "    print(\"Dataset tokenized.\")\n",
        "\n",
        "    # Rename the 'label' column to 'labels' to match the model's expected input\n",
        "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "    # Remove original text columns if no longer needed to save memory\n",
        "    tokenized_dataset = tokenized_dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "\n",
        "    # Convert the tokenized datasets to TensorFlow datasets\n",
        "    # This is useful for easy batching and shuffling\n",
        "    train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
        "        columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
        "        shuffle=True, # Shuffle training data\n",
        "        batch_size=TRAIN_BATCH_SIZE,\n",
        "        collate_fn=None, # Use default collate function\n",
        "        drop_remainder=True # Drop remainder for fixed batch size in pmap\n",
        "    )\n",
        "\n",
        "    eval_dataset = tokenized_dataset[\"validation\"].to_tf_dataset(\n",
        "        columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
        "        shuffle=False, # No shuffling for evaluation\n",
        "        batch_size=EVAL_BATCH_SIZE,\n",
        "        collate_fn=None,\n",
        "        drop_remainder=True # Drop remainder for fixed batch size in pmap\n",
        "    )\n",
        "\n",
        "    # Determine number of training and warmup steps\n",
        "    # num_train_examples is needed to calculate num_train_steps\n",
        "    # Use the size of the tokenized training dataset\n",
        "    num_train_examples = len(tokenized_dataset[\"train\"])\n",
        "    num_train_steps = int(num_train_examples / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "    WARMUP_PROPORTION = 0.1 # Assuming this value from original notebook\n",
        "    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "    print(f\"Number of training examples: {num_train_examples}\")\n",
        "    print(f\"Number of training steps: {num_train_steps}\")\n",
        "    print(f\"Number of warmup steps: {num_warmup_steps}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping tokenization and dataset conversion due to dataset loading failure.\")\n",
        "\n",
        "\n",
        "# --- Model Loading ---\n",
        "# Load a pre-trained bert-base-uncased model optimized for sequence classification using Flax\n",
        "# This requires a compatible version of transformers, jax, and jaxlib\n",
        "print(\"Loading pre-trained BERT model...\")\n",
        "# Ensure num_labels is defined - it should be 2 for MRPC\n",
        "if 'raw_datasets' in locals() and raw_datasets is not None and 'label' in raw_datasets['train'].features:\n",
        "     # Infer num_labels from the dataset features if dataset loaded successfully\n",
        "     label_list = raw_datasets['train'].features['label'].names\n",
        "     num_labels = len(label_list)\n",
        "     print(f\"Setting num_labels = {num_labels} from dataset features.\")\n",
        "elif 'num_labels' not in locals():\n",
        "     # Default to 2 for MRPC if dataset loading failed and num_labels not defined\n",
        "     num_labels = 2\n",
        "     print(f\"Assuming num_labels = {num_labels} for MRPC task (dataset loading failed).\")\n",
        "\n",
        "\n",
        "try:\n",
        "    # Check if num_labels was successfully determined\n",
        "    if 'num_labels' in locals():\n",
        "        model = FlaxBertForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased',\n",
        "            num_labels=num_labels,\n",
        "            dtype=jnp.float32 # Specify dtype for JAX/TPU compatibility\n",
        "        )\n",
        "        params = model.params # Access parameters from the model object\n",
        "        print(\"Pre-trained BERT model and parameters loaded successfully.\")\n",
        "    else:\n",
        "        print(\"Cannot load model: num_labels could not be determined.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Failed to load model due to library compatibility issue: {e}\")\n",
        "    print(\"Please ensure you have compatible versions of jax, jaxlib, and transformers installed.\")\n",
        "    # You might need to install specific versions, e.g., !pip install jax==0.4.23 jaxlib==0.4.23 transformers==4.30.0 -q\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during model loading: {e}\")\n",
        "    # Handle other potential errors during model loading\n",
        "\n",
        "# --- Optimizer and Train State ---\n",
        "# Define the learning rate schedule and optimizer\n",
        "# Using optax.join_schedules for linear warmup and linear decay\n",
        "print(\"Setting up optimizer...\")\n",
        "\n",
        "# Check if num_train_steps and num_warmup_steps are defined from Data Loading step\n",
        "if 'num_train_steps' in locals() and 'num_warmup_steps' in locals():\n",
        "    # Define linear warmup schedule\n",
        "    warmup_schedule = optax.linear_schedule(\n",
        "        init_value=0.,\n",
        "        end_value=LEARNING_RATE,\n",
        "        transition_steps=num_warmup_steps\n",
        "    )\n",
        "\n",
        "    # Define linear decay schedule (starts from LEARNING_RATE and decays to 0)\n",
        "    # The decay happens over the remaining steps after warmup\n",
        "    decay_steps_calc = num_train_steps - num_warmup_steps\n",
        "    decay_schedule = optax.linear_schedule(\n",
        "        init_value=LEARNING_RATE,\n",
        "        end_value=0.,\n",
        "        transition_steps=decay_steps_calc if decay_steps_calc > 0 else 1 # Ensure transition_steps is at least 1\n",
        "    )\n",
        "\n",
        "    # Join the two schedules\n",
        "    # The schedule switches from warmup to decay after num_warmup_steps\n",
        "    learning_rate_schedule = optax.join_schedules(\n",
        "        schedules=[warmup_schedule, decay_schedule],\n",
        "        boundaries=[num_warmup_steps]\n",
        "    )\n",
        "\n",
        "\n",
        "    # AdamW is a common optimizer used with BERT\n",
        "    optimizer = optax.adamw(\n",
        "        learning_rate=learning_rate_schedule,\n",
        "        weight_decay=0.01 # Common weight decay for BERT\n",
        "    )\n",
        "\n",
        "    # Create the training state\n",
        "    # This will hold the model parameters, optimizer state, and a PRNG key\n",
        "    # The model and params should be loaded successfully in the 'Model Loading' section.\n",
        "    # If model or params are not defined, the model loading failed.\n",
        "    if 'model' in locals() and 'params' in locals():\n",
        "        # We need a PRNG key for parameter initialization (although using pretrained params here)\n",
        "        # and for dropout.\n",
        "        key = jax.random.PRNGKey(0)\n",
        "        # Use a consistent key for dropout across training steps, folding in the step number\n",
        "        dropout_key = jax.random.fold_in(key, 0) # Initial dropout key\n",
        "\n",
        "        state = train_state.TrainState.create(\n",
        "            apply_fn=model.__call__,\n",
        "            params=params, # Use the loaded pretrained parameters\n",
        "            tx=optimizer\n",
        "        )\n",
        "\n",
        "        # Replicate the training state for pmap\n",
        "        state = jax.device_put_replicated(state, jax.local_devices())\n",
        "\n",
        "        print(\"Optimizer and training state created and replicated.\")\n",
        "    else:\n",
        "        print(\"Model or parameters not loaded, cannot create optimizer and train state.\")\n",
        "else:\n",
        "    print(\"Skipping optimizer and train state setup: num_train_steps or num_warmup_steps not defined.\")\n",
        "\n",
        "\n",
        "# --- JAX Training Step Function ---\n",
        "# Define the loss function\n",
        "def compute_loss(logits, labels):\n",
        "    # Use sparse_softmax_cross_entropy as labels are integers\n",
        "    # Ensure one_hot encoding matches the number of classes (num_labels)\n",
        "    # Check if num_labels is defined before using it\n",
        "    if 'num_labels' in locals():\n",
        "        loss = optax.softmax_cross_entropy(logits, jax.nn.one_hot(labels, num_classes=num_labels))\n",
        "        return jnp.mean(loss)\n",
        "    else:\n",
        "        print(\"Cannot compute loss: num_labels not defined.\")\n",
        "        return jnp.nan # Return NaN or handle error appropriately\n",
        "\n",
        "# Define the training step function\n",
        "# This function will compute the loss and gradients and update the model parameters.\n",
        "@partial(jax.jit) # Removed static_argnums\n",
        "def train_step(state, batch, dropout_key):\n",
        "    # Split dropout key for each step\n",
        "    new_dropout_key = jax.random.fold_in(dropout_key, state.step)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Model forward pass\n",
        "        # Check if model and num_labels are defined before applying\n",
        "        if 'model' in locals() and 'num_labels' in locals():\n",
        "            logits = state.apply_fn(\n",
        "                batch['input_ids'],\n",
        "                batch['attention_mask'],\n",
        "                batch['token_type_ids'],\n",
        "                params=params,\n",
        "                deterministic=False, # Enable dropout during training\n",
        "                rngs={'dropout': new_dropout_key} # Pass dropout key\n",
        "            )\n",
        "            loss = compute_loss(logits, batch['labels'])\n",
        "            return loss, logits # Return logits along with the loss\n",
        "        else:\n",
        "            print(\"Cannot compute loss in train_step: model or num_labels not defined.\")\n",
        "            return jnp.nan, None\n",
        "\n",
        "    # Check if state and loss_fn are defined before computing gradients\n",
        "    if 'state' in locals() and 'loss_fn' in locals():\n",
        "        # Compute gradients\n",
        "        grad_fn = jax.grad(lambda params: loss_fn(params)[0]) # Compute gradient only w.r.t. loss\n",
        "        grads = grad_fn(state.params)\n",
        "\n",
        "        # Get loss and logits from loss_fn call for accuracy calculation\n",
        "        loss, logits = loss_fn(state.params)\n",
        "\n",
        "        # Update parameters\n",
        "        new_state = state.apply_gradients(grads=grads)\n",
        "\n",
        "        # Compute training accuracy (optional, for monitoring)\n",
        "        # Check if num_labels is defined and logits is not None\n",
        "        if 'num_labels' in locals() and logits is not None:\n",
        "            predicted_labels = jnp.argmax(logits, axis=-1)\n",
        "            accuracy = jnp.mean(predicted_labels == batch['labels'])\n",
        "        else:\n",
        "            print(\"Cannot compute accuracy in train_step: num_labels not defined.\")\n",
        "            accuracy = jnp.nan\n",
        "\n",
        "        return new_state, loss, accuracy, new_dropout_key\n",
        "    else:\n",
        "        print(\"Skipping train_step execution: state or loss_fn not defined.\")\n",
        "        # Return dummy values or handle appropriately\n",
        "        return state, jnp.nan, jnp.nan, new_dropout_key\n",
        "\n",
        "\n",
        "# pmap the training step for parallel execution across TPU cores\n",
        "# Check if 'state' is defined before pmapping\n",
        "if 'state' in locals():\n",
        "    pmapped_train_step = jax.pmap(train_step, axis_name='batch')\n",
        "    print(\"JAX training step function defined and pmapped.\")\n",
        "else:\n",
        "    print(\"Train state not created, cannot pmap training step.\")\n",
        "\n",
        "\n",
        "# --- JAX Evaluation Step Function ---\n",
        "# Define the evaluation step function\n",
        "@partial(jax.jit) # Removed static_argnums\n",
        "def eval_step(state, batch): # Removed dropout_key parameter here\n",
        "    def loss_fn(params):\n",
        "         # Model forward pass in evaluation mode (deterministic=True, no dropout)\n",
        "        # Check if model and num_labels are defined before applying\n",
        "        if 'model' in locals() and 'num_labels' in locals():\n",
        "            logits = state.apply_fn(\n",
        "                batch['input_ids'],\n",
        "                batch['attention_mask'],\n",
        "                batch['token_type_ids'],\n",
        "                params=state.params, # Use state.params for evaluation\n",
        "                deterministic=True, # Explicitly pass deterministic as keyword argument\n",
        "            )\n",
        "            loss = compute_loss(logits, batch['labels'])\n",
        "            return loss, logits # Return logits for accuracy calculation\n",
        "        else:\n",
        "            print(\"Cannot compute loss in eval_step: model or num_labels not defined.\")\n",
        "            return jnp.nan, None\n",
        "\n",
        "    # Check if state and loss_fn are defined before executing\n",
        "    if 'state' in locals() and 'loss_fn' in locals():\n",
        "        loss, logits = loss_fn(state.params)\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        # Check if num_labels is defined and logits is not None\n",
        "        if 'num_labels' in locals() and logits is not None:\n",
        "            predicted_labels = jnp.argmax(logits, axis=-1)\n",
        "            accuracy = jnp.mean(predicted_labels == batch['labels'])\n",
        "        else:\n",
        "             print(\"Cannot compute metrics in eval_step: num_labels not defined or logits is None.\")\n",
        "             predicted_labels = None\n",
        "             accuracy = jnp.nan\n",
        "\n",
        "\n",
        "        return loss, accuracy, predicted_labels # Return predicted labels for potential inspection\n",
        "    else:\n",
        "        print(\"Skipping eval_step execution: state or loss_fn not defined.\")\n",
        "        return jnp.nan, jnp.nan, None\n",
        "\n",
        "\n",
        "# pmap the evaluation step for parallel execution\n",
        "# Check if 'state' is defined before pmapping\n",
        "if 'state' in locals():\n",
        "    pmapped_eval_step = jax.pmap(eval_step, axis_name='batch')\n",
        "    print(\"JAX evaluation step function defined and pmapped.\")\n",
        "else:\n",
        "    print(\"Train state not created, cannot pmap evaluation step.\")\n",
        "\n",
        "\n",
        "# --- Training and Evaluation Loops ---\n",
        "# Function to replicate and shard data for pmap\n",
        "def shard_batch(batch):\n",
        "    # Reshape batch for pmap: [batch_size] -> [num_devices, batch_size_per_device]\n",
        "    # Assumes batch_size is divisible by num_devices\n",
        "    num_devices = jax.local_device_count()\n",
        "    # Ensure batch size is divisible by number of devices\n",
        "    if batch['input_ids'].shape[0] % num_devices != 0:\n",
        "        raise ValueError(f\"Batch size ({batch['input_ids'].shape[0]}) must be divisible by the number of devices ({num_devices}) for pmap.\")\n",
        "\n",
        "    return jax.tree_util.tree_map(\n",
        "        lambda x: x.reshape(num_devices, x.shape[0] // num_devices, *x.shape[1:]),\n",
        "        batch\n",
        "    )\n",
        "\n",
        "# Check if 'state' and pmapped functions are defined before starting loops\n",
        "if 'state' in locals() and 'pmapped_train_step' in locals() and 'pmapped_eval_step' in locals():\n",
        "    print(\"Starting training...\")\n",
        "    train_metrics = []\n",
        "    eval_metrics = []\n",
        "\n",
        "    # Convert datasets to lists of NumPy arrays upfront to avoid as_numpy_iterator() RuntimeError\n",
        "    print(\"Converting datasets to NumPy arrays...\")\n",
        "    try:\n",
        "        train_data_np = list(train_dataset.as_numpy_iterator())\n",
        "        eval_data_np = list(eval_dataset.as_numpy_iterator())\n",
        "        print(\"Datasets converted to NumPy arrays.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error converting datasets to NumPy arrays: {e}\")\n",
        "         train_data_np = None\n",
        "         eval_data_np = None\n",
        "\n",
        "\n",
        "    if train_data_np is not None and eval_data_np is not None:\n",
        "        # Replication of dropout key for pmap\n",
        "        key = jax.random.PRNGKey(0) # Re-initialize key for training loop\n",
        "        dropout_key_replicated = jax.device_put_replicated(key, jax.local_devices())\n",
        "\n",
        "        # Determine total number of steps to iterate based on num_train_steps\n",
        "        # The training loop should run for num_train_steps iterations\n",
        "        # Check if num_train_steps is defined\n",
        "        if 'num_train_steps' in locals():\n",
        "            # Use a simple index for iterating over the NumPy data\n",
        "            train_data_index = 0\n",
        "\n",
        "            for step in range(num_train_steps):\n",
        "                # Get a batch and shard it for pmap\n",
        "                # Handle wrapping around the dataset for multiple epochs if needed\n",
        "                if train_data_index >= len(train_data_np):\n",
        "                    train_data_index = 0 # Reset index for next epoch\n",
        "                    print(f\"End of training dataset at step {step}. Restarting from beginning.\")\n",
        "\n",
        "                train_batch = train_data_np[train_data_index]\n",
        "                train_data_index += 1\n",
        "\n",
        "                sharded_train_batch = shard_batch(train_batch)\n",
        "\n",
        "\n",
        "                # Perform a training step\n",
        "                # Check if pmapped_train_step is defined\n",
        "                if 'pmapped_train_step' in locals():\n",
        "                    state, loss, accuracy, dropout_key_replicated = pmapped_train_step(\n",
        "                        state,\n",
        "                        sharded_train_batch,\n",
        "                        dropout_key_replicated # Pass the replicated dropout key\n",
        "                    )\n",
        "\n",
        "                    # Log metrics periodically\n",
        "                    if (step + 1) % SAVE_SUMMARY_STEPS == 0 or step == num_train_steps - 1: # Also log at the last step\n",
        "                        # Gather metrics from all devices and take the mean\n",
        "                        mean_loss = jnp.mean(loss)\n",
        "                        mean_accuracy = jnp.mean(accuracy)\n",
        "                        print(f\"Step {step+1}/{num_train_steps}, Train Loss: {mean_loss:.4f}, Train Accuracy: {mean_accuracy:.4f}\")\n",
        "                        train_metrics.append({'step': step + 1, 'loss': float(mean_loss), 'accuracy': float(mean_accuracy)}) # Convert to float for list\n",
        "\n",
        "\n",
        "                        # Run evaluation periodically\n",
        "                        print(\"Running evaluation...\")\n",
        "                        eval_losses = []\n",
        "                        eval_accuracies = []\n",
        "                        # Iterate over the evaluation dataset (NumPy list)\n",
        "                        # Check if eval_data_np is defined\n",
        "                        if eval_data_np is not None:\n",
        "                            # Replicate key for evaluation - not needed with deterministic=True passed directly\n",
        "                            # eval_key_replicated = jax.device_put_replicated(jax.random.PRNGKey(step), jax.local_devices()) # Use step for unique key per eval\n",
        "\n",
        "                            for eval_batch in eval_data_np:\n",
        "                                # Check if pmapped_eval_step is defined\n",
        "                                if 'pmapped_eval_step' in locals():\n",
        "                                    sharded_eval_batch = shard_batch(eval_batch)\n",
        "                                    # Pass a dummy key or handle rngs={'dropout': None} if deterministic=True\n",
        "                                    loss, accuracy, _ = pmapped_eval_step(state, sharded_eval_batch) # Removed key argument here\n",
        "                                    eval_losses.append(jnp.mean(loss))\n",
        "                                    eval_accuracies.append(jnp.mean(accuracy))\n",
        "                                else:\n",
        "                                    print(\"Skipping evaluation step: pmapped_eval_step not defined.\")\n",
        "                                    break # Exit inner evaluation loop\n",
        "\n",
        "\n",
        "                            # Calculate mean evaluation metrics\n",
        "                            mean_eval_loss = jnp.mean(jnp.stack(eval_losses)) if eval_losses else float('nan')\n",
        "                            mean_eval_accuracy = jnp.mean(jnp.stack(eval_accuracies)) if eval_accuracies else float('nan')\n",
        "\n",
        "                            print(f\"Evaluation Results at step {step+1}: Eval Loss: {mean_eval_loss:.4f}, Eval Accuracy: {mean_eval_accuracy:.4f}\")\n",
        "                            eval_metrics.append({'step': step + 1, 'loss': float(mean_eval_loss), 'accuracy': float(mean_eval_accuracy)}) # Convert to float for list\n",
        "                        else:\n",
        "                            print(\"Skipping evaluation: eval_data_np is None.\")\n",
        "\n",
        "                else:\n",
        "                    print(\"Skipping training step: pmapped_train_step not defined.\")\n",
        "                    break # Exit training loop\n",
        "\n",
        "            print(\"Training finished.\")\n",
        "        else:\n",
        "            print(\"Skipping training loop: num_train_steps not defined.\")\n",
        "    else:\n",
        "        print(\"Skipping training and evaluation loops: train_data_np or eval_data_np is None.\")\n",
        "else:\n",
        "    print(\"Skipping training and evaluation loops because state or pmapped functions were not created.\")\n",
        "\n",
        "\n",
        "# --- Prediction ---\n",
        "# Define the prediction step function (similar to eval_step but without loss/accuracy)\n",
        "@partial(jax.jit) # Removed static_argnums\n",
        "def predict_step(state, batch): # Removed dropout_key parameter here\n",
        "     # Model forward pass in prediction mode\n",
        "    # Check if state is defined before applying\n",
        "    if 'state' in locals():\n",
        "        logits = state.apply_fn(\n",
        "            batch['input_ids'],\n",
        "            batch['attention_mask'],\n",
        "            batch['token_type_ids'],\n",
        "            params=state.params, # Use trained parameters\n",
        "            deterministic=True, # Explicitly pass deterministic as keyword argument\n",
        "        )\n",
        "        predicted_labels = jnp.argmax(logits, axis=-1)\n",
        "        return predicted_labels, logits # Return logits for probabilities if needed\n",
        "    else:\n",
        "        print(\"Skipping prediction step: state not defined.\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# pmap the prediction step\n",
        "# Check if 'state' is defined before pmapping\n",
        "if 'state' in locals():\n",
        "    pmapped_predict_step = jax.pmap(predict_step, axis_name='batch')\n",
        "    print(\"\\nJAX prediction step function defined and pmapped.\")\n",
        "else:\n",
        "     print(\"\\nTrain state not created, cannot pmap prediction step.\")\n",
        "\n",
        "\n",
        "# Run predictions\n",
        "# Check if pmapped_predict_step is defined before running prediction loop\n",
        "if 'pmapped_predict_step' in locals():\n",
        "    print(\"Running predictions...\")\n",
        "    # Use the 'test' split of the tokenized dataset\n",
        "    # Check if tokenized_dataset is defined and has a 'test' split\n",
        "    if 'tokenized_dataset' in locals() and \"test\" in tokenized_dataset:\n",
        "        test_dataset_for_prediction = tokenized_dataset[\"test\"].to_tf_dataset(\n",
        "            columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], # Include labels if available (useful for verification)\n",
        "            shuffle=False, # No shuffling for prediction\n",
        "            batch_size=PREDICT_BATCH_SIZE,\n",
        "            collate_fn=None,\n",
        "            drop_remainder=True # Drop remainder for fixed shape in pmap\n",
        "        )\n",
        "\n",
        "        # Convert test dataset to list of NumPy arrays upfront\n",
        "        print(\"Converting test dataset to NumPy arrays...\")\n",
        "        try:\n",
        "            test_data_np = list(test_dataset_for_prediction.as_numpy_iterator())\n",
        "            print(\"Test dataset converted to NumPy arrays.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting test dataset to NumPy arrays: {e}\")\n",
        "            test_data_np = None\n",
        "\n",
        "\n",
        "        all_predictions = []\n",
        "        all_logits = []\n",
        "\n",
        "        # Use the list of NumPy arrays for prediction\n",
        "        if test_data_np is not None:\n",
        "            # Determine the number of prediction batches\n",
        "            num_predict_batches = len(test_data_np)\n",
        "\n",
        "            # Replicate key for prediction - not needed with deterministic=True passed directly\n",
        "            # predict_key = jax.random.PRNGKey(0) # Use a fixed key for prediction\n",
        "            # predict_key_replicated = jax.device_put_replicated(predict_key, jax.local_devices())\n",
        "\n",
        "\n",
        "            for predict_step_idx in range(num_predict_batches):\n",
        "                predict_batch = test_data_np[predict_step_idx]\n",
        "                sharded_predict_batch = shard_batch(predict_batch)\n",
        "\n",
        "                # Perform prediction step\n",
        "                # Check if pmapped_predict_step is defined\n",
        "                if 'pmapped_predict_step' in locals():\n",
        "                    # Pass a dummy key or handle rngs={'dropout': None} if deterministic=True\n",
        "                    sharded_predicted_labels, sharded_logits = pmapped_predict_step(state, sharded_predict_batch) # Removed key argument here\n",
        "\n",
        "                    # Gather predictions from all devices and flatten\n",
        "                    # Ensure correct reshaping and slicing based on the original batch size\n",
        "                    # Check if sharded_predicted_labels and sharded_logits is not None\n",
        "                    if sharded_predicted_labels is not None and sharded_logits is not None:\n",
        "                        predicted_labels = sharded_predicted_labels.reshape(-1)[:sharded_predict_batch['input_ids'].shape[0] * sharded_predict_batch['input_ids'].shape[1] // jax.local_device_count()]\n",
        "                        logits = sharded_logits.reshape(-1, sharded_logits.shape[-1])[:sharded_predict_batch['input_ids'].shape[0] * sharded_predict_batch['input_ids'].shape[1] // jax.local_device_count(), :]\n",
        "\n",
        "                        all_predictions.extend(predicted_labels.tolist())\n",
        "                        all_logits.extend(logits.tolist())\n",
        "                    else:\n",
        "                         print(\"Skipping prediction batch: sharded_predicted_labels or sharded_logits is None.\")\n",
        "\n",
        "                else:\n",
        "                    print(\"Skipping prediction step: pmapped_predict_step not defined.\")\n",
        "                    break # Exit prediction loop\n",
        "\n",
        "        else:\n",
        "            print(\"Skipping prediction loop: test_data_np is None.\")\n",
        "\n",
        "        print(\"Predictions finished.\")\n",
        "        # Display sample predictions and their corresponding logits\n",
        "        print(\"\\nSample Predictions (first 10):\")\n",
        "        print(all_predictions[:10])\n",
        "        print(\"\\nSample Logits (first 10 predictions):\")\n",
        "        print(all_logits[:10])\n",
        "    else:\n",
        "        print(\"Skipping prediction: tokenized_dataset or test split not defined.\")\n",
        "else:\n",
        "    print(\"\\nSkipping prediction loop because state or pmapped prediction function was not created.\")"
      ]
    }
  ]
}
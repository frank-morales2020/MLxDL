{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo/xlsxBwi76YWXqygokir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/run_mmlu-GPT4-GEMINI-MISTRAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/neulab/gemini-benchmark/tree/main"
      ],
      "metadata": {
        "id": "6gyhqLVaa4J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neulab/gemini-benchmark.git"
      ],
      "metadata": {
        "id": "e1M1jyAlatKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gemini-benchmark/benchmarking"
      ],
      "metadata": {
        "id": "72mnecxXbEC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install litellm -q"
      ],
      "metadata": {
        "id": "n3WOBNPbbu17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --upgrade -q\n",
        "!pip install openai -q\n"
      ],
      "metadata": {
        "id": "cyYnMNf-0IXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIWzQcZAj2RV",
        "outputId": "817365d9-93f9-49d0-eaf6-ab0d825e7f67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
        "#client = OpenAI()"
      ],
      "metadata": {
        "id": "Ep2anuh1eIhd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "modellist=client.models.list()\n",
        "modellist.data"
      ],
      "metadata": {
        "id": "d5G_Gu6zeTGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i9hhgxpjbIL",
        "outputId": "b41624cc-54a6-454b-8b85-c62942293c66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: run_mmlu.py [-h] [--openai_api_key OPENAI_API_KEY]\n",
            "                   [--model_name {gpt-3.5-turbo,gpt-4-1106-preview,gemini-pro,mixtral}] [--cot]\n",
            "                   [--num_examples NUM_EXAMPLES]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --openai_api_key OPENAI_API_KEY\n",
            "  --model_name {gpt-3.5-turbo,gpt-4-1106-preview,gemini-pro,mixtral}\n",
            "  --cot\n",
            "  --num_examples NUM_EXAMPLES\n",
            "                        Number of examples included in the current prompt input.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://litellm.vercel.app/docs/exception_mapping"
      ],
      "metadata": {
        "id": "hgsSXbZsmbpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBahsof4DYa4",
        "outputId": "3d082853-ac78-4ba5-eb7e-3ec14fa582c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Cloud SDK 476.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/compute/docs/regions-zones"
      ],
      "metadata": {
        "id": "4AYibmmqEwlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud init"
      ],
      "metadata": {
        "id": "4W697ejAike8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud info"
      ],
      "metadata": {
        "id": "BwV2KN9sDqz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py"
      ],
      "metadata": {
        "id": "oRCemArMiCAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/gemini-benchmark/benchmarking/MMLU/utils.py"
      ],
      "metadata": {
        "id": "6pMYcMGUwiGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GEMINI\n"
      ],
      "metadata": {
        "id": "KM_aXmmw7WzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GID=1\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "TpmqgR0j02mK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASKS = [\n",
        "    \"machine_learning\",\n",
        "    \"college_computer_science\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "JWn50cGaou_0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GEMINI\n",
        "%rm -rf /content/outputs/\n",
        "%mkdir /content/outputs\n",
        "\n",
        "!python /content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py --model_name gemini-pro --num_examples 5"
      ],
      "metadata": {
        "id": "geeyklfXrsjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8aa686f-c938-4e40-e67c-d391d93d2a87"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing machine_learning ...\n",
            "100% 112/112 [02:20<00:00,  1.25s/it]\n",
            "machine_learning acc 0.4732\n",
            "Testing college_computer_science ...\n",
            "100% 100/100 [02:14<00:00,  1.35s/it]\n",
            "college_computer_science acc 0.4800\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/outputs/gemini-pro_simple_accs.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "#print('average: %s'%data['all'])\n",
        "\n",
        "# Check if 'all' key contains a list or dictionary\n",
        "if isinstance(data['all'], (list, dict)):\n",
        "    for value in data['all']:\n",
        "        print(value)\n",
        "else:\n",
        "    # Handle the float value appropriately\n",
        "    print(\"GEMINI-MMLU Average:\", data['all'])\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGijRiB87Mj5",
        "outputId": "938217ba-9299-48d4-f8d5-d66480fb9d4e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEMINI-MMLU Average: 0.47641509433962265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAZZQdEx53VN",
        "outputId": "fd1af791-62ea-4d73-d3cd-2032efeb3658"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'machine_learning': 0.4732142857142857, 'college_computer_science': 0.48, 'all': 0.47641509433962265}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPENAI"
      ],
      "metadata": {
        "id": "SPEsGdC87afE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/outputs/\n",
        "%mkdir /content/outputs\n",
        "%mkdir -p  /content/data/dev\n",
        "%cd /content/\n",
        "\n",
        "# choose from 'gpt-3.5-turbo', 'gpt-4-1106-preview', 'gemini-pro', 'mixtral'\n",
        "!python /content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py --openai_api_key $OPENAI_API_KEY --model_name gpt-4-1106-preview  --num_examples 5"
      ],
      "metadata": {
        "id": "MZTcpSy9dqEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/outputs/gpt-4-1106-preview_simple_accs.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "#print('average: %s'%data['all'])\n",
        "\n",
        "# Check if 'all' key contains a list or dictionary\n",
        "if isinstance(data['all'], (list, dict)):\n",
        "    for value in data['all']:\n",
        "        print(value)\n",
        "else:\n",
        "    # Handle the float value appropriately\n",
        "    print(\"GEMINI-MMLU Average:\", data['all'])\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSAeXeVhE9l2",
        "outputId": "7b005ce1-fa66-4a35-bd9a-f05352c0c46e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEMINI-MMLU Average: 0.7452830188679245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57N6jgAYFFa5",
        "outputId": "c700420b-c637-4d2a-a3ea-1f710aa01cfc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'machine_learning': 0.7589285714285714, 'college_computer_science': 0.73, 'all': 0.7452830188679245}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py"
      ],
      "metadata": {
        "id": "HSiftZeYLCOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MISTRAL"
      ],
      "metadata": {
        "id": "3ok2us7NOUjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistralai --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMu9VbWuOR2i",
        "outputId": "e052e020-385b-41dc-c838-69e2a1cce6af"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### MIXTRAL\n",
        "%rm -rf /content/outputs/\n",
        "%mkdir -p /content/outputs/mistralai/\n",
        "# /content/gemini-benchmark/outputs/MMLU/mixtral\n",
        "!python /content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py --model_name mixtral --num_examples 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W0RkmPhJdx0",
        "outputId": "c206c2f1-fef2-4beb-dbd5-932145ab62cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing machine_learning ...\n",
            " 66% 74/112 [03:04<01:49,  2.88s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "#/content/outputs/mistralai/Mistral-7B-Instruct-v0.1_simple_outputs.json\n",
        "f = open('/content/outputs/mistralai/Mistral-7B-Instruct-v0.1_simple_accs.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "#print('average: %s'%data['all'])\n",
        "\n",
        "# Check if 'all' key contains a list or dictionary\n",
        "if isinstance(data['all'], (list, dict)):\n",
        "    for value in data['all']:\n",
        "        print(value)\n",
        "else:\n",
        "    # Handle the float value appropriately\n",
        "    print(\"MISTRAL-MMLU Average:\", data['all'])\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pdkCKUPtCDB",
        "outputId": "6ba73c62-b544-41e5-9a6c-a0d78e74a225"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MISTRAL-MMLU Average: 0.4339622641509434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqZhwug9tDHX",
        "outputId": "77b92911-c469-4fcf-8ac7-c61b0fe3c4f9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'machine_learning': 0.5267857142857143, 'college_computer_science': 0.33, 'all': 0.4339622641509434}\n"
          ]
        }
      ]
    }
  ]
}
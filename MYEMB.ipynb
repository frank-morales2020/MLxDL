{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF42NiuZlJWPNucMLvJVU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MYEMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4luJmKVuKah",
        "outputId": "4c6ffc5b-f2cf-4ee4-bb3a-0bfd1bb9e2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: -2.0944\n",
            "Epoch [2/100], Loss: -5.1736\n",
            "Epoch [3/100], Loss: -10.2510\n",
            "Epoch [4/100], Loss: -17.8607\n",
            "Epoch [5/100], Loss: -28.2417\n",
            "Epoch [6/100], Loss: -41.5464\n",
            "Epoch [7/100], Loss: -57.8382\n",
            "Epoch [8/100], Loss: -77.1841\n",
            "Epoch [9/100], Loss: -99.4283\n",
            "Epoch [10/100], Loss: -124.7059\n",
            "Epoch [11/100], Loss: -153.1870\n",
            "Epoch [12/100], Loss: -185.0000\n",
            "Epoch [13/100], Loss: -220.0206\n",
            "Epoch [14/100], Loss: -258.3177\n",
            "Epoch [15/100], Loss: -299.8869\n",
            "Epoch [16/100], Loss: -344.5794\n",
            "Epoch [17/100], Loss: -392.2733\n",
            "Epoch [18/100], Loss: -443.0975\n",
            "Epoch [19/100], Loss: -497.2932\n",
            "Epoch [20/100], Loss: -554.3760\n",
            "Epoch [21/100], Loss: -614.5308\n",
            "Epoch [22/100], Loss: -677.6592\n",
            "Epoch [23/100], Loss: -743.7884\n",
            "Epoch [24/100], Loss: -813.0590\n",
            "Epoch [25/100], Loss: -885.2091\n",
            "Epoch [26/100], Loss: -960.2931\n",
            "Epoch [27/100], Loss: -1038.0800\n",
            "Epoch [28/100], Loss: -1118.5536\n",
            "Epoch [29/100], Loss: -1201.6860\n",
            "Epoch [30/100], Loss: -1287.4641\n",
            "Epoch [31/100], Loss: -1375.9611\n",
            "Epoch [32/100], Loss: -1467.1417\n",
            "Epoch [33/100], Loss: -1560.9264\n",
            "Epoch [34/100], Loss: -1657.2494\n",
            "Epoch [35/100], Loss: -1756.0773\n",
            "Epoch [36/100], Loss: -1857.4530\n",
            "Epoch [37/100], Loss: -1961.3057\n",
            "Epoch [38/100], Loss: -2067.6799\n",
            "Epoch [39/100], Loss: -2176.5300\n",
            "Epoch [40/100], Loss: -2287.8335\n",
            "Epoch [41/100], Loss: -2401.6333\n",
            "Epoch [42/100], Loss: -2517.8660\n",
            "Epoch [43/100], Loss: -2636.6091\n",
            "Epoch [44/100], Loss: -2757.7622\n",
            "Epoch [45/100], Loss: -2881.2981\n",
            "Epoch [46/100], Loss: -3007.2356\n",
            "Epoch [47/100], Loss: -3135.7146\n",
            "Epoch [48/100], Loss: -3266.5581\n",
            "Epoch [49/100], Loss: -3399.7827\n",
            "Epoch [50/100], Loss: -3535.3552\n",
            "Epoch [51/100], Loss: -3673.2900\n",
            "Epoch [52/100], Loss: -3813.5010\n",
            "Epoch [53/100], Loss: -3956.0759\n",
            "Epoch [54/100], Loss: -4100.9150\n",
            "Epoch [55/100], Loss: -4248.0859\n",
            "Epoch [56/100], Loss: -4397.6260\n",
            "Epoch [57/100], Loss: -4549.4277\n",
            "Epoch [58/100], Loss: -4703.4517\n",
            "Epoch [59/100], Loss: -4859.8589\n",
            "Epoch [60/100], Loss: -5018.6079\n",
            "Epoch [61/100], Loss: -5179.6084\n",
            "Epoch [62/100], Loss: -5342.8433\n",
            "Epoch [63/100], Loss: -5508.3760\n",
            "Epoch [64/100], Loss: -5676.1748\n",
            "Epoch [65/100], Loss: -5846.2324\n",
            "Epoch [66/100], Loss: -6018.5186\n",
            "Epoch [67/100], Loss: -6193.0537\n",
            "Epoch [68/100], Loss: -6369.8389\n",
            "Epoch [69/100], Loss: -6548.9351\n",
            "Epoch [70/100], Loss: -6730.4224\n",
            "Epoch [71/100], Loss: -6914.1333\n",
            "Epoch [72/100], Loss: -7100.0454\n",
            "Epoch [73/100], Loss: -7288.1855\n",
            "Epoch [74/100], Loss: -7478.5068\n",
            "Epoch [75/100], Loss: -7671.0933\n",
            "Epoch [76/100], Loss: -7865.9272\n",
            "Epoch [77/100], Loss: -8062.9717\n",
            "Epoch [78/100], Loss: -8262.3203\n",
            "Epoch [79/100], Loss: -8463.8857\n",
            "Epoch [80/100], Loss: -8667.7051\n",
            "Epoch [81/100], Loss: -8873.7490\n",
            "Epoch [82/100], Loss: -9082.0127\n",
            "Epoch [83/100], Loss: -9292.5879\n",
            "Epoch [84/100], Loss: -9505.3574\n",
            "Epoch [85/100], Loss: -9720.3564\n",
            "Epoch [86/100], Loss: -9937.5312\n",
            "Epoch [87/100], Loss: -10156.8906\n",
            "Epoch [88/100], Loss: -10378.4453\n",
            "Epoch [89/100], Loss: -10602.2295\n",
            "Epoch [90/100], Loss: -10828.2334\n",
            "Epoch [91/100], Loss: -11056.4746\n",
            "Epoch [92/100], Loss: -11286.9102\n",
            "Epoch [93/100], Loss: -11519.5596\n",
            "Epoch [94/100], Loss: -11754.4326\n",
            "Epoch [95/100], Loss: -11991.5195\n",
            "Epoch [96/100], Loss: -12230.7773\n",
            "Epoch [97/100], Loss: -12472.2051\n",
            "Epoch [98/100], Loss: -12715.8018\n",
            "Epoch [99/100], Loss: -12961.5732\n",
            "Epoch [100/100], Loss: -13209.5381\n",
            "Energy for new sample: -20777.6055\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple energy-based model (EBM)\n",
        "class SimpleEBM(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleEBM, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 128)\n",
        "        self.linear2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        energy = self.linear2(x)\n",
        "        return energy\n",
        "\n",
        "# Generate some sample data\n",
        "input_size = 10\n",
        "data = torch.randn(100, input_size)\n",
        "\n",
        "# Initialize the EBM and optimizer\n",
        "model = SimpleEBM(input_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data:\n",
        "        optimizer.zero_grad()\n",
        "        energy = model(batch)\n",
        "        loss = torch.mean(energy)  # Minimize the energy\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Example usage: Calculate energy for a new sample\n",
        "new_sample = torch.randn(1, input_size)\n",
        "energy = model(new_sample)\n",
        "print(f\"Energy for new sample: {energy.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Load MNIST test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the ImageEBM with mixed activations\n",
        "class ImageEBM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEBM, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.fc1 = nn.Linear(8 * 28 * 28, 10)\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))  # ReLU after convolution\n",
        "        x = x.view(-1, 8 * 28 * 28)\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid in fully connected layer\n",
        "        energy = self.fc2(x)\n",
        "        return energy\n",
        "\n",
        "# Initialize the EBM, optimizer, and learning rate scheduler\n",
        "model = ImageEBM()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop with reduced energy regularization\n",
        "num_epochs = 5\n",
        "alpha = 0.1  # Reduced regularization strength\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        energy = model(data)\n",
        "\n",
        "        # Add energy regularization to the loss\n",
        "        loss = torch.mean(energy) + alpha * torch.mean(torch.relu(-energy))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    scheduler.step()  # Update learning rate\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            energy = model(data)\n",
        "            test_loss += torch.mean(energy).item()  # Sum up batch loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XiU23XyvOjY",
        "outputId": "a1991d8c-e8c4-45f1-9c23-8e5f7567a1c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [1/938], Loss: 0.1992\n",
            "Epoch [1/5], Batch [101/938], Loss: -0.7339\n",
            "Epoch [1/5], Batch [201/938], Loss: -0.7856\n",
            "Epoch [1/5], Batch [301/938], Loss: -0.8311\n",
            "Epoch [1/5], Batch [401/938], Loss: -0.8778\n",
            "Epoch [1/5], Batch [501/938], Loss: -0.9235\n",
            "Epoch [1/5], Batch [601/938], Loss: -0.9693\n",
            "Epoch [1/5], Batch [701/938], Loss: -1.0145\n",
            "Epoch [1/5], Batch [801/938], Loss: -1.0597\n",
            "Epoch [1/5], Batch [901/938], Loss: -1.1048\n",
            "Epoch [2/5], Batch [1/938], Loss: -1.1221\n",
            "Epoch [2/5], Batch [101/938], Loss: -1.1673\n",
            "Epoch [2/5], Batch [201/938], Loss: -1.2124\n",
            "Epoch [2/5], Batch [301/938], Loss: -1.2575\n",
            "Epoch [2/5], Batch [401/938], Loss: -1.3025\n",
            "Epoch [2/5], Batch [501/938], Loss: -1.3477\n",
            "Epoch [2/5], Batch [601/938], Loss: -1.3928\n",
            "Epoch [2/5], Batch [701/938], Loss: -1.4377\n",
            "Epoch [2/5], Batch [801/938], Loss: -1.4829\n",
            "Epoch [2/5], Batch [901/938], Loss: -1.5279\n",
            "Epoch [3/5], Batch [1/938], Loss: -1.5451\n",
            "Epoch [3/5], Batch [101/938], Loss: -1.5901\n",
            "Epoch [3/5], Batch [201/938], Loss: -1.6352\n",
            "Epoch [3/5], Batch [301/938], Loss: -1.6802\n",
            "Epoch [3/5], Batch [401/938], Loss: -1.7252\n",
            "Epoch [3/5], Batch [501/938], Loss: -1.7702\n",
            "Epoch [3/5], Batch [601/938], Loss: -1.8153\n",
            "Epoch [3/5], Batch [701/938], Loss: -1.8603\n",
            "Epoch [3/5], Batch [801/938], Loss: -1.9053\n",
            "Epoch [3/5], Batch [901/938], Loss: -1.9503\n",
            "Epoch [4/5], Batch [1/938], Loss: -1.9674\n",
            "Epoch [4/5], Batch [101/938], Loss: -2.0124\n",
            "Epoch [4/5], Batch [201/938], Loss: -2.0575\n",
            "Epoch [4/5], Batch [301/938], Loss: -2.1025\n",
            "Epoch [4/5], Batch [401/938], Loss: -2.1475\n",
            "Epoch [4/5], Batch [501/938], Loss: -2.1925\n",
            "Epoch [4/5], Batch [601/938], Loss: -2.2375\n",
            "Epoch [4/5], Batch [701/938], Loss: -2.2825\n",
            "Epoch [4/5], Batch [801/938], Loss: -2.3275\n",
            "Epoch [4/5], Batch [901/938], Loss: -2.3725\n",
            "Epoch [5/5], Batch [1/938], Loss: -2.3896\n",
            "Epoch [5/5], Batch [101/938], Loss: -2.4346\n",
            "Epoch [5/5], Batch [201/938], Loss: -2.4796\n",
            "Epoch [5/5], Batch [301/938], Loss: -2.5247\n",
            "Epoch [5/5], Batch [401/938], Loss: -2.5697\n",
            "Epoch [5/5], Batch [501/938], Loss: -2.6147\n",
            "Epoch [5/5], Batch [601/938], Loss: -2.6597\n",
            "Epoch [5/5], Batch [701/938], Loss: -2.7047\n",
            "Epoch [5/5], Batch [801/938], Loss: -2.7497\n",
            "Epoch [5/5], Batch [901/938], Loss: -2.7947\n",
            "Test Loss: -3.1242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            energy = model(data)\n",
        "            test_loss += torch.mean(energy).item()  # Sum up batch loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Eval Loss: {test_loss:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrkfus8a35qD",
        "outputId": "4b228e37-9e6c-4324-a25d-732da0b8b4a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Loss: -3.1242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the ImageEBM for CIFAR-10 (color images)\n",
        "class ImageEBM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEBM, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Input channels = 3 for color\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Adjust input size for CIFAR-10\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Adjust size for CIFAR-10\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        energy = self.fc2(x)\n",
        "        return energy\n",
        "\n",
        "# Initialize the EBM, optimizer, and learning rate scheduler\n",
        "model = ImageEBM()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop with energy regularization\n",
        "num_epochs = 5  # You might need more epochs for CIFAR-10\n",
        "alpha = 0.1\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        energy = model(data)\n",
        "\n",
        "        # Add energy regularization to the loss\n",
        "        loss = torch.mean(energy) + alpha * torch.mean(torch.relu(-energy))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    scheduler.step()  # Update learning rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaXsg-yI5MJU",
        "outputId": "9fa1f325-f57d-4403-d7ff-44c98d8b05d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/5], Batch [1/782], Loss: -0.3356\n",
            "Epoch [1/5], Batch [101/782], Loss: -3.6440\n",
            "Epoch [1/5], Batch [201/782], Loss: -4.3777\n",
            "Epoch [1/5], Batch [301/782], Loss: -5.0892\n",
            "Epoch [1/5], Batch [401/782], Loss: -5.8079\n",
            "Epoch [1/5], Batch [501/782], Loss: -6.5292\n",
            "Epoch [1/5], Batch [601/782], Loss: -7.2430\n",
            "Epoch [1/5], Batch [701/782], Loss: -7.9535\n",
            "Epoch [2/5], Batch [1/782], Loss: -8.5346\n",
            "Epoch [2/5], Batch [101/782], Loss: -9.2422\n",
            "Epoch [2/5], Batch [201/782], Loss: -9.9488\n",
            "Epoch [2/5], Batch [301/782], Loss: -10.6550\n",
            "Epoch [2/5], Batch [401/782], Loss: -11.3972\n",
            "Epoch [2/5], Batch [501/782], Loss: -12.1229\n",
            "Epoch [2/5], Batch [601/782], Loss: -12.8440\n",
            "Epoch [2/5], Batch [701/782], Loss: -13.5626\n",
            "Epoch [3/5], Batch [1/782], Loss: -14.1661\n",
            "Epoch [3/5], Batch [101/782], Loss: -14.9152\n",
            "Epoch [3/5], Batch [201/782], Loss: -15.6515\n",
            "Epoch [3/5], Batch [301/782], Loss: -16.4029\n",
            "Epoch [3/5], Batch [401/782], Loss: -17.1639\n",
            "Epoch [3/5], Batch [501/782], Loss: -17.9122\n",
            "Epoch [3/5], Batch [601/782], Loss: -18.6556\n",
            "Epoch [3/5], Batch [701/782], Loss: -19.3960\n",
            "Epoch [4/5], Batch [1/782], Loss: -20.0016\n",
            "Epoch [4/5], Batch [101/782], Loss: -20.7388\n",
            "Epoch [4/5], Batch [201/782], Loss: -21.4747\n",
            "Epoch [4/5], Batch [301/782], Loss: -22.2097\n",
            "Epoch [4/5], Batch [401/782], Loss: -22.9438\n",
            "Epoch [4/5], Batch [501/782], Loss: -23.6773\n",
            "Epoch [4/5], Batch [601/782], Loss: -24.4102\n",
            "Epoch [4/5], Batch [701/782], Loss: -25.1426\n",
            "Epoch [5/5], Batch [1/782], Loss: -25.7429\n",
            "Epoch [5/5], Batch [101/782], Loss: -26.4747\n",
            "Epoch [5/5], Batch [201/782], Loss: -27.2061\n",
            "Epoch [5/5], Batch [301/782], Loss: -27.9372\n",
            "Epoch [5/5], Batch [401/782], Loss: -28.6682\n",
            "Epoch [5/5], Batch [501/782], Loss: -29.3989\n",
            "Epoch [5/5], Batch [601/782], Loss: -30.1295\n",
            "Epoch [5/5], Batch [701/782], Loss: -30.8599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eval for CIFAR-10"
      ],
      "metadata": {
        "id": "dkCpZcGV58B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load CIFAR-10 test dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            energy = model(data)\n",
        "            test_loss += torch.mean(energy).item()  # Sum up batch loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Eval Loss: {test_loss:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRWXRN7752QM",
        "outputId": "38b8c492-41cf-4038-eae8-dd2306437dea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Eval Loss: -34.9541\n"
          ]
        }
      ]
    }
  ]
}
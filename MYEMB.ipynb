{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v55kGS8REZyw"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMztIaH1/YIKTNyYJMPf/ft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MYEMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Digit Recognition"
      ],
      "metadata": {
        "id": "v55kGS8REZyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Load MNIST test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the ImageEBM with mixed activations\n",
        "class ImageEBM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEBM, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.fc1 = nn.Linear(8 * 28 * 28, 10)\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))  # ReLU after convolution\n",
        "        x = x.view(-1, 8 * 28 * 28)\n",
        "        x = torch.sigmoid(self.fc1(x))  # Sigmoid in fully connected layer\n",
        "        energy = self.fc2(x)\n",
        "        return energy\n",
        "\n",
        "# Initialize the EBM, optimizer, and learning rate scheduler\n",
        "model = ImageEBM()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop with reduced energy regularization\n",
        "num_epochs = 5\n",
        "alpha = 0.1  # Reduced regularization strength\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        energy = model(data)\n",
        "\n",
        "        # Add energy regularization to the loss\n",
        "        loss = torch.mean(energy) + alpha * torch.mean(torch.relu(-energy))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    scheduler.step()  # Update learning rate\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            energy = model(data)\n",
        "            test_loss += torch.mean(energy).item()  # Sum up batch loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    #print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XiU23XyvOjY",
        "outputId": "4ee8a377-33ce-4eb8-bf7d-24a7be013062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [1/938], Loss: 0.4417\n",
            "Epoch [1/5], Batch [101/938], Loss: -0.3597\n",
            "Epoch [1/5], Batch [201/938], Loss: -0.4088\n",
            "Epoch [1/5], Batch [301/938], Loss: -0.4551\n",
            "Epoch [1/5], Batch [401/938], Loss: -0.5005\n",
            "Epoch [1/5], Batch [501/938], Loss: -0.5460\n",
            "Epoch [1/5], Batch [601/938], Loss: -0.5912\n",
            "Epoch [1/5], Batch [701/938], Loss: -0.6365\n",
            "Epoch [1/5], Batch [801/938], Loss: -0.6816\n",
            "Epoch [1/5], Batch [901/938], Loss: -0.7267\n",
            "Epoch [2/5], Batch [1/938], Loss: -0.7439\n",
            "Epoch [2/5], Batch [101/938], Loss: -0.7890\n",
            "Epoch [2/5], Batch [201/938], Loss: -0.8341\n",
            "Epoch [2/5], Batch [301/938], Loss: -0.8791\n",
            "Epoch [2/5], Batch [401/938], Loss: -0.9242\n",
            "Epoch [2/5], Batch [501/938], Loss: -0.9692\n",
            "Epoch [2/5], Batch [601/938], Loss: -1.0143\n",
            "Epoch [2/5], Batch [701/938], Loss: -1.0593\n",
            "Epoch [2/5], Batch [801/938], Loss: -1.1044\n",
            "Epoch [2/5], Batch [901/938], Loss: -1.1494\n",
            "Epoch [3/5], Batch [1/938], Loss: -1.1665\n",
            "Epoch [3/5], Batch [101/938], Loss: -1.2115\n",
            "Epoch [3/5], Batch [201/938], Loss: -1.2566\n",
            "Epoch [3/5], Batch [301/938], Loss: -1.3016\n",
            "Epoch [3/5], Batch [401/938], Loss: -1.3466\n",
            "Epoch [3/5], Batch [501/938], Loss: -1.3916\n",
            "Epoch [3/5], Batch [601/938], Loss: -1.4366\n",
            "Epoch [3/5], Batch [701/938], Loss: -1.4816\n",
            "Epoch [3/5], Batch [801/938], Loss: -1.5267\n",
            "Epoch [3/5], Batch [901/938], Loss: -1.5717\n",
            "Epoch [4/5], Batch [1/938], Loss: -1.5888\n",
            "Epoch [4/5], Batch [101/938], Loss: -1.6338\n",
            "Epoch [4/5], Batch [201/938], Loss: -1.6788\n",
            "Epoch [4/5], Batch [301/938], Loss: -1.7238\n",
            "Epoch [4/5], Batch [401/938], Loss: -1.7688\n",
            "Epoch [4/5], Batch [501/938], Loss: -1.8138\n",
            "Epoch [4/5], Batch [601/938], Loss: -1.8588\n",
            "Epoch [4/5], Batch [701/938], Loss: -1.9038\n",
            "Epoch [4/5], Batch [801/938], Loss: -1.9488\n",
            "Epoch [4/5], Batch [901/938], Loss: -1.9938\n",
            "Epoch [5/5], Batch [1/938], Loss: -2.0109\n",
            "Epoch [5/5], Batch [101/938], Loss: -2.0559\n",
            "Epoch [5/5], Batch [201/938], Loss: -2.1009\n",
            "Epoch [5/5], Batch [301/938], Loss: -2.1459\n",
            "Epoch [5/5], Batch [401/938], Loss: -2.1909\n",
            "Epoch [5/5], Batch [501/938], Loss: -2.2359\n",
            "Epoch [5/5], Batch [601/938], Loss: -2.2810\n",
            "Epoch [5/5], Batch [701/938], Loss: -2.3260\n",
            "Epoch [5/5], Batch [801/938], Loss: -2.3710\n",
            "Epoch [5/5], Batch [901/938], Loss: -2.4160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            energy = model(data)\n",
        "            test_loss += torch.mean(energy).item()  # Sum up batch loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Eval Loss: {test_loss:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrkfus8a35qD",
        "outputId": "4b228e37-9e6c-4324-a25d-732da0b8b4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Loss: -3.1242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10 Image Classification"
      ],
      "metadata": {
        "id": "-_VmeqWUEm5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the ImageEBM for CIFAR-10 (color images)\n",
        "class ImageEBM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEBM, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Input channels = 3 for color\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Adjust input size for CIFAR-10\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Adjust size for CIFAR-10\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        energy = self.fc2(x)\n",
        "        return energy\n",
        "\n",
        "# Initialize the EBM, optimizer, and learning rate scheduler\n",
        "model = ImageEBM()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop with energy regularization\n",
        "num_epochs = 5  # You might need more epochs for CIFAR-10\n",
        "alpha = 0.1\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        energy = model(data)\n",
        "\n",
        "        # Add energy regularization to the loss\n",
        "        loss = torch.mean(energy) + alpha * torch.mean(torch.relu(-energy))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    scheduler.step()  # Update learning rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaXsg-yI5MJU",
        "outputId": "9fa1f325-f57d-4403-d7ff-44c98d8b05d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/5], Batch [1/782], Loss: -0.3356\n",
            "Epoch [1/5], Batch [101/782], Loss: -3.6440\n",
            "Epoch [1/5], Batch [201/782], Loss: -4.3777\n",
            "Epoch [1/5], Batch [301/782], Loss: -5.0892\n",
            "Epoch [1/5], Batch [401/782], Loss: -5.8079\n",
            "Epoch [1/5], Batch [501/782], Loss: -6.5292\n",
            "Epoch [1/5], Batch [601/782], Loss: -7.2430\n",
            "Epoch [1/5], Batch [701/782], Loss: -7.9535\n",
            "Epoch [2/5], Batch [1/782], Loss: -8.5346\n",
            "Epoch [2/5], Batch [101/782], Loss: -9.2422\n",
            "Epoch [2/5], Batch [201/782], Loss: -9.9488\n",
            "Epoch [2/5], Batch [301/782], Loss: -10.6550\n",
            "Epoch [2/5], Batch [401/782], Loss: -11.3972\n",
            "Epoch [2/5], Batch [501/782], Loss: -12.1229\n",
            "Epoch [2/5], Batch [601/782], Loss: -12.8440\n",
            "Epoch [2/5], Batch [701/782], Loss: -13.5626\n",
            "Epoch [3/5], Batch [1/782], Loss: -14.1661\n",
            "Epoch [3/5], Batch [101/782], Loss: -14.9152\n",
            "Epoch [3/5], Batch [201/782], Loss: -15.6515\n",
            "Epoch [3/5], Batch [301/782], Loss: -16.4029\n",
            "Epoch [3/5], Batch [401/782], Loss: -17.1639\n",
            "Epoch [3/5], Batch [501/782], Loss: -17.9122\n",
            "Epoch [3/5], Batch [601/782], Loss: -18.6556\n",
            "Epoch [3/5], Batch [701/782], Loss: -19.3960\n",
            "Epoch [4/5], Batch [1/782], Loss: -20.0016\n",
            "Epoch [4/5], Batch [101/782], Loss: -20.7388\n",
            "Epoch [4/5], Batch [201/782], Loss: -21.4747\n",
            "Epoch [4/5], Batch [301/782], Loss: -22.2097\n",
            "Epoch [4/5], Batch [401/782], Loss: -22.9438\n",
            "Epoch [4/5], Batch [501/782], Loss: -23.6773\n",
            "Epoch [4/5], Batch [601/782], Loss: -24.4102\n",
            "Epoch [4/5], Batch [701/782], Loss: -25.1426\n",
            "Epoch [5/5], Batch [1/782], Loss: -25.7429\n",
            "Epoch [5/5], Batch [101/782], Loss: -26.4747\n",
            "Epoch [5/5], Batch [201/782], Loss: -27.2061\n",
            "Epoch [5/5], Batch [301/782], Loss: -27.9372\n",
            "Epoch [5/5], Batch [401/782], Loss: -28.6682\n",
            "Epoch [5/5], Batch [501/782], Loss: -29.3989\n",
            "Epoch [5/5], Batch [601/782], Loss: -30.1295\n",
            "Epoch [5/5], Batch [701/782], Loss: -30.8599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load CIFAR-10 test dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            energy = model(data)\n",
        "            test_loss += torch.mean(energy).item()  # Sum up batch loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Eval Loss: {test_loss:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRWXRN7752QM",
        "outputId": "38b8c492-41cf-4038-eae8-dd2306437dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Eval Loss: -34.9541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10 Image Classification - Avoid Overfitting  \n",
        "\n"
      ],
      "metadata": {
        "id": "NSyIMybCrqyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting is a modeling error that occurs when a model is too closely aligned to a specific set of data, resulting in poor predictions for new data"
      ],
      "metadata": {
        "id": "JmrIcjr_H56p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F  # Import functional for LeakyReLU\n",
        "\n",
        "# --- Data Augmentation ---\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# --- Data Loading ---\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class ImageEBM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEBM, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1)  # Reduced filters\n",
        "        self.bn1 = nn.BatchNorm2d(4)\n",
        "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)  # Reduced filters\n",
        "        self.bn2 = nn.BatchNorm2d(8)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 16)  # Reduced neurons\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(p=0.4)  # Increased dropout rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.01)\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.01)\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 8 * 8 * 8)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        energy = self.fc2(x)\n",
        "        return energy\n",
        "\n",
        "# --- Model Initialization ---\n",
        "model = ImageEBM()\n",
        "\n",
        "# --- Optimizer and Scheduler ---\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=0.01)  # Reduced learning rate, increased weight decay\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)  # More aggressive LR scheduling\n",
        "\n",
        "# --- Training Loop with Early Stopping ---\n",
        "def train(model, train_loader, test_loader, optimizer, scheduler, num_epochs, alpha, patience=5):\n",
        "    best_eval_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            energy = model(data)\n",
        "            loss = torch.mean(energy) + alpha * torch.mean(torch.relu(-energy))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Evaluate the model after each epoch\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, _ in test_loader:\n",
        "                energy = model(data)\n",
        "                test_loss += torch.mean(energy).item()\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        print(f\"Eval Loss: {test_loss:.4f}\")\n",
        "\n",
        "        scheduler.step(test_loss)  # Use test loss for LR scheduling\n",
        "\n",
        "        # Early stopping check\n",
        "        if test_loss < best_eval_loss:\n",
        "            best_eval_loss = test_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "# --- Train the Model ---\n",
        "num_epochs = 50\n",
        "alpha = 1000  # Significantly increased alpha\n",
        "train(model, train_loader, test_loader, optimizer, scheduler, num_epochs, alpha)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            energy = model(data)\n",
        "            test_loss += torch.mean(energy).item()  # Sum up batch loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Eval Loss: {test_loss:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7YWpd2a_uMg",
        "outputId": "3a6c01eb-001d-42e1-ff51-ddcaf7960e23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/50], Batch [1/782], Loss: 14.9660\n",
            "Epoch [1/50], Batch [101/782], Loss: 0.2232\n",
            "Epoch [1/50], Batch [201/782], Loss: 0.2384\n",
            "Epoch [1/50], Batch [301/782], Loss: 0.5560\n",
            "Epoch [1/50], Batch [401/782], Loss: 0.2187\n",
            "Epoch [1/50], Batch [501/782], Loss: 0.2130\n",
            "Epoch [1/50], Batch [601/782], Loss: 0.2062\n",
            "Epoch [1/50], Batch [701/782], Loss: 0.2179\n",
            "Eval Loss: 0.1531\n",
            "Epoch [2/50], Batch [1/782], Loss: 0.2182\n",
            "Epoch [2/50], Batch [101/782], Loss: 0.2050\n",
            "Epoch [2/50], Batch [201/782], Loss: 0.1982\n",
            "Epoch [2/50], Batch [301/782], Loss: 0.2004\n",
            "Epoch [2/50], Batch [401/782], Loss: 0.2096\n",
            "Epoch [2/50], Batch [501/782], Loss: 0.3405\n",
            "Epoch [2/50], Batch [601/782], Loss: 0.1677\n",
            "Epoch [2/50], Batch [701/782], Loss: 0.2142\n",
            "Eval Loss: 0.1458\n",
            "Epoch [3/50], Batch [1/782], Loss: 0.1773\n",
            "Epoch [3/50], Batch [101/782], Loss: 0.1832\n",
            "Epoch [3/50], Batch [201/782], Loss: 0.1833\n",
            "Epoch [3/50], Batch [301/782], Loss: 0.1685\n",
            "Epoch [3/50], Batch [401/782], Loss: 0.1594\n",
            "Epoch [3/50], Batch [501/782], Loss: 0.1827\n",
            "Epoch [3/50], Batch [601/782], Loss: 0.1744\n",
            "Epoch [3/50], Batch [701/782], Loss: 0.1605\n",
            "Eval Loss: 0.1254\n",
            "Epoch [4/50], Batch [1/782], Loss: 0.1574\n",
            "Epoch [4/50], Batch [101/782], Loss: 0.1707\n",
            "Epoch [4/50], Batch [201/782], Loss: 0.1329\n",
            "Epoch [4/50], Batch [301/782], Loss: 0.1615\n",
            "Epoch [4/50], Batch [401/782], Loss: 0.1591\n",
            "Epoch [4/50], Batch [501/782], Loss: 0.1319\n",
            "Epoch [4/50], Batch [601/782], Loss: 0.1421\n",
            "Epoch [4/50], Batch [701/782], Loss: 0.5422\n",
            "Eval Loss: 0.1104\n",
            "Epoch [5/50], Batch [1/782], Loss: 0.1406\n",
            "Epoch [5/50], Batch [101/782], Loss: 0.1243\n",
            "Epoch [5/50], Batch [201/782], Loss: 0.1055\n",
            "Epoch [5/50], Batch [301/782], Loss: 0.1326\n",
            "Epoch [5/50], Batch [401/782], Loss: 0.1362\n",
            "Epoch [5/50], Batch [501/782], Loss: 0.1142\n",
            "Epoch [5/50], Batch [601/782], Loss: 0.1125\n",
            "Epoch [5/50], Batch [701/782], Loss: 0.1056\n",
            "Eval Loss: 0.0852\n",
            "Epoch [6/50], Batch [1/782], Loss: 0.1046\n",
            "Epoch [6/50], Batch [101/782], Loss: 0.1158\n",
            "Epoch [6/50], Batch [201/782], Loss: 0.1102\n",
            "Epoch [6/50], Batch [301/782], Loss: 0.0859\n",
            "Epoch [6/50], Batch [401/782], Loss: 0.1005\n",
            "Epoch [6/50], Batch [501/782], Loss: 0.0836\n",
            "Epoch [6/50], Batch [601/782], Loss: 0.0791\n",
            "Epoch [6/50], Batch [701/782], Loss: 0.0955\n",
            "Eval Loss: 0.0811\n",
            "Epoch [7/50], Batch [1/782], Loss: 0.0833\n",
            "Epoch [7/50], Batch [101/782], Loss: 0.0854\n",
            "Epoch [7/50], Batch [201/782], Loss: 0.1018\n",
            "Epoch [7/50], Batch [301/782], Loss: 0.0792\n",
            "Epoch [7/50], Batch [401/782], Loss: 0.0738\n",
            "Epoch [7/50], Batch [501/782], Loss: 0.0784\n",
            "Epoch [7/50], Batch [601/782], Loss: 0.0645\n",
            "Epoch [7/50], Batch [701/782], Loss: 0.0655\n",
            "Eval Loss: 0.0696\n",
            "Epoch [8/50], Batch [1/782], Loss: 0.0743\n",
            "Epoch [8/50], Batch [101/782], Loss: 0.0682\n",
            "Epoch [8/50], Batch [201/782], Loss: 0.0649\n",
            "Epoch [8/50], Batch [301/782], Loss: 0.0725\n",
            "Epoch [8/50], Batch [401/782], Loss: 0.0741\n",
            "Epoch [8/50], Batch [501/782], Loss: 0.0546\n",
            "Epoch [8/50], Batch [601/782], Loss: 0.0935\n",
            "Epoch [8/50], Batch [701/782], Loss: 0.0505\n",
            "Eval Loss: 0.0572\n",
            "Epoch [9/50], Batch [1/782], Loss: 0.0644\n",
            "Epoch [9/50], Batch [101/782], Loss: 0.0520\n",
            "Epoch [9/50], Batch [201/782], Loss: 0.0631\n",
            "Epoch [9/50], Batch [301/782], Loss: 0.0561\n",
            "Epoch [9/50], Batch [401/782], Loss: 0.0582\n",
            "Epoch [9/50], Batch [501/782], Loss: 0.0630\n",
            "Epoch [9/50], Batch [601/782], Loss: 0.0447\n",
            "Epoch [9/50], Batch [701/782], Loss: 0.0417\n",
            "Eval Loss: 0.0456\n",
            "Epoch [10/50], Batch [1/782], Loss: 0.0509\n",
            "Epoch [10/50], Batch [101/782], Loss: 0.1529\n",
            "Epoch [10/50], Batch [201/782], Loss: 0.0405\n",
            "Epoch [10/50], Batch [301/782], Loss: 0.0582\n",
            "Epoch [10/50], Batch [401/782], Loss: 0.0546\n",
            "Epoch [10/50], Batch [501/782], Loss: 0.0411\n",
            "Epoch [10/50], Batch [601/782], Loss: 0.0492\n",
            "Epoch [10/50], Batch [701/782], Loss: 0.0376\n",
            "Eval Loss: 0.0419\n",
            "Epoch [11/50], Batch [1/782], Loss: 0.0481\n",
            "Epoch [11/50], Batch [101/782], Loss: 0.0871\n",
            "Epoch [11/50], Batch [201/782], Loss: 0.0407\n",
            "Epoch [11/50], Batch [301/782], Loss: 0.0460\n",
            "Epoch [11/50], Batch [401/782], Loss: 0.0418\n",
            "Epoch [11/50], Batch [501/782], Loss: 0.0422\n",
            "Epoch [11/50], Batch [601/782], Loss: 0.0251\n",
            "Epoch [11/50], Batch [701/782], Loss: 0.0251\n",
            "Eval Loss: 0.0284\n",
            "Epoch [12/50], Batch [1/782], Loss: 0.0352\n",
            "Epoch [12/50], Batch [101/782], Loss: 0.0292\n",
            "Epoch [12/50], Batch [201/782], Loss: 0.0419\n",
            "Epoch [12/50], Batch [301/782], Loss: 0.0558\n",
            "Epoch [12/50], Batch [401/782], Loss: 0.0289\n",
            "Epoch [12/50], Batch [501/782], Loss: 0.0330\n",
            "Epoch [12/50], Batch [601/782], Loss: 0.0314\n",
            "Epoch [12/50], Batch [701/782], Loss: 0.0324\n",
            "Eval Loss: 0.0252\n",
            "Epoch [13/50], Batch [1/782], Loss: 0.0337\n",
            "Epoch [13/50], Batch [101/782], Loss: 0.0376\n",
            "Epoch [13/50], Batch [201/782], Loss: 0.0334\n",
            "Epoch [13/50], Batch [301/782], Loss: 0.0486\n",
            "Epoch [13/50], Batch [401/782], Loss: 0.0374\n",
            "Epoch [13/50], Batch [501/782], Loss: 0.0318\n",
            "Epoch [13/50], Batch [601/782], Loss: 0.0293\n",
            "Epoch [13/50], Batch [701/782], Loss: 0.0380\n",
            "Eval Loss: 0.0303\n",
            "Epoch [14/50], Batch [1/782], Loss: 0.0356\n",
            "Epoch [14/50], Batch [101/782], Loss: 0.0240\n",
            "Epoch [14/50], Batch [201/782], Loss: 0.0256\n",
            "Epoch [14/50], Batch [301/782], Loss: 0.0359\n",
            "Epoch [14/50], Batch [401/782], Loss: 0.0258\n",
            "Epoch [14/50], Batch [501/782], Loss: 0.0247\n",
            "Epoch [14/50], Batch [601/782], Loss: 0.0246\n",
            "Epoch [14/50], Batch [701/782], Loss: 0.0503\n",
            "Eval Loss: 0.0189\n",
            "Epoch [15/50], Batch [1/782], Loss: 0.0813\n",
            "Epoch [15/50], Batch [101/782], Loss: 0.0321\n",
            "Epoch [15/50], Batch [201/782], Loss: 0.0238\n",
            "Epoch [15/50], Batch [301/782], Loss: 0.0269\n",
            "Epoch [15/50], Batch [401/782], Loss: 0.0206\n",
            "Epoch [15/50], Batch [501/782], Loss: 0.0272\n",
            "Epoch [15/50], Batch [601/782], Loss: 0.0214\n",
            "Epoch [15/50], Batch [701/782], Loss: 0.0233\n",
            "Eval Loss: 0.0255\n",
            "Epoch [16/50], Batch [1/782], Loss: 0.0320\n",
            "Epoch [16/50], Batch [101/782], Loss: 0.0291\n",
            "Epoch [16/50], Batch [201/782], Loss: 0.0279\n",
            "Epoch [16/50], Batch [301/782], Loss: 0.0274\n",
            "Epoch [16/50], Batch [401/782], Loss: 0.0269\n",
            "Epoch [16/50], Batch [501/782], Loss: 0.0332\n",
            "Epoch [16/50], Batch [601/782], Loss: 0.0230\n",
            "Epoch [16/50], Batch [701/782], Loss: 0.0213\n",
            "Eval Loss: 0.0241\n",
            "Epoch [17/50], Batch [1/782], Loss: 0.0310\n",
            "Epoch [17/50], Batch [101/782], Loss: 0.0202\n",
            "Epoch [17/50], Batch [201/782], Loss: 0.0237\n",
            "Epoch [17/50], Batch [301/782], Loss: 0.0229\n",
            "Epoch [17/50], Batch [401/782], Loss: 0.0179\n",
            "Epoch [17/50], Batch [501/782], Loss: 0.0296\n",
            "Epoch [17/50], Batch [601/782], Loss: 0.0213\n",
            "Epoch [17/50], Batch [701/782], Loss: 0.0230\n",
            "Eval Loss: 0.0136\n",
            "Epoch [18/50], Batch [1/782], Loss: 0.0192\n",
            "Epoch [18/50], Batch [101/782], Loss: 0.0224\n",
            "Epoch [18/50], Batch [201/782], Loss: 0.0168\n",
            "Epoch [18/50], Batch [301/782], Loss: 0.0250\n",
            "Epoch [18/50], Batch [401/782], Loss: 0.0168\n",
            "Epoch [18/50], Batch [501/782], Loss: 0.0176\n",
            "Epoch [18/50], Batch [601/782], Loss: 0.0248\n",
            "Epoch [18/50], Batch [701/782], Loss: 0.0187\n",
            "Eval Loss: 0.0134\n",
            "Epoch [19/50], Batch [1/782], Loss: 0.0159\n",
            "Epoch [19/50], Batch [101/782], Loss: 0.0201\n",
            "Epoch [19/50], Batch [201/782], Loss: 0.0163\n",
            "Epoch [19/50], Batch [301/782], Loss: 0.0158\n",
            "Epoch [19/50], Batch [401/782], Loss: 0.0229\n",
            "Epoch [19/50], Batch [501/782], Loss: 0.0157\n",
            "Epoch [19/50], Batch [601/782], Loss: 0.0155\n",
            "Epoch [19/50], Batch [701/782], Loss: 0.0207\n",
            "Eval Loss: 0.0144\n",
            "Epoch [20/50], Batch [1/782], Loss: 0.0185\n",
            "Epoch [20/50], Batch [101/782], Loss: 0.0171\n",
            "Epoch [20/50], Batch [201/782], Loss: 0.0157\n",
            "Epoch [20/50], Batch [301/782], Loss: 0.0243\n",
            "Epoch [20/50], Batch [401/782], Loss: 0.0177\n",
            "Epoch [20/50], Batch [501/782], Loss: 0.0280\n",
            "Epoch [20/50], Batch [601/782], Loss: 0.0208\n",
            "Epoch [20/50], Batch [701/782], Loss: 0.0184\n",
            "Eval Loss: 0.0090\n",
            "Epoch [21/50], Batch [1/782], Loss: 0.0097\n",
            "Epoch [21/50], Batch [101/782], Loss: 0.0153\n",
            "Epoch [21/50], Batch [201/782], Loss: 0.0153\n",
            "Epoch [21/50], Batch [301/782], Loss: 0.0110\n",
            "Epoch [21/50], Batch [401/782], Loss: 0.0121\n",
            "Epoch [21/50], Batch [501/782], Loss: 0.0141\n",
            "Epoch [21/50], Batch [601/782], Loss: 0.0166\n",
            "Epoch [21/50], Batch [701/782], Loss: 0.0146\n",
            "Eval Loss: 0.0155\n",
            "Epoch [22/50], Batch [1/782], Loss: 0.0152\n",
            "Epoch [22/50], Batch [101/782], Loss: 0.0181\n",
            "Epoch [22/50], Batch [201/782], Loss: 0.0137\n",
            "Epoch [22/50], Batch [301/782], Loss: 0.0154\n",
            "Epoch [22/50], Batch [401/782], Loss: 0.0125\n",
            "Epoch [22/50], Batch [501/782], Loss: 0.0160\n",
            "Epoch [22/50], Batch [601/782], Loss: 0.0221\n",
            "Epoch [22/50], Batch [701/782], Loss: 0.0155\n",
            "Eval Loss: 0.0118\n",
            "Epoch [23/50], Batch [1/782], Loss: 0.0137\n",
            "Epoch [23/50], Batch [101/782], Loss: 0.0116\n",
            "Epoch [23/50], Batch [201/782], Loss: 0.0110\n",
            "Epoch [23/50], Batch [301/782], Loss: 0.0157\n",
            "Epoch [23/50], Batch [401/782], Loss: 0.0152\n",
            "Epoch [23/50], Batch [501/782], Loss: 0.0110\n",
            "Epoch [23/50], Batch [601/782], Loss: 0.0143\n",
            "Epoch [23/50], Batch [701/782], Loss: 0.0155\n",
            "Eval Loss: 0.0117\n",
            "Epoch [24/50], Batch [1/782], Loss: 0.0159\n",
            "Epoch [24/50], Batch [101/782], Loss: 0.0165\n",
            "Epoch [24/50], Batch [201/782], Loss: 0.0144\n",
            "Epoch [24/50], Batch [301/782], Loss: 0.0126\n",
            "Epoch [24/50], Batch [401/782], Loss: 0.0117\n",
            "Epoch [24/50], Batch [501/782], Loss: 0.0121\n",
            "Epoch [24/50], Batch [601/782], Loss: 0.0121\n",
            "Epoch [24/50], Batch [701/782], Loss: 0.0097\n",
            "Eval Loss: 0.0077\n",
            "Epoch [25/50], Batch [1/782], Loss: 0.0096\n",
            "Epoch [25/50], Batch [101/782], Loss: 0.0085\n",
            "Epoch [25/50], Batch [201/782], Loss: 0.0091\n",
            "Epoch [25/50], Batch [301/782], Loss: 0.0102\n",
            "Epoch [25/50], Batch [401/782], Loss: 0.0073\n",
            "Epoch [25/50], Batch [501/782], Loss: 0.0081\n",
            "Epoch [25/50], Batch [601/782], Loss: 0.0093\n",
            "Epoch [25/50], Batch [701/782], Loss: 0.0110\n",
            "Eval Loss: 0.0045\n",
            "Epoch [26/50], Batch [1/782], Loss: 0.0072\n",
            "Epoch [26/50], Batch [101/782], Loss: 0.0063\n",
            "Epoch [26/50], Batch [201/782], Loss: 0.0075\n",
            "Epoch [26/50], Batch [301/782], Loss: 0.0404\n",
            "Epoch [26/50], Batch [401/782], Loss: 0.0119\n",
            "Epoch [26/50], Batch [501/782], Loss: 0.0063\n",
            "Epoch [26/50], Batch [601/782], Loss: 0.0078\n",
            "Epoch [26/50], Batch [701/782], Loss: 0.0081\n",
            "Eval Loss: 0.0066\n",
            "Epoch [27/50], Batch [1/782], Loss: 0.0081\n",
            "Epoch [27/50], Batch [101/782], Loss: 0.0063\n",
            "Epoch [27/50], Batch [201/782], Loss: 0.0086\n",
            "Epoch [27/50], Batch [301/782], Loss: 0.0076\n",
            "Epoch [27/50], Batch [401/782], Loss: 0.0088\n",
            "Epoch [27/50], Batch [501/782], Loss: 0.0080\n",
            "Epoch [27/50], Batch [601/782], Loss: 0.0077\n",
            "Epoch [27/50], Batch [701/782], Loss: 0.0087\n",
            "Eval Loss: 0.0075\n",
            "Epoch [28/50], Batch [1/782], Loss: 0.0092\n",
            "Epoch [28/50], Batch [101/782], Loss: 0.0090\n",
            "Epoch [28/50], Batch [201/782], Loss: 0.0065\n",
            "Epoch [28/50], Batch [301/782], Loss: 0.0100\n",
            "Epoch [28/50], Batch [401/782], Loss: 0.0150\n",
            "Epoch [28/50], Batch [501/782], Loss: 0.0108\n",
            "Epoch [28/50], Batch [601/782], Loss: 0.0080\n",
            "Epoch [28/50], Batch [701/782], Loss: 0.0079\n",
            "Eval Loss: 0.0049\n",
            "Epoch [29/50], Batch [1/782], Loss: 0.0055\n",
            "Epoch [29/50], Batch [101/782], Loss: 0.0073\n",
            "Epoch [29/50], Batch [201/782], Loss: 0.0073\n",
            "Epoch [29/50], Batch [301/782], Loss: 0.0071\n",
            "Epoch [29/50], Batch [401/782], Loss: 0.0048\n",
            "Epoch [29/50], Batch [501/782], Loss: 0.0053\n",
            "Epoch [29/50], Batch [601/782], Loss: 0.0053\n",
            "Epoch [29/50], Batch [701/782], Loss: 0.0064\n",
            "Eval Loss: 0.0043\n",
            "Epoch [30/50], Batch [1/782], Loss: 0.0054\n",
            "Epoch [30/50], Batch [101/782], Loss: 0.0061\n",
            "Epoch [30/50], Batch [201/782], Loss: 0.0061\n",
            "Epoch [30/50], Batch [301/782], Loss: 0.0061\n",
            "Epoch [30/50], Batch [401/782], Loss: 0.0065\n",
            "Epoch [30/50], Batch [501/782], Loss: 0.0063\n",
            "Epoch [30/50], Batch [601/782], Loss: 0.0088\n",
            "Epoch [30/50], Batch [701/782], Loss: 0.0062\n",
            "Eval Loss: 0.0066\n",
            "Epoch [31/50], Batch [1/782], Loss: 0.0086\n",
            "Epoch [31/50], Batch [101/782], Loss: 0.0069\n",
            "Epoch [31/50], Batch [201/782], Loss: 0.0285\n",
            "Epoch [31/50], Batch [301/782], Loss: 0.0048\n",
            "Epoch [31/50], Batch [401/782], Loss: 0.0063\n",
            "Epoch [31/50], Batch [501/782], Loss: 0.0056\n",
            "Epoch [31/50], Batch [601/782], Loss: 0.0051\n",
            "Epoch [31/50], Batch [701/782], Loss: 0.0067\n",
            "Eval Loss: 0.0045\n",
            "Epoch [32/50], Batch [1/782], Loss: 0.0056\n",
            "Epoch [32/50], Batch [101/782], Loss: 0.0060\n",
            "Epoch [32/50], Batch [201/782], Loss: 0.0056\n",
            "Epoch [32/50], Batch [301/782], Loss: 0.0053\n",
            "Epoch [32/50], Batch [401/782], Loss: 0.0056\n",
            "Epoch [32/50], Batch [501/782], Loss: 0.0055\n",
            "Epoch [32/50], Batch [601/782], Loss: 0.0066\n",
            "Epoch [32/50], Batch [701/782], Loss: 0.0042\n",
            "Eval Loss: 0.0047\n",
            "Epoch [33/50], Batch [1/782], Loss: 0.0055\n",
            "Epoch [33/50], Batch [101/782], Loss: 0.0046\n",
            "Epoch [33/50], Batch [201/782], Loss: 0.0134\n",
            "Epoch [33/50], Batch [301/782], Loss: 0.0047\n",
            "Epoch [33/50], Batch [401/782], Loss: 0.0052\n",
            "Epoch [33/50], Batch [501/782], Loss: 0.0044\n",
            "Epoch [33/50], Batch [601/782], Loss: 0.0040\n",
            "Epoch [33/50], Batch [701/782], Loss: 0.0056\n",
            "Eval Loss: 0.0043\n",
            "Epoch [34/50], Batch [1/782], Loss: 0.0051\n",
            "Epoch [34/50], Batch [101/782], Loss: 0.0054\n",
            "Epoch [34/50], Batch [201/782], Loss: 0.0045\n",
            "Epoch [34/50], Batch [301/782], Loss: 0.0062\n",
            "Epoch [34/50], Batch [401/782], Loss: 0.0048\n",
            "Epoch [34/50], Batch [501/782], Loss: 0.0050\n",
            "Epoch [34/50], Batch [601/782], Loss: 0.0042\n",
            "Epoch [34/50], Batch [701/782], Loss: 0.0037\n",
            "Eval Loss: 0.0034\n",
            "Epoch [35/50], Batch [1/782], Loss: 0.0047\n",
            "Epoch [35/50], Batch [101/782], Loss: 0.0055\n",
            "Epoch [35/50], Batch [201/782], Loss: 0.0041\n",
            "Epoch [35/50], Batch [301/782], Loss: 0.0039\n",
            "Epoch [35/50], Batch [401/782], Loss: 0.0037\n",
            "Epoch [35/50], Batch [501/782], Loss: 0.0046\n",
            "Epoch [35/50], Batch [601/782], Loss: 0.0046\n",
            "Epoch [35/50], Batch [701/782], Loss: 0.0039\n",
            "Eval Loss: 0.0032\n",
            "Epoch [36/50], Batch [1/782], Loss: 0.0043\n",
            "Epoch [36/50], Batch [101/782], Loss: 0.0045\n",
            "Epoch [36/50], Batch [201/782], Loss: 0.0051\n",
            "Epoch [36/50], Batch [301/782], Loss: 0.0033\n",
            "Epoch [36/50], Batch [401/782], Loss: 0.0044\n",
            "Epoch [36/50], Batch [501/782], Loss: 0.0043\n",
            "Epoch [36/50], Batch [601/782], Loss: 0.0042\n",
            "Epoch [36/50], Batch [701/782], Loss: 0.0040\n",
            "Eval Loss: 0.0024\n",
            "Epoch [37/50], Batch [1/782], Loss: 0.0039\n",
            "Epoch [37/50], Batch [101/782], Loss: 0.0040\n",
            "Epoch [37/50], Batch [201/782], Loss: 0.0041\n",
            "Epoch [37/50], Batch [301/782], Loss: 0.0043\n",
            "Epoch [37/50], Batch [401/782], Loss: 0.0069\n",
            "Epoch [37/50], Batch [501/782], Loss: 0.0038\n",
            "Epoch [37/50], Batch [601/782], Loss: 0.0035\n",
            "Epoch [37/50], Batch [701/782], Loss: 0.0050\n",
            "Eval Loss: 0.0021\n",
            "Epoch [38/50], Batch [1/782], Loss: 0.0033\n",
            "Epoch [38/50], Batch [101/782], Loss: 0.0030\n",
            "Epoch [38/50], Batch [201/782], Loss: 0.0039\n",
            "Epoch [38/50], Batch [301/782], Loss: 0.0040\n",
            "Epoch [38/50], Batch [401/782], Loss: 0.0042\n",
            "Epoch [38/50], Batch [501/782], Loss: 0.0032\n",
            "Epoch [38/50], Batch [601/782], Loss: 0.0049\n",
            "Epoch [38/50], Batch [701/782], Loss: 0.0044\n",
            "Eval Loss: 0.0025\n",
            "Epoch [39/50], Batch [1/782], Loss: 0.0037\n",
            "Epoch [39/50], Batch [101/782], Loss: 0.0033\n",
            "Epoch [39/50], Batch [201/782], Loss: 0.0030\n",
            "Epoch [39/50], Batch [301/782], Loss: 0.0041\n",
            "Epoch [39/50], Batch [401/782], Loss: 0.0045\n",
            "Epoch [39/50], Batch [501/782], Loss: 0.0032\n",
            "Epoch [39/50], Batch [601/782], Loss: 0.0033\n",
            "Epoch [39/50], Batch [701/782], Loss: 0.0047\n",
            "Eval Loss: 0.0032\n",
            "Epoch [40/50], Batch [1/782], Loss: 0.0042\n",
            "Epoch [40/50], Batch [101/782], Loss: 0.0040\n",
            "Epoch [40/50], Batch [201/782], Loss: 0.0046\n",
            "Epoch [40/50], Batch [301/782], Loss: 0.0038\n",
            "Epoch [40/50], Batch [401/782], Loss: 0.0040\n",
            "Epoch [40/50], Batch [501/782], Loss: 0.0034\n",
            "Epoch [40/50], Batch [601/782], Loss: 0.0037\n",
            "Epoch [40/50], Batch [701/782], Loss: 0.0030\n",
            "Eval Loss: 0.0027\n",
            "Epoch [41/50], Batch [1/782], Loss: 0.0035\n",
            "Epoch [41/50], Batch [101/782], Loss: 0.0034\n",
            "Epoch [41/50], Batch [201/782], Loss: 0.0035\n",
            "Epoch [41/50], Batch [301/782], Loss: 0.0031\n",
            "Epoch [41/50], Batch [401/782], Loss: 0.0034\n",
            "Epoch [41/50], Batch [501/782], Loss: 0.0035\n",
            "Epoch [41/50], Batch [601/782], Loss: 0.0034\n",
            "Epoch [41/50], Batch [701/782], Loss: 0.0033\n",
            "Eval Loss: 0.0024\n",
            "Epoch [42/50], Batch [1/782], Loss: 0.0031\n",
            "Epoch [42/50], Batch [101/782], Loss: 0.0027\n",
            "Epoch [42/50], Batch [201/782], Loss: 0.0038\n",
            "Epoch [42/50], Batch [301/782], Loss: 0.0183\n",
            "Epoch [42/50], Batch [401/782], Loss: 0.0037\n",
            "Epoch [42/50], Batch [501/782], Loss: 0.0034\n",
            "Epoch [42/50], Batch [601/782], Loss: 0.0036\n",
            "Epoch [42/50], Batch [701/782], Loss: 0.0042\n",
            "Eval Loss: 0.0023\n",
            "Early stopping at epoch 42\n",
            "Eval Loss: 0.0023\n"
          ]
        }
      ]
    }
  ]
}
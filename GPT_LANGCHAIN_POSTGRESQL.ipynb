{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L09EVOnecGEM",
        "T80QOIQucnuU",
        "XS4f7vS83LQB",
        "C6aHjg6JSmjK",
        "2fuHTt9Y3vXu",
        "igaydyysTGs9",
        "8ENJvQ8lXT5C",
        "v5aGyp75TZvu",
        "qNpdFmQRalLp"
      ],
      "authorship_tag": "ABX9TyOtVoZs/Ygvc1dMdTeshH0b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/GPT_LANGCHAIN_POSTGRESQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPENDENCIES"
      ],
      "metadata": {
        "id": "L09EVOnecGEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore\n",
        "!pip install llama_index phoenix pyvis network\n",
        "!pip install llama_hub\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "!pip install accelerate\n",
        "#!pip install typing_extensions\n",
        "\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "9TXupbcpOdCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU CONFIGURATION"
      ],
      "metadata": {
        "id": "T80QOIQucnuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml"
      ],
      "metadata": {
        "id": "PTRRX9NXc1N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHIniViOcti1",
        "outputId": "e35bcb38-c83b-4c02-d239-efbea98e1a35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-284-g95ebf68f Python-3.10.12 torch-2.1.0+cu121 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 27.6/225.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER - TEXT FILES"
      ],
      "metadata": {
        "id": "XS4f7vS83LQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/hwchase17/chat-your-data.git\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "#loader = UnstructuredFileLoader(\"/content/chat-your-data/state_of_the_union.txt\")\n",
        "loader = TextLoader(\"/content/chat-your-data/state_of_the_union.txt\")\n",
        "#/content/chat-your-data/state_of_the_union.txt\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs2 = text_splitter.split_documents(documents)\n",
        "\n",
        "collection_name0 = \"state_of_the_union\"\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9XnhQ5Pk5AF",
        "outputId": "80e2256e-cfbd-4eb4-8fd3-104d261c0f32"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'chat-your-data' already exists and is not an empty directory.\n",
            "# of Document Pages 1\n",
            "# of Document Chunks: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "%cd /content/\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpIk9s_H3RhQ",
        "outputId": "fdad19f4-78b8-41fa-d1ad-740b8613b5c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n",
            "--2024-02-24 10:16:41--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: â€˜pg_essay.txtâ€™\n",
            "\n",
            "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-24 10:16:41 (5.54 MB/s) - â€˜pg_essay.txtâ€™ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POSTGRESQL"
      ],
      "metadata": {
        "id": "C6aHjg6JSmjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/integrations/vectorstores/pgembedding\n",
        "\n",
        "# install PSQL WITH DEV Libraries AND PGVECTOR\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "sWh6pKLH3hHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/tools/pgvector\n",
        "!cp -pr /content/gdrive/MyDrive/tools/pgvector /content/\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG VECTOR COMPILATION')\n",
        "print()\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/neondatabase/pg_embedding.git\n",
        "%cd /content/pg_embedding\n",
        "print()\n",
        "print('START: PG embedding COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()\n",
        "\n",
        "#!ls /usr/share/postgresql/14/extension/*control*"
      ],
      "metadata": {
        "id": "kY7aBec_R1S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2 as ps\n",
        "\n",
        "# PostGRES SQL Settings\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id integer PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{0,1,2}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{1,2,3}'),  (3,'{1,1,1}')\"%h\n",
        "print(hh)\n",
        "\n",
        "def insert_document(id,embedding):\n",
        "    #review_embedding=get_embedding(text)\n",
        "    ### INSERT INTO DB\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO documents\n",
        "        (id, embedding)\n",
        "        VALUES ('%s',\n",
        "                '%s')\"\"\" % (id,embedding))\n",
        "\n",
        "    conn.commit()\n",
        "    print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "\n",
        "insert_document(1,'{0,1,2}')\n",
        "insert_document(2,\"{1,2,3}\")\n",
        "insert_document(3,\"{1,1,1}\")\n",
        "\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\"\n",
        "!sudo -u postgres psql -c \"SET enable_seqscan = off\"\n",
        "\n",
        "ARRAY = [3, 3, 3]\n",
        "\n",
        "def select_document(HNSW_index):\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "    SELECT id FROM documents\n",
        "    ORDER BY embedding %s ARRAY[%s,%s,%s] LIMIT 1\n",
        "    \"\"\" % (HNSW_index,str(ARRAY[0]), str(ARRAY[1]), str(ARRAY[2])))\n",
        "\n",
        "    conn.commit()\n",
        "    print(cur.fetchone())\n",
        "    #print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "# <->, <=>, and <~> operators define the distance metric, which calculates the distance between the query vector and each row of the dataset.\n",
        "select_document('<->')\n",
        "select_document('<=>')\n",
        "select_document('<~>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hprWkj7gR-jV",
        "outputId": "d69512f5-5a9e-4b05-9acf-2cb591dcc15e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "CREATE EXTENSION\n",
            "ERROR:  table \"documents\" does not exist\n",
            "CREATE TABLE\n",
            "INSERT INTO documents(id, embedding) VALUES (1,'{0,1,2}'), (2,'{1,2,3}'),  (3,'{1,1,1}')\n",
            "INSERT EMBEDDING {0,1,2} successfully\n",
            "INSERT EMBEDDING {1,2,3} successfully\n",
            "INSERT EMBEDDING {1,1,1} successfully\n",
            "CREATE INDEX\n",
            "SET\n",
            "(2,)\n",
            "(3,)\n",
            "(2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER - PDF FILES"
      ],
      "metadata": {
        "id": "2fuHTt9Y3vXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "urls = [\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "filenames = [\n",
        "    'AMZN-2022-Shareholder-Letter.pdf',\n",
        "    'AMZN-2021-Shareholder-Letter.pdf',\n",
        "    'AMZN-2020-Shareholder-Letter.pdf',\n",
        "    'AMZN-2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "metadata = [\n",
        "    dict(year=2022, source=filenames[0]),\n",
        "    dict(year=2021, source=filenames[1]),\n",
        "    dict(year=2020, source=filenames[2]),\n",
        "    dict(year=2019, source=filenames[3])]\n",
        "\n",
        "data_root = \"/content/data/\"\n",
        "\n",
        "for idx, url in enumerate(urls):\n",
        "    file_path = data_root + filenames[idx]\n",
        "    urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "IcYfsCqkRQ8g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader, PdfWriter\n",
        "import glob\n",
        "\n",
        "local_pdfs = glob.glob(data_root + '*.pdf')\n",
        "\n",
        "for local_pdf in local_pdfs:\n",
        "    pdf_reader = PdfReader(local_pdf)\n",
        "    pdf_writer = PdfWriter()\n",
        "    for pagenum in range(len(pdf_reader.pages)-3):\n",
        "        page = pdf_reader.pages[pagenum]\n",
        "        pdf_writer.add_page(page)\n",
        "\n",
        "    with open(local_pdf, 'wb') as new_file:\n",
        "        new_file.seek(0)\n",
        "        pdf_writer.write(new_file)\n",
        "        new_file.truncate()"
      ],
      "metadata": {
        "id": "C5u4-wxvRQwT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "\n",
        "documents = []\n",
        "\n",
        "for idx, file in enumerate(filenames):\n",
        "    loader = PyPDFLoader(data_root + file)\n",
        "    document = loader.load()\n",
        "    for document_fragment in document:\n",
        "        document_fragment.metadata = metadata[idx]\n",
        "\n",
        "    documents += document\n",
        "\n",
        "# - in our testing Character split works better with this PDF data set\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 100,\n",
        ")\n",
        "\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
        "### for the DB embedding\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs)}')\n",
        "\n",
        "collection_name = \"AWS\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCe6lxBRQeW",
        "outputId": "e4a12cf0-c9ae-4637-fe47-811143ec39d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 25\n",
            "# of Document Chunks: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI - SETTINGS"
      ],
      "metadata": {
        "id": "igaydyysTGs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "y_40orcxOeqW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_reponse(query):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    #model=\"gpt-3.5-turbo\"\n",
        "    #response_format={ \"type\": \"json_object\" },\n",
        "    messages=[\n",
        "      #{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output text.\"},\n",
        "      {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "9Xk5wTFBOtpP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59graM4yOvqr",
        "outputId": "88ca51a8-70d3-4574-ee32-4f4e8381e6ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "Answer: First, calculate the total cost of the ice cream cones. Since each ice cream cone is $1.25 and you bought 6, multiply 1.25 by 6, which equals $7.50. Then, subtract that amount from the $10 bill you used to pay. This provides the amount of change you received from the transaction. \n",
            "\n",
            "The calculation is as follows: $10.00 - $7.50 = $2.50. \n",
            "\n",
            "So, you got $2.50 back in change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT MODEL"
      ],
      "metadata": {
        "id": "8ENJvQ8lXT5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_reponse(query):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    #model=\"gpt-3.5-turbo\"\n",
        "    #response_format={ \"type\": \"json_object\" },\n",
        "    messages=[\n",
        "      #{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output text.\"},\n",
        "      {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "X7tB-ZA7XZxU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "aSvfTUGnXd0Z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGnMRGTwXmNp",
        "outputId": "4196fadd-7a72-43c1-d813-783c35ce301a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "Answer: First, you have to find out the total cost of the ice cream cones. As each cone costs $1.25 and you bought 6 cones, you multiply $1.25 by 6, which equals $7.5. \n",
            "\n",
            "You then subtract this amount from $10, which is the amount you paid with. \n",
            "\n",
            "$10 - $7.5 equals $2.5. So, you got back $2.5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING - WITH PDF FILES"
      ],
      "metadata": {
        "id": "v5aGyp75TZvu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZeKswKQkvp6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20x faster than pgvector: introducing pg_embedding extension for vector search in Postgres and LangChain\n",
        "# https://neon.tech/blog/pg-embedding-extension-for-vector-search\n",
        "\n",
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "# https://supabase.com/blog/fewer-dimensions-are-better-pgvector\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "collection_name='AWS'\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "import llama_index.core.readers as readers\n",
        "reader = readers.SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "## for the RAG\n",
        "#docs = reader.load_data()\n",
        "\n",
        "### FOR PDF FILES\n",
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")\n",
        "\n",
        "#db.create_hnsw_index(dims = 1536, m = 8, ef_construction = 16, ef_search = 16)"
      ],
      "metadata": {
        "id": "ZCgibzj42E8e"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from typing import List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "query='AI'\n",
        "\n",
        "docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)\n",
        "\n",
        "print()\n",
        "print(query)\n",
        "print()\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5vpS3e82WQS",
        "outputId": "2e4f4da2-c1d7-4eca-d5c0-a0ac192fe398"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AI\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6193112\n",
            "drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.62688583\n",
            "assistantlike Alexa (launched in 2014) that you could use to access entertainment, control your smart home, shop,and retrieve all sorts of information.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6347305\n",
            "as weâ€™ve done for years inAWS, weâ€™re democratizing this technology so companies of all sizes can leverage Generative AI. AWS isoffering the most price-performant machine learning chips in Trainium and Inferentia so small and largecompanies can afford to train and run their LLMs in production. We enable companies to choose fromvarious LLMs and build applications with all of the AWS security, privacy and other features that customersare accustomed to using. And, weâ€™re delivering applications like AWSâ€™s\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.64382213\n",
            "like Amazon overa hundred million dollars in capital expense already. Our Inferentia2 chip, which just launched, offers upto four times higher throughput and ten times lower latency than our first Inferentia processor. With theenormous upcoming growth in machine learning, customers will be able to get a lot more done with AWSâ€™straining and inference chips at a significantly lower cost. Weâ€™re not close to being done innovating here,and this long-term investment should prove fruitful for both customers and\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD CHAIN"
      ],
      "metadata": {
        "id": "qNpdFmQRalLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "b2uEZxw2arsD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "# create a chain to answer questions\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "     llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n"
      ],
      "metadata": {
        "id": "9q3Exb_7bDXY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI()\n",
        "query = \"How AWS has evolved?\"\n",
        "#query = \"How many AI publications in 2022?\"\n",
        "result = llm(query)\n",
        "\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "\n",
        "print()\n",
        "print('chain to answer questions')\n",
        "print(\"-\" * 80)\n",
        "result = qa({\"query\": query})\n",
        "print(f'Query: {result[\"query\"]}\\n')\n",
        "print(f'Result: {result[\"result\"]}\\n')\n",
        "print(f'Context Documents: ')\n",
        "for srcdoc in result[\"source_documents\"]:\n",
        "      print(f'{srcdoc}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "29ajr1Cja03Q",
        "outputId": "555998fd-8537-48a9-f17f-71c36a3728ae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>How AWS has evolved?</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>\n\n1. Expansion of services: AWS started with just a few basic services such as EC2 and S3. Over time, it has expanded its offerings to include over 200 services, covering various areas such as compute, storage, database, analytics, machine learning, and more.\n\n2. Global presence: AWS has expanded its global presence with 24 geographic regions and 77 availability zones, making it one of the most widely available cloud platforms.\n\n3. Increased security: AWS has continuously worked on improving its security features, offering services such as Amazon Inspector, AWS WAF, and AWS Shield to protect against cyber threats.\n\n4. Hybrid and multi-cloud solutions: AWS has evolved to provide hybrid and multi-cloud solutions, allowing businesses to seamlessly integrate their on-premises infrastructure with AWS services.\n\n5. Focus on AI and ML: AWS has heavily invested in artificial intelligence (AI) and machine learning (ML) services, making it easier for businesses to incorporate these technologies into their applications.\n\n6. Serverless computing: With the introduction of serverless computing, AWS has revolutionized the way applications are developed and deployed, reducing the need for managing servers and infrastructure.\n\n7. Internet of Things (IoT): AWS has expanded its services to cater to the growing demand for IoT solutions</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "chain to answer questions\n",
            "--------------------------------------------------------------------------------\n",
            "Query: How AWS has evolved?\n",
            "\n",
            "Result:  Over the course of 15 years, AWS has evolved from having a head start on potential competitors to becoming a $85B annual revenue run rate business with strong profitability. This transformation has significantly impacted how customers, from startups to multinational companies to public sector organizations, manage their technology infrastructure.\n",
            "\n",
            "Context Documents: \n",
            "page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.' metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "\n",
            "page_content='We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifweâ€™d slowed investment in AWS during that 2008-2009' metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
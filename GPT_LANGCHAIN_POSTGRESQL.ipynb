{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v5aGyp75TZvu"
      ],
      "authorship_tag": "ABX9TyM9K6S5gLAn0pcVV5Wz3Gtn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/GPT_LANGCHAIN_POSTGRESQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPENDENCIES"
      ],
      "metadata": {
        "id": "L09EVOnecGEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore\n",
        "!pip install llama_index phoenix pyvis network\n",
        "!pip install llama_hub\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "!pip install accelerate\n",
        "#!pip install typing_extensions\n",
        "\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "9TXupbcpOdCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU CONFIGURATION"
      ],
      "metadata": {
        "id": "T80QOIQucnuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml"
      ],
      "metadata": {
        "id": "PTRRX9NXc1N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHIniViOcti1",
        "outputId": "c158da05-ef16-4151-ca6c-3e1c748f7ddf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-284-g95ebf68f Python-3.10.12 torch-2.1.0+cu121 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 27.6/225.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER - TEXT FILES"
      ],
      "metadata": {
        "id": "XS4f7vS83LQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State of the union"
      ],
      "metadata": {
        "id": "Lh78KWA1X--H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/hwchase17/chat-your-data.git\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "#loader = UnstructuredFileLoader(\"/content/chat-your-data/state_of_the_union.txt\")\n",
        "loader = TextLoader(\"/content/chat-your-data/state_of_the_union.txt\")\n",
        "#/content/chat-your-data/state_of_the_union.txt\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs2 = text_splitter.split_documents(documents)\n",
        "\n",
        "collection_name0 = \"state_of_the_union\"\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9XnhQ5Pk5AF",
        "outputId": "87e6fb6f-2a71-47e8-c8eb-8c3637c4208c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'chat-your-data'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 62 (delta 17), reused 15 (delta 13), pack-reused 34\u001b[K\n",
            "Receiving objects: 100% (62/62), 24.22 MiB | 31.20 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "# of Document Pages 1\n",
            "# of Document Chunks: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paul Graham Essay"
      ],
      "metadata": {
        "id": "phhHhN67YIUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "%cd /content/\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpIk9s_H3RhQ",
        "outputId": "2eebd0f5-cceb-4767-8b80-612d0886a87d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n",
            "--2024-02-24 13:14:55--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: â€˜pg_essay.txtâ€™\n",
            "\n",
            "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-24 13:14:55 (5.76 MB/s) - â€˜pg_essay.txtâ€™ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POSTGRESQL"
      ],
      "metadata": {
        "id": "C6aHjg6JSmjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/integrations/vectorstores/pgembedding\n",
        "\n",
        "# install PSQL WITH DEV Libraries AND PGVECTOR\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "sWh6pKLH3hHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/tools/pgvector\n",
        "!cp -pr /content/gdrive/MyDrive/tools/pgvector /content/\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG VECTOR COMPILATION')\n",
        "print()\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/neondatabase/pg_embedding.git\n",
        "%cd /content/pg_embedding\n",
        "print()\n",
        "print('START: PG embedding COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()\n",
        "\n",
        "#!ls /usr/share/postgresql/14/extension/*control*"
      ],
      "metadata": {
        "id": "kY7aBec_R1S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2 as ps\n",
        "\n",
        "# PostGRES SQL Settings\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id integer PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{0,1,2}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{1,2,3}'),  (3,'{1,1,1}')\"%h\n",
        "print(hh)\n",
        "\n",
        "def insert_document(id,embedding):\n",
        "    #review_embedding=get_embedding(text)\n",
        "    ### INSERT INTO DB\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO documents\n",
        "        (id, embedding)\n",
        "        VALUES ('%s',\n",
        "                '%s')\"\"\" % (id,embedding))\n",
        "\n",
        "    conn.commit()\n",
        "    print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "\n",
        "insert_document(1,'{0,1,2}')\n",
        "insert_document(2,\"{1,2,3}\")\n",
        "insert_document(3,\"{1,1,1}\")\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=8, efconstruction=16, efsearch=16)\"\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\"\n",
        "!sudo -u postgres psql -c \"SET enable_seqscan = off\"\n",
        "\n",
        "ARRAY = [3, 3, 3]\n",
        "\n",
        "def select_document(HNSW_index):\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "    SELECT id FROM documents\n",
        "    ORDER BY embedding %s ARRAY[%s,%s,%s] LIMIT 1\n",
        "    \"\"\" % (HNSW_index,str(ARRAY[0]), str(ARRAY[1]), str(ARRAY[2])))\n",
        "\n",
        "    conn.commit()\n",
        "    print(cur.fetchone())\n",
        "    #print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "# <->, <=>, and <~> operators define the distance metric, which calculates the distance between the query vector and each row of the dataset.\n",
        "select_document('<->')\n",
        "select_document('<=>')\n",
        "select_document('<~>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hprWkj7gR-jV",
        "outputId": "b063c34b-c788-4dca-bddb-ad2133cbd155"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "CREATE EXTENSION\n",
            "ERROR:  table \"documents\" does not exist\n",
            "CREATE TABLE\n",
            "INSERT INTO documents(id, embedding) VALUES (1,'{0,1,2}'), (2,'{1,2,3}'),  (3,'{1,1,1}')\n",
            "INSERT EMBEDDING {0,1,2} successfully\n",
            "INSERT EMBEDDING {1,2,3} successfully\n",
            "INSERT EMBEDDING {1,1,1} successfully\n",
            "CREATE INDEX\n",
            "SET\n",
            "(2,)\n",
            "(3,)\n",
            "(2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER - PDF FILES"
      ],
      "metadata": {
        "id": "2fuHTt9Y3vXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AMAZON Shareholder Letters"
      ],
      "metadata": {
        "id": "yYJrsvktYTN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "urls = [\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "filenames = [\n",
        "    'AMZN-2022-Shareholder-Letter.pdf',\n",
        "    'AMZN-2021-Shareholder-Letter.pdf',\n",
        "    'AMZN-2020-Shareholder-Letter.pdf',\n",
        "    'AMZN-2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "metadata = [\n",
        "    dict(year=2022, source=filenames[0]),\n",
        "    dict(year=2021, source=filenames[1]),\n",
        "    dict(year=2020, source=filenames[2]),\n",
        "    dict(year=2019, source=filenames[3])]\n",
        "\n",
        "data_root = \"/content/data/\"\n",
        "\n",
        "for idx, url in enumerate(urls):\n",
        "    file_path = data_root + filenames[idx]\n",
        "    urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "IcYfsCqkRQ8g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader, PdfWriter\n",
        "import glob\n",
        "\n",
        "local_pdfs = glob.glob(data_root + '*.pdf')\n",
        "\n",
        "for local_pdf in local_pdfs:\n",
        "    pdf_reader = PdfReader(local_pdf)\n",
        "    pdf_writer = PdfWriter()\n",
        "    for pagenum in range(len(pdf_reader.pages)-3):\n",
        "        page = pdf_reader.pages[pagenum]\n",
        "        pdf_writer.add_page(page)\n",
        "\n",
        "    with open(local_pdf, 'wb') as new_file:\n",
        "        new_file.seek(0)\n",
        "        pdf_writer.write(new_file)\n",
        "        new_file.truncate()"
      ],
      "metadata": {
        "id": "C5u4-wxvRQwT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "\n",
        "documents = []\n",
        "\n",
        "for idx, file in enumerate(filenames):\n",
        "    loader = PyPDFLoader(data_root + file)\n",
        "    document = loader.load()\n",
        "    for document_fragment in document:\n",
        "        document_fragment.metadata = metadata[idx]\n",
        "\n",
        "    documents += document\n",
        "\n",
        "# - in our testing Character split works better with this PDF data set\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 100,\n",
        ")\n",
        "\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
        "### for the DB embedding\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs)}')\n",
        "\n",
        "collection_name = \"AWS\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCe6lxBRQeW",
        "outputId": "1e60fb7f-f37f-452d-950d-cd7a7df979ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 25\n",
            "# of Document Chunks: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI - SETTINGS"
      ],
      "metadata": {
        "id": "igaydyysTGs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "y_40orcxOeqW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_reponse(query):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    #model=\"gpt-3.5-turbo\"\n",
        "    #response_format={ \"type\": \"json_object\" },\n",
        "    messages=[\n",
        "      #{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output text.\"},\n",
        "      {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "9Xk5wTFBOtpP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who won the world series in 2009 and who lost, explained?, who were the managers?\"\n",
        "response=gpt_reponse(query)\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "print('Answer: %s'%response.choices[0].message.content)\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59graM4yOvqr",
        "outputId": "ee40f75c-0b61-40b8-966c-f49ba1c8897e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: Who won the world series in 2009 and who lost, explained?, who were the managers?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: The 2009 World Series was won by the New York Yankees and the team that lost was the Philadelphia Phillies. The series concluded in six games with the Yankees winning four games to the Phillies' two.\n",
            "\n",
            "The manager for the New York Yankees in 2009 was Joe Girardi. This was Girardi's second season as the Yankees' manager and this victory marked his first World Series win as a manager.\n",
            "\n",
            "On the other hand, the Philadelphia Phillies were managed by Charlie Manuel. 2009 represented Manuel's fifth year as the Phillies' manager and he had led the team to victory in the previous World Series in 2008.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT4 MODEL - EXAMPLE"
      ],
      "metadata": {
        "id": "8ENJvQ8lXT5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How AWS has evolved?\"\n",
        "response=gpt_reponse(query)\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "print('Answer: %s'%response.choices[0].message.content)\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCMp7DiRel6Q",
        "outputId": "d189700c-8ab7-41b2-82ad-8a999f7b3151"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Question: How AWS has evolved?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: Amazon Web Services (AWS), a subsidiary of Amazon.com, launched in 2006. Since then, it has continuously evolved and expanded its services. Here is a brief overview:\n",
            "\n",
            "1. Inception (2002-2006): AWS platform, born out of Amazon's need to scale operations and its retail application, was initially a collection of tools and services for developers. The official launch of AWS occurred in 2006.\n",
            "\n",
            "2. The Birth of EC2 and S3 (2006): Arguably the two most well-known AWS services, EC2 (Elastic Compute Cloud) and S3 (Simple Storage Service) were launched, marking the birth of cloud infrastructure services.\n",
            "\n",
            "3. Service Expansion (2007-2010): AWS launched various services like SimpleDB, Elastic Block Store (EBS), Content Delivery Network (CDN), and more.\n",
            "\n",
            "4. Global Expansion (2010-2011): AWS began its global expansion by setting up infrastructure around the world, establishing its first Asian Pacific region in Singapore in 2010. \n",
            "\n",
            "5. Introduction of First Managed Database and VPC (2009-2011): AWS launched RDS, its first managed database service. They also introduced Virtual Private Cloud (VPC) letting users create logically isolated sections of AWS.\n",
            "\n",
            "6. Launch of AWS Marketplace and Glacier (2012): AWS launched the AWS Marketplace, allowing customers to find, buy, and immediately start using the software and services they need. AWS Glacier was also launched for long-term storage.\n",
            "\n",
            "7. Continuous Innovation (2013-present): AWS continued to innovate with new products like WorkSpaces (DaaS), Lambda (event-driven computing), Alexa Voice Service, and more. They also launched machine learning and AI services, IoT services, and analytics services among others.\n",
            "\n",
            "8. Growing Enterprise Adoption (2015-present): As AWS expanded its services and global infrastructure, it attracted large enterprises to move their operations to the cloud.\n",
            "\n",
            "9. Push into Hybrid Cloud (2017-present): AWS announced a series of hybrid cloud products including the Snowball Edge, Greengrass, and most notably Outposts, marking their pivot towards hybrid cloud.\n",
            "\n",
            "10. Dominance in Cloud Market (Present): AWS holds a dominant position in the cloud market with millions of active customers and multiple data centers across the globe.\n",
            "\n",
            "Throughout its evolution, AWS has focused on improving and expanding its services, working to ensure operational security and efficiency, and nurturing a growing ecosystem of partners and developers.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "response=gpt_reponse(query)\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "print('Answer: %s'%response.choices[0].message.content)\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGnMRGTwXmNp",
        "outputId": "a25ea1f7-a45e-49e0-99ef-2380fe214344"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: First, you have to calculate the total cost of the ice cream cones for the 6 kids. Given that each cone costs $1.25, you simply multiply this price by the number of kids, which is 6. So, $1.25 x 6 equals $7.50. This is the total cost for the ice cream cones.\n",
            "\n",
            "Next, we subtract the total cost of the ice cream from the amount you paid with which was a $10 bill. So, $10 - $7.50 equals $2.50.\n",
            "\n",
            "Therefore, you received $2.50 back.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING - WITH PDF FILES"
      ],
      "metadata": {
        "id": "v5aGyp75TZvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20x faster than pgvector: introducing pg_embedding extension for vector search in Postgres and LangChain\n",
        "# https://neon.tech/blog/pg-embedding-extension-for-vector-search\n",
        "\n",
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "# https://supabase.com/blog/fewer-dimensions-are-better-pgvector\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "collection_name='AWS'\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "import llama_index.core.readers as readers\n",
        "reader = readers.SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "## for the RAG\n",
        "#docs = reader.load_data()\n",
        "\n",
        "### FOR PDF FILES\n",
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")\n",
        "\n",
        "#db.create_hnsw_index(dims = 1536, m = 8, ef_construction = 16, ef_search = 16)\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=8, efconstruction=16, efsearch=16)\"\n"
      ],
      "metadata": {
        "id": "ZCgibzj42E8e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from typing import List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "query='AI'\n",
        "\n",
        "docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)\n",
        "\n",
        "print()\n",
        "print(query)\n",
        "print()\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5vpS3e82WQS",
        "outputId": "44478635-4c27-4f0a-a9c8-bc7270d506b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AI\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6194219\n",
            "drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.62688583\n",
            "assistantlike Alexa (launched in 2014) that you could use to access entertainment, control your smart home, shop,and retrieve all sorts of information.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6347305\n",
            "as weâ€™ve done for years inAWS, weâ€™re democratizing this technology so companies of all sizes can leverage Generative AI. AWS isoffering the most price-performant machine learning chips in Trainium and Inferentia so small and largecompanies can afford to train and run their LLMs in production. We enable companies to choose fromvarious LLMs and build applications with all of the AWS security, privacy and other features that customersare accustomed to using. And, weâ€™re delivering applications like AWSâ€™s\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.64382213\n",
            "like Amazon overa hundred million dollars in capital expense already. Our Inferentia2 chip, which just launched, offers upto four times higher throughput and ten times lower latency than our first Inferentia processor. With theenormous upcoming growth in machine learning, customers will be able to get a lot more done with AWSâ€™straining and inference chips at a significantly lower cost. Weâ€™re not close to being done innovating here,and this long-term investment should prove fruitful for both customers and\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANG CHAIN"
      ],
      "metadata": {
        "id": "qNpdFmQRalLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "b2uEZxw2arsD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a chain to answer questions**"
      ],
      "metadata": {
        "id": "tqu0gPTHraBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mf7Ptz9bSy5N",
        "outputId": "4cad65f2-ffc8-4f85-8749-0168523da54d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "# create a chain to answer questions\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "     llm=OpenAI(model_name='gpt-3.5-turbo-instruct',max_tokens=1512,temperature=0.9), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n"
      ],
      "metadata": {
        "id": "9q3Exb_7bDXY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL answer questions**"
      ],
      "metadata": {
        "id": "VIJ2ZGHrrH_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL\tDESCRIPTION\tCONTEXT WINDOW\tTRAINING DATA\n",
        "#gpt-3.5-turbo-0125\n",
        "#gpt-3.5-turbo-instruct\n",
        "# https://platform.openai.com/docs/models/overview\n",
        "# https://pypi.org/project/openai/\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "\n",
        "#llm=OpenAI(model='gpt-4',max_tokens=1512,temperature=0.9)\n",
        "llm=OpenAI(model='gpt-3.5-turbo-instruct',max_tokens=1512,temperature=0.9)\n",
        "\n",
        "\n",
        "#from langchain_community.llms import OpenAI as OpenAIv1\n",
        "#llm = OpenAIv1(model_name=\"gpt-4\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "query = \"How AWS has evolved?\"\n",
        "#query = \"How many AI publications in 2022?\"\n",
        "result = llm(query)\n",
        "display(Markdown(f\"<p>{query}</p>\"))\n",
        "print(\"-\" * 80)\n",
        "display(Markdown(f\"<p>{result}</p>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "-wbgU1maq2eT",
        "outputId": "cc59d245-4bef-43e2-85ed-d1d0c519a83b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>How AWS has evolved?</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>\n\n1. Cloud Computing: AWS revolutionized the computing industry by introducing the concept of cloud computing. With AWS, businesses can easily access computing resources as needed, rather than investing in expensive and complex on-premises infrastructure.\n\n2. Expanding Services: From its humble beginnings as a simple storage service, AWS has expanded its offerings to include a wide range of services, including compute, storage, databases, networking, analytics, machine learning, Internet of Things (IoT), security, and more.\n\n3. Global Infrastructure: AWS has massively expanded its global infrastructure, with data centers in over 25 regions around the world, enabling businesses to easily reach their customers wherever they may be located.\n\n4. Constant Innovation: AWS is constantly innovating and releasing new services and features, such as serverless computing, containers, artificial intelligence, and more. This allows businesses to stay ahead of the curve and adapt to changing market needs.\n\n5. Hybrid and Multi-cloud Capabilities: To meet the diverse needs of its customers, AWS offers hybrid and multi-cloud capabilities, allowing businesses to seamlessly integrate on-premises infrastructure with AWS cloud services.\n\n6. Cost Optimization: AWS has introduced several cost-saving features, such as auto-scaling, reserved instances, and spot instances, helping businesses reduce their overall IT costs.\n\n7. Enterprise Focus: AWS has made significant efforts to cater to the needs of large enterprises, with services such as AWS Enterprise Support, AWS Managed Services, and AWS Control Tower, making it easier for businesses to adopt and manage cloud services at scale.\n\n8. Educating the Market: AWS offers a variety of training and certification programs for individuals and businesses to learn and build expertise on its services. This has helped to create a broad base of skilled professionals who are well-versed in AWS technologies.\n\n9. Partner Ecosystem: AWS has built a vast partner ecosystem, including Independent Software Vendors (ISVs), System Integrators (SIs), Managed Service Providers (MSPs), and Consulting Partners, offering businesses a wide range of options to meet their specific needs.\n\n10. Impact on other Cloud Providers: AWS's success has not only revolutionized the cloud computing market, but it has also pushed other cloud providers to innovate and improve their services, benefiting customers across the industry.</p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHAIN answer questions**"
      ],
      "metadata": {
        "id": "fSrv29xnq-mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "#print('chain to answer questions')\n",
        "print(\"-\" * 80)\n",
        "print()\n",
        "result = qa({\"query\": query})\n",
        "print(f'Query: {result[\"query\"]}\\n')\n",
        "print(f'Result: {result[\"result\"]}\\n')\n",
        "print(f'Context Documents: ')\n",
        "for srcdoc in result[\"source_documents\"]:\n",
        "      print(f'{srcdoc}\\n')\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ajr1Cja03Q",
        "outputId": "bfbfea51-2b86-4558-f5c8-266544de7829"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query: How AWS has evolved?\n",
            "\n",
            "Result:  AWS has evolved into an $85B annual revenue run rate business with strong profitability that has transformed how customers manage their technology infrastructure. \n",
            "\n",
            "Context Documents: \n",
            "page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.' metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "\n",
            "page_content='We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifweâ€™d slowed investment in AWS during that 2008-2009' metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
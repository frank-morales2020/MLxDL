{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v5aGyp75TZvu"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP0S86bzhKcAHEVfpxfV/bi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/GPT_LANGCHAIN_POSTGRESQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPENDENCIES"
      ],
      "metadata": {
        "id": "L09EVOnecGEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM_v6Ctm0KSP",
        "outputId": "c699038d-a35e-4c52-e684-8af484b5f52a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May  1 11:01:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   57C    P8              17W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore\n",
        "!pip install llama_index phoenix pyvis network\n",
        "!pip install llama_hub\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "!pip install accelerate\n",
        "#!pip install typing_extensions\n",
        "\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "9TXupbcpOdCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU CONFIGURATION"
      ],
      "metadata": {
        "id": "T80QOIQucnuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml"
      ],
      "metadata": {
        "id": "PTRRX9NXc1N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "id": "JHIniViOcti1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER - TEXT FILES"
      ],
      "metadata": {
        "id": "XS4f7vS83LQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State of the union"
      ],
      "metadata": {
        "id": "Lh78KWA1X--H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/hwchase17/chat-your-data.git\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "#loader = UnstructuredFileLoader(\"/content/chat-your-data/state_of_the_union.txt\")\n",
        "loader = TextLoader(\"/content/chat-your-data/state_of_the_union.txt\")\n",
        "#/content/chat-your-data/state_of_the_union.txt\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs2 = text_splitter.split_documents(documents)\n",
        "\n",
        "collection_name0 = \"state_of_the_union\"\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs2)}')"
      ],
      "metadata": {
        "id": "X9XnhQ5Pk5AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paul Graham Essay"
      ],
      "metadata": {
        "id": "phhHhN67YIUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dbredvick/paul-graham-to-kindle.git"
      ],
      "metadata": {
        "id": "PTXEgthEzR_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "import llama_index.core.readers as readers\n",
        "reader = readers.SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "## for the RAG\n",
        "docs0 = reader.load_data()\n",
        "\n",
        "collection_name0 = \"pg_essay\"\n",
        "print(f'# of Document Pages {len(docs0)}')\n",
        "print(f'# of Document Chunks: {len(docs0)}')\n",
        "\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "loader = TextLoader(\"/content/pg_essay.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "### for the DB embedding\n",
        "docs0 = text_splitter.split_documents(documents)\n",
        "\n",
        "collection_name0 = \"pg_essay\"\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs0)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "mpIk9s_H3RhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POSTGRESQL"
      ],
      "metadata": {
        "id": "C6aHjg6JSmjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/integrations/vectorstores/pgembedding\n",
        "\n",
        "# install PSQL WITH DEV Libraries AND PGVECTOR\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "sWh6pKLH3hHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/tools/pgvector\n",
        "!cp -pr /content/gdrive/MyDrive/tools/pgvector /content/\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG VECTOR COMPILATION')\n",
        "print()\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/neondatabase/pg_embedding.git\n",
        "%cd /content/pg_embedding\n",
        "print()\n",
        "print('START: PG embedding COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()\n",
        "\n",
        "#!ls /usr/share/postgresql/14/extension/*control*"
      ],
      "metadata": {
        "id": "kY7aBec_R1S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2 as ps\n",
        "\n",
        "# PostGRES SQL Settings\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id integer PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{0,1,2}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{1,2,3}'),  (3,'{1,1,1}')\"%h\n",
        "print(hh)\n",
        "\n",
        "def insert_document(id,embedding):\n",
        "    #review_embedding=get_embedding(text)\n",
        "    ### INSERT INTO DB\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO documents\n",
        "        (id, embedding)\n",
        "        VALUES ('%s',\n",
        "                '%s')\"\"\" % (id,embedding))\n",
        "\n",
        "    conn.commit()\n",
        "    print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "\n",
        "insert_document(1,'{0,1,2}')\n",
        "insert_document(2,\"{1,2,3}\")\n",
        "insert_document(3,\"{1,1,1}\")\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=8, efconstruction=16, efsearch=16)\"\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\"\n",
        "!sudo -u postgres psql -c \"SET enable_seqscan = off\"\n",
        "\n",
        "ARRAY = [3, 3, 3]\n",
        "\n",
        "def select_document(HNSW_index):\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "    SELECT id FROM documents\n",
        "    ORDER BY embedding %s ARRAY[%s,%s,%s] LIMIT 1\n",
        "    \"\"\" % (HNSW_index,str(ARRAY[0]), str(ARRAY[1]), str(ARRAY[2])))\n",
        "\n",
        "    conn.commit()\n",
        "    print(cur.fetchone())\n",
        "    #print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "# <->, <=>, and <~> operators define the distance metric, which calculates the distance between the query vector and each row of the dataset.\n",
        "select_document('<->')\n",
        "select_document('<=>')\n",
        "select_document('<~>')"
      ],
      "metadata": {
        "id": "hprWkj7gR-jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER - PDF FILES"
      ],
      "metadata": {
        "id": "2fuHTt9Y3vXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AMAZON Shareholder Letters"
      ],
      "metadata": {
        "id": "yYJrsvktYTN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "urls = [\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "filenames = [\n",
        "    'AMZN-2022-Shareholder-Letter.pdf',\n",
        "    'AMZN-2021-Shareholder-Letter.pdf',\n",
        "    'AMZN-2020-Shareholder-Letter.pdf',\n",
        "    'AMZN-2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "metadata = [\n",
        "    dict(year=2022, source=filenames[0]),\n",
        "    dict(year=2021, source=filenames[1]),\n",
        "    dict(year=2020, source=filenames[2]),\n",
        "    dict(year=2019, source=filenames[3])]\n",
        "\n",
        "data_root = \"/content/data/\"\n",
        "\n",
        "for idx, url in enumerate(urls):\n",
        "    file_path = data_root + filenames[idx]\n",
        "    urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "IcYfsCqkRQ8g"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader, PdfWriter\n",
        "import glob\n",
        "\n",
        "local_pdfs = glob.glob(data_root + '*.pdf')\n",
        "\n",
        "for local_pdf in local_pdfs:\n",
        "    pdf_reader = PdfReader(local_pdf)\n",
        "    pdf_writer = PdfWriter()\n",
        "    for pagenum in range(len(pdf_reader.pages)-3):\n",
        "        page = pdf_reader.pages[pagenum]\n",
        "        pdf_writer.add_page(page)\n",
        "\n",
        "    with open(local_pdf, 'wb') as new_file:\n",
        "        new_file.seek(0)\n",
        "        pdf_writer.write(new_file)\n",
        "        new_file.truncate()"
      ],
      "metadata": {
        "id": "C5u4-wxvRQwT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "\n",
        "documents = []\n",
        "\n",
        "for idx, file in enumerate(filenames):\n",
        "    loader = PyPDFLoader(data_root + file)\n",
        "    document = loader.load()\n",
        "    for document_fragment in document:\n",
        "        document_fragment.metadata = metadata[idx]\n",
        "\n",
        "    documents += document\n",
        "\n",
        "# - in our testing Character split works better with this PDF data set\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 100,\n",
        ")\n",
        "\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
        "### for the DB embedding\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs)}')\n",
        "\n",
        "collection_name = \"AWS\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCe6lxBRQeW",
        "outputId": "60f79112-5c2c-4ca3-d217-5416ee3b4709"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 25\n",
            "# of Document Chunks: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI - SETTINGS"
      ],
      "metadata": {
        "id": "igaydyysTGs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "y_40orcxOeqW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_reponse(query):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    #model=\"gpt-3.5-turbo\"\n",
        "    #response_format={ \"type\": \"json_object\" },\n",
        "    messages=[\n",
        "      #{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output text.\"},\n",
        "      {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "9Xk5wTFBOtpP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who won the world series in 2009 and who lost, explained?, who were the managers?\"\n",
        "response=gpt_reponse(query)\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "print('Answer: %s'%response.choices[0].message.content)\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59graM4yOvqr",
        "outputId": "d6f8f3c5-1aa4-46f2-d617-6799ea86aeb2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: Who won the world series in 2009 and who lost, explained?, who were the managers?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: The 2009 World Series was won by the New York Yankees, defeating the Philadelphia Phillies. The Yankees secured their 27th championship title, winning the series 4 games to 2.\n",
            "\n",
            "The manager of the New York Yankees in 2009 was Joe Girardi. Having joined the team in 2008, Girardi led the Yankees to their first World Series win since 2000. His strategy, leadership, and decision-making played a significant role in their success.\n",
            "\n",
            "On the other side, the Philadelphia Phillies were managed by Charlie Manuel. Despite losing the series, Manuel was at the helm of the Phillies during a successful period in their history. In 2008, he had led the team to a World Series victory.\n",
            "\n",
            "Both teams showed exceptional quality and resilience, delivering a thrilling World Series. However, the New York Yankees emerged as the champions.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT4 MODEL - EXAMPLE"
      ],
      "metadata": {
        "id": "8ENJvQ8lXT5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How AWS has evolved?\"\n",
        "response=gpt_reponse(query)\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "print('Answer: %s'%response.choices[0].message.content)\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCMp7DiRel6Q",
        "outputId": "1dbc376c-b72f-4fb9-b6e9-d2aa47609ed7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Question: How AWS has evolved?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: Amazon Web Services (AWS) has undergone significant evolution since its inception in 2006, growing into the most comprehensive, broadly adopted, and mature cloud platform globally. Here's a brief overview of AWS's evolution:\n",
            "\n",
            "1. Launch Phase (2006-2010): AWS launched with just three services -- Amazon Simple Queue Service (SQS), Amazon Simple Storage Service (S3), and Amazon Elastic Compute Cloud (EC2).\n",
            "\n",
            "2. Expansion Phase (2010-2013): AWS expanded rapidly, launching new services like Elastic Load Balancing, Auto Scaling, and Amazon Relational Database Service (RDS). Amazon also introduced Elastic Beanstalk, a platform as a service (PaaS), and CloudFormation, an IaaS service.\n",
            "\n",
            "3. Global Expansion (2013-2016): AWS began expanding globally, opening regions in Asia (Tokyo, Sydney, Beijing) and Europe (Frankfurt, London). In this period, services like Amazon Kinesis (real-time data streaming), Lambda (serverless computing), and Machine Learning (predictive analytics) were launched.\n",
            "\n",
            "4. Specialization Phase (2016-Present): AWS has continuously released new specialized services in areas including Artificial Intelligence (AI), Machine Learning (ML), business productivity, desktop streaming, mobile services, AR/VR, game development, Internet of Things, security, and more. AWS also enhanced their offerings for hybrid cloud environments.\n",
            "\n",
            "5. Reinvention (2020-Present): AWS brought several innovations like AWS Outposts (for operating a hybrid environment), AWS Wavelength (for 5G), and AWS Local Zones (bringing AWS services to large cities). They've also begun more industry-focused solutions, like AWS for Industrial.\n",
            "\n",
            "Throughout this evolution, AWS has maintained a strong focus on security, compliance, and governance. They have aimed to meet the different needs of startups, small and medium businesses, and large enterprises, across various industries. The flexibility, scalability, and cost-effectiveness of AWS services have been key in its evolution and success.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "response=gpt_reponse(query)\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print('Question: %s'%query)\n",
        "print(\"-\" * 80)\n",
        "print('Answer: %s'%response.choices[0].message.content)\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGnMRGTwXmNp",
        "outputId": "752b771f-91ab-46d5-ae8b-e9ba32470596"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: First, to find the total cost of the ice cream cones, you need to multiply the price of each cone, $1.25, by the number of kids, which is 6. So, $1.25 times 6 equals $7.50, which is the total cost of the cones.\n",
            "\n",
            "Next, you need to calculate how much change you should get back. You do this by taking the amount you paid, $10, and subtracting the total cost of the cones, $7.50.\n",
            "\n",
            "So, $10 - $7.50 = $2.50  \n",
            "\n",
            "So you got back $2.50 in change after buying ice cream cones for 6 kids.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING - WITH PDF FILES"
      ],
      "metadata": {
        "id": "v5aGyp75TZvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20x faster than pgvector: introducing pg_embedding extension for vector search in Postgres and LangChain\n",
        "# https://neon.tech/blog/pg-embedding-extension-for-vector-search\n",
        "\n",
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "# https://supabase.com/blog/fewer-dimensions-are-better-pgvector\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "collection_name='AWS'\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "import llama_index.core.readers as readers\n",
        "reader = readers.SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "## for the RAG\n",
        "#docs = reader.load_data()\n",
        "\n",
        "### FOR PDF FILES\n",
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")\n",
        "\n",
        "#db.create_hnsw_index(dims = 1536, m = 8, ef_construction = 16, ef_search = 16)\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=8, efconstruction=16, efsearch=16)\"\n"
      ],
      "metadata": {
        "id": "ZCgibzj42E8e"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from typing import List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "query='AI'\n",
        "\n",
        "docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)\n",
        "\n",
        "print()\n",
        "print(query)\n",
        "print()\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5vpS3e82WQS",
        "outputId": "89d6fc44-7f28-4661-ebc4-a4e8e1a99848"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AI\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.61912847\n",
            "drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.61912847\n",
            "drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6268243\n",
            "assistantlike Alexa (launched in 2014) that you could use to access entertainment, control your smart home, shop,and retrieve all sorts of information.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6268243\n",
            "assistantlike Alexa (launched in 2014) that you could use to access entertainment, control your smart home, shop,and retrieve all sorts of information.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANG CHAIN"
      ],
      "metadata": {
        "id": "qNpdFmQRalLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "b2uEZxw2arsD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a chain to answer questions**"
      ],
      "metadata": {
        "id": "tqu0gPTHraBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mf7Ptz9bSy5N",
        "outputId": "e10f5f29-47db-4c43-c7c8-8396e4a553d5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.25.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "# create a chain to answer questions\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "     llm=OpenAI(model_name='gpt-3.5-turbo-instruct',max_tokens=1512,temperature=0.9), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n"
      ],
      "metadata": {
        "id": "9q3Exb_7bDXY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL answer questions**"
      ],
      "metadata": {
        "id": "VIJ2ZGHrrH_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL\tDESCRIPTION\tCONTEXT WINDOW\tTRAINING DATA\n",
        "#gpt-3.5-turbo-0125\n",
        "#gpt-3.5-turbo-instruct\n",
        "# https://platform.openai.com/docs/models/overview\n",
        "# https://pypi.org/project/openai/\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "\n",
        "#llm=OpenAI(model='gpt-4',max_tokens=1512,temperature=0.9)\n",
        "llm=OpenAI(model='gpt-3.5-turbo-instruct',max_tokens=1512,temperature=0.9)\n",
        "\n",
        "\n",
        "#from langchain_community.llms import OpenAI as OpenAIv1\n",
        "#llm = OpenAIv1(model_name=\"gpt-4\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "query = \"How AWS has evolved?\"\n",
        "#query = \"How many AI publications in 2022?\"\n",
        "result = llm(query)\n",
        "display(Markdown(f\"<p>{query}</p>\"))\n",
        "print(\"-\" * 80)\n",
        "display(Markdown(f\"<p>{result}</p>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "-wbgU1maq2eT",
        "outputId": "2b40ed2d-97a2-41f8-bc62-2194bb2772a2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>How AWS has evolved?</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>\n\n1. Increased Services and Capabilities: When AWS was first launched in 2006, it offered only 3 services. Today, it offers over 175 services and continues to add more. These services range from computing, storage, networking, databases, analytics, AI/ML, IoT, security, and many more.\n\n2. Global Infrastructure: AWS has expanded its infrastructure globally with the addition of multiple regions and availability zones. This allows customers to have their services hosted in different regions for better redundancy and improved performance.\n\n3. Hybrid Cloud Capabilities: AWS has evolved to support hybrid cloud architectures, where customers can connect their on-premises infrastructure to the AWS cloud. This allows organizations to have a mix of both on-premises and cloud environments, giving them more flexibility in managing their workloads.\n\n4. Industry-Specific Solutions: AWS has developed solutions tailored for specific industries such as healthcare, finance, government, education, and more. These solutions help organizations in these industries comply with regulations and address their specific needs.\n\n5. AI and ML Capabilities: AWS has heavily invested in AI and ML capabilities, making it easier for customers to build and deploy machine learning models. This has opened up new opportunities for businesses to use AI and ML in their applications and services.\n\n6. Serverless Computing: AWS has introduced serverless computing through its services such as AWS Lambda, which allows customers to run code without managing servers. This has greatly reduced the complexity and cost of managing servers for organizations.\n\n7. DevOps Integration: AWS has integrated its services with DevOps tools, allowing customers to automate their software delivery processes. This has helped organizations to accelerate their software development and deployment cycles.\n\n8. Focus on Security: AWS has put a strong emphasis on security and compliance, offering a wide range of security services and compliance programs to help customers secure their applications and data on the cloud.\n\n9. Partner Ecosystem: AWS has built a strong partner ecosystem, with thousands of technology and consulting partners offering solutions and services on top of AWS. This has helped customers to easily find and implement the solutions they need for their business.\n\n10. User-Friendly Interfaces: AWS has evolved its user interface to be more user-friendly and intuitive, making it easier for customers to manage and monitor their services on the cloud.</p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHAIN answer questions**"
      ],
      "metadata": {
        "id": "fSrv29xnq-mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "#print('chain to answer questions')\n",
        "print(\"-\" * 80)\n",
        "print()\n",
        "result = qa({\"query\": query})\n",
        "print(f'Query: {result[\"query\"]}\\n')\n",
        "print(f'Result: {result[\"result\"]}\\n')\n",
        "print(f'Context Documents: ')\n",
        "for srcdoc in result[\"source_documents\"]:\n",
        "      print(f'{srcdoc}\\n')\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ajr1Cja03Q",
        "outputId": "8bfbc098-cec1-47b6-c5f0-45a136104e38"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query: How AWS has evolved?\n",
            "\n",
            "Result:  AWS has evolved by offering customers more functionality and becoming a game-changing platform. \n",
            "\n",
            "Context Documents: \n",
            "page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.' metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "\n",
            "page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.' metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
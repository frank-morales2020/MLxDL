{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "F3Zb2mqu3AcA",
        "Ucspq-tBeq-X",
        "B2l5Jf1dvfHn",
        "HpnoN8VZqlQz",
        "whs9_Qwsq0kw"
      ],
      "authorship_tag": "ABX9TyP4fK+A8S9MvqGIZfLCOSch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/cmapss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installation"
      ],
      "metadata": {
        "id": "Ucspq-tBeq-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 OR L4 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install accelerate --quiet"
      ],
      "metadata": {
        "id": "JARmny3ZO_5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "B2l5Jf1dvfHn"
      }
    },
    {
      "source": [
        "# Dynamically check for sliding window support in flash_attn\n",
        "_flash_supports_window_size = False  # Initialize to False\n",
        "try:\n",
        "    import flash_attn  # Try to import flash_attn\n",
        "\n",
        "    if hasattr(flash_attn, \"flash_attn_func\"):\n",
        "        from flash_attn.flash_attn_interface import _flash_supports_window_size\n",
        "    else:\n",
        "        from flash_attn.flash_attention import _flash_supports_window_size\n",
        "except ImportError:\n",
        "    pass  # If flash_attn is not installed, keep _flash_supports_window_size as False"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gU76uaBdEQCz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TrainingArguments\n",
        "import accelerate\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = accelerate.Accelerator()\n",
        "\n",
        "#!pip install diffusers safetensors  --quiet\n",
        "#!pip install colab-env --quiet\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\")\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "8jEIt5AjErhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Device: {device}')"
      ],
      "metadata": {
        "id": "TrjM_pByFAcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "HpnoN8VZqlQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# --- Data Loading from Google Drive ---\n",
        "zip_path = '/content/gdrive/MyDrive/datasets/CMAPSSData.zip'\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "!mkdir -p /content/gdrive/MyDrive/datasets/CMAPSSData/\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(f\"Error: CMAPSSData.zip not found at {zip_path}. Please ensure the file is correctly located in your Google Drive.\")\n",
        "    raise FileNotFoundError(f\"CMAPSSData.zip not found at {zip_path}\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        if zip_ref.testzip() is None:  # Check for ZIP file integrity\n",
        "            zip_ref.extractall(extract_dir)\n",
        "            print(f\"Extracted dataset files to: {extract_dir}\")\n",
        "        else:\n",
        "            print(\"Error: ZIP file integrity check failed. The file may not be a valid ZIP file.\")\n",
        "            raise zipfile.BadZipFile(\"ZIP file integrity check failed.\")\n",
        "\n",
        "except zipfile.BadZipFile as e:\n",
        "    print(f\"Error extracting ZIP file: {e}\")\n",
        "    print(\n",
        "        \"The uploaded file may not be a valid or complete ZIP file. \"\n",
        "        \"Please ensure you have uploaded the correct file, that it is not corrupted, \"\n",
        "        \"and that it is a standard ZIP archive.\"\n",
        "    )\n",
        "    raise  # Stop execution if extraction fails\n",
        "\n",
        "# --- Prepare NASA CMAPSS Data and Save to JSONL in GCS ---\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Process all four subsets\n",
        "data_subsets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "\n",
        "for data_subset in data_subsets:\n",
        "    train_file = os.path.join(extract_dir, f'train_{data_subset}.txt')\n",
        "    test_file = os.path.join(extract_dir, f'test_{data_subset}.txt')\n",
        "    rul_file = os.path.join(extract_dir, f'RUL_{data_subset}.txt')\n",
        "\n",
        "    SENSOR_COLUMNS = ['sensor' + str(i).zfill(2) for i in range(1, 22)]\n",
        "    OP_SETTING_COLUMNS = ['op_setting_' + str(i) for i in range(1, 4)]\n",
        "    DATA_COLUMNS = ['unit_nr', 'time_cycles'] + OP_SETTING_COLUMNS + SENSOR_COLUMNS\n",
        "\n",
        "    # Load training data\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        test_df = pd.read_csv(test_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        rul_df = pd.read_csv(rul_file, names=['RUL'], delim_whitespace=True, header=None)\n",
        "\n",
        "        train_df.columns = DATA_COLUMNS\n",
        "        test_df.columns = DATA_COLUMNS\n",
        "\n",
        "        print(f\"\\nProcessing data subset: {data_subset}\")\n",
        "        print(\"Shape of train_df after loading:\", train_df.shape)\n",
        "        print(\"train_df head after loading:\\n\", train_df.head())\n",
        "        print(\"Shape of test_df:\", test_df.shape)\n",
        "        print(\"test_df head after loading:\\n\", test_df.head())\n",
        "        print(\"Shape of RUL data:\", rul_df.shape)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data files for subset {data_subset}: {e}\")\n",
        "        raise  # Stop execution if a file is missing\n",
        "\n",
        "    def create_jsonl(df, rul_df, output_path, sequence_length=30, is_test=False):\n",
        "        grouped_data = df.groupby('unit_nr')\n",
        "        rul_values = rul_df.values.tolist()  # Convert RUL DataFrame to list\n",
        "        engine_count = 0  # To track which RUL value to use\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            for unit_nr, unit_data in grouped_data:\n",
        "                num_cycles = len(unit_data)\n",
        "                data_values = unit_data.drop(['unit_nr'], axis=1).values.tolist()\n",
        "                json_data = []  # Initialize an empty list to hold JSON objects\n",
        "\n",
        "                for i in range(max(0, num_cycles - sequence_length + 1)):\n",
        "                    sequence = data_values[i:i + sequence_length]\n",
        "                    rul = num_cycles - (i + sequence_length)\n",
        "\n",
        "                    # Ensure RUL is not out of bounds\n",
        "                    if engine_count < len(rul_values):\n",
        "                        current_rul = rul_values[engine_count][0]  # Get the RUL value\n",
        "                    else:\n",
        "                        current_rul = 0  # Or some default value if RUL data is exhausted\n",
        "\n",
        "                    if len(sequence) == sequence_length:\n",
        "                        json_record = {\"sequence\": sequence, \"sequence_length\": len(sequence), \"rul\": current_rul}  # Include sequence length\n",
        "                        json_data.append(json_record)\n",
        "\n",
        "                # Write all JSON objects to the file at once\n",
        "                with open(output_path, 'w') as f:\n",
        "                    for json_record in json_data:\n",
        "                        f.write(json.dumps(json_record) + '\\n')\n",
        "\n",
        "                engine_count += 1  # Increment engine counter\n",
        "\n",
        "    local_train_jsonl_path = f\"cmapss_{data_subset}_train_sequences.jsonl\"\n",
        "    local_test_jsonl_path = f\"cmapss_{data_subset}_test_sequences.jsonl\"\n",
        "\n",
        "    # Create JSONL for training\n",
        "    create_jsonl(train_df, rul_df, local_train_jsonl_path, is_test=False)\n",
        "    print(f\"Created {local_train_jsonl_path}\")\n",
        "\n",
        "    # Create JSONL for testing\n",
        "    create_jsonl(test_df, rul_df, local_test_jsonl_path, is_test=True)\n",
        "    print(f\"Created {local_test_jsonl_path}\")\n",
        "\n",
        "!cp *.jsonl /content/gdrive/MyDrive/datasets/CMAPSSData/\n",
        "print(\"JSONL files created and uploaded.\")"
      ],
      "metadata": {
        "id": "smsD0LA2wvUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def create_textual_dataset(input_file, output_file):\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                sequence = data.get(\"sequence\")\n",
        "                rul = data.get(\"rul\") # Assuming your data has an RUL\n",
        "\n",
        "                if sequence:\n",
        "                    # Create a simple textual description (you can make this more sophisticated)\n",
        "                    description = f\"Engine sensor readings over time: {np.array(sequence).flatten().tolist()}\"\n",
        "                    if rul is not None:\n",
        "                        output_data = {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": description}]}, {\"role\": \"model\", \"parts\": [{\"text\": f\"Remaining Useful Life: {rul}\"}]}]}\n",
        "                        outfile.write(json.dumps(output_data) + '\\n')\n",
        "                    else:\n",
        "                        output_data = {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": description}]}, {\"role\": \"model\", \"parts\": [{\"text\": \"RUL prediction needed.\"}]}]}\n",
        "                        outfile.write(json.dumps(output_data) + '\\n')\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Define your input and output file paths\n",
        "input_train_file = \"cmapss_FD004_train_sequences.jsonl\"\n",
        "output_train_file_text = \"cmapss_FD004_train_text.jsonl\"\n",
        "\n",
        "input_test_file = \"cmapss_FD004_test_sequences.jsonl\"\n",
        "output_test_file_text = \"cmapss_FD004_test_text.jsonl\"\n",
        "\n",
        "# Create the textual datasets\n",
        "create_textual_dataset(input_train_file, output_train_file_text)\n",
        "create_textual_dataset(input_test_file, output_test_file_text)\n",
        "\n",
        "print(f\"Textual training data created: {output_train_file_text}\")\n",
        "print(f\"Textual testing data created: {output_test_file_text}\")"
      ],
      "metadata": {
        "id": "p5ROyp9Omqt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def transform_jsonl_to_prompt_completion(input_file_path, output_file_path):\n",
        "    \"\"\"Transforms chat-style JSONL to prompt-completion JSONL.\"\"\"\n",
        "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                # Extract prompt and completion from 'contents'\n",
        "                prompt = \"\".join([part[\"text\"] for part in data[\"contents\"][0][\"parts\"]])  # Assumes user role is first\n",
        "                completion = str(data.get(\"completion\", \"\")) # Handle if completion is missing\n",
        "\n",
        "                # Construct prompt-completion dictionary\n",
        "                prompt_completion_data = {\"prompt\": prompt, \"completion\": completion}\n",
        "\n",
        "                # Write to output file\n",
        "                outfile.write(json.dumps(prompt_completion_data) + \"\\n\")\n",
        "\n",
        "            except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
        "                print(f\"Skipping invalid or unprocessable line: {line.strip()}, Error: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "input_file_path = \"cmapss_FD004_train_text.jsonl\"\n",
        "output_file_path = \"cmapss_FD004_train_text_transformed.jsonl\"\n",
        "\n",
        "transform_jsonl_to_prompt_completion(input_file_path, output_file_path)\n",
        "print(f\"Transformed data written to: {output_file_path}\")\n",
        "\n",
        "input_file_path = \"cmapss_FD004_test_text.jsonl\"\n",
        "output_file_path = \"cmapss_FD004_test_text_transformed.jsonl\"\n",
        "\n",
        "transform_jsonl_to_prompt_completion(input_file_path, output_file_path)\n",
        "print(f\"Transformed data written to: {output_file_path}\")"
      ],
      "metadata": {
        "id": "4aFALsICkXKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"contents\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"Engine sensor readings over time: [1.0, 41.9993, 0.8409, 100.0, 445.0, 548.68, 1343.85, 1111.03, 3.91, 5.69, 137.26, 2211.96, 8296.96, ..., 8054.65, 9.2728, 0.02, 331.0, 2223.0, 100.0, 14.78, 8.8922]\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"Remaining Useful Life: 0\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "ll_5xp9ZfpXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "whs9_Qwsq0kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "#model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# Set padding token if not present (common requirement for training)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Instead of using the unk_token, add a dedicated padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model.resize_token_embeddings(len(tokenizer)) #Important: update the model's embedding layer to accommodate the new padding token.\n",
        "print(\"Model and tokenizer loaded.\")"
      ],
      "metadata": {
        "id": "90P0CGcWCAe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning"
      ],
      "metadata": {
        "id": "5e8SYqr9qN9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset # Example for loading data\n",
        "from trl import SFTTrainer # Simplified Fine-tuning Trainer\n",
        "from peft import LoraConfig # For LoRA efficient tuning\n",
        "from transformers import TrainingArguments\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import gc # Import the garbage collector\n",
        "\n",
        "\n",
        "# Ignore all future warnings\n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=DeprecationWarning)\n",
        "simplefilter(action='ignore', category=UserWarning)\n",
        "simplefilter(action='ignore', category=RuntimeWarning)\n",
        "simplefilter(action='ignore', category=Warning)\n",
        "simplefilter(action='ignore', category=ResourceWarning)\n",
        "simplefilter(action='ignore')\n",
        "simplefilter(action='ignore', category=UnicodeWarning)\n",
        "\n",
        "# 1. --- Prepare your Dataset ---\n",
        "# Needs to be in a format the trainer understands (e.g., instruction/response pairs)\n",
        "# Example: Load a dataset from Hugging Face Hub\n",
        "# dataset = load_dataset(\"your_dataset_name\", split=\"train\")\n",
        "# Or create your own Dataset object\n",
        "# Formatted dataset usually has a 'text' column with structured prompts/responses\n",
        "print(\"Preparing dataset...\")\n",
        "\n",
        "\n",
        "#Load your datasets\n",
        "train_dataset = load_dataset(\"json\", data_files=\"/content/cmapss_FD004_train_text.jsonl\", split=\"train\")\n",
        "test_dataset = load_dataset(\"json\", data_files=\"/content/cmapss_FD004_test_text.jsonl\", split=\"train\") # Using 'train' split for test as well for simplicity\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Process each example individually within the batch\n",
        "    processed_examples = []  # Store processed examples here\n",
        "\n",
        "    for example in examples['contents']:\n",
        "        prompt = \"\".join([part[\"text\"] for part in example[0][\"parts\"]])\n",
        "        completion = \"\".join([part[\"text\"] for part in example[1][\"parts\"]])\n",
        "        #Combine the prompt and completion and then tokenize\n",
        "        inputs = tokenizer(prompt + completion, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "        # Extract RUL information from the completion instead of the prompt\n",
        "        # Assume the completion has the format \"Remaining Useful Life: {RUL}\"\n",
        "        try:\n",
        "            rul = int(completion.split(\"Remaining Useful Life: \")[-1])\n",
        "        except ValueError:\n",
        "            # If the completion format is incorrect or RUL is missing, set rul to 0\n",
        "            rul = 0\n",
        "\n",
        "        #Add the RUL to the inputs to be passed to the model\n",
        "        inputs['labels'] = inputs['input_ids'].clone()\n",
        "        inputs['rul'] = rul\n",
        "\n",
        "        processed_examples.append(inputs)  # Append the dictionary\n",
        "\n",
        "    # Stack tensors to get correct dimensions\n",
        "    input_ids = torch.stack([d['input_ids'] for d in processed_examples]).squeeze(1)\n",
        "    labels = torch.stack([d['labels'] for d in processed_examples]).squeeze(1)\n",
        "    ruls = torch.tensor([d['rul'] for d in processed_examples])\n",
        "\n",
        "    return {'input_ids': input_ids, 'labels': labels, 'rul': ruls}  # Return the final dictionary\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# 2. --- Configure LoRA (Optional but recommended for efficiency) ---\n",
        "lora_config = LoraConfig(\n",
        "     r=16, # Rank\n",
        "     lora_alpha=32,\n",
        "     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Adapt these layers\n",
        "     lora_dropout=0.05,\n",
        "     bias=\"none\",\n",
        "     task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# 3. --- Define Training Arguments ---\n",
        "output_dir = \"./llama3-8b-finetuned\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=5,            # Adjust as needed\n",
        "    per_device_train_batch_size=1, # Adjust based on your GPU memory\n",
        "    gradient_accumulation_steps=8, # Increased gradient accumulation steps\n",
        "    gradient_checkpointing=True,   # Enabled gradient checkpointing\n",
        "    learning_rate=2e-4,            # Adjust as needed\n",
        "    max_grad_norm=0.3,             # Gradient clipping\n",
        "    weight_decay= 0.01,           # Regularization\n",
        "    logging_steps=10,\n",
        "    optim=\"paged_adamw_8bit\",      # Optimizer for quantized models\n",
        "    save_strategy=\"steps\",\n",
        "    eval_strategy=\"steps\",  # Evaluate at specified intervals\n",
        "    eval_steps=10,             # Evaluate every 100 steps (adjust as needed)\n",
        "    #eval_strategy=\"no\",  # Disable evaluation during training to save memory\n",
        "    load_best_model_at_end=True,  # Load the best model based on validation\n",
        "    metric_for_best_model=\"loss\",  # Metric to use for selecting the best model\n",
        "    report_to=\"none\",\n",
        "    # Add more arguments as needed (fp16, etc.)\n",
        ")\n",
        "\n",
        "# 4. --- Create the Trainer ---\n",
        "# Using SFTTrainer for supervised fine-tuning on conversational/instruction data\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,  # Pass LoRA config if using PEFT/LoRA\n",
        "    train_dataset=train_dataset, # Your formatted training dataset\n",
        "    eval_dataset=test_dataset,   # Your formatted testing dataset (for evaluation)\n",
        ")\n",
        "\n",
        "# 5. --- Run Fine-Tuning ---\n",
        "print(\"Starting fine-tuning...\")\n",
        "# This is the core training step\n",
        "trainer.train()\n",
        "print(\"Fine-tuning finished (Conceptual - train() call commented out).\")\n",
        "\n",
        "# Delete unused variables to free up memory\n",
        "del train_dataset, test_dataset\n",
        "gc.collect()  # Run garbage collection\n",
        "\n",
        "\n",
        "# 6. --- Save the Fine-Tuned Model (Adapter or Full Model) ---\n",
        "print(f\"Saving fine-tuned model to {output_dir}...\")\n",
        "# trainer.save_model(output_dir) # Saves adapter config (if LoRA) & weights\n",
        "# Alternatively, if not using LoRA or want to merge weights:\n",
        "# merged_model = model.merge_and_unload() # Merge LoRA weights back if needed\n",
        "# merged_model.save_pretrained(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)\n",
        "print(\"Model saved.\")"
      ],
      "metadata": {
        "id": "0bLhaqiYcbih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step\tTraining Loss\tValidation Loss\n",
        "10\t1.380800\t1.063199\n",
        "20\t0.957200\t0.898772\n",
        "30\t0.773000\t0.742648\n",
        "40\t0.633900\t0.658089\n",
        "50\t0.578800\t0.605854\n",
        "60\t0.531600\t0.603501\n",
        "70\t0.511300\t0.560742\n",
        "80\t0.477200\t0.554871\n",
        "90\t0.465600\t0.555977\n",
        "100\t0.455800\t0.551059\n",
        "110\t0.450000\t0.550248\n",
        "120\t0.447200\t0.546723\n",
        "130\t0.431200\t0.558591\n",
        "140\t0.433100\t0.556804"
      ],
      "metadata": {
        "id": "KrSdyNMRRKW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation"
      ],
      "metadata": {
        "id": "LV6GXgaCx1DU"
      }
    },
    {
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = load_dataset(\"json\", data_files=\"/content/cmapss_FD004_test_text.jsonl\", split=\"train\")\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Get predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Get actual RUL values\n",
        "actual_rul = test_dataset['rul']\n",
        "\n",
        "# Get predicted RUL values (logits for the last token)\n",
        "predicted_rul = predictions.predictions[0][:, -1]\n",
        "predicted_rul = predicted_rul[:len(actual_rul)]  # Truncate to match actual RUL length\n",
        "\n",
        "# Get predicted token IDs using argmax\n",
        "predicted_token_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
        "\n",
        "# Extract the last token ID as the predicted RUL\n",
        "#predicted_rul_decoded = [seq[-1] for seq in predicted_token_ids]  # Removed this line\n",
        "predicted_rul_decoded = predicted_token_ids[-1]  # Get the last element\n",
        "\n",
        "# Decode the predicted token ID to get the RUL value as a string\n",
        "predicted_rul_decoded = tokenizer.decode(predicted_rul_decoded, skip_special_tokens=True)\n",
        "\n",
        "# Convert the decoded RUL string to an integer\n",
        "try:\n",
        "    predicted_rul_decoded = int(predicted_rul_decoded)\n",
        "except ValueError:\n",
        "    # Handle cases where the decoded string is not a valid integer (e.g., empty string)\n",
        "    predicted_rul_decoded = 0  # Or any other default value you prefer\n",
        "\n",
        "# Duplicate the single predicted RUL to match the length of actual RUL\n",
        "predicted_rul_decoded = [predicted_rul_decoded] * len(actual_rul) # Modified this line to to fill predicted_rul_decoded using predicted_rul\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(actual_rul, predicted_rul_decoded)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(actual_rul, predicted_rul_decoded)\n",
        "\n",
        "print(f\"MSE: {mse}, RMSE: {rmse}, R2: {r2}\")\n",
        "#MSE: 676.0, RMSE: 26.0, R2: 0.0"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mfppeSSLpAKM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnuK8HAF0USPTjs3SCY3Kw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/multi_GPU_computing_lambda_ai_cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBXSy2xuCans"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import cuda\n",
        "import math\n",
        "import time\n",
        "from scipy.stats import norm # For Black-Scholes comparison\n",
        "\n",
        "# --- Option Parameters ---\n",
        "S0 = 100.0  # Start Price\n",
        "K = 100.0   # Strike Price\n",
        "r = 0.02    # Risk-free Interest Rate\n",
        "sigma = 0.20  # Volatility\n",
        "T = 1.0     # Time to Maturity (years)\n",
        "N_STEPS = 100 # Number of Time Steps\n",
        "N_SIMULATIONS = 1_000_000 # Use 1 million simulations\n",
        "\n",
        "# --- Numba CUDA Kernel ---\n",
        "@cuda.jit\n",
        "def monte_carlo_option_kernel(s0, k, r, sigma, dt, n_steps, paths_device, payoffs_device):\n",
        "    \"\"\"\n",
        "    CUDA kernel to simulate stock paths and calculate option payoffs.\n",
        "    Each thread simulates one path.\n",
        "    \"\"\"\n",
        "    # Get the index of the current thread (which corresponds to a simulation path)\n",
        "    tid = cuda.grid(1)\n",
        "\n",
        "    # Only process if the thread index is within the number of simulations\n",
        "    if tid < paths_device.shape[0]:\n",
        "        s_t = s0\n",
        "        # Use the pre-generated random numbers specific to this path\n",
        "        path_random_numbers = paths_device[tid, :]\n",
        "\n",
        "        # Simulate the stock price path using Geometric Brownian Motion\n",
        "        for i in range(n_steps):\n",
        "            # Ensure numerical stability: Check for potential issues before exp\n",
        "            # This check is often not strictly necessary for standard GBM with\n",
        "            # reasonable parameters, but can help debug infinities.\n",
        "            # The primary fix for inf with N=100 was increasing N_SIMULATIONS\n",
        "            # which makes the average more stable.\n",
        "            diffusion_term = sigma * math.sqrt(dt) * path_random_numbers[i]\n",
        "            drift_term = (r - 0.5 * sigma**2) * dt\n",
        "\n",
        "            # Potential point of numerical error if drift_term + diffusion_term is huge\n",
        "            # However, with standard parameters, this is unlikely.\n",
        "            log_return = drift_term + diffusion_term\n",
        "\n",
        "            # Check if log_return is finite before exponentiating\n",
        "            # if not math.isfinite(log_return):\n",
        "            #     # Handle non-finite case - for MC, often means this path is discarded\n",
        "            #     # or results in a 0 payoff. Setting payoff to 0 is safer than inf.\n",
        "            #     s_t = 0.0 # Will result in 0 payoff\n",
        "            #     break # Stop simulating this path\n",
        "\n",
        "            s_t = s_t * math.exp(log_return)\n",
        "\n",
        "        # Calculate the payoff at maturity T\n",
        "        payoff = max(0.0, s_t - k)\n",
        "\n",
        "        # Store the calculated payoff for this path\n",
        "        payoffs_device[tid] = payoff\n",
        "\n",
        "# --- CPU Monte Carlo Function (for comparison) ---\n",
        "def monte_carlo_option_cpu(s0, k, r, sigma, t, n_steps, n_simulations):\n",
        "    dt = t / n_steps\n",
        "    total_payoff = 0.0\n",
        "\n",
        "    for i in range(n_simulations):\n",
        "        s_t = s0\n",
        "        # Generate random numbers for each step within the loop (less efficient than batching)\n",
        "        # Or generate batch beforehand for fairer comparison with GPU preparation\n",
        "        random_numbers = np.random.standard_normal(n_steps)\n",
        "        for j in range(n_steps):\n",
        "            s_t = s_t * np.exp((r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * random_numbers[j])\n",
        "\n",
        "        payoff = max(0.0, s_t - k)\n",
        "        total_payoff += payoff\n",
        "\n",
        "    # Discount the average payoff back to present value\n",
        "    option_price = (total_payoff / n_simulations) * np.exp(-r * t)\n",
        "    return option_price\n",
        "\n",
        "# --- Black-Scholes Formula (for theoretical benchmark) ---\n",
        "def black_scholes_price(S, K, T, r, sigma):\n",
        "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "    call_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
        "    return call_price\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(\"----------------------------------------\")\n",
        "print(\"European Call Option Pricing using Monte Carlo Simulation\")\n",
        "print(\"----------------------------------------\")\n",
        "# Note: The script detected 8 GPUs in your output. This Numba code\n",
        "# targets one GPU per process. To use 8 GPUs, you would launch this\n",
        "# script 8 times, each on a different GPU, or use a framework like\n",
        "# PyTorch/Accelerate that orchestrates this. The Numba kernel itself\n",
        "# doesn't manage multiple devices simultaneously unless explicitly coded.\n",
        "# We'll correct the single-GPU Numba part which was causing the error/warning.\n",
        "print(f\"Number of Simulations: {N_SIMULATIONS:,}\")\n",
        "print(f\"Start Price: {S0:.2f}\")\n",
        "print(f\"Strike Price: {K:.2f}\")\n",
        "print(f\"Expected Return (mu): {r:.2f}\") # Using r as drift proxy for printing consistency\n",
        "print(f\"Volatility (sigma): {sigma:.2f}\")\n",
        "print(f\"Time to Maturity (T): {T:.2f} years\")\n",
        "print(f\"Number of Time Steps: {N_STEPS}\")\n",
        "print(f\"Risk-free Interest Rate (r): {r:.2f}\")\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "\n",
        "# --- CPU Calculation ---\n",
        "start_time_cpu = time.time()\n",
        "option_price_cpu = monte_carlo_option_cpu(S0, K, r, sigma, T, N_STEPS, N_SIMULATIONS)\n",
        "end_time_cpu = time.time()\n",
        "cpu_time = end_time_cpu - start_time_cpu\n",
        "\n",
        "print(f\"Option Price (CPU): {option_price_cpu:.4f}\")\n",
        "print(f\"Execution Time (CPU): {cpu_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# --- GPU Calculation ---\n",
        "# Prepare data and kernel launch configuration\n",
        "dt = T / N_STEPS\n",
        "# Generate all random numbers needed on the CPU first\n",
        "# Shape is (N_SIMULATIONS, N_STEPS)\n",
        "random_numbers_cpu = np.random.standard_normal((N_SIMULATIONS, N_STEPS)).astype(np.float64)\n",
        "\n",
        "# Determine kernel launch configuration\n",
        "threads_per_block = 512 # Common choice, can be tuned\n",
        "# Calculate the required number of blocks\n",
        "blocks_per_grid = (N_SIMULATIONS + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "# Allocate device memory and transfer data\n",
        "# Use float64 matching numpy default and standard for financial calcs\n",
        "random_numbers_device = cuda.to_device(random_numbers_cpu)\n",
        "# Allocate space for payoffs on the device\n",
        "payoffs_device = cuda.device_array(N_SIMULATIONS, dtype=np.float64)\n",
        "\n",
        "start_time_gpu = time.time()\n",
        "\n",
        "# Launch the CUDA kernel\n",
        "# Pass the launch configuration (blocks_per_grid, threads_per_block) before arguments\n",
        "monte_carlo_option_kernel[blocks_per_grid, threads_per_block](\n",
        "    S0, K, r, sigma, dt, N_STEPS, random_numbers_device, payoffs_device\n",
        ")\n",
        "\n",
        "# Synchronize device to ensure kernel completes before stopping timer\n",
        "cuda.synchronize()\n",
        "\n",
        "end_time_gpu = time.time()\n",
        "gpu_time = end_time_gpu - start_time_gpu\n",
        "\n",
        "# Transfer payoffs back to host (CPU)\n",
        "payoffs_cpu = payoffs_device.copy_to_host()\n",
        "\n",
        "# Calculate the final option price on the CPU (summation/average)\n",
        "# This reduction could also be done on the GPU for larger speedup on very many simulations\n",
        "total_payoff_gpu = np.sum(payoffs_cpu)\n",
        "option_price_gpu = (total_payoff_gpu / N_SIMULATIONS) * np.exp(-r * T)\n",
        "\n",
        "print(f\"Option Price (GPU): {option_price_gpu:.4f}\")\n",
        "print(f\"Execution Time (GPU): {gpu_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# --- Speedup and Black-Scholes Comparison ---\n",
        "speedup = cpu_time / gpu_time if gpu_time > 0 else float('inf')\n",
        "print(f\"Speedup (CPU vs. GPU): {speedup:.2f}x\")\n",
        "\n",
        "black_scholes = black_scholes_price(S0, K, T, r, sigma)\n",
        "print(f\"Option Price (Black-Scholes): {black_scholes:.4f}\")\n",
        "\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Device used for calculations: GPU (via Numba CUDA kernel)\")\n",
        "print(\"----------------------------------------\")"
      ]
    }
  ]
}
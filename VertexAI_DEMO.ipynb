{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "airPyx-2aoHe",
        "6pmjTE1-ZGBW",
        "srLTVr_QZPXR"
      ],
      "authorship_tag": "ABX9TyOQBYaZaDu9tLVygWcIJDqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/VertexAI_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "airPyx-2aoHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpsTOa9FzURt"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install --upgrade --quiet gcsfs==2024.3.1\n",
        "!pip install --upgrade --quiet accelerate==0.31.0\n",
        "!pip install --upgrade --quiet transformers==4.45.2\n",
        "!pip install --upgrade --quiet datasets==2.19.2\n",
        "!pip install --upgrade google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai --upgrade -q\n",
        "!pip install tensorflow_datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "QstV8F_f0lbE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset - Vertex AI"
      ],
      "metadata": {
        "id": "6pmjTE1-ZGBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "# ** Data Preparation**\n",
        "\n",
        "# Authentication and Initialization**\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Convert to JSONL format with prompt and completion\n",
        "def convert_to_jsonl(data, filename):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for row in data:\n",
        "            data_point = {\n",
        "                \"prompt\": row[\"input\"],\n",
        "                \"completion\": str(row[\"label\"]),  # Convert label to string\n",
        "            }\n",
        "            f.write(json.dumps(data_point) + \"\\n\")\n",
        "\n",
        "# Convert the Hugging Face Dataset to a list of dictionaries\n",
        "dataset_list = list(dataset[\"train\"])\n",
        "\n",
        "# Split the dataset into training and evaluation sets\n",
        "train_data, eval_data = train_test_split(dataset_list, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert and save to JSONL files\n",
        "convert_to_jsonl(train_data, \"/content/training_data.jsonl\")\n",
        "convert_to_jsonl(eval_data, \"/content/eval_data.jsonl\")\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
        "print('\\n\\n')\n",
        "\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "print('\\n\\n')\n",
        "# Upload to GCS\n",
        "!gsutil ls gs://{BUCKET_NAME}/\n",
        "print('\\n\\n')\n",
        "!gsutil cp  /content/training_data.jsonl gs://{BUCKET_NAME}/\n",
        "!gsutil cp  /content/eval_data.jsonl gs://{BUCKET_NAME}/\n",
        "print('\\n\\n')"
      ],
      "metadata": {
        "id": "gP6sYGY2OY1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train_data: {len(train_data)}\")\n",
        "print(train_data[0])\n",
        "print(f\"Length of eval_data: {len(eval_data)}\")\n",
        "print(eval_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wka0fLPbOvGm",
        "outputId": "62370599-1a5c-4304-a57c-b7723b0ead43"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train_data: 1600\n",
            "{'input': 'Calculate the waypoints from JFK to DUB. Departure: 2024-09-14, Aircraft: Boeing 757, Weather: Sunny', 'label': 7, 'distance': 6176.2203486565095, 'distance_category': 'long', 'waypoints': [[40.642947899999996, -73.7793733748521], [45.298594095237384, -17.78586892190225], [42.855448053630305, -47.16961137636025], [43.690065289044455, -37.13166134152348], [42.692446676984524, -49.13003060202848], [44.71641545571105, -24.787737379620857], [44.56731446277484, -26.580976548106896], [45.52410760535687, -15.0736156269691], [47.2956874, 6.2331927]], 'waypoint_names': ['JFK', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'DUB']}\n",
            "Length of eval_data: 400\n",
            "{'input': 'Calculate the waypoints from SAN to LAS. Departure: 2024-01-23, Aircraft: Boeing 777, Weather: Stormy', 'label': 7, 'distance': 16993.35324438778, 'distance_category': 'longhaul', 'waypoints': [[7.0000085, -73.2500086], [17.469538816793573, 68.81040414006927], [17.59575212940935, 70.52298481379142], [12.65812849864566, 3.524673550843133], [11.117766492667563, -17.37640373887689], [10.942810020379422, -19.750377347313957], [11.885466287035268, -6.95953246114577], [13.515304876727356, 15.155647088022548], [20.0171109, 103.378253]], 'waypoint_names': ['SAN', 'Waypoint', 'Waypoint', 'Touloumay', 'Waypoint', 'Waypoint', 'Conze', 'Boui Alhadji Kalari', 'LAS']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create Vertex AI TextDatasets**\n",
        "\n",
        "# Training dataset\n",
        "train_dataset = aiplatform.TextDataset.create(\n",
        "    display_name=\"waypoints-train\",\n",
        "    gcs_source=[f\"gs://{BUCKET_NAME}/training_data.jsonl\"],  # Replace with your GCS bucket and path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
        ")\n",
        "\n",
        "# Evaluation dataset\n",
        "eval_dataset = aiplatform.TextDataset.create(\n",
        "    display_name=\"waypoints-eval\",\n",
        "    gcs_source=[f\"gs://{BUCKET_NAME}/eval_data.jsonl\"],  # Replace with your GCS bucket and path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
        ")\n",
        "\n",
        "print(f\"Training dataset created: {train_dataset.resource_name}\")\n",
        "print(f\"Evaluation dataset created: {eval_dataset.resource_name}\")"
      ],
      "metadata": {
        "id": "JM8WNwbQPcou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import json\n",
        "\n",
        "# Initialize a GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Get the GCS bucket and file path from gcs_source\n",
        "bucket_name = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "file_path = \"training_data.jsonl\"  # Replace if your file path is different\n",
        "\n",
        "# Get the bucket and blob (file)\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "\n",
        "# Download the file content as a string\n",
        "file_content = blob.download_as_string().decode(\"utf-8\")\n",
        "\n",
        "# Iterate through the first five lines (JSON objects) and print\n",
        "for i, line in enumerate(file_content.splitlines()):\n",
        "    if i >= 5:\n",
        "        break  # Stop after 5 records\n",
        "    data_point = json.loads(line)\n",
        "    print(f\"Record {i + 1}:\")\n",
        "    print(f\"  Prompt: {data_point['prompt']}\")\n",
        "    print(f\"  Completion: {data_point['completion']}\")\n",
        "    print(\"-\" * 20)  # Separator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUVrnCndX9_K",
        "outputId": "f7f9e62d-cf5e-45f4-9d42-505d006df8df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record 1:\n",
            "  Prompt: Calculate the waypoints from JFK to DUB. Departure: 2024-09-14, Aircraft: Boeing 757, Weather: Sunny\n",
            "  Completion: 7\n",
            "--------------------\n",
            "Record 2:\n",
            "  Prompt: Calculate the waypoints from ORD to SEA. Departure: 2024-03-14, Aircraft: Boeing 757, Weather: Snowy\n",
            "  Completion: 4\n",
            "--------------------\n",
            "Record 3:\n",
            "  Prompt: Calculate the waypoints from SEA to KUL. Departure: 2024-03-23, Aircraft: Airbus A330, Weather: Rainy\n",
            "  Completion: 8\n",
            "--------------------\n",
            "Record 4:\n",
            "  Prompt: Calculate the waypoints from MUC to SCL. Departure: 2024-11-22, Aircraft: Boeing 777, Weather: Overcast\n",
            "  Completion: 7\n",
            "--------------------\n",
            "Record 5:\n",
            "  Prompt: Calculate the waypoints from DEN to NAS. Departure: 2024-04-08, Aircraft: Boeing 767, Weather: Snowy\n",
            "  Completion: 5\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Docker - Artifact Registry  "
      ],
      "metadata": {
        "id": "srLTVr_QZPXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud artifacts repositories create my-repo-tmp \\\n",
        "    --repository-format=docker \\\n",
        "    --location=us-central1 \\\n",
        "    --description=\"My Docker repository\" \\\n",
        "    --project={project_id}"
      ],
      "metadata": {
        "id": "1W7kJMMgBPLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import colab_env\n",
        "\n",
        "from google.cloud import storage # Import the storage module\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# 1. Get Project ID and Region from Environment Variables\n",
        "project_id = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "\n",
        "IMAGE_URI = os.environ.get(\n",
        "    \"IMAGE_URI\", f\"us-central1-docker.pkg.dev/{project_id}/my-repo-tmp/my-pytorch-image:latest\"\n",
        ")\n",
        "\n",
        "### ---------\n",
        "\n",
        "\n",
        "# *** Define and create trainer/train.py ***\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer  # Import tokenizer (example using transformers)\n",
        "import torch.distributed as dist\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# CustomDataset definition\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.data = []\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    example = json.loads(line)\n",
        "\n",
        "                    # Validate and cast 'completion' to int\n",
        "                    completion_value = example[\"completion\"]\n",
        "                    if not isinstance(completion_value, int):\n",
        "                        try:\n",
        "                            completion_value = int(completion_value)\n",
        "                        except ValueError:\n",
        "                            logging.warning(f\"Skipping invalid 'completion' value: {completion_value} in line: {line}\")\n",
        "                            continue  # Skip this line\n",
        "\n",
        "                    # Flatten and stringify 'prompt'\n",
        "                    prompt = example[\"prompt\"]\n",
        "                    if isinstance(prompt, (list, tuple)):\n",
        "                        prompt = \" \".join([str(item) for item in prompt])\n",
        "                    elif isinstance(prompt, dict):\n",
        "                        prompt = json.dumps(prompt)\n",
        "                    else:\n",
        "                        prompt = str(prompt)\n",
        "\n",
        "                    self.data.append((prompt, completion_value))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(f\"Skipping invalid JSON line: {line}, Error: {e}\")\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, completion = self.data[idx]\n",
        "        return prompt, completion\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "# MyModel definition\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, hidden_size=128):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is a batch of tokenized prompts (indices)\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Take the last hidden state\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_model(model_name, dataset_uri, eval_dataset_uri, staging_bucket, rank, world_size):\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Dataset URI: {dataset_uri}\")\n",
        "    logging.info(f\"Eval Dataset URI: {eval_dataset_uri}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "\n",
        "    # Initialize process group for distributed training\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    # Data loading\n",
        "    train_dataset = CustomDataset(dataset_uri)\n",
        "    eval_dataset = CustomDataset(eval_dataset_uri)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset, num_replicas=world_size, rank=rank\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32)\n",
        "\n",
        "    # Model, optimizer, and loss\n",
        "    model = MyModel()\n",
        "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()  # Or other appropriate loss function\n",
        "\n",
        "    # Tokenization (example using AutoTokenizer)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your tokenizer\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Adjust number of epochs as needed\n",
        "        model.train()\n",
        "        train_sampler.set_epoch(epoch)  # Important for shuffling data across epochs in DDP\n",
        "        for batch_idx, (prompts, completions) in enumerate(train_loader):\n",
        "            # Tokenize prompts\n",
        "            inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            prompts = inputs[\"input_ids\"].to(device)  # Use tokenized input IDs and move to device\n",
        "\n",
        "            # Convert completions to PyTorch tensor and move to device\n",
        "            completions = torch.tensor(completions, dtype=torch.float32).to(device)  # Adjust dtype if needed\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(prompts)  # Pass tokenized prompts to the model\n",
        "            loss = criterion(outputs, completions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0 and rank == 0:  # Only print on rank 0 to avoid duplicate logs\n",
        "                logging.info(f\"Epoch [{epoch + 1}/{10}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # ... (Evaluation on eval_loader) ...\n",
        "        # Save the model if you are on the main process (rank 0)\n",
        "        if rank == 0:\n",
        "            torch.save(model.module.state_dict(), '/content/model.pth') # Save only the model parameters\n",
        "            # Upload to GCS\n",
        "            client = storage.Client()\n",
        "            bucket = client.bucket(BUCKET_NAME)\n",
        "            blob = bucket.blob('model.pth')  # Assuming 'model.pth' is your desired filename in GCS\n",
        "            blob.upload_from_filename('/content/model.pth')\n",
        "            logging.info(f\"Uploaded model to gs://{BUCKET_NAME}/model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, required=True, help=\"Name of the base Gemini model to fine-tune.\")\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"URI of the training dataset.\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"URI of the evaluation dataset.\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"GCS bucket for staging model artifacts.\")\n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank for distributed training')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    world_size = torch.cuda.device_count()\n",
        "    train_model(args.model, args.dataset, args.eval_dataset, args.staging_bucket, args.local_rank, world_size)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "os.makedirs('/content/drive/myproject/', exist_ok=True)\n",
        "\n",
        "# Create trainer/train.py\n",
        "with open('/content/drive/myproject/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# Create setup.py\n",
        "setup_py_content = \"\"\"\n",
        "from setuptools import find_packages, setup\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        'google-cloud-aiplatform',\n",
        "        'torch',\n",
        "        'transformers'\n",
        "    ]\n",
        ")\n",
        "\"\"\"\n",
        "with open('/content/drive/myproject/setup.py', 'w') as f:\n",
        "    f.write(setup_py_content)\n",
        "\n",
        "print('\\n\\n')\n",
        "print(setup_py_content)\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "# Package and upload training code\n",
        "!python setup.py sdist --formats=gztar\n",
        "PACKAGE_NAME = \"trainer-0.1.tar.gz\"\n",
        "\n",
        "# **Authenticate explicitly for Google Cloud Storage**\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# **Then create the storage client**\n",
        "storage_client = storage.Client()\n",
        "\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(f\"custom_training_models/{PACKAGE_NAME}\")\n",
        "blob.upload_from_filename(os.path.join(\"dist\", PACKAGE_NAME))\n",
        "\n",
        "\n",
        "# *** Create and upload cloudbuild.yaml and Dockerfile ***\n",
        "cloudbuild_content = f\"\"\"\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', '{IMAGE_URI}', '.']\n",
        "images:\n",
        "- '{IMAGE_URI}'\n",
        "\"\"\"\n",
        "\n",
        "# Write cloudbuild.yaml locally\n",
        "with open('cloudbuild.yaml', 'w') as f:\n",
        "    f.write(cloudbuild_content)\n",
        "\n",
        "blob = bucket.blob('/content/drive/myproject/cloudbuild.yaml')\n",
        "blob.upload_from_filename('cloudbuild.yaml')\n",
        "\n",
        "\n",
        "dockerfile_content = \"\"\"\n",
        "FROM pytorch/pytorch:latest\n",
        "\n",
        "# Install other dependencies\n",
        "RUN pip install google-cloud-aiplatform transformers\n",
        "\n",
        "# Copy your training code\n",
        "COPY . /trainer\n",
        "\n",
        "# Set the entry point\n",
        "ENTRYPOINT [\"python\", \"-m\", \"torch.distributed.launch\", \"--nproc_per_node=NUM_GPUS_YOU_HAVE\",  \"/trainer/train.py\"]\n",
        "\"\"\"\n",
        "with open('/content/drive/myproject/Dockerfile', 'w') as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "#print('\\n\\n')\n",
        "#!gsutil cp -pr trainer gs://{BUCKET_NAME}/\n",
        "#print('\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Create requirements.txt\n",
        "requirements_content = \"\"\"\n",
        "google-cloud-aiplatform\n",
        "torch\n",
        "transformers\n",
        "\"\"\"\n",
        "\n",
        "# Write requirements.txt\n",
        "with open(\"/content/drive/myproject/requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "\n",
        "\n",
        "# Verify Artifact Registry Permissions\n",
        "# Get Project Number (using project_id)\n",
        "project_number_output = !gcloud projects describe {project_id} --format=\"value(projectNumber)\"\n",
        "project_number = project_number_output[0]\n",
        "\n",
        "# Grant Storage Admin role to Cloud Build service account (using project_number)\n",
        "!gsutil iam ch serviceAccount:{project_number}@cloudbuild.gserviceaccount.com:roles/storage.admin gs://{BUCKET_NAME}  # Replace YOUR_BUCKET\n",
        "\n",
        "\n",
        "# Grant Artifact Registry Writer role to Cloud Build service account (using project_id and project_number)\n",
        "!gcloud projects add-iam-policy-binding {project_id} \\\n",
        "  --member=serviceAccount:{project_number}@cloudbuild.gserviceaccount.com \\\n",
        "  --role=roles/artifactregistry.writer\n",
        "\n",
        "# You can now print the configuration files or use them in your build process\n",
        "print('\\n')\n",
        "print(\"Cloud Build Configuration:\")\n",
        "print('\\n')\n",
        "print(\"cloudbuild.yaml content:\")\n",
        "print(cloudbuild_content)\n",
        "print('\\n')\n",
        "print(\"\\nDockerfile content:\")\n",
        "print(dockerfile_content)\n",
        "print('\\n\\n')\n",
        "\n",
        "!gsutil cp gs://{BUCKET_NAME}/cloudbuild.yaml cloudbuild.yaml\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/trainer\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/cloudbuild.yaml\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/logs\n",
        "!gsutil cp -pr gs://{BUCKET_NAME}/logs .\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"Project ID: {project_id}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
        "print(f\"Image URI: {IMAGE_URI}\")\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "# Navigate to your project directory in Google Drive\n",
        "project_path = '/content/drive/myproject'\n",
        "os.chdir(project_path)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "NlMAkBxJ0d_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud builds submit . --config cloudbuild.yaml --project {project_id}"
      ],
      "metadata": {
        "id": "iFzBKV0ZBpW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES                                                                                    STATUS\n",
        "ad38b690-XXXXX-4b8d-xxxx-xxxxxxxxx  2025-03-31T02:29:07+00:00  3M39S     gs://LLL-KKKK-MMMMM-XXXXXXXXXX_cloudbuild/source/1743388147.235874-4248758bc9d2411cb5b407d317d2a1c2.tgz  us-central1-docker.pkg.dev/LLL-KKKK-MMMMM-XXXXXXXXXX/my-repo/my-pytorch-image (+1 more)  SUCCESS\n"
      ],
      "metadata": {
        "id": "ejQhMjtrE5af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning with CPU"
      ],
      "metadata": {
        "id": "uOQ_4EhUoEZ2"
      }
    },
    {
      "source": [
        "### FINE TUNING ###\n",
        "\n",
        "import colab_env\n",
        "\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "import vertexai\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Google Cloud Project and Region\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "IMAGE_URI = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/my-repo/my-pytorch-image:latest\"  # Your image URI in Artifact Registry\n",
        "\n",
        "#print('\\n\\n')\n",
        "#print(f\"Project ID: {PROJECT_ID}\")\n",
        "#print(f\"Region: {REGION}\")\n",
        "#print(f\"Bucket Name: {BUCKET_NAME}\")\n",
        "#print(f\"Image URI: {IMAGE_URI}\")\n",
        "#print('\\n\\n')\n",
        "\n",
        "# Initialize Vertex AI and GCS\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Create or verify GCS bucket\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "if not bucket.exists():\n",
        "    bucket.create(location=REGION)\n",
        "    logging.info(f\"Bucket {BUCKET_NAME} created in {REGION}.\")\n",
        "else:\n",
        "    logging.info(f\"Bucket {BUCKET_NAME} already exists.\")\n",
        "\n",
        "# Configure staging bucket in aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}/staging\")\n",
        "\n",
        "# Print SDK version\n",
        "logging.info(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "\n",
        "# Path to your saved model in GCS\n",
        "MODEL_PATH = f\"gs://{BUCKET_NAME}/model_output/\" # Updated path\n",
        "\n",
        "\n",
        "# *** Define and create trainer/train.py *** FOR CPU ONLY\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer\n",
        "import time  # For retry logic\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# CustomDataset definition\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self.data = []\n",
        "        self.tokenizer = tokenizer\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    example = json.loads(line)\n",
        "                    # Validate and cast 'completion' to int\n",
        "                    completion_value = example[\"completion\"]\n",
        "                    if not isinstance(completion_value, int):\n",
        "                        try:\n",
        "                            completion_value = int(completion_value)\n",
        "                        except ValueError:\n",
        "                            logging.warning(\n",
        "                                f\"Skipping invalid 'completion' value: {completion_value} in line: {line}\")\n",
        "                            continue  # Skip this line\n",
        "                    # Flatten and stringify 'prompt'\n",
        "                    prompt = example[\"prompt\"]\n",
        "                    if isinstance(prompt, (list, tuple)):\n",
        "                        prompt = \"\".join([str(item) for item in prompt])\n",
        "                    elif isinstance(prompt, dict):\n",
        "                        prompt = json.dumps(prompt)\n",
        "                    else:\n",
        "                        prompt = str(prompt)\n",
        "                    self.data.append((prompt, completion_value))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(f\"Skipping invalid JSON line: {line}, error: {e}\")\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, completion = self.data[idx]\n",
        "        # Ensure 'prompt' is a string before tokenization\n",
        "        if isinstance(prompt, (list, tuple)):\n",
        "            prompt = \"\".join([str(item) for item in prompt])  # Join list/tuple elements\n",
        "        elif not isinstance(prompt, str):\n",
        "            prompt = str(prompt)  # Convert to string if not already\n",
        "        # Tokenize the prompt using the provided tokenizer\n",
        "        inputs = self.tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        # Return the input IDs and completion\n",
        "        # inputs[\"input_ids\"] will be a tensor of shape [1, sequence_length]\n",
        "        # We squeeze it to get a tensor of shape [sequence_length]\n",
        "        return inputs[\"input_ids\"].squeeze(0), completion\n",
        "\n",
        "# MyModel definition\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, hidden_size=128, tokenizer=None):  # Add tokenizer\n",
        "        super(MyModel, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, embedding_dim)  # Use self.tokenizer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No need to move x to device here, it's already on the correct device\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Take the last hidden state\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_model(model_name, dataset_uri, eval_dataset_uri, staging_bucket, bucket_name, base_output_dir):\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Dataset URI: {dataset_uri}\")\n",
        "    logging.info(f\"Eval Dataset URI: {eval_dataset_uri}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "    logging.info(f\"Bucket Name: {bucket_name}\")\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    # Tokenization (example using AutoTokenizer)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace if needed\n",
        "\n",
        "    # Data loading\n",
        "    dataset = CustomDataset(dataset_uri, tokenizer)  # Pass tokenizer to CustomDataset\n",
        "    train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "    eval_size = len(dataset) - train_size  # Remaining 20% for evaluation\n",
        "    train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32)\n",
        "\n",
        "    # Model, optimizer, and loss\n",
        "    model = MyModel(tokenizer=tokenizer)  # Pass tokenizer to MyModel\n",
        "    device = torch.device(\"cpu\")  # Explicitly use CPU\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()  # Or other appropriate loss function\n",
        "\n",
        "    # Debugging: Print working directory\n",
        "    logging.info(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "    # Simplified Training Loop\n",
        "    try:\n",
        "        for epoch in range(10):  # Train for only 1 epoch for debugging\n",
        "            model.train()\n",
        "            #num_training_steps = 10  # Limit training steps per epoch\n",
        "            for batch_idx, (prompts, completions) in enumerate(train_loader):\n",
        "                #if batch_idx >= num_training_steps:\n",
        "                    #break  # Exit inner loop after the desired number of steps\n",
        "                prompts = prompts.to(device)  # Move prompts to device\n",
        "                completions = torch.tensor(completions, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(prompts)\n",
        "                loss = criterion(outputs, completions)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                if batch_idx % 100 == 0:  # Adjust logging frequency if needed\n",
        "                    logging.info(f\"Epoch [{epoch + 1}/1], Batch [{batch_idx}], Loss: {loss.item()}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during training: {e}\")\n",
        "    finally:\n",
        "        # Directory creation and model saving\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(bucket_name)\n",
        "\n",
        "        # Construct the directory path *carefully* with a trailing slash\n",
        "        output_dir = base_output_dir.replace(f\"gs://{bucket_name}/\", \"\")  # Path within the bucket\n",
        "        output_dir = output_dir.rstrip('/') + '/'  # Ensure trailing slash\n",
        "\n",
        "        # Check if the directory exists and create it if necessary\n",
        "        blob = bucket.blob(output_dir)\n",
        "        if not blob.exists():\n",
        "            logging.info(f\"Creating directory: gs://{bucket_name}/{output_dir}\")\n",
        "            bucket.blob(output_dir).upload_from_string(\"\")  # Create an empty object to simulate a directory\n",
        "\n",
        "        # Construct the full model save path\n",
        "        model_save_path = os.path.join(f\"gs://{bucket_name}/\" + output_dir, 'model.pth')\n",
        "        logging.info(f\"Saving model to: {model_save_path}\")  # Log the final path\n",
        "\n",
        "        # Retry logic for saving the model\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                torch.save(model.state_dict(), \"/tmp/model.pth\")  # Save locally first\n",
        "                # Upload the model from the local temp file to GCS\n",
        "                bucket.blob(output_dir + 'model.pth').upload_from_filename(\"/tmp/model.pth\")\n",
        "                logging.info(f\"Model saved to: {model_save_path}\")\n",
        "                break  # Success, exit the loop\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error saving model (attempt {attempt + 1}): {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(5)  # Wait a few seconds before retrying\n",
        "                else:\n",
        "                    raise  # Re-raise the exception if retries fail\n",
        "\n",
        "        # Create a symbolic link to satisfy Vertex AI's directory expectations\n",
        "        symlink_path = os.path.join(base_output_dir, \"model\", \"model.pth\")\n",
        "        os.makedirs(os.path.dirname(symlink_path), exist_ok=True)  # Create directory if it doesn't exist\n",
        "        os.symlink(model_save_path, symlink_path)  # Create the symbolic link\n",
        "        logging.info(f\"Created symbolic link from {model_save_path} to {symlink_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, required=True, help=\"Name of the model\")\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"URI of the training dataset\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"URI of the evaluation dataset\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"Staging bucket URI\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Bucket Name\")\n",
        "    parser.add_argument(\"--base_output_dir\", type=str, required=True, help=\"Base output directory\")\n",
        "    args = parser.parse_args()\n",
        "    train_model(args.model, args.dataset, args.eval_dataset, args.staging_bucket, args.bucket_name,\n",
        "                args.base_output_dir)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create trainer/train.py\n",
        "os.makedirs('trainer', exist_ok=True)\n",
        "with open('trainer/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# Create setup.py\n",
        "setup_py_content = \"\"\"\n",
        "from setuptools import find_packages, setup\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        'google-cloud-aiplatform',\n",
        "        'torch',\n",
        "        'transformers'\n",
        "    ]\n",
        ")\n",
        "\"\"\"\n",
        "with open('setup.py', 'w') as f:\n",
        "    f.write(setup_py_content)\n",
        "\n",
        "# Package and upload training code\n",
        "!python setup.py sdist --formats=gztar\n",
        "PACKAGE_NAME = \"trainer-0.1.tar.gz\"\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(f\"custom_training_models/{PACKAGE_NAME}\")\n",
        "blob.upload_from_filename(os.path.join(\"dist\", PACKAGE_NAME))\n",
        "\n",
        "# *** Create and upload cloudbuild.yaml and Dockerfile ***\n",
        "cloudbuild_content = f\"\"\"\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', '{IMAGE_URI}', '.']\n",
        "images:\n",
        "- '{IMAGE_URI}'\n",
        "\"\"\"\n",
        "\n",
        "# Write cloudbuild.yaml locally\n",
        "with open('cloudbuild.yaml', 'w') as f:\n",
        "    f.write(cloudbuild_content)\n",
        "\n",
        "blob = bucket.blob('cloudbuild.yaml')\n",
        "blob.upload_from_filename('cloudbuild.yaml')\n",
        "\n",
        "# Dockerfile content - Specify requirements and copy necessary files\n",
        "dockerfile_content = f\"\"\"\n",
        "# Or a GPU-enabled PyTorch image if needed\n",
        "FROM pytorch/pytorch:1.11-cpu\n",
        "WORKDIR /app\n",
        "COPY requirements.txt requirements.txt\n",
        "\n",
        "# Install the required libraries in the Docker image\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the training code into the Docker image\n",
        "COPY trainer trainer\n",
        "\n",
        "# Set the entry point for the Docker image\n",
        "ENTRYPOINT [\"python\", \"trainer/train.py\"]\n",
        "\"\"\"\n",
        "# Write Dockerfile locally\n",
        "with open('Dockerfile', 'w') as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "blob = bucket.blob('Dockerfile')\n",
        "blob.upload_from_filename('Dockerfile')\n",
        "\n",
        "# Create requirements.txt\n",
        "requirements_content = \"\"\"\n",
        "google-cloud-aiplatform\n",
        "torch\n",
        "transformers\n",
        "\"\"\"\n",
        "\n",
        "# Write requirements.txt\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "\n",
        "!gsutil cp -pr trainer gs://{BUCKET_NAME}/\n",
        "!gsutil cp -pr Dockerfile gs://{BUCKET_NAME}/trainer/\n",
        "!gsutil cp -pr requirements.txt gs://{BUCKET_NAME}/trainer/\n",
        "\n",
        "\n",
        "# Define and run custom training job\n",
        "TRAINING_DATA_PATH = f\"gs://{BUCKET_NAME}/training_data.jsonl\"\n",
        "EVAL_DATA_PATH = f\"gs://{BUCKET_NAME}/eval_data.jsonl\"\n",
        "BASE_MODEL_NAME = \"chat-bison@001\"  # Or your desired base model\n",
        "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}/model_output\"  # Define base output directory\n",
        "\n",
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"POC-my-custom-training-job\",\n",
        "    script_path=\"trainer/train.py\",\n",
        "\n",
        "    #container_uri=IMAGE_URI,  # my image URI in Artifact Registry by vertex ai does not validate it\n",
        "\n",
        "    container_uri='us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest',\n",
        "\n",
        "    requirements=[\"google-cloud-aiplatform\", \"torch\", \"transformers\"],\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\"\n",
        ")\n",
        "\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--model\", BASE_MODEL_NAME,\n",
        "        \"--dataset\", TRAINING_DATA_PATH,\n",
        "        \"--eval_dataset\", EVAL_DATA_PATH,\n",
        "        \"--staging_bucket\", f\"gs://{BUCKET_NAME}/staging\",\n",
        "        \"--bucket_name\", BUCKET_NAME,\n",
        "        \"--base_output_dir\", BASE_OUTPUT_DIR,\n",
        "    ],\n",
        "    replica_count=1,\n",
        "    machine_type=\"n1-standard-8\",\n",
        "    model_display_name=\"my-pytorch-model\",\n",
        "    base_output_dir=BASE_OUTPUT_DIR,\n",
        ")\n",
        "\n",
        "logging.info(f\"Fine-tuned model: {model}\")\n",
        "print(f\"Fine-tuned model: {model}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Fc6ELqZL2Ev1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Fine tuned model is successfully created')\n",
        "print(f\"Fine-tuned model: {model.display_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIn99FFbdxSw",
        "outputId": "96698b33-6d5e-496f-9c50-7bdfc35e707c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine tuned model is successfully created\n",
            "Fine-tuned model: my-pytorch-model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - VERTEX AI IAM"
      ],
      "metadata": {
        "id": "4j9A3m-hdYKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://{BUCKET_NAME}/model_output"
      ],
      "metadata": {
        "id": "OZqWSDMIi2ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp /content/requirements.txt gs://{BUCKET_NAME}/model_output/model/"
      ],
      "metadata": {
        "id": "_y2naG4Vd9wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://{BUCKET_NAME}/model_output/model"
      ],
      "metadata": {
        "id": "w_Q4KYUReKGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://{BUCKET_NAME}/model_output/"
      ],
      "metadata": {
        "id": "CyP83joneu1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil acl get gs://{BUCKET_NAME}"
      ],
      "metadata": {
        "id": "cJcpUpyRWgN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil iam get gs://{BUCKET_NAME}"
      ],
      "metadata": {
        "id": "sVIIUn0mW2fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil iam ch serviceAccount:{project_number}@cloudbuild.gserviceaccount.com:roles/storage.objectCreator gs://{BUCKET_NAME}"
      ],
      "metadata": {
        "id": "pXmxWvvuXQOk"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}
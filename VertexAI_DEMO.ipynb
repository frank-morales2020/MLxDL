{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "airPyx-2aoHe",
        "6pmjTE1-ZGBW",
        "srLTVr_QZPXR"
      ],
      "authorship_tag": "ABX9TyNSDGwj4WEk08S3I0h2h0l+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/VertexAI_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "airPyx-2aoHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpsTOa9FzURt"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install --upgrade --quiet gcsfs==2024.3.1\n",
        "!pip install --upgrade --quiet accelerate==0.31.0\n",
        "!pip install --upgrade --quiet transformers==4.45.2\n",
        "!pip install --upgrade --quiet datasets==2.19.2\n",
        "!pip install --upgrade google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai --upgrade -q\n",
        "!pip install tensorflow_datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "QstV8F_f0lbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset - Vertex AI"
      ],
      "metadata": {
        "id": "6pmjTE1-ZGBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "# ** Data Preparation**\n",
        "\n",
        "# Authentication and Initialization**\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Convert to JSONL format with prompt and completion\n",
        "def convert_to_jsonl(data, filename):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for row in data:\n",
        "            data_point = {\n",
        "                \"prompt\": row[\"input\"],\n",
        "                \"completion\": str(row[\"label\"]),  # Convert label to string\n",
        "            }\n",
        "            f.write(json.dumps(data_point) + \"\\n\")\n",
        "\n",
        "# Convert the Hugging Face Dataset to a list of dictionaries\n",
        "dataset_list = list(dataset[\"train\"])\n",
        "\n",
        "# Split the dataset into training and evaluation sets\n",
        "train_data, eval_data = train_test_split(dataset_list, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert and save to JSONL files\n",
        "convert_to_jsonl(train_data, \"/content/training_data.jsonl\")\n",
        "convert_to_jsonl(eval_data, \"/content/eval_data.jsonl\")\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
        "print('\\n\\n')\n",
        "\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "print('\\n\\n')\n",
        "# Upload to GCS\n",
        "!gsutil ls gs://{BUCKET_NAME}/\n",
        "print('\\n\\n')\n",
        "!gsutil cp  /content/training_data.jsonl gs://{BUCKET_NAME}/\n",
        "!gsutil cp  /content/eval_data.jsonl gs://{BUCKET_NAME}/\n",
        "print('\\n\\n')"
      ],
      "metadata": {
        "id": "gP6sYGY2OY1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train_data: {len(train_data)}\")\n",
        "print(train_data[0])\n",
        "print(f\"Length of eval_data: {len(eval_data)}\")\n",
        "print(eval_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wka0fLPbOvGm",
        "outputId": "c1720d8f-3d5b-4f1e-e1c1-fe6f825fb2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train_data: 1600\n",
            "{'input': 'Calculate the waypoints from JFK to DUB. Departure: 2024-09-14, Aircraft: Boeing 757, Weather: Sunny', 'label': 7, 'distance': 6176.2203486565095, 'distance_category': 'long', 'waypoints': [[40.642947899999996, -73.7793733748521], [45.298594095237384, -17.78586892190225], [42.855448053630305, -47.16961137636025], [43.690065289044455, -37.13166134152348], [42.692446676984524, -49.13003060202848], [44.71641545571105, -24.787737379620857], [44.56731446277484, -26.580976548106896], [45.52410760535687, -15.0736156269691], [47.2956874, 6.2331927]], 'waypoint_names': ['JFK', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'Waypoint', 'DUB']}\n",
            "Length of eval_data: 400\n",
            "{'input': 'Calculate the waypoints from SAN to LAS. Departure: 2024-01-23, Aircraft: Boeing 777, Weather: Stormy', 'label': 7, 'distance': 16993.35324438778, 'distance_category': 'longhaul', 'waypoints': [[7.0000085, -73.2500086], [17.469538816793573, 68.81040414006927], [17.59575212940935, 70.52298481379142], [12.65812849864566, 3.524673550843133], [11.117766492667563, -17.37640373887689], [10.942810020379422, -19.750377347313957], [11.885466287035268, -6.95953246114577], [13.515304876727356, 15.155647088022548], [20.0171109, 103.378253]], 'waypoint_names': ['SAN', 'Waypoint', 'Waypoint', 'Touloumay', 'Waypoint', 'Waypoint', 'Conze', 'Boui Alhadji Kalari', 'LAS']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create Vertex AI TextDatasets**\n",
        "\n",
        "# Training dataset\n",
        "train_dataset = aiplatform.TextDataset.create(\n",
        "    display_name=\"waypoints-train\",\n",
        "    gcs_source=[f\"gs://{BUCKET_NAME}/training_data.jsonl\"],  # Replace with your GCS bucket and path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
        ")\n",
        "\n",
        "# Evaluation dataset\n",
        "eval_dataset = aiplatform.TextDataset.create(\n",
        "    display_name=\"waypoints-eval\",\n",
        "    gcs_source=[f\"gs://{BUCKET_NAME}/eval_data.jsonl\"],  # Replace with your GCS bucket and path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
        ")\n",
        "\n",
        "print(f\"Training dataset created: {train_dataset.resource_name}\")\n",
        "print(f\"Evaluation dataset created: {eval_dataset.resource_name}\")"
      ],
      "metadata": {
        "id": "JM8WNwbQPcou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import json\n",
        "\n",
        "# Initialize a GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Get the GCS bucket and file path from gcs_source\n",
        "bucket_name = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "file_path = \"training_data.jsonl\"  # Replace if your file path is different\n",
        "\n",
        "# Get the bucket and blob (file)\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(file_path)\n",
        "\n",
        "# Download the file content as a string\n",
        "file_content = blob.download_as_string().decode(\"utf-8\")\n",
        "\n",
        "# Iterate through the first five lines (JSON objects) and print\n",
        "for i, line in enumerate(file_content.splitlines()):\n",
        "    if i >= 5:\n",
        "        break  # Stop after 5 records\n",
        "    data_point = json.loads(line)\n",
        "    print(f\"Record {i + 1}:\")\n",
        "    print(f\"  Prompt: {data_point['prompt']}\")\n",
        "    print(f\"  Completion: {data_point['completion']}\")\n",
        "    print(\"-\" * 20)  # Separator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUVrnCndX9_K",
        "outputId": "9062e517-f7dd-437e-8ffa-fb91f9f4a064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record 1:\n",
            "  Prompt: Calculate the waypoints from JFK to DUB. Departure: 2024-09-14, Aircraft: Boeing 757, Weather: Sunny\n",
            "  Completion: 7\n",
            "--------------------\n",
            "Record 2:\n",
            "  Prompt: Calculate the waypoints from ORD to SEA. Departure: 2024-03-14, Aircraft: Boeing 757, Weather: Snowy\n",
            "  Completion: 4\n",
            "--------------------\n",
            "Record 3:\n",
            "  Prompt: Calculate the waypoints from SEA to KUL. Departure: 2024-03-23, Aircraft: Airbus A330, Weather: Rainy\n",
            "  Completion: 8\n",
            "--------------------\n",
            "Record 4:\n",
            "  Prompt: Calculate the waypoints from MUC to SCL. Departure: 2024-11-22, Aircraft: Boeing 777, Weather: Overcast\n",
            "  Completion: 7\n",
            "--------------------\n",
            "Record 5:\n",
            "  Prompt: Calculate the waypoints from DEN to NAS. Departure: 2024-04-08, Aircraft: Boeing 767, Weather: Snowy\n",
            "  Completion: 5\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Docker - Artifact Registry  "
      ],
      "metadata": {
        "id": "srLTVr_QZPXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud artifacts repositories create my-repo-tmp \\\n",
        "    --repository-format=docker \\\n",
        "    --location=us-central1 \\\n",
        "    --description=\"My Docker repository\" \\\n",
        "    --project={project_id}"
      ],
      "metadata": {
        "id": "1W7kJMMgBPLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import colab_env\n",
        "\n",
        "from google.cloud import storage # Import the storage module\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# 1. Get Project ID and Region from Environment Variables\n",
        "project_id = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "\n",
        "IMAGE_URI = os.environ.get(\n",
        "    \"IMAGE_URI\", f\"us-central1-docker.pkg.dev/{project_id}/my-repo-tmp/my-pytorch-image:latest\"\n",
        ")\n",
        "\n",
        "### ---------\n",
        "\n",
        "\n",
        "# *** Define and create trainer/train.py ***\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer  # Import tokenizer (example using transformers)\n",
        "import torch.distributed as dist\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# CustomDataset definition\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.data = []\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    example = json.loads(line)\n",
        "\n",
        "                    # Validate and cast 'completion' to int\n",
        "                    completion_value = example[\"completion\"]\n",
        "                    if not isinstance(completion_value, int):\n",
        "                        try:\n",
        "                            completion_value = int(completion_value)\n",
        "                        except ValueError:\n",
        "                            logging.warning(f\"Skipping invalid 'completion' value: {completion_value} in line: {line}\")\n",
        "                            continue  # Skip this line\n",
        "\n",
        "                    # Flatten and stringify 'prompt'\n",
        "                    prompt = example[\"prompt\"]\n",
        "                    if isinstance(prompt, (list, tuple)):\n",
        "                        prompt = \" \".join([str(item) for item in prompt])\n",
        "                    elif isinstance(prompt, dict):\n",
        "                        prompt = json.dumps(prompt)\n",
        "                    else:\n",
        "                        prompt = str(prompt)\n",
        "\n",
        "                    self.data.append((prompt, completion_value))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(f\"Skipping invalid JSON line: {line}, Error: {e}\")\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, completion = self.data[idx]\n",
        "        return prompt, completion\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "# MyModel definition\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, hidden_size=128):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is a batch of tokenized prompts (indices)\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Take the last hidden state\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_model(model_name, dataset_uri, eval_dataset_uri, staging_bucket, rank, world_size):\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Dataset URI: {dataset_uri}\")\n",
        "    logging.info(f\"Eval Dataset URI: {eval_dataset_uri}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "\n",
        "    # Initialize process group for distributed training\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    # Data loading\n",
        "    train_dataset = CustomDataset(dataset_uri)\n",
        "    eval_dataset = CustomDataset(eval_dataset_uri)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset, num_replicas=world_size, rank=rank\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32)\n",
        "\n",
        "    # Model, optimizer, and loss\n",
        "    model = MyModel()\n",
        "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()  # Or other appropriate loss function\n",
        "\n",
        "    # Tokenization (example using AutoTokenizer)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your tokenizer\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Adjust number of epochs as needed\n",
        "        model.train()\n",
        "        train_sampler.set_epoch(epoch)  # Important for shuffling data across epochs in DDP\n",
        "        for batch_idx, (prompts, completions) in enumerate(train_loader):\n",
        "            # Tokenize prompts\n",
        "            inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            prompts = inputs[\"input_ids\"].to(device)  # Use tokenized input IDs and move to device\n",
        "\n",
        "            # Convert completions to PyTorch tensor and move to device\n",
        "            completions = torch.tensor(completions, dtype=torch.float32).to(device)  # Adjust dtype if needed\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(prompts)  # Pass tokenized prompts to the model\n",
        "            loss = criterion(outputs, completions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0 and rank == 0:  # Only print on rank 0 to avoid duplicate logs\n",
        "                logging.info(f\"Epoch [{epoch + 1}/{10}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # ... (Evaluation on eval_loader) ...\n",
        "        # Save the model if you are on the main process (rank 0)\n",
        "        if rank == 0:\n",
        "            torch.save(model.module.state_dict(), '/content/model.pth') # Save only the model parameters\n",
        "            # Upload to GCS\n",
        "            client = storage.Client()\n",
        "            bucket = client.bucket(BUCKET_NAME)\n",
        "            blob = bucket.blob('model.pth')  # Assuming 'model.pth' is your desired filename in GCS\n",
        "            blob.upload_from_filename('/content/model.pth')\n",
        "            logging.info(f\"Uploaded model to gs://{BUCKET_NAME}/model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, required=True, help=\"Name of the base Gemini model to fine-tune.\")\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"URI of the training dataset.\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"URI of the evaluation dataset.\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"GCS bucket for staging model artifacts.\")\n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank for distributed training')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    world_size = torch.cuda.device_count()\n",
        "    train_model(args.model, args.dataset, args.eval_dataset, args.staging_bucket, args.local_rank, world_size)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "os.makedirs('/content/drive/myproject/', exist_ok=True)\n",
        "\n",
        "# Create trainer/train.py\n",
        "with open('/content/drive/myproject/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# Create setup.py\n",
        "setup_py_content = \"\"\"\n",
        "from setuptools import find_packages, setup\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        'google-cloud-aiplatform',\n",
        "        'torch',\n",
        "        'transformers'\n",
        "    ]\n",
        ")\n",
        "\"\"\"\n",
        "with open('/content/drive/myproject/setup.py', 'w') as f:\n",
        "    f.write(setup_py_content)\n",
        "\n",
        "print('\\n\\n')\n",
        "print(setup_py_content)\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "# Package and upload training code\n",
        "!python setup.py sdist --formats=gztar\n",
        "PACKAGE_NAME = \"trainer-0.1.tar.gz\"\n",
        "\n",
        "# **Authenticate explicitly for Google Cloud Storage**\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# **Then create the storage client**\n",
        "storage_client = storage.Client()\n",
        "\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(f\"custom_training_models/{PACKAGE_NAME}\")\n",
        "blob.upload_from_filename(os.path.join(\"dist\", PACKAGE_NAME))\n",
        "\n",
        "\n",
        "# *** Create and upload cloudbuild.yaml and Dockerfile ***\n",
        "cloudbuild_content = f\"\"\"\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', '{IMAGE_URI}', '.']\n",
        "images:\n",
        "- '{IMAGE_URI}'\n",
        "\"\"\"\n",
        "\n",
        "# Write cloudbuild.yaml locally\n",
        "with open('cloudbuild.yaml', 'w') as f:\n",
        "    f.write(cloudbuild_content)\n",
        "\n",
        "blob = bucket.blob('/content/drive/myproject/cloudbuild.yaml')\n",
        "blob.upload_from_filename('cloudbuild.yaml')\n",
        "\n",
        "\n",
        "dockerfile_content = \"\"\"\n",
        "FROM pytorch/pytorch:latest\n",
        "\n",
        "# Install other dependencies\n",
        "RUN pip install google-cloud-aiplatform transformers\n",
        "\n",
        "# Copy your training code\n",
        "COPY . /trainer\n",
        "\n",
        "# Set the entry point\n",
        "ENTRYPOINT [\"python\", \"-m\", \"torch.distributed.launch\", \"--nproc_per_node=NUM_GPUS_YOU_HAVE\",  \"/trainer/train.py\"]\n",
        "\"\"\"\n",
        "with open('/content/drive/myproject/Dockerfile', 'w') as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "#print('\\n\\n')\n",
        "#!gsutil cp -pr trainer gs://{BUCKET_NAME}/\n",
        "#print('\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Create requirements.txt\n",
        "requirements_content = \"\"\"\n",
        "google-cloud-aiplatform\n",
        "torch\n",
        "transformers\n",
        "\"\"\"\n",
        "\n",
        "# Write requirements.txt\n",
        "with open(\"/content/drive/myproject/requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "\n",
        "\n",
        "# Verify Artifact Registry Permissions\n",
        "# Get Project Number (using project_id)\n",
        "project_number_output = !gcloud projects describe {project_id} --format=\"value(projectNumber)\"\n",
        "project_number = project_number_output[0]\n",
        "\n",
        "# Grant Storage Admin role to Cloud Build service account (using project_number)\n",
        "!gsutil iam ch serviceAccount:{project_number}@cloudbuild.gserviceaccount.com:roles/storage.admin gs://{BUCKET_NAME}  # Replace YOUR_BUCKET\n",
        "\n",
        "\n",
        "# Grant Artifact Registry Writer role to Cloud Build service account (using project_id and project_number)\n",
        "!gcloud projects add-iam-policy-binding {project_id} \\\n",
        "  --member=serviceAccount:{project_number}@cloudbuild.gserviceaccount.com \\\n",
        "  --role=roles/artifactregistry.writer\n",
        "\n",
        "# You can now print the configuration files or use them in your build process\n",
        "print('\\n')\n",
        "print(\"Cloud Build Configuration:\")\n",
        "print('\\n')\n",
        "print(\"cloudbuild.yaml content:\")\n",
        "print(cloudbuild_content)\n",
        "print('\\n')\n",
        "print(\"\\nDockerfile content:\")\n",
        "print(dockerfile_content)\n",
        "print('\\n\\n')\n",
        "\n",
        "!gsutil cp gs://{BUCKET_NAME}/cloudbuild.yaml cloudbuild.yaml\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/trainer\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/cloudbuild.yaml\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/logs\n",
        "!gsutil cp -pr gs://{BUCKET_NAME}/logs .\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"Project ID: {project_id}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
        "print(f\"Image URI: {IMAGE_URI}\")\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "# Navigate to your project directory in Google Drive\n",
        "project_path = '/content/drive/myproject'\n",
        "os.chdir(project_path)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "NlMAkBxJ0d_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud builds submit . --config cloudbuild.yaml --project {project_id}"
      ],
      "metadata": {
        "id": "iFzBKV0ZBpW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES                                                                                    STATUS\n",
        "ad38b690-XXXXX-4b8d-xxxx-xxxxxxxxx  2025-03-31T02:29:07+00:00  3M39S     gs://LLL-KKKK-MMMMM-XXXXXXXXXX_cloudbuild/source/1743388147.235874-4248758bc9d2411cb5b407d317d2a1c2.tgz  us-central1-docker.pkg.dev/LLL-KKKK-MMMMM-XXXXXXXXXX/my-repo/my-pytorch-image (+1 more)  SUCCESS\n"
      ],
      "metadata": {
        "id": "ejQhMjtrE5af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning with CPU"
      ],
      "metadata": {
        "id": "uOQ_4EhUoEZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FINE TUNING ###\n",
        "\n",
        "import colab_env\n",
        "\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "import vertexai\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Google Cloud Project and Region\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "IMAGE_URI = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/my-repo/my-pytorch-image:latest\"  # Your image URI in Artifact Registry\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
        "print(f\"Image URI: {IMAGE_URI}\")\n",
        "print('\\n\\n')\n",
        "\n",
        "# Initialize Vertex AI and GCS\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Create or verify GCS bucket\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "if not bucket.exists():\n",
        "    bucket.create(location=REGION)\n",
        "    logging.info(f\"Bucket {BUCKET_NAME} created in {REGION}.\")\n",
        "else:\n",
        "    logging.info(f\"Bucket {BUCKET_NAME} already exists.\")\n",
        "\n",
        "# Configure staging bucket in aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}/staging\")\n",
        "\n",
        "# Print SDK version\n",
        "logging.info(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "# *** Define and create trainer/train.py *** FOR CPU ONLY\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split # Import random_split\n",
        "\n",
        "from transformers import AutoTokenizer  # Import tokenizer (example using transformers)\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# CustomDataset definition\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self.data = []\n",
        "        self.tokenizer = tokenizer\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    example = json.loads(line)\n",
        "\n",
        "                    # Validate and cast 'completion' to int\n",
        "                    completion_value = example[\"completion\"]\n",
        "                    if not isinstance(completion_value, int):\n",
        "                        try:\n",
        "                            completion_value = int(completion_value)\n",
        "                        except ValueError:\n",
        "                            logging.warning(f\"Skipping invalid 'completion' value: {completion_value} in line: {line}\")\n",
        "                            continue  # Skip this line\n",
        "\n",
        "                    # Flatten and stringify 'prompt'\n",
        "                    prompt = example[\"prompt\"]\n",
        "                    if isinstance(prompt, (list, tuple)):\n",
        "                        prompt = \" \".join([str(item) for item in prompt])\n",
        "                    elif isinstance(prompt, dict):\n",
        "                        prompt = json.dumps(prompt)\n",
        "                    else:\n",
        "                        prompt = str(prompt)\n",
        "\n",
        "                    self.data.append((prompt, completion_value))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(f\"Skipping invalid JSON line: {line}, Error: {e}\")\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "           prompt, completion = self.data[idx]\n",
        "           # Ensure 'prompt' is a string before tokenization\n",
        "           if isinstance(prompt, (list, tuple)):\n",
        "               prompt = \" \".join([str(item) for item in prompt])  # Join list elements into a string\n",
        "           elif not isinstance(prompt, str):\n",
        "               prompt = str(prompt)  # Convert to string if not already\n",
        "\n",
        "           # Tokenize the prompt using the provided tokenizer\n",
        "           inputs = self.tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "           # Return the input IDs and completion\n",
        "           # inputs[\"input_ids\"] will be a tensor of shape [1, sequence_length]\n",
        "           # We squeeze it to get a tensor of shape [sequence_length]\n",
        "           return inputs[\"input_ids\"].squeeze(0), completion\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "# MyModel definition\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, hidden_size=128):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No need to move x to device here, it's already on the correct device\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Take the last hidden state\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_model(model_name, dataset_uri, eval_dataset_uri, staging_bucket, bucket_name):\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Dataset URI: {dataset_uri}\")\n",
        "    logging.info(f\"Eval Dataset URI: {eval_dataset_uri}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "    logging.info(f\"Bucket Name: {bucket_name}\")\n",
        "\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "\n",
        "\n",
        "    # Tokenization (example using AutoTokenizer)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your tokenizer\n",
        "\n",
        "    # Data loading\n",
        "    dataset = CustomDataset(dataset_uri, tokenizer)  # Pass tokenizer to CustomDataset\n",
        "    train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "    eval_size = len(dataset) - train_size  # Remaining 20% for evaluation\n",
        "    train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32)\n",
        "\n",
        "    # Model, optimizer, and loss\n",
        "    model = MyModel()\n",
        "    device = torch.device(\"cpu\") # Explicitly use CPU\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()  # Or other appropriate loss function\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Adjust number of epochs as needed\n",
        "        model.train()\n",
        "        for batch_idx, (prompts, completions) in enumerate(train_loader):\n",
        "           # Move prompts and completions to the correct device\n",
        "            prompts = prompts.to(device)  # Move prompts to device\n",
        "            completions = torch.tensor(completions, dtype=torch.float32).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(prompts)\n",
        "            loss = criterion(outputs, completions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                logging.info(f\"Epoch [{epoch + 1}/{10}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # ... (Evaluation on eval_loader) ...\n",
        "        torch.save(model.state_dict(), '/tmp/model.pth')\n",
        "\n",
        "\n",
        "        # Upload to GCS\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(bucket_name)\n",
        "\n",
        "\n",
        "        blob = bucket.blob('model.pth')\n",
        "        blob.upload_from_filename('/tmp/model.pth')\n",
        "        logging.info(f\"Uploaded model to gs://{BUCKET_NAME}/model.pth\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, required=True, help=\"Name of the base Gemini model to fine-tune.\")\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"URI of the training dataset.\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"URI of the evaluation dataset.\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"GCS bucket for staging model artifacts.\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Name of the GCS bucket.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(args.model, args.dataset, args.eval_dataset, args.staging_bucket, args.bucket_name)  # Now pass args.bucket_name\n",
        "    print('\\n')\n",
        "    print(\"Training completed.\")\n",
        "    print('\\n')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create trainer/train.py\n",
        "os.makedirs('trainer', exist_ok=True)\n",
        "with open('trainer/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# Create setup.py\n",
        "setup_py_content = \"\"\"\n",
        "from setuptools import find_packages, setup\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        'google-cloud-aiplatform',\n",
        "        'torch',\n",
        "        'transformers'\n",
        "    ]\n",
        ")\n",
        "\"\"\"\n",
        "with open('setup.py', 'w') as f:\n",
        "    f.write(setup_py_content)\n",
        "\n",
        "# Package and upload training code\n",
        "!python setup.py sdist --formats=gztar\n",
        "PACKAGE_NAME = \"trainer-0.1.tar.gz\"\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(f\"custom_training_models/{PACKAGE_NAME}\")\n",
        "blob.upload_from_filename(os.path.join(\"dist\", PACKAGE_NAME))\n",
        "\n",
        "# *** Create and upload cloudbuild.yaml and Dockerfile ***\n",
        "cloudbuild_content = f\"\"\"\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', '{IMAGE_URI}', '.']\n",
        "images:\n",
        "- '{IMAGE_URI}'\n",
        "\"\"\"\n",
        "\n",
        "# Write cloudbuild.yaml locally\n",
        "with open('cloudbuild.yaml', 'w') as f:\n",
        "    f.write(cloudbuild_content)\n",
        "\n",
        "blob = bucket.blob('cloudbuild.yaml')\n",
        "blob.upload_from_filename('cloudbuild.yaml')\n",
        "\n",
        "# Dockerfile content - Specify requirements and copy necessary files\n",
        "dockerfile_content = f\"\"\"\n",
        "FROM pytorch/pytorch:1.11-cpu  # Or a GPU-enabled PyTorch image if needed\n",
        "WORKDIR /app\n",
        "COPY requirements.txt requirements.txt\n",
        "\n",
        "# Install the required libraries in the Docker image\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the training code into the Docker image\n",
        "COPY trainer trainer\n",
        "\n",
        "# Set the entry point for the Docker image\n",
        "ENTRYPOINT [\"python\", \"trainer/train.py\"]\n",
        "\"\"\"\n",
        "\n",
        "# Create requirements.txt\n",
        "requirements_content = \"\"\"\n",
        "google-cloud-aiplatform\n",
        "torch\n",
        "transformers\n",
        "\"\"\"\n",
        "\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "\n",
        "\n",
        "# Write requirements.txt\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "# Write Dockerfile locally\n",
        "with open('Dockerfile', 'w') as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "blob = bucket.blob('Dockerfile')\n",
        "blob.upload_from_filename('Dockerfile')\n",
        "\n",
        "!gsutil cp -pr trainer gs://{BUCKET_NAME}/\n",
        "!gsutil cp -pr Dockerfile gs://{BUCKET_NAME}/trainer/\n",
        "!gsutil cp -pr requirements.txt gs://{BUCKET_NAME}/trainer/\n",
        "\n",
        "\n",
        "# Define and run custom training job\n",
        "TRAINING_DATA_PATH = f\"gs://{BUCKET_NAME}/training_data.jsonl\"\n",
        "EVAL_DATA_PATH = f\"gs://{BUCKET_NAME}/eval_data.jsonl\"\n",
        "BASE_MODEL_NAME = \"chat-bison@001\"  # Or your desired base model\n",
        "\n",
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"POC-my-custom-training-job\",\n",
        "    script_path=\"trainer/train.py\",\n",
        "\n",
        "    #container_uri=IMAGE_URI,  # Your image URI in Artifact Registry\n",
        "\n",
        "    container_uri='us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest',\n",
        "\n",
        "    requirements=[\"google-cloud-aiplatform\", \"torch\", \"transformers\"],\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\"\n",
        ")\n",
        "\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--model\", BASE_MODEL_NAME,\n",
        "        \"--dataset\", TRAINING_DATA_PATH,\n",
        "        \"--eval_dataset\", EVAL_DATA_PATH,\n",
        "        \"--staging_bucket\", f\"gs://{BUCKET_NAME}/staging\",\n",
        "        \"--bucket_name\", BUCKET_NAME,\n",
        "    ],\n",
        "    replica_count=1,\n",
        "    machine_type=\"n1-standard-8\",\n",
        "    model_display_name=\"my-pytorch-model\"\n",
        ")\n",
        "\n",
        "logging.info(f\"Fine-tuned model: {model}\")"
      ],
      "metadata": {
        "id": "U4ux4vcRoL7H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
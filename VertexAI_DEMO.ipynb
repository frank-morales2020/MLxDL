{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS4Wcwfl6PyZvTWi2s8+1H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/VertexAI_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpsTOa9FzURt"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install --upgrade --quiet gcsfs==2024.3.1\n",
        "!pip install --upgrade --quiet accelerate==0.31.0\n",
        "!pip install --upgrade --quiet transformers==4.45.2\n",
        "!pip install --upgrade --quiet datasets==2.19.2\n",
        "!pip install --upgrade google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai --upgrade -q\n",
        "!pip install tensorflow_datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "QstV8F_f0lbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud artifacts repositories create my-repo-tmp \\\n",
        "    --repository-format=docker \\\n",
        "    --location=us-central1 \\\n",
        "    --description=\"My Docker repository\" \\\n",
        "    --project={project_id}"
      ],
      "metadata": {
        "id": "1W7kJMMgBPLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import colab_env\n",
        "\n",
        "from google.cloud import storage # Import the storage module\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# 1. Get Project ID and Region from Environment Variables\n",
        "project_id = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "BUCKET_NAME = os.environ[\"GOOGLE_CLOUD_BUCKET_NAME\"]\n",
        "\n",
        "IMAGE_URI = os.environ.get(\n",
        "    \"IMAGE_URI\", f\"us-central1-docker.pkg.dev/{project_id}/my-repo-tmp/my-pytorch-image:latest\"\n",
        ")\n",
        "\n",
        "### ---------\n",
        "\n",
        "\n",
        "# *** Define and create trainer/train.py ***\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer  # Import tokenizer (example using transformers)\n",
        "import torch.distributed as dist\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# CustomDataset definition\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.data = []\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    example = json.loads(line)\n",
        "\n",
        "                    # Validate and cast 'completion' to int\n",
        "                    completion_value = example[\"completion\"]\n",
        "                    if not isinstance(completion_value, int):\n",
        "                        try:\n",
        "                            completion_value = int(completion_value)\n",
        "                        except ValueError:\n",
        "                            logging.warning(f\"Skipping invalid 'completion' value: {completion_value} in line: {line}\")\n",
        "                            continue  # Skip this line\n",
        "\n",
        "                    # Flatten and stringify 'prompt'\n",
        "                    prompt = example[\"prompt\"]\n",
        "                    if isinstance(prompt, (list, tuple)):\n",
        "                        prompt = \" \".join([str(item) for item in prompt])\n",
        "                    elif isinstance(prompt, dict):\n",
        "                        prompt = json.dumps(prompt)\n",
        "                    else:\n",
        "                        prompt = str(prompt)\n",
        "\n",
        "                    self.data.append((prompt, completion_value))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(f\"Skipping invalid JSON line: {line}, Error: {e}\")\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, completion = self.data[idx]\n",
        "        return prompt, completion\n",
        "\n",
        "# MyModel definition\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, vocab_size=10000, embedding_dim=128, hidden_size=128):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is a batch of tokenized prompts (indices)\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Take the last hidden state\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_model(model_name, dataset_uri, eval_dataset_uri, staging_bucket, rank, world_size):\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Dataset URI: {dataset_uri}\")\n",
        "    logging.info(f\"Eval Dataset URI: {eval_dataset_uri}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "\n",
        "    # Initialize process group for distributed training\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    # Data loading\n",
        "    train_dataset = CustomDataset(dataset_uri)\n",
        "    eval_dataset = CustomDataset(eval_dataset_uri)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset, num_replicas=world_size, rank=rank\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32)\n",
        "\n",
        "    # Model, optimizer, and loss\n",
        "    model = MyModel()\n",
        "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()  # Or other appropriate loss function\n",
        "\n",
        "    # Tokenization (example using AutoTokenizer)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your tokenizer\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Adjust number of epochs as needed\n",
        "        model.train()\n",
        "        train_sampler.set_epoch(epoch)  # Important for shuffling data across epochs in DDP\n",
        "        for batch_idx, (prompts, completions) in enumerate(train_loader):\n",
        "            # Tokenize prompts\n",
        "            inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            prompts = inputs[\"input_ids\"].to(device)  # Use tokenized input IDs and move to device\n",
        "\n",
        "            # Convert completions to PyTorch tensor and move to device\n",
        "            completions = torch.tensor(completions, dtype=torch.float32).to(device)  # Adjust dtype if needed\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(prompts)  # Pass tokenized prompts to the model\n",
        "            loss = criterion(outputs, completions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0 and rank == 0:  # Only print on rank 0 to avoid duplicate logs\n",
        "                logging.info(f\"Epoch [{epoch + 1}/{10}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # ... (Evaluation on eval_loader) ...\n",
        "        # Save the model if you are on the main process (rank 0)\n",
        "        if rank == 0:\n",
        "            torch.save(model.module.state_dict(), '/content/model.pth') # Save only the model parameters\n",
        "            # Upload to GCS\n",
        "            client = storage.Client()\n",
        "            bucket = client.bucket(BUCKET_NAME)\n",
        "            blob = bucket.blob('model.pth')  # Assuming 'model.pth' is your desired filename in GCS\n",
        "            blob.upload_from_filename('/content/model.pth')\n",
        "            logging.info(f\"Uploaded model to gs://{BUCKET_NAME}/model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, required=True, help=\"Name of the base Gemini model to fine-tune.\")\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"URI of the training dataset.\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"URI of the evaluation dataset.\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"GCS bucket for staging model artifacts.\")\n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank for distributed training')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    world_size = torch.cuda.device_count()\n",
        "    train_model(args.model, args.dataset, args.eval_dataset, args.staging_bucket, args.local_rank, world_size)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "os.makedirs('/content/drive/myproject/', exist_ok=True)\n",
        "\n",
        "# Create trainer/train.py\n",
        "with open('/content/drive/myproject/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# Create setup.py\n",
        "setup_py_content = \"\"\"\n",
        "from setuptools import find_packages, setup\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        'google-cloud-aiplatform',\n",
        "        'torch',\n",
        "        'transformers'\n",
        "    ]\n",
        ")\n",
        "\"\"\"\n",
        "with open('/content/drive/myproject/setup.py', 'w') as f:\n",
        "    f.write(setup_py_content)\n",
        "\n",
        "print('\\n\\n')\n",
        "print(setup_py_content)\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "# Package and upload training code\n",
        "!python setup.py sdist --formats=gztar\n",
        "PACKAGE_NAME = \"trainer-0.1.tar.gz\"\n",
        "\n",
        "# **Authenticate explicitly for Google Cloud Storage**\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# **Then create the storage client**\n",
        "storage_client = storage.Client()\n",
        "\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(f\"custom_training_models/{PACKAGE_NAME}\")\n",
        "blob.upload_from_filename(os.path.join(\"dist\", PACKAGE_NAME))\n",
        "\n",
        "\n",
        "# *** Create and upload cloudbuild.yaml and Dockerfile ***\n",
        "cloudbuild_content = f\"\"\"\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', '{IMAGE_URI}', '.']\n",
        "images:\n",
        "- '{IMAGE_URI}'\n",
        "\"\"\"\n",
        "\n",
        "# Write cloudbuild.yaml locally\n",
        "with open('cloudbuild.yaml', 'w') as f:\n",
        "    f.write(cloudbuild_content)\n",
        "\n",
        "blob = bucket.blob('/content/drive/myproject/cloudbuild.yaml')\n",
        "blob.upload_from_filename('cloudbuild.yaml')\n",
        "\n",
        "\n",
        "dockerfile_content = \"\"\"\n",
        "FROM pytorch/pytorch:latest\n",
        "\n",
        "# Install other dependencies\n",
        "RUN pip install google-cloud-aiplatform transformers\n",
        "\n",
        "# Copy your training code\n",
        "COPY . /trainer\n",
        "\n",
        "# Set the entry point\n",
        "ENTRYPOINT [\"python\", \"-m\", \"torch.distributed.launch\", \"--nproc_per_node=NUM_GPUS_YOU_HAVE\",  \"/trainer/train.py\"]\n",
        "\"\"\"\n",
        "with open('/content/drive/myproject/Dockerfile', 'w') as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "#print('\\n\\n')\n",
        "#!gsutil cp -pr trainer gs://{BUCKET_NAME}/\n",
        "#print('\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Create requirements.txt\n",
        "requirements_content = \"\"\"\n",
        "google-cloud-aiplatform\n",
        "torch\n",
        "transformers\n",
        "\"\"\"\n",
        "\n",
        "# Write requirements.txt\n",
        "with open(\"/content/drive/myproject/requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "\n",
        "\n",
        "# Verify Artifact Registry Permissions\n",
        "# Get Project Number (using project_id)\n",
        "project_number_output = !gcloud projects describe {project_id} --format=\"value(projectNumber)\"\n",
        "project_number = project_number_output[0]\n",
        "\n",
        "# Grant Storage Admin role to Cloud Build service account (using project_number)\n",
        "!gsutil iam ch serviceAccount:{project_number}@cloudbuild.gserviceaccount.com:roles/storage.admin gs://{BUCKET_NAME}  # Replace YOUR_BUCKET\n",
        "\n",
        "\n",
        "# Grant Artifact Registry Writer role to Cloud Build service account (using project_id and project_number)\n",
        "!gcloud projects add-iam-policy-binding {project_id} \\\n",
        "  --member=serviceAccount:{project_number}@cloudbuild.gserviceaccount.com \\\n",
        "  --role=roles/artifactregistry.writer\n",
        "\n",
        "# You can now print the configuration files or use them in your build process\n",
        "print('\\n')\n",
        "print(\"Cloud Build Configuration:\")\n",
        "print('\\n')\n",
        "print(\"cloudbuild.yaml content:\")\n",
        "print(cloudbuild_content)\n",
        "print('\\n')\n",
        "print(\"\\nDockerfile content:\")\n",
        "print(dockerfile_content)\n",
        "print('\\n\\n')\n",
        "\n",
        "!gsutil cp gs://{BUCKET_NAME}/cloudbuild.yaml cloudbuild.yaml\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/trainer\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/cloudbuild.yaml\n",
        "print('\\n')\n",
        "\n",
        "!gsutil ls gs://{BUCKET_NAME}/logs\n",
        "!gsutil cp -pr gs://{BUCKET_NAME}/logs .\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"Project ID: {project_id}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Bucket Name: {BUCKET_NAME}\")\n",
        "print(f\"Image URI: {IMAGE_URI}\")\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "# Navigate to your project directory in Google Drive\n",
        "project_path = '/content/drive/myproject'\n",
        "os.chdir(project_path)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "NlMAkBxJ0d_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud builds submit . --config cloudbuild.yaml --project {project_id}"
      ],
      "metadata": {
        "id": "iFzBKV0ZBpW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES                                                                                    STATUS\n",
        "ad38b690-XXXXX-4b8d-xxxx-xxxxxxxxx  2025-03-31T02:29:07+00:00  3M39S     gs://LLL-KKKK-MMMMM-XXXXXXXXXX_cloudbuild/source/1743388147.235874-4248758bc9d2411cb5b407d317d2a1c2.tgz  us-central1-docker.pkg.dev/LLL-KKKK-MMMMM-XXXXXXXXXX/my-repo/my-pytorch-image (+1 more)  SUCCESS\n"
      ],
      "metadata": {
        "id": "ejQhMjtrE5af"
      }
    }
  ]
}
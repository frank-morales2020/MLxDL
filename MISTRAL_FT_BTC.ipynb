{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPTCiN+e0+pEXlcKR+Kj3i9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_FT_BTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlzcPp_ElvKq",
        "outputId": "76844794-9dac-4d43-b016-e7fd510c2a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct  4 05:42:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   34C    P0             56W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas pandas_ta -q"
      ],
      "metadata": {
        "id": "ohmzO886h5EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga3oCA9ehutz",
        "outputId": "ea557fdb-bb0b-4bd7-89c1-b9202a35f374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3EspHYJhm0p",
        "outputId": "6c5f5994-549d-431a-80b0-a4741d1553e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Attempting to load data from SQLite: /content/gdrive/MyDrive/TradingBotLogs/ohlcv_data_BTC.db - Table: btcusd_1h_data_12y ---\n",
            "--- Successfully loaded BTC data from SQLite ---\n",
            "Columns in loaded data: ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
            "New Index Type: datetime64[ns, UTC]\n",
            "--- Calculating Technical Indicators (SMA, EMA, RSI, Log Returns) ---\n",
            "Data ready. Remaining rows after cleaning: 89769\n",
            "--- Generating samples (Window: 72h, Horizon: 12h) ---\n",
            "\n",
            "Successfully generated 89685 hourly fine-tuning samples.\n",
            "Example of one training sample (first entry):\n",
            "\n",
            "================================================================================\n",
            "<s>[INST] Analyze the 5 most recent 1-hour BTC bars in the table below, focusing on the Close price, Volume, RSI (Relative Strength Index), and 20-period SMA. Predict the price direction (UP or DOWN) for the next 12 hours and provide a brief technical rationale.\n",
            "\n",
            "| datetime                  | close   | volume   | rsi_14   | sma_20   |\n",
            "|:--------------------------|:--------|:---------|:---------|:---------|\n",
            "| 2013-10-26 08:00:00+00:00 | 184.58  | 4        | 48.8856  | 187.968  |\n",
            "| 2013-10-27 08:00:00+00:00 | 182.21  | 15       | 47.3294  | 186.801  |\n",
            "| 2013-10-27 12:00:00+00:00 | 179.47  | 9        | 45.5251  | 186.232  |\n",
            "| 2013-10-27 14:00:00+00:00 | 180.276 | 8        | 46.1755  | 186.75   |\n",
            "| 2013-10-27 15:00:00+00:00 | 179.236 | 4        | 45.4222  | 185.771  | [/INST] The 12-hour prediction is **UP**. The final bar's RSI of 45.42 suggests room to run. The current Close is below the 20-period SMA, which supports a UP bias. The price ultimately moved $21.53.</s>\n",
            "================================================================================\n",
            "\n",
            "Dataset saved to 'btc_instruction_dataset.jsonl'. You are now ready for the fine-tuning stage (QLoRA/SFTTrainer).\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings from pandas_ta when calculating indicators\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DB_PATH = '/content/gdrive/MyDrive/TradingBotLogs/ohlcv_data_BTC.db'\n",
        "TABLE_NAME = 'btcusd_1h_data_12y'\n",
        "TIME_COLUMN = 'timestamp' # Name of the column containing the datetime/timestamp\n",
        "WINDOW_SIZE = 72\n",
        "PREDICTION_HORIZON = 12\n",
        "\n",
        "# --- STEP 1: LOAD AND INITIALIZE DATA ---\n",
        "print(f\"--- Attempting to load data from SQLite: {DB_PATH} - Table: {TABLE_NAME} ---\")\n",
        "try:\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    query = f\"SELECT * FROM {TABLE_NAME}\"\n",
        "    df = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "    print(\"--- Successfully loaded BTC data from SQLite ---\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Ensure column names are lowercased for pandas_ta compatibility\n",
        "df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "# --- FIX IS HERE ---\n",
        "# 1. Convert the string timestamp column to datetime WITHOUT specifying unit='ms'.\n",
        "#    Pandas will infer the format from the string 'YYYY-MM-DD HH:MM:SS+00:00'.\n",
        "df['datetime'] = pd.to_datetime(df[TIME_COLUMN])\n",
        "# --------------------\n",
        "\n",
        "# Set the datetime index and sort\n",
        "df = df.set_index('datetime').sort_index()\n",
        "\n",
        "# Check column names after lowercasing\n",
        "print(\"Columns in loaded data:\", df.columns.tolist())\n",
        "# Check the index type to confirm conversion\n",
        "print(\"New Index Type:\", df.index.dtype)\n",
        "\n",
        "\n",
        "# --- STEP 2: FEATURE ENGINEERING (ADD TECHNICAL INDICATORS) ---\n",
        "\n",
        "print(\"--- Calculating Technical Indicators (SMA, EMA, RSI, Log Returns) ---\")\n",
        "\n",
        "# Simple Moving Average (SMA) - 20 periods\n",
        "df['sma_20'] = df['close'].rolling(20).mean()\n",
        "\n",
        "# Exponential Moving Average (EMA) - 50 periods\n",
        "df['ema_50'] = df['close'].ewm(span=50, adjust=False).mean()\n",
        "\n",
        "# Relative Strength Index (RSI) - 14 periods\n",
        "df['rsi_14'] = ta.rsi(df['close'], length=14)\n",
        "\n",
        "# Log Returns (Percentage change in price, then natural log)\n",
        "df['log_return'] = df['close'].pct_change()\n",
        "# Use np.log1p for numerical stability, though the original np.log(1+x) works if you handle zeros/negative carefully\n",
        "df['log_return'] = df['log_return'].apply(lambda x: np.log(1 + x) if 1 + x > 0 else 0)\n",
        "\n",
        "# Drop initial NaN rows created by rolling window calculations\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"Data ready. Remaining rows after cleaning: {len(df)}\")\n",
        "\n",
        "# --- STEP 3: INSTRUCTIONAL DATASET CREATION (SLIDING WINDOW) ---\n",
        "\n",
        "def format_for_llm(data_window, full_df):\n",
        "    \"\"\"Generates a single instruction-tuning sample.\"\"\"\n",
        "\n",
        "    # --- 1. Define the Context (Input Features) ---\n",
        "    context_data = data_window[['close', 'volume', 'rsi_14', 'sma_20']].tail(5)\n",
        "    context_table = context_data.to_markdown(numalign=\"left\", stralign=\"left\")\n",
        "\n",
        "    # --- 2. Define the Instruction ---\n",
        "    instruction = (\n",
        "        f\"Analyze the 5 most recent 1-hour BTC bars in the table below, focusing on the Close price, \"\n",
        "        f\"Volume, RSI (Relative Strength Index), and 20-period SMA. \"\n",
        "        f\"Predict the price direction (UP or DOWN) for the next {PREDICTION_HORIZON} hours and provide a brief technical rationale.\"\n",
        "    )\n",
        "\n",
        "    # --- 3. Define the Response (Ground Truth Label) ---\n",
        "    try:\n",
        "        current_close = data_window['close'].iloc[-1]\n",
        "\n",
        "        # Look up the close price after the prediction horizon\n",
        "        # Use .iloc[] index to safely get the bar (row) that is 12 steps after the end of the current window\n",
        "        target_index_loc = full_df.index.get_loc(data_window.index[-1]) + PREDICTION_HORIZON\n",
        "        target_close = full_df['close'].iloc[target_index_loc]\n",
        "\n",
        "        # Calculate final movement\n",
        "        movement = target_close - current_close\n",
        "        direction = \"UP\" if movement > 0 else \"DOWN\"\n",
        "\n",
        "        # Craft the detailed response\n",
        "        response = (\n",
        "            f\"The {PREDICTION_HORIZON}-hour prediction is **{direction}**. \"\n",
        "            f\"The final bar's RSI of {data_window['rsi_14'].iloc[-1]:.2f} suggests \"\n",
        "            f\"{'overbought pressure' if data_window['rsi_14'].iloc[-1] > 70 else 'room to run'}. \"\n",
        "            f\"The current Close is {'above' if current_close > data_window['sma_20'].iloc[-1] else 'below'} the 20-period SMA, \"\n",
        "            f\"which supports a {direction} bias. The price ultimately moved ${movement:.2f}.\"\n",
        "        )\n",
        "\n",
        "    except IndexError:\n",
        "        return None # Skip if there's not enough future data\n",
        "\n",
        "    # Final Instruction-Tuning Format (Mistral/Llama standard)\n",
        "    template = f\"<s>[INST] {instruction}\\n\\n{context_table} [/INST] {response}</s>\"\n",
        "    return {'text': template}\n",
        "\n",
        "# --- Generate the full dataset by sliding the window ---\n",
        "\n",
        "print(f\"--- Generating samples (Window: {WINDOW_SIZE}h, Horizon: {PREDICTION_HORIZON}h) ---\")\n",
        "fine_tuning_samples = []\n",
        "\n",
        "# Iterate, leaving enough bars at the end for the prediction horizon\n",
        "for i in range(WINDOW_SIZE, len(df) - PREDICTION_HORIZON):\n",
        "    history_window = df.iloc[i - WINDOW_SIZE : i]\n",
        "\n",
        "    # Pass the full df to the function for target lookup\n",
        "    sample = format_for_llm(history_window, df)\n",
        "\n",
        "    if sample:\n",
        "        fine_tuning_samples.append(sample)\n",
        "\n",
        "# --- STEP 4: SAVE DATASET ---\n",
        "\n",
        "output_file = 'btc_instruction_dataset.jsonl'\n",
        "with open(output_file, 'w') as f:\n",
        "    for sample in fine_tuning_samples:\n",
        "        f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "print(f\"\\nSuccessfully generated {len(fine_tuning_samples)} hourly fine-tuning samples.\")\n",
        "if fine_tuning_samples:\n",
        "    print(f\"Example of one training sample (first entry):\\n\")\n",
        "    print(\"=\"*80)\n",
        "    print(fine_tuning_samples[0]['text'])\n",
        "    print(\"=\"*80)\n",
        "print(f\"\\nDataset saved to '{output_file}'. You are now ready for the fine-tuning stage (QLoRA/SFTTrainer).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for QLoRA and SFTTrainer\n",
        "!pip install -q -U bitsandbytes transformers peft accelerate trl datasets -q"
      ],
      "metadata": {
        "id": "CepkKdb9jfMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lth /content/gdrive/MyDrive/CryptoFT/dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gMf8deflH9S",
        "outputId": "6fbb7072-aa48-43e0-aeb5-86441f5f5b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 86M\n",
            "-rw------- 1 root root 86M Oct  4 04:52 btc_instruction_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lth /content/gdrive/MyDrive/CryptoFT/models"
      ],
      "metadata": {
        "id": "Y7EmzrEBjwIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# --- 0. GPU SETUP ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"--- Running on device: {device} ---\")\n",
        "if device.type == 'cpu':\n",
        "    raise RuntimeError(\"GPU not found. QLoRA fine-tuning is not feasible without a CUDA-enabled GPU.\")\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "DATASET_PATH = \"/content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset.jsonl\"\n",
        "NEW_MODEL_NAME = \"Mistral-7B-BTC-Expert\"\n",
        "OUTPUT_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/results_btc_finetune\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "# --- Training Arguments (Hyperparameters) ---\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_steps=500,\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # CRITICAL ADDITIONS TO SHOW EVAL LOSS\n",
        "    eval_strategy=\"steps\", # Enable evaluation at specified step intervals\n",
        "    eval_steps=500,              # Evaluate every 500 steps\n",
        "    load_best_model_at_end=True, # Load the best checkpoint based on eval_loss at the end\n",
        "    metric_for_best_model=\"eval_loss\", # Use validation loss as the metric\n",
        ")\n",
        "\n",
        "# --- 2. LOAD DATASET ---\n",
        "print(\"\\n--- Loading and Splitting Custom Dataset ---\")\n",
        "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "dataset = dataset.train_test_split(test_size=0.01, seed=42)\n",
        "\n",
        "# --- 3. LOAD MODEL & TOKENIZER (with 4-bit Quantization on GPU) ---\n",
        "print(\"\\n--- Loading Base Model and Tokenizer with QLoRA Configuration ---\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "# --- CRITICAL FIX: Pre-tokenize the Dataset ---\n",
        "print(f\"--- Pre-tokenizing dataset to MAX_SEQ_LENGTH: {MAX_SEQ_LENGTH} ---\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized_output = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n",
        "    return tokenized_output\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing samples\"\n",
        ")\n",
        "\n",
        "# --- 4. INITIALIZE SFTTRAINER AND START TRAINING (FINAL CLEAN INPUT) ---\n",
        "print(\"\\n--- Initializing and Starting SFTTrainer (Fine-Tuning) ---\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Fine-Tuning ---\")\n",
        "# Start training!\n",
        "trainer.train()\n",
        "print(\"\\n--- Fine-Tuning Complete! ---\")\n",
        "\n",
        "# --- 5. SAVE MODEL ---\n",
        "print(\"\\n--- Saving Final Adapter Weights ---\")\n",
        "# Save the LoRA adapter weights\n",
        "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
        "tokenizer.save_pretrained(NEW_MODEL_NAME) # Save tokenizer alongside the adapter\n",
        "\n",
        "print(f\"Fine-tuning complete! LoRA adapter and tokenizer saved to './{NEW_MODEL_NAME}'\")\n",
        "print(f\"Checkpoints and logs are saved to '{OUTPUT_DIR}'\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nrxVcymujbxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Starting Fine-Tuning ---\n",
        " [5550/5550 7:34:55, Epoch 1/1]\n",
        "\n",
        "Step\tTraining Loss\tValidation Loss\tEntropy\tNum Tokens\tMean Token Accuracy\n",
        "500\t0.215800\t0.215459\t0.215031\t8192000.000000\t0.916472\n",
        "\n",
        "1000\t0.217400\t0.214608\t0.214947\t16384000.000000\t0.916857\n",
        "\n",
        "1500\t0.212100\t0.212424\t0.212158\t24576000.000000\t0.917407\n",
        "\n",
        "2000\t0.211400\t0.211059\t0.210042\t32768000.000000\t0.917807\n",
        "\n",
        "2500\t0.209900\t0.210005\t0.208038\t40960000.000000\t0.918461\n",
        "\n",
        "3000\t0.207400\t0.208606\t0.208269\t49152000.000000\t0.919172\n",
        "\n",
        "3500\t0.206600\t0.207157\t0.207384\t57344000.000000\t0.919553\n",
        "\n",
        "4000\t0.204900\t0.205344\t0.205466\t65536000.000000\t0.920340\n",
        "\n",
        "4500\t0.203300\t0.203777\t0.203547\t73728000.000000\t0.921309\n",
        "\n",
        "5000\t0.201200\t0.202193\t0.202084\t81920000.000000\t0.921827\n",
        "\n",
        "5500\t0.200400\t0.201922\t0.201656\t90112000.000000\t0.922047\n"
      ],
      "metadata": {
        "id": "k9JcayBCiXbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lth /content/gdrive/MyDrive/CryptoFT/models/results_btc_finetune/checkpoint-5550/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T02B4AkoMoXE",
        "outputId": "c41566ca-4315-4462-ed8e-efae1bad6cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 970M\n",
            "-rw------- 1 root root  15K Oct  4 13:21 rng_state.pth\n",
            "-rw------- 1 root root 1.4K Oct  4 13:21 scaler.pt\n",
            "-rw------- 1 root root 1.5K Oct  4 13:21 scheduler.pt\n",
            "-rw------- 1 root root  37K Oct  4 13:21 trainer_state.json\n",
            "-rw------- 1 root root  936 Oct  4 13:21 adapter_config.json\n",
            "-rw------- 1 root root 326M Oct  4 13:21 optimizer.pt\n",
            "-rw------- 1 root root  414 Oct  4 13:21 special_tokens_map.json\n",
            "-rw------- 1 root root 1.1K Oct  4 13:21 tokenizer_config.json\n",
            "-rw------- 1 root root 3.4M Oct  4 13:21 tokenizer.json\n",
            "-rw------- 1 root root 482K Oct  4 13:21 tokenizer.model\n",
            "-rw------- 1 root root 6.2K Oct  4 13:21 training_args.bin\n",
            "-rw------- 1 root root 641M Oct  4 13:21 adapter_model.safetensors\n",
            "-rw------- 1 root root 5.1K Oct  4 13:21 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL HUGGING FACE DEPLOYMENT SCRIPT (Robust with Repo Creation) ---\n",
        "# --- Final Hugging Face Deployment Script for Crypto Oracle ---\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from google.colab import drive, userdata\n",
        "from huggingface_hub import login, HfApi, create_repo, Repository\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub.utils import RepositoryNotFoundError # Import for error checking\n",
        "\n",
        "# --- 1. AUTHENTICATION ---\n",
        "print(\"--- Authenticating with Hugging Face Hub using Colab Secret ---\")\n",
        "try:\n",
        "    drive.mount('/content/gdrive')\n",
        "    access_token_write = userdata.get('HF_TOKEN')\n",
        "    if not access_token_write:\n",
        "        raise ValueError(\"HF_TOKEN secret not found or is empty.\")\n",
        "    login(\n",
        "        token=access_token_write,\n",
        "        add_to_git_credential=True\n",
        "    )\n",
        "    # Initialize HfApi client for programmatic operations\n",
        "    api = HfApi(token=access_token_write)\n",
        "    print(\"✅ Successfully logged into Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during login: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "LOCAL_ADAPTER_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/results_btc_finetune/checkpoint-5550\"\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-Expert\"\n",
        "\n",
        "# --- 3. REPOSITORY SETUP ---\n",
        "print(f\"\\n--- Checking/Creating Repository: {HUB_MODEL_ID} ---\")\n",
        "try:\n",
        "    # Check if the repo exists by trying to get its info\n",
        "    api.repo_info(repo_id=HUB_MODEL_ID, repo_type=\"model\")\n",
        "    print(f\"✅ Repository {HUB_MODEL_ID} already exists.\")\n",
        "except RepositoryNotFoundError:\n",
        "    # If the repo does not exist, create it\n",
        "    print(f\"⚠️ Repository {HUB_MODEL_ID} not found. Creating it now...\")\n",
        "    create_repo(repo_id=HUB_MODEL_ID, repo_type=\"model\", private=False, token=access_token_write)\n",
        "    print(f\"✅ Repository {HUB_MODEL_ID} created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during repository check/creation: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 4. PUSH ADAPTER AND TOKENIZER ---\n",
        "print(f\"\\n--- Pushing LoRA Adapter and Tokenizer ---\")\n",
        "\n",
        "# Validate directory\n",
        "if not os.path.exists(LOCAL_ADAPTER_DIR):\n",
        "    raise FileNotFoundError(f\"Adapter directory not found at {LOCAL_ADAPTER_DIR}.\")\n",
        "\n",
        "try:\n",
        "    # Attempt Method A: Push using in-memory trainer (This will still fail with NameError if run separately)\n",
        "    print(\"Attempting Method A (Trainer object)...\")\n",
        "    trainer.model.push_to_hub(\n",
        "        repo_id=HUB_MODEL_ID,\n",
        "        commit_message=\"Initial QLoRA adapter for Bitcoin price prediction (Trainer Method)\",\n",
        "        private=False\n",
        "    )\n",
        "    tokenizer.push_to_hub(HUB_MODEL_ID)\n",
        "    print(\"\\n✅ SUCCESS (Method A): Adapter and tokenizer uploaded to Hugging Face Hub.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\n⚠️ WARNING: 'trainer' object out of scope. Proceeding with robust file-based fallback.\")\n",
        "\n",
        "    # Fallback Method: Use HfApi.upload_folder to transfer checkpoint contents directly.\n",
        "    try:\n",
        "        # Push all necessary files in the checkpoint directory\n",
        "        print(f\"Uploading all files from {LOCAL_ADAPTER_DIR} using HfApi...\")\n",
        "        api.upload_folder(\n",
        "            folder_path=LOCAL_ADAPTER_DIR,\n",
        "            repo_id=HUB_MODEL_ID,\n",
        "            commit_message=\"LoRA adapter and tokenizer pushed from Google Drive checkpoint\",\n",
        "            # Uploading all files in the checkpoint is safest.\n",
        "            ignore_patterns=[\"*.pt\", \"*.bin\", \"optimizer.pt\", \"scheduler.pt\", \"rng_state.pth\"] # Ignore large training state files\n",
        "        )\n",
        "        print(\"\\n✅ SUCCESS (Fallback): Adapter and tokenizer uploaded to Hugging Face Hub.\")\n",
        "\n",
        "    except Exception as fallback_e:\n",
        "        print(f\"\\nFATAL ERROR: Upload failed. Error: {fallback_e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "weKPWa01U0hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nDeployment Complete. Model available at: https://huggingface.co/{HUB_MODEL_ID}\")"
      ],
      "metadata": {
        "id": "oPiqDEmZi2V-",
        "outputId": "909c4d74-0dc9-42ce-a82c-f8335535b2c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Deployment Complete. Model available at: https://huggingface.co/frankmorales2020/Mistral-7B-BTC-Expert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVAL"
      ],
      "metadata": {
        "id": "VIX2vXhyaWUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP AND CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Install necessary libraries (if not already installed)\n",
        "!pip install -q torch transformers peft accelerate bitsandbytes datasets pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ENQF3ZQaRiz",
        "outputId": "88c5a098-4173-4a74-fb59-206a9b4e0872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNodhQ92bcRy",
        "outputId": "ba71e866-9e83-4b78-a775-078cf7390a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL WORKING CODE: SINGLE INFERENCE TEST ---\n",
        "# This code block successfully loads the model and retrieves the prediction.\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Configuration\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-Expert\"\n",
        "BASE_MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# The user prompt designed to get a direct answer (based on successful structure)\n",
        "USER_PROMPT_CONTENT = \"Current BTC data: [O:30000, H:30500, C:30200]. Give ONLY the 12-hour direction (UP, DOWN, or FLAT).\"\n",
        "\n",
        "# --- 1. Load Model with Quantization ---\n",
        "print(f\"--- 1. Loading Model onto {DEVICE} ---\")\n",
        "try:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(HUB_MODEL_ID)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, HUB_MODEL_ID).eval()\n",
        "    print(\"✅ Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during model loading: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Run Single Inference ---\n",
        "\n",
        "# Manually construct the Mistral prompt string\n",
        "input_text = f\"<s>[INST] {USER_PROMPT_CONTENT} [/INST]\"\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Use max_new_tokens=120 or similar value to allow for the full analysis\n",
        "# The model generated 120 tokens previously, so we use 120 as a safe value.\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,\n",
        "        do_sample=False,\n",
        "        num_beams=1\n",
        "    )\n",
        "\n",
        "# Decode and clean the output\n",
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Slice the response to show only the generated part\n",
        "prompt_end_marker = \"[/INST]\"\n",
        "response_start_index = response_text.rfind(prompt_end_marker) + len(prompt_end_marker)\n",
        "prediction_output = response_text[response_start_index:].strip()"
      ],
      "metadata": {
        "id": "h78CJnXpaF8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 2. Running Single Inference Test ---\")\n",
        "print(f\"\\n✅ Inference Complete.\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"INPUT PROMPT:\\n{input_text.strip()}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"MODEL RESPONSE (Full Output):\\n{prediction_output}\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjV6r3efhNBl",
        "outputId": "22399f1d-8e34-4507-cb86-3f3dfc712d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. Running Single Inference Test ---\n",
            "\n",
            "✅ Inference Complete.\n",
            "----------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "<s>[INST] Current BTC data: [O:30000, H:30500, C:30200]. Give ONLY the 12-hour direction (UP, DOWN, or FLAT). [/INST]\n",
            "----------------------------------------------------------------------\n",
            "MODEL RESPONSE (Full Output):\n",
            "The 12-hour direction is **UP**. The current price is above the 20-period SMA but below the 20-period BTC bars' Average Price. The price is above the 20-period BTC bars' RSI (Relative Strength Index). The current price is below the 20-period BTC bars' 20-period SMA. The price is below the 20-period BTC bars' 20-period RSI (Relative Strength Index). The current price is above the 20\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
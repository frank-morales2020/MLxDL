{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMDL0f4e1QDh0+zDD2b0Hvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/deepseek_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwA3AVQ3uK1m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
        "\n",
        "text = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n",
        "\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n",
        "\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo1yV-rAv-Eq",
        "outputId": "bdf04fe9-28dd-4a1d-d955-e8a9beee2ae7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is a vector of real numbers, and the attention function is a function that maps the query and key-value pairs to the output.\n",
            "\n",
            "The attention function is a key component of the attention mechanism, which is a type of neural network architecture that allows the network to focus on specific parts of the input data. The attention mechanism is used in various applications, such as machine translation, image captioning, and question answering.\n",
            "\n",
            "The attention function is typically implemented using a neural network architecture, where the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "!pip install bitsandbytes -q\n",
        "!pip install -U bitsandbytes -q"
      ],
      "metadata": {
        "id": "ILccAs7vyh8E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"You seem to be using the pipelines sequentially on GPU\")\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "xWcef_467xJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import bitsandbytes\n",
        "from peft import LoraConfig, PeftModel  # Import LoraConfig from peft\n",
        "\n",
        "# Load the tokenizer and model with 4-bit quantization\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the LoRA update matrices\n",
        "    lora_alpha=32,  # Scaling factor for the LoRA updates\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # Specific target modules for DeepSeek LLM\n",
        "    lora_dropout=0.05,  # Dropout probability for the LoRA layers\n",
        "    bias=\"none\",  # No bias for the LoRA layers\n",
        ")\n",
        "\n",
        "\n",
        "# Apply LoRA\n",
        "# The model is already quantized during loading using bnb_config, and compute dtype is set to bfloat16.)\n",
        "from peft import get_peft_model # Import get_peft_model\n",
        "model = get_peft_model(model, lora_config) # Use get_peft_model to add LoRA to the model\n",
        "\n",
        "# Configure the model for training\n",
        "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
        "\n",
        "\n",
        "# Load the Spider dataset\n",
        "spider_dataset = load_dataset(\"spider\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"Question: {q} SQL: \" for q in examples[\"question\"]]\n",
        "    targets = examples[\"query\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_spider = spider_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"frankmorales2020/deepseek-llm-7b-base-spider\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_spider[\"train\"],\n",
        "    eval_dataset=tokenized_spider[\"validation\"],\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save and push the fine-tuned model to Hugging Face Hub\n",
        "trainer.save_model()\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "TstpYb3m4or7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
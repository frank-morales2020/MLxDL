{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FINAL_LLM_JEPA_MISTRAL_FT_BTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlzcPp_ElvKq",
        "outputId": "2d9e5e81-adff-46fe-a6db-b3b9ba103b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 25 05:07:20 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   42C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNM1CxFeHhUc"
      },
      "outputs": [],
      "source": [
        "# --- 1. ENVIRONMENT SETUP AND DEPENDENCY INSTALLATION ---\n",
        "print(\"\\n--- Installing dependencies and cleaning environment ---\")\n",
        "!pip cache purge\n",
        "!rm -rf /root/.cache/huggingface/* /root/.cache/pip/* /tmp/*\n",
        "!pip uninstall -y transformers peft bitsandbytes accelerate trl datasets pandas pandas_ta huggingface_hub quanto triton numpy scipy scikit-learn torch\n",
        "!pip install -q -U numpy==1.26.4\n",
        "!pip install -q -U scipy==1.14.1\n",
        "!pip install -q -U scikit-learn==1.5.1\n",
        "!pip install -q -U torch==2.3.0\n",
        "!pip install -q -U transformers==4.44.2\n",
        "!pip install -q -U peft==0.12.0\n",
        "!pip install -q -U bitsandbytes==0.43.3\n",
        "!pip install -q -U accelerate==0.34.2\n",
        "!pip install -q -U trl==0.11.1\n",
        "!pip install -q -U huggingface_hub==0.25.1\n",
        "!pip install -q -U quanto==0.2.0\n",
        "!pip install -q -U datasets==3.0.1\n",
        "!pip install -q -U triton==2.3.0\n",
        "!pip install -q -U pandas pandas_ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga3oCA9ehutz",
        "outputId": "75871730-e71d-4add-c36c-4261ed9b7ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVJJ5I2FK_Fc",
        "outputId": "be008446-8ce7-4bdf-d56e-961cd4eb9787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "btc_instruction_dataset.jsonl  temp_tokenized\n"
          ]
        }
      ],
      "source": [
        "!ls /content/gdrive/MyDrive/CryptoFT/dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0InbGmsTVfYW",
        "outputId": "d083d0bf-7749-4c38-ac5f-5280cf474345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "89685\n"
          ]
        }
      ],
      "source": [
        "!cat /content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset.jsonl | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI1C_OB0Vodg"
      },
      "outputs": [],
      "source": [
        "!more /content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset.jsonl | head -n 89000 > /content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset_sample.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kRJNhkrnXYG4"
      },
      "outputs": [],
      "source": [
        "!cat /content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset_sample.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxKX4CMlXh5X",
        "outputId": "863b4c79-b956-4cff-f48a-717f2761a238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42469\n"
          ]
        }
      ],
      "source": [
        "!cat /content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset_sample.jsonl |grep 'which supports a DOWN'|wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuAy4D-51KRb",
        "outputId": "c7626fa4-8136-4c10-86b7-df48f4b17fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   49G  188G  21% /\n",
            "du: cannot access '/root/.cache/huggingface/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!df -h /\n",
        "!du -sh /root/.cache/huggingface/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Oo_7RHC5M5L",
        "outputId": "52e2baf4-0ccc-4899-f83f-3a03b359da3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cleaning up temporary storage ---\n",
            "rm: cannot remove '/tmp/colab_runtime.sock': Device or resource busy\n",
            "âœ… Temporary storage cleared.\n"
          ]
        }
      ],
      "source": [
        "#--- CLEAN UP TEMPORARY STORAGE ---\n",
        "print(\"\\n--- Cleaning up temporary storage ---\")\n",
        "!rm -rf /root/.cache/huggingface/*\n",
        "!rm -rf /tmp/*\n",
        "print(\"âœ… Temporary storage cleared.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPVYKpBrQCWU",
        "outputId": "d9fe869b-3748-451f-ca2a-c288f32cbcb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "access_token_write = userdata.get('HF_TOKEN')\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIO8dphFSTjV"
      },
      "source": [
        "## MODEL-BASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK4rulypCq82"
      },
      "outputs": [],
      "source": [
        "# --- FULL CODE: TRAINING + DEPLOYMENT + EVALUATION FOR MISTRAL-7B-BTC-JEPA-LLM-EXPERT ---\n",
        "\n",
        "# --- 1. IMPORTS AND CUSTOM CLASSES ---\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from huggingface_hub.utils import RepositoryNotFoundError\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# --- LLM-JEPA CONFIGURATION ---\n",
        "LLM_JEPA_CONFIG = {\n",
        "    \"lbd\": 2.0,               # Lambda (JEPA loss weight)\n",
        "    \"gamma\": 1.0,             # Gamma (LLM loss weight)\n",
        "    \"predictors\": 3,          # k (Number of predictor tokens)\n",
        "    \"last_token\": -1,         # Index of last token for embedding (typically EOS/</s> or -1)\n",
        "}\n",
        "SPECIAL_PREDICTOR_TOKENS = [f\"<|predictor_{i}|>\" for i in range(1, LLM_JEPA_CONFIG[\"predictors\"] + 1)]\n",
        "\n",
        "# --- FILE PATHS ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "DATASET_PATH = \"/content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset.jsonl\"\n",
        "OUTPUT_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/results_btc_jepa\"\n",
        "FINAL_MODEL_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/Mistral-7B-BTC-JEPA_FINAL\"\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-JEPA-LLM-Expert\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "TEMP_DATASET_DIR = \"/content/gdrive/MyDrive/CryptoFT/dataset/temp_tokenized\"\n",
        "\n",
        "# --- UTILITIES ---\n",
        "def get_messages_from_sample(sample_text):\n",
        "    \"\"\"Parses the Mistral template: '<s>[INST] Instruction [/INST] Response</s>'\"\"\"\n",
        "    inst_match = re.search(r\"\\[INST\\](.*?)\\[/INST\\]\", sample_text, re.DOTALL)\n",
        "    user_content = inst_match.group(1).strip() if inst_match else \"\"\n",
        "    resp_match = re.search(r\"\\[/INST\\](.*?)\\s*</s>\", sample_text, re.DOTALL)\n",
        "    assistant_content = resp_match.group(1).strip() if resp_match else \"\"\n",
        "    return {\"user_content\": user_content, \"assistant_content\": assistant_content}\n",
        "\n",
        "# --- CUSTOM DATA COLLATOR ---\n",
        "class CustomJEPABatchCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.keys = [\n",
        "            \"input_ids\", \"labels\", \"attention_mask\",\n",
        "            \"input_ids_user\", \"labels_user\", \"attention_mask_user\",\n",
        "            \"input_ids_assistant\", \"labels_assistant\", \"attention_mask_assistant\"\n",
        "        ]\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = {key: [] for key in self.keys}\n",
        "        if not features or not any(features):\n",
        "            return None\n",
        "        for feature in features:\n",
        "            if all(key in feature for key in self.keys):\n",
        "                for key in self.keys:\n",
        "                    batch[key].append(feature[key])\n",
        "        if not any(batch.values()):\n",
        "            return None\n",
        "        for key in self.keys:\n",
        "            if not batch[key]:\n",
        "                raise ValueError(f\"No valid samples for key '{key}' in batch. Check dataset tokenization.\")\n",
        "            batch[key] = torch.LongTensor(batch[key])\n",
        "        return batch\n",
        "\n",
        "# --- CUSTOM TRAINER ---\n",
        "class RepresentationTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.lbd = kwargs.pop('lbd', 1.0)\n",
        "        self.gamma = kwargs.pop('gamma', 1.0)\n",
        "        self.last_token = kwargs.pop('last_token', -1)\n",
        "        self._last_metrics = {}  # Store metrics for logging\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.args.per_device_train_batch_size,\n",
        "            collate_fn=self.data_collator,\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset=None):\n",
        "        dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.args.per_device_eval_batch_size,\n",
        "            collate_fn=self.data_collator,\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "    def _last_token_index(self, input_ids, attention_mask):\n",
        "        last_indices = torch.sum(attention_mask, dim=1) + self.last_token\n",
        "        return torch.clamp(last_indices, min=0)\n",
        "\n",
        "    def forward(self, model, inputs):\n",
        "        batch_size = inputs[\"input_ids\"].shape[0]\n",
        "        llm_inputs = {\n",
        "            \"input_ids\": torch.cat([inputs[\"input_ids\"], inputs[\"input_ids_user\"], inputs[\"input_ids_assistant\"]], dim=0),\n",
        "            \"labels\": torch.cat([inputs[\"labels\"], inputs[\"labels_user\"], inputs[\"labels_assistant\"]], dim=0),\n",
        "            \"attention_mask\": torch.cat([inputs[\"attention_mask\"], inputs[\"attention_mask_user\"], inputs[\"attention_mask_assistant\"]], dim=0),\n",
        "        }\n",
        "        with torch.enable_grad():\n",
        "            outputs = model(**llm_inputs, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states[-1].requires_grad_(True)\n",
        "        user_hidden_states = hidden_states[batch_size: batch_size * 2]\n",
        "        assistant_hidden_states = hidden_states[batch_size * 2:]\n",
        "        return {\n",
        "            'main_outputs': outputs,\n",
        "            'user_hidden_states': user_hidden_states,\n",
        "            'assistant_hidden_states': assistant_hidden_states,\n",
        "        }\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        batch_size = inputs[\"input_ids\"].shape[0]\n",
        "        index_user = self._last_token_index(inputs[\"input_ids_user\"], inputs[\"attention_mask_user\"])\n",
        "        index_assistant = self._last_token_index(inputs[\"input_ids_assistant\"], inputs[\"attention_mask_assistant\"])\n",
        "        forward_results = self.forward(model, inputs)\n",
        "        lm_loss = forward_results['main_outputs'].loss\n",
        "        user_hidden_states = forward_results['user_hidden_states']\n",
        "        assistant_hidden_states = forward_results['assistant_hidden_states']\n",
        "        user_embedding = user_hidden_states[range(batch_size), index_user, :]\n",
        "        assistant_embedding = assistant_hidden_states[range(batch_size), index_assistant, :]\n",
        "        cosine_similarity = F.cosine_similarity(user_embedding, assistant_embedding, dim=-1)\n",
        "        jepa_loss = 1.0 - torch.mean(cosine_similarity)\n",
        "        total_loss = self.gamma * lm_loss + self.lbd * jepa_loss\n",
        "        self._last_metrics = {'jepa_loss': jepa_loss, 'lm_loss': lm_loss}\n",
        "\n",
        "        # FIXED: Return only loss tensor for training, metrics are stored separately\n",
        "        if return_outputs:\n",
        "            return (total_loss, forward_results['main_outputs'])\n",
        "        return total_loss\n",
        "\n",
        "    def log(self, logs):\n",
        "        if self._last_metrics:\n",
        "            logs[\"jepa_loss\"] = self._nested_gather(self._last_metrics[\"jepa_loss\"]).mean().item()\n",
        "            logs[\"lm_loss\"] = self._nested_gather(self._last_metrics[\"lm_loss\"]).mean().item()\n",
        "        super().log(logs)\n",
        "\n",
        "# --- 2. MODEL AND DATASET SETUP ---\n",
        "print(\"\\n--- Setting up model and dataset ---\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# Clear any existing dataset cache\n",
        "if os.path.exists(TEMP_DATASET_DIR):\n",
        "    shutil.rmtree(TEMP_DATASET_DIR)\n",
        "os.makedirs(TEMP_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# PEFT (LoRA) Config\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "\n",
        "# Training Arguments - OPTIMIZED FOR JEPA MONITORING\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_steps=0,  # Disable saving during demo\n",
        "    logging_steps=50,  # See JEPA metrics every 10 steps\n",
        "    max_steps=500,  # Just enough to see JEPA loss decreasing\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"none\",\n",
        "    # â†“â†“â†“ CRITICAL CHANGES FOR DEMO â†“â†“â†“\n",
        "    evaluation_strategy=\"no\",  # Disable evaluation during training\n",
        "    eval_steps=None,  # No evaluation steps\n",
        "    metric_for_best_model=None,  # Not needed for demo\n",
        "    # â†‘â†‘â†‘ CRITICAL CHANGES FOR DEMO â†‘â†‘â†‘\n",
        "    dataloader_drop_last=True,\n",
        "    dataloader_num_workers=0,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    resume_from_checkpoint=True\n",
        ")\n",
        "\n",
        "# Load Model with Quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_PREDICTOR_TOKENS})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = get_peft_model(model, peft_config)\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name:\n",
        "        param.requires_grad = True\n",
        "print(\"âœ… PEFT (LoRA) configuration applied\")\n",
        "\n",
        "# Load and Prepare Dataset\n",
        "print(\"\\n--- Loading and Splitting Custom Dataset ---\")\n",
        "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "\n",
        "# Filter malformed samples\n",
        "def filter_valid_samples(example):\n",
        "    text = example[\"text\"]\n",
        "    parsed = get_messages_from_sample(text)\n",
        "    valid = bool(parsed[\"user_content\"] and parsed[\"assistant_content\"])\n",
        "    if not valid:\n",
        "        print(f\"Skipping malformed sample: {text[:50]}...\")\n",
        "    return valid\n",
        "\n",
        "filtered_dataset = dataset.filter(filter_valid_samples, desc=\"Filtering valid samples\")\n",
        "if len(filtered_dataset) < 3000:\n",
        "    raise ValueError(f\"Filtered dataset has {len(filtered_dataset)} samples, need at least 3000 for 2500 train + 500 eval.\")\n",
        "print(f\"Filtered dataset size: {len(filtered_dataset)} samples\")\n",
        "\n",
        "# Select 2500 for train, 500 for eval\n",
        "train_dataset = filtered_dataset.select(range(2500))\n",
        "test_dataset = filtered_dataset.select(range(2500, 3000))\n",
        "tokenized_dataset = {\"train\": train_dataset, \"test\": test_dataset}\n",
        "\n",
        "def tokenize_jepa_views(example):\n",
        "    sample_text = example[\"text\"]\n",
        "    parsed = get_messages_from_sample(sample_text)\n",
        "    predictor_str = \"\".join(SPECIAL_PREDICTOR_TOKENS)\n",
        "    user_text_with_pred = f\"<s>[INST] {parsed['user_content']} {predictor_str} [/INST]\"\n",
        "    assistant_text = f\"{parsed['assistant_content']}</s>\"\n",
        "    tokenized_full = tokenizer(sample_text, truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", return_tensors=\"np\")\n",
        "    tokenized_user = tokenizer(user_text_with_pred, truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", return_tensors=\"np\")\n",
        "    tokenized_assistant = tokenizer(assistant_text, truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", return_tensors=\"np\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": tokenized_full[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask\": tokenized_full[\"attention_mask\"][0].tolist(),\n",
        "        \"labels\": tokenized_full[\"input_ids\"][0].copy().tolist(),\n",
        "        \"input_ids_user\": tokenized_user[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask_user\": tokenized_user[\"attention_mask\"][0].tolist(),\n",
        "        \"labels_user\": [-100] * MAX_SEQ_LENGTH,\n",
        "        \"input_ids_assistant\": tokenized_assistant[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask_assistant\": tokenized_assistant[\"attention_mask\"][0].tolist(),\n",
        "        \"labels_assistant\": [-100] * MAX_SEQ_LENGTH\n",
        "    }\n",
        "\n",
        "# Tokenize and save to disk to avoid caching issues\n",
        "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].map(\n",
        "    tokenize_jepa_views,\n",
        "    batched=False,\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing train samples for LLM-JEPA views\",\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(\n",
        "    tokenize_jepa_views,\n",
        "    batched=False,\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing test samples for LLM-JEPA views\",\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "tokenized_dataset[\"train\"].save_to_disk(os.path.join(TEMP_DATASET_DIR, \"train\"))\n",
        "tokenized_dataset[\"test\"].save_to_disk(os.path.join(TEMP_DATASET_DIR, \"test\"))\n",
        "\n",
        "# Reload dataset to ensure consistency\n",
        "from datasets import load_from_disk\n",
        "tokenized_dataset = {\n",
        "    \"train\": load_from_disk(os.path.join(TEMP_DATASET_DIR, \"train\")),\n",
        "    \"test\": load_from_disk(os.path.join(TEMP_DATASET_DIR, \"test\"))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWZIrwGLSiGD"
      },
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez8THznFwjJM",
        "outputId": "b128ab37-6274-4060-d562-e3f8999a3c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Validating Dataset Keys ---\n",
            "\n",
            "Checking train split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [00:10<00:00, 242.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train split: All 2500 samples validated.\n",
            "\n",
            "Checking test split...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 239.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test split: All 500 samples validated.\n",
            "Dataset columns filtered to: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Validate all samples in dataset\n",
        "print(\"\\n--- Validating Dataset Keys ---\")\n",
        "required_keys = [\n",
        "    \"input_ids\", \"labels\", \"attention_mask\",\n",
        "    \"input_ids_user\", \"labels_user\", \"attention_mask_user\",\n",
        "    \"input_ids_assistant\", \"labels_assistant\", \"attention_mask_assistant\"\n",
        "]\n",
        "for split in [\"train\", \"test\"]:\n",
        "    print(f\"\\nChecking {split} split...\")\n",
        "    missing_samples = []\n",
        "    for idx, sample in tqdm(enumerate(tokenized_dataset[split]), total=len(tokenized_dataset[split]), desc=f\"Validating {split} split\"):\n",
        "        missing_keys = [key for key in required_keys if key not in sample]\n",
        "        if missing_keys:\n",
        "            missing_samples.append((idx, missing_keys))\n",
        "    if missing_samples:\n",
        "        print(f\"Found {len(missing_samples)} problematic samples in {split} split:\")\n",
        "        for idx, missing_keys in missing_samples[:5]:\n",
        "            print(f\"Sample {idx} missing keys: {missing_keys}\")\n",
        "        raise ValueError(f\"Dataset validation failed in {split} split.\")\n",
        "    print(f\"{split} split: All {len(tokenized_dataset[split])} samples validated.\")\n",
        "\n",
        "COLUMNS_TO_KEEP = required_keys\n",
        "try:\n",
        "    tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].select_columns(COLUMNS_TO_KEEP)\n",
        "    tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].select_columns(COLUMNS_TO_KEEP)\n",
        "    print(f\"Dataset columns filtered to: {COLUMNS_TO_KEEP}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during column selection: {e}\")\n",
        "    print(f\"Available columns in train: {tokenized_dataset['train'].column_names}\")\n",
        "    print(f\"Available columns in test: {tokenized_dataset['test'].column_names}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QWOSpkBS7jY"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oau3g7m79mFw",
        "outputId": "ef70a967-837a-4535-b3e8-f3ff00838bb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Debugging Dataset Before Training ---\n",
            "\n",
            "train split sample keys:\n",
            "Sample 0 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 1 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 2 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 3 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 4 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "\n",
            "test split sample keys:\n",
            "Sample 0 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 1 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 2 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 3 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 4 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "\n",
            "--- Initializing and Starting RepresentationTrainer (LLM-JEPA) ---\n",
            "\n",
            "--- GPU Memory Before Training ---\n",
            "GPU Memory Allocated: 5166.18 MiB\n",
            "GPU Memory Reserved: 5240.00 MiB\n",
            "GPU Memory Free: 17452.88 MiB\n",
            "\n",
            "--- Trainer Initialized with Separate Logging and Control Callbacks ---\n"
          ]
        }
      ],
      "source": [
        "# Debug dataset before training\n",
        "print(\"\\n--- Debugging Dataset Before Training ---\")\n",
        "for split in [\"train\", \"test\"]:\n",
        "    print(f\"\\n{split} split sample keys:\")\n",
        "    for idx in range(min(5, len(tokenized_dataset[split]))):\n",
        "        sample = tokenized_dataset[split][idx]\n",
        "        print(f\"Sample {idx} keys: {list(sample.keys())}\")\n",
        "\n",
        "# --- 3. TRAINER INITIALIZATION AND EXECUTION ---\n",
        "print(\"\\n--- Initializing and Starting RepresentationTrainer (LLM-JEPA) ---\")\n",
        "custom_collator = CustomJEPABatchCollator(tokenizer)\n",
        "\n",
        "# Log GPU memory before training\n",
        "def log_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MiB\")\n",
        "        print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MiB\")\n",
        "        print(f\"GPU Memory Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**2:.2f} MiB\")\n",
        "\n",
        "print(\"\\n--- GPU Memory Before Training ---\")\n",
        "log_gpu_memory()\n",
        "\n",
        "# ADD FIXED JEPA MONITORING CALLBACK HERE\n",
        "\n",
        "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "# --- CALLBACK 1: MONITORING AND LOGGING (Your Original Role) ---\n",
        "class JEPAMonitorCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        # Force JEPA metrics display every 10 steps (matching logging_steps=10)\n",
        "        if state.global_step % 50 == 0 and hasattr(trainer, '_last_metrics'):\n",
        "            metrics = trainer._last_metrics\n",
        "            jepa_loss = metrics.get('jepa_loss')\n",
        "            lm_loss = metrics.get('lm_loss')\n",
        "            if jepa_loss is not None and lm_loss is not None:\n",
        "                print(f\"\\nðŸŽ¯ JEPA Metrics [Step {state.global_step}]:\")\n",
        "                print(f\"   JEPA Loss: {jepa_loss.item():.4f}\")\n",
        "                print(f\"   LM Loss: {lm_loss.item():.4f}\")\n",
        "                print(f\"   Cosine Sim: {1 - jepa_loss.item():.4f}\")\n",
        "                print(f\"   Total Loss: {trainer.gamma * lm_loss + trainer.lbd * jepa_loss:.4f}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Also add to official logs for potential WandB/etc.\n",
        "        if logs is not None and hasattr(trainer, '_last_metrics'):\n",
        "            jepa_loss = trainer._last_metrics.get('jepa_loss')\n",
        "            lm_loss = trainer._last_metrics.get('lm_loss')\n",
        "            if jepa_loss is not None:\n",
        "                logs[\"jepa_loss\"] = jepa_loss.item()\n",
        "                logs[\"lm_loss\"] = lm_loss.item()\n",
        "\n",
        "# --- CALLBACK 2: CONTROL AND EARLY STOPPING (The New Role) ---\n",
        "# This class handles the conditional stopping logic.\n",
        "class JEPALossStabilityCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Stops training when the JEPA Loss on the training batch drops below a threshold\n",
        "    and stabilizes for a specified number of logging steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, jepa_threshold=0.0001, stability_steps=3):\n",
        "        self.jepa_threshold = jepa_threshold\n",
        "        self.stability_steps = stability_steps\n",
        "        self.stable_count = 0\n",
        "\n",
        "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        trainer = kwargs.get('trainer')\n",
        "\n",
        "        # Check only at logging steps (every 50 steps)\n",
        "        if state.global_step > 0 and state.global_step % args.logging_steps == 0 and hasattr(trainer, '_last_metrics'):\n",
        "            metrics = trainer._last_metrics\n",
        "            jepa_loss = metrics.get('jepa_loss')\n",
        "\n",
        "            if jepa_loss is not None:\n",
        "                current_jepa_loss = jepa_loss.item()\n",
        "\n",
        "                # Check for Early Stop Condition\n",
        "                if current_jepa_loss <= self.jepa_threshold:\n",
        "                    self.stable_count += 1\n",
        "\n",
        "                    if self.stable_count >= self.stability_steps:\n",
        "                        # SET THE CONTROL FLAG TO STOP\n",
        "                        control.should_stop = True\n",
        "                else:\n",
        "                    self.stable_count = 0\n",
        "\n",
        "        return control\n",
        "\n",
        "# Ensure both classes are defined in a single block before proceeding.\n",
        "# --- RE-INITIALIZE TRAINER WITH TWO CUSTOM CALLBACKS ---\n",
        "\n",
        "# Ensure the custom collator is ready\n",
        "custom_collator = CustomJEPABatchCollator(tokenizer)\n",
        "\n",
        "# Pass both callback instances in a list\n",
        "trainer = RepresentationTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"], # Keep this here for structure, even if eval_strategy=\"no\" is in TrainingArguments\n",
        "    args=training_arguments,\n",
        "    data_collator=custom_collator,\n",
        "    lbd=LLM_JEPA_CONFIG[\"lbd\"],\n",
        "    gamma=LLM_JEPA_CONFIG[\"gamma\"],\n",
        "    last_token=LLM_JEPA_CONFIG[\"last_token\"],\n",
        "    callbacks=[\n",
        "        JEPAMonitorCallback(),                     # 1. For Logging (Monitors progress)\n",
        "        JEPALossStabilityCallback(jepa_threshold=0.0001, stability_steps=3) # 2. For Control (Stops the process)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Trainer Initialized with Separate Logging and Control Callbacks ---\")\n",
        "# You can now call trainer.train() in the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SwG6nFwobLvg",
        "outputId": "7f3bdeda-7494-432c-80f4-66717aa2e7ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… JEPA Monitoring Enabled - will show metrics every 50/500 steps\n",
            "\n",
            "--- Starting LLM-JEPA Fine-Tuning with Monitoring ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 10:55:37, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.876400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.235400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.224500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.229200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.231300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.221500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.229300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.220400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.224200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 50]:\n",
            "   JEPA Loss: 0.0001\n",
            "   LM Loss: 0.2534\n",
            "   Cosine Sim: 0.9999\n",
            "   Total Loss: 0.2535\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 100]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.2319\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.2320\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 150]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.2438\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.2438\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 200]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.2384\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.2384\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 250]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.2216\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.2217\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 300]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.1579\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.1580\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 350]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.2287\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.2287\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 400]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.2151\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.2152\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 450]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.1994\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.1994\n",
            "\n",
            "ðŸŽ¯ JEPA Metrics [Step 500]:\n",
            "   JEPA Loss: 0.0000\n",
            "   LM Loss: 0.2188\n",
            "   Cosine Sim: 1.0000\n",
            "   Total Loss: 0.2189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fine-Tuning Complete! ---\n",
            "\n",
            "--- Cleaning Up Old Checkpoints ---\n",
            "\n",
            "--- Saving Final Adapter Weights to Google Drive ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… LoRA adapter and tokenizer saved to: '/content/gdrive/MyDrive/CryptoFT/models/Mistral-7B-BTC-JEPA_FINAL'\n",
            "Checkpoints and logs saved to '/content/gdrive/MyDrive/CryptoFT/models/results_btc_jepa'\n"
          ]
        }
      ],
      "source": [
        "print(f\"âœ… JEPA Monitoring Enabled - will show metrics every {training_arguments.logging_steps}/{training_arguments.max_steps} steps\")\n",
        "print(\"\\n--- Starting LLM-JEPA Fine-Tuning with Monitoring ---\")\n",
        "trainer.train()\n",
        "print(\"\\n--- Fine-Tuning Complete! ---\")\n",
        "\n",
        "# Clean up old checkpoints\n",
        "print(\"\\n--- Cleaning Up Old Checkpoints ---\")\n",
        "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save to Google Drive\n",
        "print(\"\\n--- Saving Final Adapter Weights to Google Drive ---\")\n",
        "trainer.model.save_pretrained(FINAL_MODEL_DIR)\n",
        "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "print(f\"\\nâœ… LoRA adapter and tokenizer saved to: '{FINAL_MODEL_DIR}'\")\n",
        "print(f\"Checkpoints and logs saved to '{OUTPUT_DIR}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQFjS7d3TE0y"
      },
      "source": [
        "## DEPLOYMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toQZ-0Ox22aY",
        "outputId": "0eaf1411-18bb-457a-e42b-5f0ef9575f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Authenticating with Hugging Face Hub ---\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# --- 4. DEPLOYMENT TO HUGGING FACE ---\n",
        "print(\"\\n--- Authenticating with Hugging Face Hub ---\")\n",
        "try:\n",
        "    access_token_write = userdata.get('HF_TOKEN')\n",
        "    if not access_token_write:\n",
        "        raise ValueError(\"HF_TOKEN secret not found or is empty.\")\n",
        "    login(token=access_token_write, add_to_git_credential=True)\n",
        "    api = HfApi(token=access_token_write)\n",
        "    print(\"âœ… Successfully logged into Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during login: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\n--- Checking/Creating Repository: {HUB_MODEL_ID} ---\")\n",
        "try:\n",
        "    api.repo_info(repo_id=HUB_MODEL_ID, repo_type=\"model\")\n",
        "    print(f\"âœ… Repository {HUB_MODEL_ID} already exists.\")\n",
        "except RepositoryNotFoundError:\n",
        "    print(f\"âš ï¸ Repository {HUB_MODEL_ID} not found. Creating it now...\")\n",
        "    create_repo(repo_id=HUB_MODEL_ID, repo_type=\"model\", private=False, token=access_token_write)\n",
        "    print(f\"âœ… Repository {HUB_MODEL_ID} created successfully.\")\n",
        "\n",
        "print(f\"\\n--- Pushing LoRA Adapter and Tokenizer ---\")\n",
        "if not os.path.exists(FINAL_MODEL_DIR):\n",
        "    raise FileNotFoundError(f\"Adapter directory not found at {FINAL_MODEL_DIR}.\")\n",
        "try:\n",
        "    trainer.model.push_to_hub(\n",
        "        repo_id=HUB_MODEL_ID,\n",
        "        commit_message=\"Initial QLoRA adapter for Bitcoin price prediction\",\n",
        "        private=False\n",
        "    )\n",
        "    tokenizer.push_to_hub(HUB_MODEL_ID)\n",
        "    print(\"\\nâœ… SUCCESS: Adapter and tokenizer uploaded to Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸ WARNING: Trainer push failed. Using HfApi fallback. Error: {e}\")\n",
        "    api.upload_folder(\n",
        "        folder_path=FINAL_MODEL_DIR,\n",
        "        repo_id=HUB_MODEL_ID,\n",
        "        commit_message=\"LoRA adapter and tokenizer pushed from Google Drive checkpoint\",\n",
        "        ignore_patterns=[\"*.pt\", \"*.bin\", \"optimizer.pt\", \"scheduler.pt\", \"rng_state.pth\"]\n",
        "    )\n",
        "    print(\"\\nâœ… SUCCESS (Fallback): Adapter and tokenizer uploaded to Hugging Face Hub.\")\n",
        "print(f\"\\nDeployment Complete. Model available at: https://huggingface.co/{HUB_MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqXmJ3ecTQQB"
      },
      "source": [
        "## EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O9seIeCEUZss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614b9664-2f61-4824-ca50-b03de6e5bf6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: peft 0.12.0\n",
            "Uninstalling peft-0.12.0:\n",
            "  Successfully uninstalled peft-0.12.0\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ðŸš¨ FIRST: COMPLETELY RESET PEFT INSTALLATION\n",
        "!pip uninstall -y peft\n",
        "!pip install peft==0.10.0  -q # Use stable version\n",
        "\n",
        "# Restart runtime after this command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIr4DOt_V9SJ"
      },
      "outputs": [],
      "source": [
        "# --- FILE PATHS ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "DATASET_PATH = \"/content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset.jsonl\"\n",
        "OUTPUT_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/results_btc_jepa\"\n",
        "FINAL_MODEL_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/Mistral-7B-BTC-JEPA_FINAL\"\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-JEPA-LLM-Expert\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "TEMP_DATASET_DIR = \"/content/gdrive/MyDrive/CryptoFT/dataset/temp_tokenized\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf7NgN2kVusD"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaqlhdvFo0SN"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries for QLoRA and SFTTrainer\n",
        "!pip install -q -U bitsandbytes transformers peft accelerate trl datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-S2qQKao4TQ"
      },
      "outputs": [],
      "source": [
        "# --- FILE PATHS ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "FINAL_MODEL_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/Mistral-7B-BTC-JEPA_FINAL\"\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-JEPA-LLM-Expert\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch"
      ],
      "metadata": {
        "id": "6G_T-2yH7IhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. CLEAN ENVIRONMENT SETUP ---\n",
        "!pip uninstall -y bitsandbytes triton\n",
        "!pip install -q bitsandbytes==0.43.3 triton==2.3.0 transformers==4.44.2 peft==0.12.0 accelerate==0.34.2"
      ],
      "metadata": {
        "id": "O88qVI64GOSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Uninstall the conflicting packages\n",
        "!pip uninstall -y bitsandbytes triton\n",
        "\n",
        "# 2. Install specific compatible versions\n",
        "# We use triton 2.3.0 which contains the 'ops' module expected by bnb 0.43.3\n",
        "!pip install -q bitsandbytes==0.43.3 triton==2.3.0\n"
      ],
      "metadata": {
        "id": "Jvn_mZcy86e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Ensure the system sees the CUDA HOME (Crucial for Colab/Linux)\n",
        "import os\n",
        "import torch\n",
        "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
        "os.environ[\"LD_LIBRARY_PATH\"] += \":/usr/local/cuda/lib64\"\n",
        "\n",
        "# 4. Verify if bitsandbytes can find CUDA now\n",
        "import bitsandbytes as bnb\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkcUN09H9fyH",
        "outputId": "61820ca7-13c2-443b-8711-a546ff1f048a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBgzk3UW28F4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-JEPA-LLM-Expert\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SPECIAL_TOKENS = [\"<|predictor_1|>\", \"<|predictor_2|>\", \"<|predictor_3|>\"]\n",
        "\n",
        "# --- 3. THE FIXED LOADING SEQUENCE ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, HUB_MODEL_ID).eval()\n",
        "print(\"ðŸŽ‰ SUCCESS: Model and JEPA Expert loaded!\")\n",
        "\n",
        "# --- 4. CALIBRATED INFERENCE ENGINE (DEMO-FOCUSED) ---\n",
        "def run_expert_inference(ohlc_data, rsi=50.0, sma=60000.0):\n",
        "    target_words = [\"UP\", \"DOWN\", \"FLAT\"]\n",
        "    target_ids = [tokenizer.encode(word, add_special_tokens=False)[-1] for word in target_words]\n",
        "\n",
        "    instruction = (\n",
        "        f\"Analyze BTC Market Data: {ohlc_data}. \"\n",
        "        f\"Technicals: RSI(14) is {rsi:.2f}, SMA(20) is {sma:.2f}. \"\n",
        "        f\"Direction (UP/DOWN/FLAT):\"\n",
        "    )\n",
        "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        null_prompt = f\"<s>[INST] Analyze BTC Market Data: [O:60000, H:60100, C:60050]. Technicals: RSI(14) is 50.00, SMA(20) is 60000.00. Direction (UP/DOWN/FLAT): [/INST]\"\n",
        "        n_inputs = tokenizer(null_prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "        null_logits = model(**n_inputs).logits[0, -1, target_ids]\n",
        "\n",
        "        r_inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "        real_logits = model(**r_inputs).logits[0, -1, target_ids]\n",
        "\n",
        "        # Temperature 1.0 for natural balance in demo cases\n",
        "        calibrated_logits = (real_logits - null_logits) / 1.0\n",
        "        probs = F.softmax(calibrated_logits, dim=-1)\n",
        "\n",
        "    confidences = {word: round(prob.item(), 3) for word, prob in zip(target_words, probs)}\n",
        "    prediction = max(confidences, key=confidences.get)\n",
        "    return prediction, confidences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸŽ‰ SUCCESS: Model and JEPA Expert loaded!\")\n",
        "#--- 5. FINAL DEMO CASES TO SHOWCASE UP, DOWN, and FLAT ---\n",
        "demo_cases = [\n",
        "    # Clear bearish crash (strong DOWN signal)\n",
        "    {\"label\": \"BEARISH CRASH (DOWN Demo)\", \"data\": \"[O:100000, H:101000, C:70000]\", \"rsi\": 20.0, \"sma\": 95000.0},\n",
        "\n",
        "    # Pure neutral flat (FLAT Demo)\n",
        "    {\"label\": \"NEUTRAL FLAT (FLAT Demo)\", \"data\": \"[O:87000, H:87100, C:87050]\", \"rsi\": 50.0, \"sma\": 87000.0},\n",
        "\n",
        "    # Current real market Dec 25, 2025 (~$87,700, neutral RSI ~43, mild weakness below SMA ~$88,500)\n",
        "    {\"label\": \"CURRENT REAL (FLAT/DOWN-ish)\", \"data\": \"[O:87700, H:87800, C:87700]\", \"rsi\": 43.0, \"sma\": 88500.0},\n",
        "\n",
        "    # Mild bullish (to trigger UP â€“ the adapter sometimes favors UP in very neutral cases)\n",
        "    {\"label\": \"MILD BULLISH (UP Demo)\", \"data\": \"[O:87000, H:88000, C:87500]\", \"rsi\": 55.0, \"sma\": 86800.0},\n",
        "\n",
        "    # Strong bullish rally (UP Demo fallback if needed)\n",
        "    {\"label\": \"STRONG BULLISH (UP Demo)\", \"data\": \"[O:80000, H:95000, C:94000]\", \"rsi\": 75.0, \"sma\": 82000.0},\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*95)\n",
        "print(f\"{'Demo Scenario':<30} | {'Prediction':<10} | {'Confidence (UP / DOWN / FLAT)'}\")\n",
        "print(\"-\" * 95)\n",
        "for test in demo_cases:\n",
        "    pred, scores = run_expert_inference(test['data'], rsi=test['rsi'], sma=test['sma'])\n",
        "    score_str = f\"U:{scores['UP']:.3f} D:{scores['DOWN']:.3f} F:{scores['FLAT']:.3f}\"\n",
        "    print(f\"{test['label']:<30} | {pred:<10} | {score_str}\")\n",
        "print(\"=\"*95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOhhFJKxPSCJ",
        "outputId": "90dd9ca0-b0af-43dd-904b-3b34af25086c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ‰ SUCCESS: Model and JEPA Expert loaded!\n",
            "\n",
            "===============================================================================================\n",
            "Demo Scenario                  | Prediction | Confidence (UP / DOWN / FLAT)\n",
            "-----------------------------------------------------------------------------------------------\n",
            "BEARISH CRASH (DOWN Demo)      | FLAT       | U:0.328 D:0.332 F:0.340\n",
            "NEUTRAL FLAT (FLAT Demo)       | UP         | U:0.337 D:0.335 F:0.329\n",
            "CURRENT REAL (FLAT/DOWN-ish)   | DOWN       | U:0.332 D:0.338 F:0.330\n",
            "MILD BULLISH (UP Demo)         | DOWN       | U:0.331 D:0.337 F:0.331\n",
            "STRONG BULLISH (UP Demo)       | DOWN       | U:0.331 D:0.336 F:0.332\n",
            "===============================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PQFjS7d3TE0y",
        "nqXmJ3ecTQQB"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNk0Y8YOC/fsUnCO4aWPhqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
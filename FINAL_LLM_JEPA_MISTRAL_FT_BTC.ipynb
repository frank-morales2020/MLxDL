{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "wWZIrwGLSiGD",
        "PQFjS7d3TE0y",
        "nqXmJ3ecTQQB"
      ],
      "authorship_tag": "ABX9TyNJJIQs/QGBT2Q2Mt/WLKNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FINAL_LLM_JEPA_MISTRAL_FT_BTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlzcPp_ElvKq",
        "outputId": "31daf692-9a87-4c74-b838-917c8d77490d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct  7 13:34:55 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   48C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. ENVIRONMENT SETUP AND DEPENDENCY INSTALLATION ---\n",
        "print(\"\\n--- Installing dependencies and cleaning environment ---\")\n",
        "!pip cache purge\n",
        "!rm -rf /root/.cache/huggingface/* /root/.cache/pip/* /tmp/*\n",
        "!pip uninstall -y transformers peft bitsandbytes accelerate trl datasets pandas pandas_ta huggingface_hub quanto triton numpy scipy scikit-learn torch\n",
        "!pip install -q -U numpy==1.26.4\n",
        "!pip install -q -U scipy==1.14.1\n",
        "!pip install -q -U scikit-learn==1.5.1\n",
        "!pip install -q -U torch==2.3.0\n",
        "!pip install -q -U transformers==4.44.2\n",
        "!pip install -q -U peft==0.12.0\n",
        "!pip install -q -U bitsandbytes==0.43.3\n",
        "!pip install -q -U accelerate==0.34.2\n",
        "!pip install -q -U trl==0.11.1\n",
        "!pip install -q -U huggingface_hub==0.25.1\n",
        "!pip install -q -U quanto==0.2.0\n",
        "!pip install -q -U datasets==3.0.1\n",
        "!pip install -q -U triton==2.3.0\n",
        "!pip install -q -U pandas pandas_ta"
      ],
      "metadata": {
        "id": "nNM1CxFeHhUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga3oCA9ehutz",
        "outputId": "e7638218-ecf8-4cc2-a061-84768d2a0a73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/CryptoFT/dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVJJ5I2FK_Fc",
        "outputId": "7b9d5b50-27e6-4fce-d0b3-120f2813951e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "btc_instruction_dataset.jsonl  temp_tokenized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h /\n",
        "!du -sh /root/.cache/huggingface/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuAy4D-51KRb",
        "outputId": "54734d2d-1d00-44e3-d8ab-8c85ce8f9b04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   49G  188G  21% /\n",
            "du: cannot access '/root/.cache/huggingface/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--- CLEAN UP TEMPORARY STORAGE ---\n",
        "print(\"\\n--- Cleaning up temporary storage ---\")\n",
        "!rm -rf /root/.cache/huggingface/*\n",
        "!rm -rf /tmp/*\n",
        "print(\"✅ Temporary storage cleared.\")"
      ],
      "metadata": {
        "id": "6Oo_7RHC5M5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86aef670-06df-429c-c4f7-85c800360f2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cleaning up temporary storage ---\n",
            "rm: cannot remove '/tmp/colab_runtime.sock': Device or resource busy\n",
            "✅ Temporary storage cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "access_token_write = userdata.get('HF_TOKEN')\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPVYKpBrQCWU",
        "outputId": "39927974-8ffa-4c8d-d7d9-0461c29e3436"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL-BASE"
      ],
      "metadata": {
        "id": "cIO8dphFSTjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FULL CODE: TRAINING + DEPLOYMENT + EVALUATION FOR MISTRAL-7B-BTC-JEPA-LLM-EXPERT ---\n",
        "\n",
        "# --- 1. IMPORTS AND CUSTOM CLASSES ---\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from huggingface_hub.utils import RepositoryNotFoundError\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# --- LLM-JEPA CONFIGURATION ---\n",
        "LLM_JEPA_CONFIG = {\n",
        "    \"lbd\": 2.0,               # Lambda (JEPA loss weight)\n",
        "    \"gamma\": 1.0,             # Gamma (LLM loss weight)\n",
        "    \"predictors\": 3,          # k (Number of predictor tokens)\n",
        "    \"last_token\": -1,         # Index of last token for embedding (typically EOS/</s> or -1)\n",
        "}\n",
        "SPECIAL_PREDICTOR_TOKENS = [f\"<|predictor_{i}|>\" for i in range(1, LLM_JEPA_CONFIG[\"predictors\"] + 1)]\n",
        "\n",
        "# --- FILE PATHS ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "DATASET_PATH = \"/content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset.jsonl\"\n",
        "OUTPUT_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/results_btc_jepa\"\n",
        "FINAL_MODEL_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/Mistral-7B-BTC-JEPA_FINAL\"\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-JEPA-LLM-Expert\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "TEMP_DATASET_DIR = \"/content/gdrive/MyDrive/CryptoFT/dataset/temp_tokenized\"\n",
        "\n",
        "# --- UTILITIES ---\n",
        "def get_messages_from_sample(sample_text):\n",
        "    \"\"\"Parses the Mistral template: '<s>[INST] Instruction [/INST] Response</s>'\"\"\"\n",
        "    inst_match = re.search(r\"\\[INST\\](.*?)\\[/INST\\]\", sample_text, re.DOTALL)\n",
        "    user_content = inst_match.group(1).strip() if inst_match else \"\"\n",
        "    resp_match = re.search(r\"\\[/INST\\](.*?)\\s*</s>\", sample_text, re.DOTALL)\n",
        "    assistant_content = resp_match.group(1).strip() if resp_match else \"\"\n",
        "    return {\"user_content\": user_content, \"assistant_content\": assistant_content}\n",
        "\n",
        "# --- CUSTOM DATA COLLATOR ---\n",
        "class CustomJEPABatchCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.keys = [\n",
        "            \"input_ids\", \"labels\", \"attention_mask\",\n",
        "            \"input_ids_user\", \"labels_user\", \"attention_mask_user\",\n",
        "            \"input_ids_assistant\", \"labels_assistant\", \"attention_mask_assistant\"\n",
        "        ]\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = {key: [] for key in self.keys}\n",
        "        if not features or not any(features):\n",
        "            return None\n",
        "        for feature in features:\n",
        "            if all(key in feature for key in self.keys):\n",
        "                for key in self.keys:\n",
        "                    batch[key].append(feature[key])\n",
        "        if not any(batch.values()):\n",
        "            return None\n",
        "        for key in self.keys:\n",
        "            if not batch[key]:\n",
        "                raise ValueError(f\"No valid samples for key '{key}' in batch. Check dataset tokenization.\")\n",
        "            batch[key] = torch.LongTensor(batch[key])\n",
        "        return batch\n",
        "\n",
        "# --- CUSTOM TRAINER ---\n",
        "class RepresentationTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.lbd = kwargs.pop('lbd', 1.0)\n",
        "        self.gamma = kwargs.pop('gamma', 1.0)\n",
        "        self.last_token = kwargs.pop('last_token', -1)\n",
        "        self._last_metrics = {}  # Store metrics for logging\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.args.per_device_train_batch_size,\n",
        "            collate_fn=self.data_collator,\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset=None):\n",
        "        dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.args.per_device_eval_batch_size,\n",
        "            collate_fn=self.data_collator,\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "    def _last_token_index(self, input_ids, attention_mask):\n",
        "        last_indices = torch.sum(attention_mask, dim=1) + self.last_token\n",
        "        return torch.clamp(last_indices, min=0)\n",
        "\n",
        "    def forward(self, model, inputs):\n",
        "        batch_size = inputs[\"input_ids\"].shape[0]\n",
        "        llm_inputs = {\n",
        "            \"input_ids\": torch.cat([inputs[\"input_ids\"], inputs[\"input_ids_user\"], inputs[\"input_ids_assistant\"]], dim=0),\n",
        "            \"labels\": torch.cat([inputs[\"labels\"], inputs[\"labels_user\"], inputs[\"labels_assistant\"]], dim=0),\n",
        "            \"attention_mask\": torch.cat([inputs[\"attention_mask\"], inputs[\"attention_mask_user\"], inputs[\"attention_mask_assistant\"]], dim=0),\n",
        "        }\n",
        "        with torch.enable_grad():\n",
        "            outputs = model(**llm_inputs, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states[-1].requires_grad_(True)\n",
        "        user_hidden_states = hidden_states[batch_size: batch_size * 2]\n",
        "        assistant_hidden_states = hidden_states[batch_size * 2:]\n",
        "        return {\n",
        "            'main_outputs': outputs,\n",
        "            'user_hidden_states': user_hidden_states,\n",
        "            'assistant_hidden_states': assistant_hidden_states,\n",
        "        }\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        batch_size = inputs[\"input_ids\"].shape[0]\n",
        "        index_user = self._last_token_index(inputs[\"input_ids_user\"], inputs[\"attention_mask_user\"])\n",
        "        index_assistant = self._last_token_index(inputs[\"input_ids_assistant\"], inputs[\"attention_mask_assistant\"])\n",
        "        forward_results = self.forward(model, inputs)\n",
        "        lm_loss = forward_results['main_outputs'].loss\n",
        "        user_hidden_states = forward_results['user_hidden_states']\n",
        "        assistant_hidden_states = forward_results['assistant_hidden_states']\n",
        "        user_embedding = user_hidden_states[range(batch_size), index_user, :]\n",
        "        assistant_embedding = assistant_hidden_states[range(batch_size), index_assistant, :]\n",
        "        cosine_similarity = F.cosine_similarity(user_embedding, assistant_embedding, dim=-1)\n",
        "        jepa_loss = 1.0 - torch.mean(cosine_similarity)\n",
        "        total_loss = self.gamma * lm_loss + self.lbd * jepa_loss\n",
        "        self._last_metrics = {'jepa_loss': jepa_loss, 'lm_loss': lm_loss}\n",
        "\n",
        "        # FIXED: Return only loss tensor for training, metrics are stored separately\n",
        "        if return_outputs:\n",
        "            return (total_loss, forward_results['main_outputs'])\n",
        "        return total_loss\n",
        "\n",
        "    def log(self, logs):\n",
        "        if self._last_metrics:\n",
        "            logs[\"jepa_loss\"] = self._nested_gather(self._last_metrics[\"jepa_loss\"]).mean().item()\n",
        "            logs[\"lm_loss\"] = self._nested_gather(self._last_metrics[\"lm_loss\"]).mean().item()\n",
        "        super().log(logs)\n",
        "\n",
        "# --- 2. MODEL AND DATASET SETUP ---\n",
        "print(\"\\n--- Setting up model and dataset ---\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# Clear any existing dataset cache\n",
        "if os.path.exists(TEMP_DATASET_DIR):\n",
        "    shutil.rmtree(TEMP_DATASET_DIR)\n",
        "os.makedirs(TEMP_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# PEFT (LoRA) Config\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "\n",
        "# Training Arguments - OPTIMIZED FOR JEPA MONITORING\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_steps=0,  # Disable saving during demo\n",
        "    logging_steps=50,  # See JEPA metrics every 10 steps\n",
        "    max_steps=500,  # Just enough to see JEPA loss decreasing\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"none\",\n",
        "    # ↓↓↓ CRITICAL CHANGES FOR DEMO ↓↓↓\n",
        "    evaluation_strategy=\"no\",  # Disable evaluation during training\n",
        "    eval_steps=None,  # No evaluation steps\n",
        "    metric_for_best_model=None,  # Not needed for demo\n",
        "    # ↑↑↑ CRITICAL CHANGES FOR DEMO ↑↑↑\n",
        "    dataloader_drop_last=True,\n",
        "    dataloader_num_workers=0,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    resume_from_checkpoint=True\n",
        ")\n",
        "\n",
        "# Load Model with Quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_PREDICTOR_TOKENS})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = get_peft_model(model, peft_config)\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name:\n",
        "        param.requires_grad = True\n",
        "print(\"✅ PEFT (LoRA) configuration applied\")\n",
        "\n",
        "# Load and Prepare Dataset\n",
        "print(\"\\n--- Loading and Splitting Custom Dataset ---\")\n",
        "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "\n",
        "# Filter malformed samples\n",
        "def filter_valid_samples(example):\n",
        "    text = example[\"text\"]\n",
        "    parsed = get_messages_from_sample(text)\n",
        "    valid = bool(parsed[\"user_content\"] and parsed[\"assistant_content\"])\n",
        "    if not valid:\n",
        "        print(f\"Skipping malformed sample: {text[:50]}...\")\n",
        "    return valid\n",
        "\n",
        "filtered_dataset = dataset.filter(filter_valid_samples, desc=\"Filtering valid samples\")\n",
        "if len(filtered_dataset) < 3000:\n",
        "    raise ValueError(f\"Filtered dataset has {len(filtered_dataset)} samples, need at least 3000 for 2500 train + 500 eval.\")\n",
        "print(f\"Filtered dataset size: {len(filtered_dataset)} samples\")\n",
        "\n",
        "# Select 2500 for train, 500 for eval\n",
        "train_dataset = filtered_dataset.select(range(2500))\n",
        "test_dataset = filtered_dataset.select(range(2500, 3000))\n",
        "tokenized_dataset = {\"train\": train_dataset, \"test\": test_dataset}\n",
        "\n",
        "def tokenize_jepa_views(example):\n",
        "    sample_text = example[\"text\"]\n",
        "    parsed = get_messages_from_sample(sample_text)\n",
        "    predictor_str = \"\".join(SPECIAL_PREDICTOR_TOKENS)\n",
        "    user_text_with_pred = f\"<s>[INST] {parsed['user_content']} {predictor_str} [/INST]\"\n",
        "    assistant_text = f\"{parsed['assistant_content']}</s>\"\n",
        "    tokenized_full = tokenizer(sample_text, truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", return_tensors=\"np\")\n",
        "    tokenized_user = tokenizer(user_text_with_pred, truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", return_tensors=\"np\")\n",
        "    tokenized_assistant = tokenizer(assistant_text, truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", return_tensors=\"np\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": tokenized_full[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask\": tokenized_full[\"attention_mask\"][0].tolist(),\n",
        "        \"labels\": tokenized_full[\"input_ids\"][0].copy().tolist(),\n",
        "        \"input_ids_user\": tokenized_user[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask_user\": tokenized_user[\"attention_mask\"][0].tolist(),\n",
        "        \"labels_user\": [-100] * MAX_SEQ_LENGTH,\n",
        "        \"input_ids_assistant\": tokenized_assistant[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask_assistant\": tokenized_assistant[\"attention_mask\"][0].tolist(),\n",
        "        \"labels_assistant\": [-100] * MAX_SEQ_LENGTH\n",
        "    }\n",
        "\n",
        "# Tokenize and save to disk to avoid caching issues\n",
        "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].map(\n",
        "    tokenize_jepa_views,\n",
        "    batched=False,\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing train samples for LLM-JEPA views\",\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(\n",
        "    tokenize_jepa_views,\n",
        "    batched=False,\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing test samples for LLM-JEPA views\",\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "tokenized_dataset[\"train\"].save_to_disk(os.path.join(TEMP_DATASET_DIR, \"train\"))\n",
        "tokenized_dataset[\"test\"].save_to_disk(os.path.join(TEMP_DATASET_DIR, \"test\"))\n",
        "\n",
        "# Reload dataset to ensure consistency\n",
        "from datasets import load_from_disk\n",
        "tokenized_dataset = {\n",
        "    \"train\": load_from_disk(os.path.join(TEMP_DATASET_DIR, \"train\")),\n",
        "    \"test\": load_from_disk(os.path.join(TEMP_DATASET_DIR, \"test\"))\n",
        "}\n"
      ],
      "metadata": {
        "id": "oK4rulypCq82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATASET"
      ],
      "metadata": {
        "id": "wWZIrwGLSiGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate all samples in dataset\n",
        "print(\"\\n--- Validating Dataset Keys ---\")\n",
        "required_keys = [\n",
        "    \"input_ids\", \"labels\", \"attention_mask\",\n",
        "    \"input_ids_user\", \"labels_user\", \"attention_mask_user\",\n",
        "    \"input_ids_assistant\", \"labels_assistant\", \"attention_mask_assistant\"\n",
        "]\n",
        "for split in [\"train\", \"test\"]:\n",
        "    print(f\"\\nChecking {split} split...\")\n",
        "    missing_samples = []\n",
        "    for idx, sample in tqdm(enumerate(tokenized_dataset[split]), total=len(tokenized_dataset[split]), desc=f\"Validating {split} split\"):\n",
        "        missing_keys = [key for key in required_keys if key not in sample]\n",
        "        if missing_keys:\n",
        "            missing_samples.append((idx, missing_keys))\n",
        "    if missing_samples:\n",
        "        print(f\"Found {len(missing_samples)} problematic samples in {split} split:\")\n",
        "        for idx, missing_keys in missing_samples[:5]:\n",
        "            print(f\"Sample {idx} missing keys: {missing_keys}\")\n",
        "        raise ValueError(f\"Dataset validation failed in {split} split.\")\n",
        "    print(f\"{split} split: All {len(tokenized_dataset[split])} samples validated.\")\n",
        "\n",
        "COLUMNS_TO_KEEP = required_keys\n",
        "try:\n",
        "    tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].select_columns(COLUMNS_TO_KEEP)\n",
        "    tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].select_columns(COLUMNS_TO_KEEP)\n",
        "    print(f\"Dataset columns filtered to: {COLUMNS_TO_KEEP}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during column selection: {e}\")\n",
        "    print(f\"Available columns in train: {tokenized_dataset['train'].column_names}\")\n",
        "    print(f\"Available columns in test: {tokenized_dataset['test'].column_names}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez8THznFwjJM",
        "outputId": "b31017a8-5c65-4ebd-aae5-5558e00e610a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Validating Dataset Keys ---\n",
            "\n",
            "Checking train split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating train split: 100%|██████████| 2500/2500 [00:09<00:00, 260.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train split: All 2500 samples validated.\n",
            "\n",
            "Checking test split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating test split: 100%|██████████| 500/500 [00:01<00:00, 262.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test split: All 500 samples validated.\n",
            "Dataset columns filtered to: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "2QWOSpkBS7jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug dataset before training\n",
        "print(\"\\n--- Debugging Dataset Before Training ---\")\n",
        "for split in [\"train\", \"test\"]:\n",
        "    print(f\"\\n{split} split sample keys:\")\n",
        "    for idx in range(min(5, len(tokenized_dataset[split]))):\n",
        "        sample = tokenized_dataset[split][idx]\n",
        "        print(f\"Sample {idx} keys: {list(sample.keys())}\")\n",
        "\n",
        "# --- 3. TRAINER INITIALIZATION AND EXECUTION ---\n",
        "print(\"\\n--- Initializing and Starting RepresentationTrainer (LLM-JEPA) ---\")\n",
        "custom_collator = CustomJEPABatchCollator(tokenizer)\n",
        "\n",
        "# Log GPU memory before training\n",
        "def log_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MiB\")\n",
        "        print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MiB\")\n",
        "        print(f\"GPU Memory Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**2:.2f} MiB\")\n",
        "\n",
        "print(\"\\n--- GPU Memory Before Training ---\")\n",
        "log_gpu_memory()\n",
        "\n",
        "# ADD FIXED JEPA MONITORING CALLBACK HERE\n",
        "\n",
        "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "# --- CALLBACK 1: MONITORING AND LOGGING (Your Original Role) ---\n",
        "class JEPAMonitorCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        # Force JEPA metrics display every 10 steps (matching logging_steps=10)\n",
        "        if state.global_step % 50 == 0 and hasattr(trainer, '_last_metrics'):\n",
        "            metrics = trainer._last_metrics\n",
        "            jepa_loss = metrics.get('jepa_loss')\n",
        "            lm_loss = metrics.get('lm_loss')\n",
        "            if jepa_loss is not None and lm_loss is not None:\n",
        "                print(f\"\\n🎯 JEPA Metrics [Step {state.global_step}]:\")\n",
        "                print(f\"   JEPA Loss: {jepa_loss.item():.4f}\")\n",
        "                print(f\"   LM Loss: {lm_loss.item():.4f}\")\n",
        "                print(f\"   Cosine Sim: {1 - jepa_loss.item():.4f}\")\n",
        "                print(f\"   Total Loss: {trainer.gamma * lm_loss + trainer.lbd * jepa_loss:.4f}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Also add to official logs for potential WandB/etc.\n",
        "        if logs is not None and hasattr(trainer, '_last_metrics'):\n",
        "            jepa_loss = trainer._last_metrics.get('jepa_loss')\n",
        "            lm_loss = trainer._last_metrics.get('lm_loss')\n",
        "            if jepa_loss is not None:\n",
        "                logs[\"jepa_loss\"] = jepa_loss.item()\n",
        "                logs[\"lm_loss\"] = lm_loss.item()\n",
        "\n",
        "# --- CALLBACK 2: CONTROL AND EARLY STOPPING (The New Role) ---\n",
        "# This class handles the conditional stopping logic.\n",
        "class JEPALossStabilityCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Stops training when the JEPA Loss on the training batch drops below a threshold\n",
        "    and stabilizes for a specified number of logging steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, jepa_threshold=0.0001, stability_steps=3):\n",
        "        self.jepa_threshold = jepa_threshold\n",
        "        self.stability_steps = stability_steps\n",
        "        self.stable_count = 0\n",
        "\n",
        "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        trainer = kwargs.get('trainer')\n",
        "\n",
        "        # Check only at logging steps (every 50 steps)\n",
        "        if state.global_step > 0 and state.global_step % args.logging_steps == 0 and hasattr(trainer, '_last_metrics'):\n",
        "            metrics = trainer._last_metrics\n",
        "            jepa_loss = metrics.get('jepa_loss')\n",
        "\n",
        "            if jepa_loss is not None:\n",
        "                current_jepa_loss = jepa_loss.item()\n",
        "\n",
        "                # Check for Early Stop Condition\n",
        "                if current_jepa_loss <= self.jepa_threshold:\n",
        "                    self.stable_count += 1\n",
        "\n",
        "                    if self.stable_count >= self.stability_steps:\n",
        "                        # SET THE CONTROL FLAG TO STOP\n",
        "                        control.should_stop = True\n",
        "                else:\n",
        "                    self.stable_count = 0\n",
        "\n",
        "        return control\n",
        "\n",
        "# Ensure both classes are defined in a single block before proceeding.\n",
        "# --- RE-INITIALIZE TRAINER WITH TWO CUSTOM CALLBACKS ---\n",
        "\n",
        "# Ensure the custom collator is ready\n",
        "custom_collator = CustomJEPABatchCollator(tokenizer)\n",
        "\n",
        "# Pass both callback instances in a list\n",
        "trainer = RepresentationTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"], # Keep this here for structure, even if eval_strategy=\"no\" is in TrainingArguments\n",
        "    args=training_arguments,\n",
        "    data_collator=custom_collator,\n",
        "    lbd=LLM_JEPA_CONFIG[\"lbd\"],\n",
        "    gamma=LLM_JEPA_CONFIG[\"gamma\"],\n",
        "    last_token=LLM_JEPA_CONFIG[\"last_token\"],\n",
        "    callbacks=[\n",
        "        JEPAMonitorCallback(),                     # 1. For Logging (Monitors progress)\n",
        "        JEPALossStabilityCallback(jepa_threshold=0.0001, stability_steps=3) # 2. For Control (Stops the process)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Trainer Initialized with Separate Logging and Control Callbacks ---\")\n",
        "# You can now call trainer.train() in the next cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oau3g7m79mFw",
        "outputId": "c6dae973-258d-4790-c4ea-c64565dbb4cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Debugging Dataset Before Training ---\n",
            "\n",
            "train split sample keys:\n",
            "Sample 0 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 1 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 2 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 3 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 4 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "\n",
            "test split sample keys:\n",
            "Sample 0 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 1 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 2 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 3 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "Sample 4 keys: ['input_ids', 'labels', 'attention_mask', 'input_ids_user', 'labels_user', 'attention_mask_user', 'input_ids_assistant', 'labels_assistant', 'attention_mask_assistant']\n",
            "\n",
            "--- Initializing and Starting RepresentationTrainer (LLM-JEPA) ---\n",
            "\n",
            "--- GPU Memory Before Training ---\n",
            "GPU Memory Allocated: 5166.18 MiB\n",
            "GPU Memory Reserved: 5240.00 MiB\n",
            "GPU Memory Free: 17452.88 MiB\n",
            "\n",
            "--- Trainer Initialized with Separate Logging and Control Callbacks ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"✅ JEPA Monitoring Enabled - will show metrics every {training_arguments.logging_steps}/{training_arguments.max_steps} steps\")\n",
        "print(\"\\n--- Starting LLM-JEPA Fine-Tuning with Monitoring ---\")\n",
        "trainer.train()\n",
        "print(\"\\n--- Fine-Tuning Complete! ---\")\n",
        "\n",
        "# Clean up old checkpoints\n",
        "print(\"\\n--- Cleaning Up Old Checkpoints ---\")\n",
        "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save to Google Drive\n",
        "print(\"\\n--- Saving Final Adapter Weights to Google Drive ---\")\n",
        "trainer.model.save_pretrained(FINAL_MODEL_DIR)\n",
        "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "print(f\"\\n✅ LoRA adapter and tokenizer saved to: '{FINAL_MODEL_DIR}'\")\n",
        "print(f\"Checkpoints and logs saved to '{OUTPUT_DIR}'\")"
      ],
      "metadata": {
        "id": "SwG6nFwobLvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "43d683f6-44f3-413e-946e-e1eef1e43c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JEPA Monitoring Enabled - will show metrics every 50/500 steps\n",
            "\n",
            "--- Starting LLM-JEPA Fine-Tuning with Monitoring ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='143' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [143/500 3:06:22 < 7:51:53, 0.01 it/s, Epoch 0.91/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.885500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.235800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 JEPA Metrics [Step 50]:\n",
            "   JEPA Loss: 0.0001\n",
            "   LM Loss: 0.2543\n",
            "   Cosine Sim: 0.9999\n",
            "   Total Loss: 0.2545\n",
            "\n",
            "🎯 JEPA Metrics [Step 100]:\n",
            "   JEPA Loss: 0.0001\n",
            "   LM Loss: 0.2285\n",
            "   Cosine Sim: 0.9999\n",
            "   Total Loss: 0.2286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEPLOYMENT"
      ],
      "metadata": {
        "id": "PQFjS7d3TE0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# --- 4. DEPLOYMENT TO HUGGING FACE ---\n",
        "print(\"\\n--- Authenticating with Hugging Face Hub ---\")\n",
        "try:\n",
        "    access_token_write = userdata.get('HF_TOKEN')\n",
        "    if not access_token_write:\n",
        "        raise ValueError(\"HF_TOKEN secret not found or is empty.\")\n",
        "    login(token=access_token_write, add_to_git_credential=True)\n",
        "    api = HfApi(token=access_token_write)\n",
        "    print(\"✅ Successfully logged into Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR during login: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\n--- Checking/Creating Repository: {HUB_MODEL_ID} ---\")\n",
        "try:\n",
        "    api.repo_info(repo_id=HUB_MODEL_ID, repo_type=\"model\")\n",
        "    print(f\"✅ Repository {HUB_MODEL_ID} already exists.\")\n",
        "except RepositoryNotFoundError:\n",
        "    print(f\"⚠️ Repository {HUB_MODEL_ID} not found. Creating it now...\")\n",
        "    create_repo(repo_id=HUB_MODEL_ID, repo_type=\"model\", private=False, token=access_token_write)\n",
        "    print(f\"✅ Repository {HUB_MODEL_ID} created successfully.\")\n",
        "\n",
        "print(f\"\\n--- Pushing LoRA Adapter and Tokenizer ---\")\n",
        "if not os.path.exists(FINAL_MODEL_DIR):\n",
        "    raise FileNotFoundError(f\"Adapter directory not found at {FINAL_MODEL_DIR}.\")\n",
        "try:\n",
        "    trainer.model.push_to_hub(\n",
        "        repo_id=HUB_MODEL_ID,\n",
        "        commit_message=\"Initial QLoRA adapter for Bitcoin price prediction\",\n",
        "        private=False\n",
        "    )\n",
        "    tokenizer.push_to_hub(HUB_MODEL_ID)\n",
        "    print(\"\\n✅ SUCCESS: Adapter and tokenizer uploaded to Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠️ WARNING: Trainer push failed. Using HfApi fallback. Error: {e}\")\n",
        "    api.upload_folder(\n",
        "        folder_path=FINAL_MODEL_DIR,\n",
        "        repo_id=HUB_MODEL_ID,\n",
        "        commit_message=\"LoRA adapter and tokenizer pushed from Google Drive checkpoint\",\n",
        "        ignore_patterns=[\"*.pt\", \"*.bin\", \"optimizer.pt\", \"scheduler.pt\", \"rng_state.pth\"]\n",
        "    )\n",
        "    print(\"\\n✅ SUCCESS (Fallback): Adapter and tokenizer uploaded to Hugging Face Hub.\")\n",
        "print(f\"\\nDeployment Complete. Model available at: https://huggingface.co/{HUB_MODEL_ID}\")"
      ],
      "metadata": {
        "id": "toQZ-0Ox22aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION"
      ],
      "metadata": {
        "id": "nqXmJ3ecTQQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚨 FIRST: COMPLETELY RESET PEFT INSTALLATION\n",
        "!pip uninstall -y peft\n",
        "!pip install peft==0.10.0  # Use stable version\n",
        "\n",
        "# Restart runtime after this command"
      ],
      "metadata": {
        "id": "O9seIeCEUZss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FILE PATHS ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "DATASET_PATH = \"/content/gdrive/MyDrive/CryptoFT/dataset/btc_instruction_dataset.jsonl\"\n",
        "OUTPUT_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/results_btc_jepa\"\n",
        "FINAL_MODEL_DIR = \"/content/gdrive/MyDrive/CryptoFT/models/Mistral-7B-BTC-JEPA_FINAL\"\n",
        "HUB_MODEL_ID = \"frankmorales2020/Mistral-7B-BTC-JEPA-LLM-Expert\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "TEMP_DATASET_DIR = \"/content/gdrive/MyDrive/CryptoFT/dataset/temp_tokenized\"\n"
      ],
      "metadata": {
        "id": "yIr4DOt_V9SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, PeftModel"
      ],
      "metadata": {
        "id": "Xf7NgN2kVusD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPROVED TEST WITH CLEANER PROMPTS ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔧 IMPROVED JEPA MODEL TESTING - CLEAN PROMPTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "improved_tests = [\n",
        "    {\n",
        "        \"name\": \"BULLISH SCENARIO\",\n",
        "        \"data\": \"[O:30000, H:31000, C:30900]\",\n",
        "        \"expected\": \"UP\",\n",
        "        \"reason\": \"closing near highs with strong upward momentum\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"BEARISH SCENARIO\",\n",
        "        \"data\": \"[O:30500, H:30600, C:30000]\",\n",
        "        \"expected\": \"DOWN\",\n",
        "        \"reason\": \"closing at lows after being rejected at highs\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"CONSOLIDATION SCENARIO\",\n",
        "        \"data\": \"[O:30200, H:30300, C:30250]\",\n",
        "        \"expected\": \"FLAT\",\n",
        "        \"reason\": \"trading in tight range with minimal movement\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"🤖 Testing with CLEANER prompts to prevent hallucinations...\\n\")\n",
        "\n",
        "for test in improved_tests:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"📊 {test['name']} (Expecting: {test['expected']})\")\n",
        "    print(f\"📈 Data: {test['data']}\")\n",
        "    print(f\"💡 Reason: {test['reason']}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # 🚨 CLEANER PROMPT - No historical references\n",
        "    prompt_content = f\"BTC price data: {test['data']}. Based ONLY on this OHLC data, the 12-hour direction is:\"\n",
        "    input_text = f\"<s>[INST] {prompt_content} [/INST]\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=30,  # 🚨 Shorter to prevent rambling\n",
        "            do_sample=False,    # 🚨 Greedy decoding for consistency\n",
        "            temperature=0.3,    # 🚨 Lower temperature\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prediction = response.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in response else response\n",
        "\n",
        "    print(f\"🎯 MODEL PREDICTION: {prediction}\")\n",
        "\n",
        "    # Check if prediction matches expectation\n",
        "    if test['expected'] in prediction.upper():\n",
        "        print(f\"✅ CORRECT - Matched expected: {test['expected']}\")\n",
        "    else:\n",
        "        print(f\"❌ UNEXPECTED - Wanted {test['expected']}, got different\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📝 ANALYSIS: The model may need more focused fine-tuning on\")\n",
        "print(\"   directional prediction without historical data contamination.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "qBgzk3UW28F4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
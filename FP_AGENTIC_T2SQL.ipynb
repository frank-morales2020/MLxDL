{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO3b191NzgZdSWkzZKH0PA/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FP_AGENTIC_T2SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "1UKww0CDSPMm",
        "outputId": "568ae6ff-beab-437e-d8aa-632a67d95416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 16 07:36:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   42C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAxxuWSDCUDA"
      },
      "outputs": [],
      "source": [
        "# From the provided reference:\n",
        "# Assume these are already installed as per the notebook:\n",
        "!pip install -U langchain-community -q\n",
        "!pip install -U crewai -q\n",
        "!pip install 'crewai [tools]' -q\n",
        "!pip install transformers -U -q\n",
        "!pip install colab-env -q\n",
        "!pip install unsloth -q\n",
        "!pip install torch -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "from typing import Any, List, Dict, Optional\n",
        "\n",
        "# Ensure all necessary Langchain/Transformers/Unsloth imports are here\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.outputs import ChatResult, ChatGeneration\n",
        "\n",
        "# Import PromptTemplate and LLMChain for the new approach\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Unsloth and Transformers imports for model loading\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import pipeline, AutoConfig # Make sure AutoConfig is imported\n",
        "from langchain.tools import BaseTool # Import BaseTool if you still want to use your tool class structure\n"
      ],
      "metadata": {
        "id": "ffDMb5TDkd3i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "source": [
        "# 1. Custom LLM Wrapper (UnslothCrewAILLM)\n",
        "# This class makes your fine-tuned model compatible with Langchain.\n",
        "# (Keep the same class definition from the last attempt as it's the most compliant)\n",
        "class UnslothCrewAILLM(BaseChatModel):\n",
        "    model: Any\n",
        "    tokenizer: Any\n",
        "    pipeline: Any = None\n",
        "    max_new_tokens: int = 1024\n",
        "    temperature: float = 0.1\n",
        "    do_sample: bool = False\n",
        "    trust_remote_code: bool = True # Changed True to bool\n",
        "\n",
        "    def __init__(self, model, tokenizer, pipeline=None, max_new_tokens=1024, temperature=0.1, do_sample=False, trust_remote_code=True):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            pipeline=pipeline,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=do_sample,\n",
        "            trust_remote_code=trust_remote_code\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        if not messages:\n",
        "            raise ValueError(\"No messages provided to the LLM wrapper.\")\n",
        "\n",
        "        # Langchain often sends a list of messages, take the last one as the primary prompt\n",
        "        final_message_content = messages[-1].content\n",
        "\n",
        "        if self.pipeline:\n",
        "            try:\n",
        "                response = self.pipeline(\n",
        "                    final_message_content,\n",
        "                    num_return_sequences=1,\n",
        "                    return_full_text=False,\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    temperature=self.temperature,\n",
        "                    do_sample=self.do_sample,\n",
        "                )\n",
        "                generated_text = response[0].get('generated_text', '').strip() if response else \"\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error during pipeline generation in wrapper: {e}\")\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "        elif self.model and self.tokenizer:\n",
        "            try:\n",
        "                max_input_length = getattr(self.tokenizer, 'model_max_length', self.max_new_tokens)\n",
        "                inputs = self.tokenizer(final_message_content, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
        "\n",
        "                if self.tokenizer.pad_token_id is None:\n",
        "                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    temperature=self.temperature,\n",
        "                    do_sample=self.do_sample,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    stopping_criteria=stop,\n",
        "                    # input_length = inputs.input_ids.shape[1] # This line was causing an error, removed for clarity.\n",
        "                )\n",
        "                # Ensure outputs has a shape to work with before slicing\n",
        "                if outputs.shape[1] > inputs.input_ids.shape[1]:\n",
        "                    generated_ids = outputs[0, inputs.input_ids.shape[1]:]\n",
        "                else:\n",
        "                    generated_ids = outputs[0] # If output is shorter, take the whole thing\n",
        "\n",
        "                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "            except Exception as e:\n",
        "                print(f\"Error during manual generation in wrapper: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "        else:\n",
        "            generated_text = \"Error: Model or pipeline not loaded in wrapper.\"\n",
        "\n",
        "        message = AIMessage(content=generated_text)\n",
        "        generation = ChatGeneration(message=message)\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"unsloth_transformer_wrapper\"\n",
        "\n",
        "    def supports_stop_words(self) -> bool:\n",
        "        \"\"\"Returns whether the model supports stop words.\"\"\"\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def supports_control_chars(self) -> bool:\n",
        "        \"\"\"Returns whether the model supports control characters.\"\"\"\n",
        "        return False\n",
        "\n",
        "    # Add dummy implementations for other BaseChatModel methods for compatibility\n",
        "    # Implement stream, invoke, batch methods for better Langchain compatibility\n",
        "    # For this example, we can delegate _invoke to generate\n",
        "    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None):\n",
        "        \"\"\"Implement stream method (not used in this wrapper's logic, but required by BaseChatModel).\"\"\"\n",
        "        raise NotImplementedError(\"Streaming is not implemented for this wrapper.\")\n",
        "\n",
        "    def _invoke(self, prompt: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement invoke method (required by BaseChatModel).\"\"\"\n",
        "        # Delegate to generate and return the first message\n",
        "        return self._generate(prompt, stop=stop, run_manager=run_manager, **kwargs).generations[0].message\n",
        "\n",
        "    def _batch(self, messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement batch method (required by BaseChatModel).\"\"\"\n",
        "        return [self._generate(msgs, stop=stop, run_manager=run_manager, **kwargs) for msgs in messages]\n",
        "\n",
        "    async def _agenerate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        return self._generate(messages, stop, run_manager, **kwargs)\n",
        "\n",
        "    async def _astream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None):\n",
        "        \"\"\"Implement async stream method.\"\"\"\n",
        "        raise NotImplementedError(\"Async streaming is not implemented for this wrapper.\")\n",
        "\n",
        "    async def _ainvoke(self, prompt: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement async invoke method.\"\"\"\n",
        "        return (await self._agenerate(prompt, stop=stop, run_manager=run_manager, **kwargs)).generations[0].message\n",
        "\n",
        "    async def _abatch(self, messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement async batch method.\"\"\"\n",
        "        import asyncio\n",
        "        return await asyncio.gather(*[self._agenerate(msgs, stop=stop, run_manager=run_manager, **kwargs) for msgs in messages])\n",
        "\n",
        "# 2. Database Schema Definition for Flight Planning\n",
        "db_schema = {\n",
        "    \"tables\": {\n",
        "        \"flights\": ['flight_id', 'departure_airport', 'arrival_airport', 'departure_time', 'arrival_time', 'aircraft_type', 'status', 'price'],\n",
        "        \"airports\": ['airport_code', 'airport_name', 'city', 'country'],\n",
        "        \"passengers\": ['passenger_id', 'first_name', 'last_name', 'email'],\n",
        "        \"bookings\": ['booking_id', 'flight_id', 'passenger_id', 'booking_date', 'seat_number']\n",
        "    }\n",
        "}\n",
        "db_schema_string_for_prompt = str(db_schema)\n",
        "\n",
        "# 3. Model Loading (using the model from the reference)\n",
        "fine_tuned_model_id = \"frankmorales2020/deepseek_r1_text2sql_finetuned\"\n",
        "max_seq_length = 2048\n",
        "load_in_4bit = True\n",
        "\n",
        "print(f\"\\n--- Attempting Direct LLM Loading for {fine_tuned_model_id} using Unsloth ---\")\n",
        "\n",
        "# Determine optimal dtype for Unsloth\n",
        "unsloth_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "unsloth_wrapper_pipeline = None\n",
        "llm_instance = None # Renamed from llm_for_agents for clarity in this new approach\n",
        "\n",
        "try:\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=fine_tuned_model_id,\n",
        "            max_seq_length=max_seq_length,\n",
        "            dtype=unsloth_dtype,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        print(\"Model and Tokenizer loaded successfully using Unsloth.\")\n",
        "\n",
        "    try:\n",
        "        # You can still create the pipeline if you prefer, or rely solely on manual generation\n",
        "        unsloth_wrapper_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            return_full_text=False,\n",
        "        )\n",
        "        print(\"Text generation pipeline created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create transformers pipeline: {e}. Falling back to manual generation.\")\n",
        "        unsloth_wrapper_pipeline = None # Ensure pipeline is None if creation fails\n",
        "\n",
        "    # Instantiate your custom LLM\n",
        "    llm_instance = UnslothCrewAILLM(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        pipeline=unsloth_wrapper_pipeline, # Pass the pipeline or None\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.1,\n",
        "        do_sample=False,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    print(\"Unsloth CrewAILLL instance created.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"\\n-- Skipping model loading: Unsloth or necessary libraries not installed, or compatible GPU not found.\")\n",
        "    print(\"Please ensure you have 'unsloth' and 'torch' installed and a compatible GPU/CUDA setup.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- An error occurred during model loading (Unsloth): {e} ---\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# 4. Define the SQL Query Executor Tool (as a Langchain BaseTool)\n",
        "class SQLQueryExecutorTool(BaseTool):\n",
        "    name: str = \"SQL Query Executor\"\n",
        "    description: str = \"Executes a given SQL query against the flight database and returns the results.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        print(f\"\\n--- Attempting to execute SQL query: \\n{query}\\n---\")\n",
        "\n",
        "        # Simple validation/simulation\n",
        "        if \"DROP TABLE\" in query.upper() or \"DELETE FROM\" in query.upper():\n",
        "            return \"Error: Harmful SQL query detected and blocked for safety.\"\n",
        "\n",
        "        # Add a check for the specific flight query pattern\n",
        "        if \"SELECT\" in query.upper() and \"FROM flights\" in query.lower() and \"'JFK'\" in query and \"'LAX'\" in query:\n",
        "            return \"SQL executed successfully. Sample results for flight query: [{'flight_id': 101, 'departure_airport': 'JFK', 'arrival_airport': 'LAX', 'departure_time': '2025-07-01 10:00:00', 'price': 250.00}]\"\n",
        "        # Removed other irrelevant simulated results to focus on the flight planning example\n",
        "        elif not query.strip().lower().startswith(\"select\"):\n",
        "            return \"Error: Only SELECT queries are supported by this tool for safety and simplicity in this simulation.\"\n",
        "        else:\n",
        "            if \"SELECT\" in query.upper() and \"FROM\" in query.upper():\n",
        "                return \"SQL executed successfully. (Simulated) No specific results available for this general query.\"\n",
        "            else:\n",
        "                return \"Error: Invalid or unexecutable SQL query format (simulated error).\"\n",
        "\n",
        "sql_executor_tool = SQLQueryExecutorTool()\n",
        "print(\"\\nSQL Query Executor Tool defined.\")\n",
        "\n",
        "# 5. Define the Flight Optimizer Tool (NEW)\n",
        "class FlightOptimizerTool(BaseTool):\n",
        "    name: str = \"Flight Optimizer\"\n",
        "    description: str = \"Optimizes a flight route between two airports (e.g., shortest, most efficient, considering real-time weather and air traffic) and returns a simulated optimal flight plan including estimated duration, fuel efficiency, and potential waypoints. Input should be two comma-separated airport codes and optionally optimization criteria (e.g., 'JFK,LAX,fuel_efficient,2025-07-01') or just 'JFK,LAX'.\"\n",
        "\n",
        "    def _run(self, optimization_params: str) -> str:\n",
        "        print(f\"\\n--- Attempting to optimize route for: {optimization_params} ---\")\n",
        "        try:\n",
        "            parts = [p.strip().upper() for p in optimization_params.split(',')]\n",
        "            origin = parts[0]\n",
        "            destination = parts[1]\n",
        "            optimization_criteria = \"DEFAULT\" # Default to a general optimization\n",
        "            date_str = \"TODAY\"\n",
        "\n",
        "            if len(parts) > 2:\n",
        "                for part in parts[2:]:\n",
        "                    if re.match(r\"\\d{4}-\\d{2}-\\d{2}\", part) : # Simple YYYY-MM-DD date detection\n",
        "                        date_str = part\n",
        "                    elif part in [\"FUEL_EFFICIENT\", \"SHORTEST_TIME\", \"LOW_TURBULENCE\", \"OPTIMAL\"]:\n",
        "                        optimization_criteria = part\n",
        "\n",
        "            # Simulate optimization based on the article's concepts for YUL-ZSPD and other routes\n",
        "            if origin == 'YUL' and destination == 'ZSPD':\n",
        "                if optimization_criteria == \"FUEL_EFFICIENT\":\n",
        "                    return (\n",
        "                        f\"**Optimized Flight Plan: {origin} to {destination} (Fuel Efficient)**\\n\"\n",
        "                        f\"  Date: {date_str}\\n\"\n",
        "                        f\"  Route: {origin} -> (AI Predicted Waypoint: 46.6180°N, -74.1754°W near Saint-Michel-des-Saints) -> {destination}\\n\"\n",
        "                        f\"  Estimated Flight Time: 12h 57m (aligned with AI-based Linear Regression model from study)\\n\"\n",
        "                        f\"  Estimated Fuel Consumption: 77340.77 kg (Excellent efficiency)\\n\"\n",
        "                        f\"  Considerations: Leveraging favorable wind patterns, avoiding major air traffic zones. (Source: 'AI-Driven Flight Path Optimization' Case Study 3)\"\n",
        "                    )\n",
        "                elif optimization_criteria == \"SHORTEST_TIME\":\n",
        "                    return (\n",
        "                        f\"**Optimized Flight Plan: {origin} to {destination} (Shortest Time)**\\n\"\n",
        "                        f\"  Date: {date_str}\\n\"\n",
        "                        f\"  Route: {origin} -> (Hypothetical Advanced AI Waypoint for speed) -> {destination}\\n\"\n",
        "                        f\"  Estimated Flight Time: ~12h 00m (Hypothetically even better with advanced AI and real-time data)\\n\"\n",
        "                        f\"  Estimated Fuel Consumption: Very Low\\n\"\n",
        "                        f\"  Considerations: Optimal altitudes for speed, dynamic adaptation to real-time weather and air traffic for minimal delays. (Source: 'AI-Driven Flight Path Optimization' Case Study 4)\"\n",
        "                    )\n",
        "                elif optimization_criteria == \"LOW_TURBULENCE\":\n",
        "                    return (\n",
        "                        f\"**Optimized Flight Plan: {origin} to {destination} (Low Turbulence)**\\n\"\n",
        "                        f\"  Date: {date_str}\\n\"\n",
        "                        f\"  Route: {origin} -> (Optimized for smoother air, e.g., slightly longer path to avoid storms) -> {destination}\\n\"\n",
        "                        f\"  Estimated Flight Time: ~14h 30m\\n\"\n",
        "                        f\"  Estimated Fuel Consumption: Moderate\\n\"\n",
        "                        f\"  Considerations: Proactive avoidance of known turbulence zones based on forecast data.\"\n",
        "                    )\n",
        "                else: # Default or general case for YUL-ZSPD (e.g., Vancouver layover)\n",
        "                    return (\n",
        "                        f\"**Optimized Flight Plan: {origin} to {destination} (General Optimization)**\\n\"\n",
        "                        f\"  Date: {date_str}\\n\"\n",
        "                        f\"  Route: {origin} -> Vancouver (Layover: 49.2827°N, -123.1207°W) -> {destination}\\n\"\n",
        "                        f\"  Estimated Flight Time: 14h 08m (aligned with Vancouver layover study)\\n\"\n",
        "                        f\"  Estimated Fuel Consumption: 86644.55 kg (Good efficiency due to jet streams)\\n\"\n",
        "                        f\"  Considerations: Balancing time and fuel, potential benefits of layovers for advantageous conditions. (Source: 'AI-Driven Flight Path Optimization' Case Study 2)\"\n",
        "                    )\n",
        "            # General JFK to LAX case\n",
        "            elif origin == 'JFK' and destination == 'LAX':\n",
        "                return f\"**Optimized Flight Plan: {origin} to {destination}**\\n  Date: {date_str}\\n  Optimal route: JFK -> DEN -> LAX. Estimated duration: 5h 15m, Fuel efficiency: Very High. Weather conditions: Clear. Current air traffic: Moderate.\"\n",
        "            else:\n",
        "                return f\"**Optimized Flight Plan: {origin} to {destination}**\\n  Date: {date_str}\\n  Direct route available. Estimated duration: ~4h, Fuel efficiency: Normal. Real-time data considered: Basic.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error optimizing route: Invalid input format. Please use 'ORIGIN,DESTINATION' or 'ORIGIN,DESTINATION,CRITERIA,YYYY-MM-DD'. Error: {e}\"\n",
        "\n",
        "flight_optimizer_tool = FlightOptimizerTool()\n",
        "print(\"\\nFlight Optimizer Tool defined.\")\n",
        "\n",
        "\n",
        "# 6. Define the Prompt Template for SQL Generation\n",
        "# The prompt is now more explicit about considering optimization parameters for the LLM\n",
        "sql_gen_template = \"\"\"Translate the following natural language query into a precise SQL query based on the provided database schema.\n",
        "If the query asks for flight planning, route optimization, or efficiency, identify the origin, destination, and any specific date/time or optimization criteria (e.g., 'most fuel-efficient', 'shortest time', 'low turbulence').\n",
        "Prioritize extracting airport codes and any optimization criteria for the Flight Optimizer Tool.\n",
        "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
        "\n",
        "Database Schema:\n",
        "{db_schema}\n",
        "\n",
        "Natural Language Query:\n",
        "{query}\n",
        "\n",
        "SQL:\n",
        "\"\"\"\n",
        "\n",
        "sql_gen_prompt = PromptTemplate(\n",
        "    input_variables=[\"db_schema\", \"query\"],\n",
        "    template=sql_gen_template,\n",
        ")\n",
        "print(\"\\nSQL Generation Prompt Template defined.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ioDC-JUarzd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Create the LLM Chain for SQL Generation\n",
        "if llm_instance is None:\n",
        "    print(\"\\nERROR: LLM instance is NOT available. Cannot create LLM Chain.\")\n",
        "else:\n",
        "    try:\n",
        "        sql_gen_chain = LLMChain(\n",
        "            llm=llm_instance,\n",
        "            prompt=sql_gen_prompt,\n",
        "            verbose=True, # Set verbose to True to see the prompt sent to the LLM\n",
        "        )\n",
        "        print(\"\\nLLMChain for SQL generation created.\")\n",
        "\n",
        "        # 8. Define the Natural Language Query - specifically targeting optimization and efficiency\n",
        "        # Using YUL and ZSPD to match the article's detailed case studies\n",
        "        combined_query = \"What is the most fuel-efficient flight route from 'Montreal (YUL)' to 'Shanghai (ZSPD)' for today, and what are the flight prices for that date?\"\n",
        "\n",
        "        print(f\"\\n--- Running Langchain Flow for combined query: \\\"{combined_query}\\\"\")\n",
        "\n",
        "        # 9. Run the LLMChain to generate SQL for flight details/airport/date extraction\n",
        "        print(\"\\n--- Generating SQL using LLMChain for initial flight details/airport/date extraction ---\")\n",
        "        generated_sql_result = sql_gen_chain.run(db_schema=db_schema_string_for_prompt, query=combined_query)\n",
        "        generated_sql = generated_sql_result.strip()\n",
        "        final_generated_sql = generated_sql.split(';')[0].strip() if ';' in generated_sql else generated_sql.strip()\n",
        "\n",
        "        print(f\"\\n--- Generated SQL: ---\")\n",
        "        print(final_generated_sql)\n",
        "\n",
        "        # Extract origin/destination and date/optimization criteria from the original query for the optimizer\n",
        "        import re\n",
        "        # Enhanced regex to capture origin/destination, date, and potential optimization keywords\n",
        "        match = re.search(r\"from\\s+'(?:[^()]+?\\((\\w+)\\)|([^']+))'\\s+to\\s+'(?:[^()]+?\\((\\w+)\\)|([^']+))'(?:.*for\\s+(.*?))?(?:.*(most fuel-efficient|shortest time|low turbulence))?\", combined_query, re.IGNORECASE)\n",
        "\n",
        "        origin_airport = (match.group(1) or match.group(2)).upper() if match else None\n",
        "        destination_airport = (match.group(3) or match.group(4)).upper() if match else None\n",
        "        date_str_extracted = match.group(5).strip() if match and match.group(5) else \"TODAY\"\n",
        "        optimization_criteria_extracted = match.group(6).replace(\" \", \"_\").upper() if match and match.group(6) else \"DEFAULT\"\n",
        "\n",
        "        # Simple date normalization for the tool if \"today\" is mentioned\n",
        "        if \"today\" in date_str_extracted.lower():\n",
        "            import datetime\n",
        "            date_str_extracted = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "        if origin_airport and destination_airport:\n",
        "            # 10. Execute the generated SQL using the SQL Executor Tool\n",
        "            # This part still runs to fulfill the \"show me existing flight prices\" part of the query.\n",
        "            print(\"\\n--- Executing Generated SQL using SQL Executor Tool ---\")\n",
        "            sql_tool_execution_result = sql_executor_tool.run(final_generated_sql)\n",
        "            print(f\"\\n--- SQL Tool Execution Result: ---\")\n",
        "            print(sql_tool_execution_result)\n",
        "\n",
        "            # 11. Call the Flight Optimizer Tool with extracted airports, date, and optimization criteria\n",
        "            print(f\"\\n--- Calling Flight Optimizer Tool for {origin_airport},{destination_airport},{optimization_criteria_extracted},{date_str_extracted} ---\")\n",
        "            optimization_result = flight_optimizer_tool.run(f\"{origin_airport},{destination_airport},{optimization_criteria_extracted},{date_str_extracted}\")\n",
        "            print(f\"\\n--- Flight Optimization Result: ---\")\n",
        "            print(optimization_result)\n",
        "        else:\n",
        "            print(\"\\nCould not extract origin and destination airports for optimization from the query.\")\n",
        "            print(\"\\n--- Executing Generated SQL using SQL Executor Tool (optimization not possible) ---\")\n",
        "            sql_tool_execution_result = sql_executor_tool.run(final_generated_sql)\n",
        "            print(f\"\\n--- SQL Tool Execution Result: ---\")\n",
        "            print(sql_tool_execution_result)\n",
        "\n",
        "\n",
        "        print(\"\\n### Combined Agentic Flow Finished ###\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- An error occurred during the Langchain flow: {e} ---\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1om0U1CqtbDF",
        "outputId": "e34dc50e-7aef-4500-c0a7-a2d7c1ff8d80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LLMChain for SQL generation created.\n",
            "\n",
            "--- Running Langchain Flow for combined query: \"What is the most fuel-efficient flight route from 'Montreal (YUL)' to 'Shanghai (ZSPD)' for today, and what are the flight prices for that date?\"\n",
            "\n",
            "--- Generating SQL using LLMChain for initial flight details/airport/date extraction ---\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mTranslate the following natural language query into a precise SQL query based on the provided database schema.\n",
            "If the query asks for flight planning, route optimization, or efficiency, identify the origin, destination, and any specific date/time or optimization criteria (e.g., 'most fuel-efficient', 'shortest time', 'low turbulence').\n",
            "Prioritize extracting airport codes and any optimization criteria for the Flight Optimizer Tool.\n",
            "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
            "\n",
            "Database Schema:\n",
            "{'tables': {'flights': ['flight_id', 'departure_airport', 'arrival_airport', 'departure_time', 'arrival_time', 'aircraft_type', 'status', 'price'], 'airports': ['airport_code', 'airport_name', 'city', 'country'], 'passengers': ['passenger_id', 'first_name', 'last_name', 'email'], 'bookings': ['booking_id', 'flight_id', 'passenger_id', 'booking_date', 'seat_number']}}\n",
            "\n",
            "Natural Language Query:\n",
            "What is the most fuel-efficient flight route from 'Montreal (YUL)' to 'Shanghai (ZSPD)' for today, and what are the flight prices for that date?\n",
            "\n",
            "SQL:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "--- Generated SQL: ---\n",
            "SELECT \n",
            "    flight_id, departure_airport, arrival_airport, departure_time, arrival_time, \n",
            "    aircraft_type, status, price\n",
            "FROM \n",
            "    flights\n",
            "WHERE \n",
            "    departure_airport = 'YUL' AND arrival_airport = 'ZSPD' AND \n",
            "    date('departure_time') = date('today')\n",
            "ORDER BY \n",
            "    price ASC\n",
            "\n",
            "--- Executing Generated SQL using SQL Executor Tool ---\n",
            "\n",
            "--- Attempting to execute SQL query: \n",
            "SELECT \n",
            "    flight_id, departure_airport, arrival_airport, departure_time, arrival_time, \n",
            "    aircraft_type, status, price\n",
            "FROM \n",
            "    flights\n",
            "WHERE \n",
            "    departure_airport = 'YUL' AND arrival_airport = 'ZSPD' AND \n",
            "    date('departure_time') = date('today')\n",
            "ORDER BY \n",
            "    price ASC\n",
            "---\n",
            "\n",
            "--- SQL Tool Execution Result: ---\n",
            "SQL executed successfully. (Simulated) No specific results available for this general query.\n",
            "\n",
            "--- Calling Flight Optimizer Tool for YUL,ZSPD,DEFAULT,2025-06-16 ---\n",
            "\n",
            "--- Attempting to optimize route for: YUL,ZSPD,DEFAULT,2025-06-16 ---\n",
            "\n",
            "--- Flight Optimization Result: ---\n",
            "**Optimized Flight Plan: YUL to ZSPD (General Optimization)**\n",
            "  Date: 2025-06-16\n",
            "  Route: YUL -> Vancouver (Layover: 49.2827°N, -123.1207°W) -> ZSPD\n",
            "  Estimated Flight Time: 14h 08m (aligned with Vancouver layover study)\n",
            "  Estimated Fuel Consumption: 86644.55 kg (Good efficiency due to jet streams)\n",
            "  Considerations: Balancing time and fuel, potential benefits of layovers for advantageous conditions. (Source: 'AI-Driven Flight Path Optimization' Case Study 2)\n",
            "\n",
            "### Combined Agentic Flow Finished ###\n"
          ]
        }
      ]
    }
  ]
}
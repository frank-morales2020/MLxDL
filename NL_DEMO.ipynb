{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNsDQHeSeHPGEdhRQbNMH5F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/NL_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxeR5fS7oEp6",
        "outputId": "7ea369e1-82b7-45b6-f73e-5a2a694be877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Nested Learning Simulation Start (Target: 10.0) ---\n",
            "\n",
            "--- Epoch 4 (Deep Optimizer Update) ---\n",
            "Prediction: 6.3866\n",
            "Avg Loss (Meta-Signal): 6.4941\n",
            "Learned Consolidation Factor: 0.895059\n",
            "\n",
            "--- Epoch 8 (Deep Optimizer Update) ---\n",
            "Prediction: 7.8771\n",
            "Avg Loss (Meta-Signal): 2.4701\n",
            "Learned Consolidation Factor: 0.880358\n",
            "\n",
            "--- Epoch 12 (Deep Optimizer Update) ---\n",
            "Prediction: 8.2407\n",
            "Avg Loss (Meta-Signal): 1.4512\n",
            "Learned Consolidation Factor: 0.875845\n",
            "\n",
            "--- Epoch 16 (Deep Optimizer Update) ---\n",
            "Prediction: 8.3222\n",
            "Avg Loss (Meta-Signal): 1.2026\n",
            "Learned Consolidation Factor: 0.873819\n",
            "\n",
            "--- Epoch 20 (Deep Optimizer Update) ---\n",
            "Prediction: 8.3329\n",
            "Avg Loss (Meta-Signal): 1.1470\n",
            "Learned Consolidation Factor: 0.872350\n",
            "\n",
            "--- Final Results ---\n",
            "Final Prediction: 8.3329\n",
            "Final Learned Consolidation Factor (Learned Rule): 0.872350\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. The Deep Optimizer (Higher Level / Slow Update) ---\n",
        "# This simulates the \"Learning-to-Learn\" mechanism.\n",
        "class DeepOptimizer:\n",
        "    \"\"\"\n",
        "    The Outer Loop: Learns the optimal 'consolidation factor' based on long-term performance.\n",
        "    \"\"\"\n",
        "    def __init__(self, slow_update_rate=0.01):\n",
        "        # The parameter to be learned by the Optimizer:\n",
        "        # The optimal 'consolidation factor' for preserving knowledge.\n",
        "        self.consolidation_factor = 0.95\n",
        "        self.meta_rate = slow_update_rate # Rate at which the Optimizer learns\n",
        "\n",
        "    def get_factor(self):\n",
        "        \"\"\"Provides the learned consolidation rule to the lower-level memory system.\"\"\"\n",
        "        return self.consolidation_factor\n",
        "\n",
        "    def update(self, long_term_loss):\n",
        "        \"\"\"\n",
        "        Adjusts the consolidation factor based on the average long-term performance loss.\n",
        "        If long_term_loss is high, the optimizer learns to increase the factor (consolidate more).\n",
        "        \"\"\"\n",
        "        # We model the update: If the average loss is low (good), the factor moves towards 1.0.\n",
        "        # We use (1.0 - loss) as a proxy for 'goodness' of performance over time.\n",
        "        self.consolidation_factor += self.meta_rate * (1.0 - long_term_loss)\n",
        "\n",
        "        # Ensure the factor stays between reasonable bounds (0.5 and 1.0)\n",
        "        self.consolidation_factor = np.clip(self.consolidation_factor, 0.5, 1.0)\n",
        "        return self.consolidation_factor\n",
        "\n",
        "# --- 2. The Continuum Memory System (Lower Level / Fast Update) ---\n",
        "# This simulates the core NL architecture with multi-time-scale modules.\n",
        "class NLMemorySystem:\n",
        "    \"\"\"\n",
        "    The Inner Loop: Contains two memory modules with different update frequencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, initial_factor):\n",
        "        # FAST Memory: Simulates quick adaptation (high update frequency)\n",
        "        self.fast_memory = 0.5\n",
        "        self.fast_rate = 0.5\n",
        "\n",
        "        # SLOW Memory: Simulates consolidated knowledge (low update frequency)\n",
        "        self.slow_memory = 0.5\n",
        "        self.slow_rate = 0.05\n",
        "\n",
        "        # The consolidation rule is provided by the Deep Optimizer\n",
        "        self.consolidation_factor = initial_factor\n",
        "\n",
        "    def predict(self):\n",
        "        \"\"\"The model's output is a blending of its memories.\"\"\"\n",
        "        return (self.fast_memory + self.slow_memory) / 2\n",
        "\n",
        "    # Inner Loop (Fast Task Update)\n",
        "    def fast_update(self, error):\n",
        "        \"\"\"Updates the fast memory module every step based on immediate error.\"\"\"\n",
        "        self.fast_memory -= self.fast_rate * error\n",
        "\n",
        "    # Outer Loop Rule Application (Slow Consolidation)\n",
        "    def consolidate_knowledge(self, task_error, new_factor):\n",
        "        \"\"\"\n",
        "        Applies the consolidation rule determined by the Deep Optimizer.\n",
        "        This is the mechanism that mitigates catastrophic forgetting.\n",
        "        \"\"\"\n",
        "        self.consolidation_factor = new_factor\n",
        "\n",
        "        # 1. Update SLOW Memory (Core Knowledge Consolidation)\n",
        "        # This update is slow and steady.\n",
        "        self.slow_memory -= self.slow_rate * task_error\n",
        "\n",
        "        # 2. Modulate FAST Memory\n",
        "        # The consolidation_factor (learned by the Deep Optimizer) controls\n",
        "        # how much fast knowledge is 'dampened' or preserved before the next cycle.\n",
        "        self.fast_memory *= self.consolidation_factor\n",
        "\n",
        "# --- 3. Nested Training Process Simulation ---\n",
        "def nested_learning_demo(epochs=20):\n",
        "    optimizer = DeepOptimizer()\n",
        "    model = NLMemorySystem(initial_factor=optimizer.get_factor())\n",
        "\n",
        "    TARGET = 10.0\n",
        "    CONSOLIDATION_FREQUENCY = 4\n",
        "\n",
        "    print(\"--- Nested Learning Simulation Start (Target: 10.0) ---\")\n",
        "\n",
        "    cumulative_loss = 0.0\n",
        "\n",
        "    for i in range(1, epochs + 1):\n",
        "        # --- INNER LOOP (Task Execution: FAST Time Scale) ---\n",
        "        target = TARGET\n",
        "        prediction = model.predict()\n",
        "        task_error = prediction - target\n",
        "        absolute_error = np.abs(task_error)\n",
        "\n",
        "        # 1. Update the Learner's Fast Memory based on immediate error\n",
        "        model.fast_update(task_error)\n",
        "\n",
        "        # 2. Accumulate Loss for the Outer Loop (The meta-signal)\n",
        "        cumulative_loss += absolute_error\n",
        "\n",
        "        # --- OUTER LOOP (Meta-Learning: SLOW Time Scale) ---\n",
        "        if i % CONSOLIDATION_FREQUENCY == 0:\n",
        "            avg_loss = cumulative_loss / CONSOLIDATION_FREQUENCY\n",
        "\n",
        "            # --- Deep Optimizer Update ---\n",
        "            # The Deep Optimizer learns from the sustained performance signal.\n",
        "            new_factor = optimizer.update(avg_loss)\n",
        "\n",
        "            # --- CMS Rule Application ---\n",
        "            # The model applies the new learned consolidation rule for slow consolidation.\n",
        "            model.consolidate_knowledge(task_error, new_factor)\n",
        "\n",
        "            print(f\"\\n--- Epoch {i} (Deep Optimizer Update) ---\")\n",
        "            print(f\"Prediction: {model.predict():.4f}\")\n",
        "            print(f\"Avg Loss (Meta-Signal): {avg_loss:.4f}\")\n",
        "            print(f\"Learned Consolidation Factor: {new_factor:.6f}\")\n",
        "\n",
        "            # Reset cumulative loss\n",
        "            cumulative_loss = 0.0\n",
        "\n",
        "    print(\"\\n--- Final Results ---\")\n",
        "    print(f\"Final Prediction: {model.predict():.4f}\")\n",
        "    print(f\"Final Learned Consolidation Factor (Learned Rule): {optimizer.get_factor():.6f}\")\n",
        "\n",
        "# Run the simulation\n",
        "nested_learning_demo()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# --- 1. The Deep Optimizer (Meta-Learning / Outer Loop) ---\n",
        "class DeepOptimizer:\n",
        "    \"\"\"\n",
        "    Simulates the slow-time-scale meta-optimizer.\n",
        "    It learns a meta-parameter (e.g., consolidation factor) based on long-term loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, meta_learning_rate=0.01):\n",
        "        # This parameter represents a learned rule (e.g., how much to consolidate)\n",
        "        self.consolidation_factor = torch.tensor(0.95, requires_grad=True)\n",
        "        self.meta_optimizer = torch.optim.SGD([self.consolidation_factor], lr=meta_learning_rate)\n",
        "\n",
        "    def get_factor(self):\n",
        "        \"\"\"Returns the current learned consolidation rule.\"\"\"\n",
        "        return self.consolidation_factor.item()\n",
        "\n",
        "    def update(self, long_term_loss):\n",
        "        \"\"\"\n",
        "        Updates the consolidation factor based on the sustained long-term performance loss.\n",
        "        (In a real scenario, this involves second-order derivatives or reinforcement learning.)\n",
        "        \"\"\"\n",
        "        self.meta_optimizer.zero_grad()\n",
        "\n",
        "        # Conceptual Meta-Loss: We define the Meta-Loss as (1.0 - Loss)\n",
        "        # to encourage the factor to change when the average loss is high.\n",
        "        meta_loss = (1.0 - long_term_loss) * self.consolidation_factor # Example proxy loss\n",
        "        meta_loss.backward()\n",
        "        self.meta_optimizer.step()\n",
        "\n",
        "        # Clamp the factor to keep it within sensible bounds\n",
        "        self.consolidation_factor.data.clamp_(0.5, 1.0)\n",
        "        return self.get_factor()\n",
        "\n",
        "# --- 2. The NL Transformer Feed-Forward Network (CMS) ---\n",
        "class NLTransformerFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simulates the Continuum Memory System (CMS) component of a Transformer.\n",
        "    It contains parallel Fast and Slow memory modules with separate optimization.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- SLOW MEMORY MODULE (Consolidation) ---\n",
        "        self.slow_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # --- FAST MEMORY MODULE (Adaptation) ---\n",
        "        self.fast_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Optimizer for the Fast Module (Inner Loop)\n",
        "        self.fast_optimizer = torch.optim.Adam(self.fast_linear.parameters(), lr=1e-3)\n",
        "\n",
        "        # Optimizer for the Slow Module (Outer Loop / Consolidation)\n",
        "        self.slow_optimizer = torch.optim.Adam(self.slow_linear.parameters(), lr=1e-5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The output is the combination of fast and slow memory modules.\"\"\"\n",
        "        fast_out = torch.relu(self.fast_linear(x))\n",
        "        slow_out = torch.relu(self.slow_linear(x))\n",
        "        return fast_out + slow_out\n",
        "\n",
        "    # Inner Loop Update (FAST)\n",
        "    def fast_update(self, loss_grad):\n",
        "        \"\"\"Updates the fast memory module based on task gradient.\"\"\"\n",
        "        self.fast_optimizer.zero_grad()\n",
        "        # In a real model, we would backprop through the whole model.\n",
        "        # Here we conceptually update the fast module using the overall loss gradient.\n",
        "        # This is the standard, frequent Transformer weight update.\n",
        "        self.fast_optimizer.step() # Requires loss.backward() to run first\n",
        "\n",
        "    # Outer Loop Update (SLOW / Consolidation)\n",
        "    def slow_update(self, consolidation_factor):\n",
        "        \"\"\"Updates the slow memory module and applies the consolidation rule.\"\"\"\n",
        "        # 1. Update SLOW Memory (lock in consolidated knowledge)\n",
        "        # This step is conceptually driven by the Meta-Loss in a real NL model,\n",
        "        # but here we use the small slow_optimizer rate to represent consolidation.\n",
        "        self.slow_optimizer.step() # Requires a backward pass based on Meta-Loss\n",
        "\n",
        "        # 2. Apply Deep Optimizer's Rule (Consolidation Factor)\n",
        "        # The factor modulates the Fast Memory to prevent its parameters from straying too far\n",
        "        # from the Slow Memory's consolidated state, mitigating forgetting.\n",
        "        for param in self.fast_linear.parameters():\n",
        "            param.data.mul_(consolidation_factor)\n",
        "\n",
        "\n",
        "# --- 3. Nested Training Simulation ---\n",
        "\n",
        "def nl_transformer_demo(epochs=20, inner_steps=10):\n",
        "    # Hyperparameters for simulation\n",
        "    D_MODEL = 64\n",
        "    CONSOLIDATION_FREQUENCY = 5\n",
        "\n",
        "    # 1. Initialize Components\n",
        "    deep_optimizer = DeepOptimizer()\n",
        "    nl_ffn = NLTransformerFFN(D_MODEL)\n",
        "\n",
        "    # Simulated Target Data (e.g., an encoded sequence)\n",
        "    target_data = torch.randn(1, D_MODEL)\n",
        "\n",
        "    print(\"--- Conceptual NL-Transformer Training Start ---\")\n",
        "\n",
        "    cumulative_loss_history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        avg_loss_for_meta = 0.0\n",
        "\n",
        "        for step in range(inner_steps):\n",
        "            # --- INNER LOOP (FAST TIME SCALE) ---\n",
        "\n",
        "            # Simulated Input (sequence data)\n",
        "            input_data = torch.randn(1, D_MODEL)\n",
        "\n",
        "            # Forward Pass and Task Loss (Simulated)\n",
        "            prediction = nl_ffn(input_data)\n",
        "            task_loss = nn.MSELoss()(prediction, target_data)\n",
        "\n",
        "            # Backpropagation for Gradients\n",
        "            task_loss.backward(retain_graph=True) # Retain graph for simplicity\n",
        "\n",
        "            # 1. FAST UPDATE (Standard Transformer Update)\n",
        "            nl_ffn.fast_update(task_loss)\n",
        "\n",
        "            avg_loss_for_meta += task_loss.item()\n",
        "\n",
        "        # --- OUTER LOOP (SLOW TIME SCALE / CONSOLIDATION) ---\n",
        "\n",
        "        avg_loss_for_meta /= inner_steps\n",
        "        cumulative_loss_history.append(avg_loss_for_meta)\n",
        "\n",
        "        if epoch % CONSOLIDATION_FREQUENCY == 0:\n",
        "\n",
        "            # 2. DEEP OPTIMIZER UPDATE\n",
        "            # The Meta-Optimizer learns from the sustained performance (avg_loss)\n",
        "            learned_factor = deep_optimizer.update(torch.tensor(avg_loss_for_meta))\n",
        "\n",
        "            # 3. SLOW UPDATE / CONSOLIDATION\n",
        "            # The Slow Memory consolidates knowledge, guided by the learned factor\n",
        "            nl_ffn.slow_update(learned_factor)\n",
        "\n",
        "            print(f\"\\n--- Epoch {epoch} (CONSOLIDATION) ---\")\n",
        "            print(f\"Avg Task Loss (Meta-Signal): {avg_loss_for_meta:.4f}\")\n",
        "            print(f\"Learned Consolidation Factor: {learned_factor:.6f}\")\n",
        "            print(f\"Fast Module Norm: {nl_ffn.fast_linear.weight.norm().item():.4f}\")\n",
        "            print(f\"Slow Module Norm: {nl_ffn.slow_linear.weight.norm().item():.4f}\")\n",
        "\n",
        "    print(\"\\n--- NL-Transformer Simulation Complete ---\")\n",
        "    print(f\"Final Learned Consolidation Factor (Deep Optimizer's Rule): {deep_optimizer.get_factor():.6f}\")\n",
        "\n",
        "# Run the simulation\n",
        "nl_transformer_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFq6c9IfowOq",
        "outputId": "16cdb5a3-8d0f-4a78-d3e0-53af34cb7363"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Conceptual NL-Transformer Training Start ---\n",
            "\n",
            "--- Epoch 5 (CONSOLIDATION) ---\n",
            "Avg Task Loss (Meta-Signal): 1.6427\n",
            "Learned Consolidation Factor: 0.956427\n",
            "Fast Module Norm: 4.4180\n",
            "Slow Module Norm: 4.6270\n",
            "\n",
            "--- Epoch 10 (CONSOLIDATION) ---\n",
            "Avg Task Loss (Meta-Signal): 1.5223\n",
            "Learned Consolidation Factor: 0.961650\n",
            "Fast Module Norm: 4.2486\n",
            "Slow Module Norm: 4.6268\n",
            "\n",
            "--- Epoch 15 (CONSOLIDATION) ---\n",
            "Avg Task Loss (Meta-Signal): 1.6075\n",
            "Learned Consolidation Factor: 0.967725\n",
            "Fast Module Norm: 4.1114\n",
            "Slow Module Norm: 4.6266\n",
            "\n",
            "--- Epoch 20 (CONSOLIDATION) ---\n",
            "Avg Task Loss (Meta-Signal): 1.5365\n",
            "Learned Consolidation Factor: 0.973090\n",
            "Fast Module Norm: 4.0008\n",
            "Slow Module Norm: 4.6264\n",
            "\n",
            "--- NL-Transformer Simulation Complete ---\n",
            "Final Learned Consolidation Factor (Deep Optimizer's Rule): 0.973090\n"
          ]
        }
      ]
    }
  ]
}
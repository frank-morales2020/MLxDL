{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8ZVY+pr6ZioNivhui7mNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/NEWSOLUTION_FINE_TUNING_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets networkx -q\n",
        "\n",
        "!pip install torch_geometric -q\n",
        "\n",
        "\n",
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install mistral_inference -q\n",
        "\n",
        "!pip install trl==0.8.6 -q\n",
        "\n",
        "\n",
        "!pip install torch-geometric -q\n",
        "!pip install sqlparse networkx -q\n",
        "\n",
        "!pip install bitsandbytes -q\n",
        "\n",
        "\n",
        "!pip uninstall -y torchvision -q\n",
        "!pip install torchvision --no-cache-dir -q\n",
        "import evaluate\n",
        "\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "x9cqNuPkh_aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "f\n",
        "rom transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "from trl import setup_chat_format\n"
      ],
      "metadata": {
        "id": "8Ak2KGyrYOro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6horVfRxXtU8"
      },
      "outputs": [],
      "source": [
        "# Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    # Download if not already downloaded\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# 1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\").shuffle(seed=42)\n",
        ", val_dataset, test_dataset = dataset.train_test_split(test_size=0.3, seed=42).values()\n",
        "\n",
        "# Load dataset from disk\n",
        "#train_dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/datasets/train_dataset_gnn.json\", split=\"train\")\n",
        "#test_dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/datasets/test_dataset_gnn.json\", split=\"train\")\n",
        "\n",
        "# 2. Load Mistral Model and Tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "### MODEL CONFIG\n",
        "config = AutoModelForCausalLM.from_pretrained(model_id).config\n",
        "config.output_hidden_states = True\n",
        "config.use_cache = False\n",
        "# Explicitly set the output type for hidden states in config\n",
        "config.torch_dtype = torch.float32\n",
        "\n",
        "# Load model and tokenizer\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    #device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, skip_special_tokens=True )\n",
        "tokenizer.padding_side = \"left\"  # Pad on the left side\n",
        "\n",
        "# Tokenizer Setup\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# Set chat template to OAI chatML (if needed)\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "\n",
        "\n",
        "# Device configuration (set once at the top level)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Create PyTorch Dataset for Text-to-SQL\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        #text = item['question'] + \" \" + item['context']\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "        tokenized_input = self.tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "        tokenized_target = self.tokenizer(target_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Dependency Parsing for Edge Index\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        for token in doc:\n",
        "            if token.i < len(doc) - 1:\n",
        "                edges.append([token.i, token.head.i])\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokenized_input['input_ids'].flatten(),\n",
        "            'attention_mask': tokenized_input['attention_mask'].flatten(),\n",
        "            'labels': tokenized_target['input_ids'].flatten(),\n",
        "            'edge_index': edge_index,\n",
        "        }\n",
        "\n",
        "# create all dataset from the  TextToSQLDataset\n",
        "train_dataset = TextToSQLDataset(train_dataset, tokenizer)\n",
        "val_dataset = TextToSQLDataset(val_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)\n",
        "\n",
        "\n",
        "# 4. GAT (Graph Attention Network) Layer\n",
        "class GATLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_heads=8):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.gat = GAT(in_features, out_features, heads=num_heads, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        return self.gat(x, edge_index)\n",
        "\n",
        "# 5. Model integrating GAT\n",
        "class GraphModel(torch.nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(GraphModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.gat = GATLayer(4096, 4096)\n",
        "        # PyTorch Implementation of scatter_mean\n",
        "        self.pool = lambda x, batch: torch.mean(x, dim=0, keepdim=True)\n",
        "        self.lm_head = torch.nn.Linear(4096, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        x = inputs['input_ids'].to(mistral_model.device)\n",
        "        edge_index = inputs['edge_index'].to(mistral_model.device)\n",
        "        embeddings = self.encoder(x).last_hidden_state.cpu()\n",
        "        graph_out = self.gat(embeddings, edge_index.cpu())\n",
        "        pooled = self.pool(graph_out, inputs.get('batch', torch.zeros(graph_out.size(0), dtype=torch.long)))\n",
        "        out = self.lm_head(pooled.to(mistral_model.device))\n",
        "        return {\"logits\": out}\n",
        "\n",
        "# 6. Model Setup, PEFT/LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=['gat']\n",
        ")\n",
        "model = GraphModel(mistral_model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# 7. Evaluation Metric (Semantic Similarity)\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
        "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Compute embeddings for predictions and references\n",
        "    prediction_embeddings = sentence_transformer_model.encode(predictions, convert_to_tensor=True)\n",
        "    reference_embeddings = sentence_transformer_model.encode(references, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = util.cos_sim(prediction_embeddings, reference_embeddings)\n",
        "    similarities = torch.diag(cosine_similarities).cpu().numpy()  # Extract similarities for corresponding pairs\n",
        "\n",
        "    # Return average similarity as the metric\n",
        "    return {\"semantic_similarity\": np.mean(similarities)}\n",
        "\n",
        "\n",
        "# 9. Training Arguments and Trainer\n",
        "training_args = TrainingArguments(\n",
        "    \"graph-T2SQL\",\n",
        "    logging_dir=\"graph-T2SQL\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    push_to_hub=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_semantic_similarity\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint"
      ],
      "metadata": {
        "id": "q9xVfjpHjI9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    data_collator=lambda x: x,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "# TO BE TEST IT WORK FOR trainer = SFTTrainer( ; from trl import SFTTrainer; max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
        "# https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Mistral_7B_Instruct_v0_1_for_text_to_SQL_EVALDATA.ipynb\n",
        "\n",
        "\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')"
      ],
      "metadata": {
        "id": "7cgOkDmUjQbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
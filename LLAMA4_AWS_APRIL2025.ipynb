{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yHCOvGVoLoRi"
      ],
      "authorship_tag": "ABX9TyNZ38t8jLFE+/W6kRxC88Cf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/LLAMA4_AWS_APRIL2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Information\n",
        "\n",
        "* The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
        "\n",
        "* These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n",
        "\n",
        "* Model developer: Meta\n",
        "\n",
        "* Model Architecture: The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\n",
        "\n",
        "* Model Name\tTraining Data\tParams\tInput modalities\tOutput modalities\tContext length\tToken count\tKnowledge cutoff\n",
        "Llama 4 Scout (17Bx16E)\tA mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI. Learn more in our Privacy Center.\t17B (Activated) 109B (Total)\tMultilingual text and image\tMultilingual text and code\t10M\t~40T\tAugust 2024\n",
        "Llama 4 Maverick (17Bx128E)\t\t17B (Activated) 400B (Total)\tMultilingual text and image\tMultilingual text and code\t1M\t~22T\tAugust 2024\n",
        "Supported languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\n",
        "\n",
        "* Model Release Date: April 5, 2025\n",
        "\n",
        "* Status: This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\n",
        "\n",
        "https://studio-d-yesr9g64bv2p.studio.us-east-1.sagemaker.aws/jumpstart/SageMakerPublicHub/Model/meta-vlm-llama-4-scout-17b-16e-instruct"
      ],
      "metadata": {
        "id": "-stkFFEYzvmh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaWcS7jcG5VF"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install sagemaker boto3 --quiet\n",
        "\n",
        "%pip install langchain --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#print(aws_access_key_id)\n",
        "#print()\n",
        "#print(f\"aws_access_key_id: '{aws_access_key_id}'\")\n",
        "#print(f\"aws_secret_access_key: '{aws_secret_access_key}'\")\n",
        "\n",
        "#print(f\"region: '{region}'\")\n",
        "#print()"
      ],
      "metadata": {
        "id": "AcZLELCdKFcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import boto3\n",
        "import os\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "\n",
        "#'ml.p5.48xlarge for endpoint usage\n",
        "llm_model_id = \"meta-vlm-llama-4-scout-17b-16e-instruct\"\n",
        "llm_model = JumpStartModel(model_id=llm_model_id, env={\"MAX_SEQ_LEN\": \"1048576\"}, role=ROLE_ARN, region='us-east-1')\n",
        "llm_predictor = llm_model.deploy(accept_eula=True)\n"
      ],
      "metadata": {
        "id": "5P1dwGh5KWRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the model endpoint NAME, not the ARN\n",
        "llm_model_endpoint_name = llm_predictor.endpoint_name\n",
        "llm_model_endpoint_name"
      ],
      "metadata": {
        "id": "2XRGT2iULD1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "####  CASE#1\n",
        "import json\n",
        "#query = \"who is the best French Poet?\"\n",
        "query = \"Write a program to compute factorial in python:\"\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "#print(response_body)\n",
        "\n",
        "print(f'Query #1:', query)\n",
        "print('\\n')\n",
        "\n",
        "# Check if the expected key exists before accessing it\n",
        "if 'generated_text' in response_body:\n",
        "    print(f'Response #1:', response_body['generated_text'])  # Access directly if it's a dictionary\n",
        "    print('\\n')\n",
        "elif isinstance(response_body, list) and len(response_body) > 0 and 'generated_text' in response_body[0]:\n",
        "    print(f'Response #2:', response_body[0]['generated_text'])  # Access using index if it's a list of dictionaries\n",
        "    print('\\n')\n",
        "else:\n",
        "    print(\"Unexpected response format:\", response_body)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jOXxLlKO-b0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### CASE#2\n",
        "\n",
        "#query = \"who is the best French Poet?\"\n",
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# Prepare the input for the model\n",
        "#input_data = {\"inputs\": query}\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "\n",
        "print(f'Query #2:', query)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "if 'generated_text' in response_body:\n",
        "    print(f'Response #1:', response_body['generated_text'])\n",
        "elif isinstance(response_body, list) and len(response_body) > 0 and 'generated_text' in response_body[0]:\n",
        "    print(f'Response #2:', response_body[0]['generated_text'])\n",
        "else:\n",
        "    print(\"Unexpected response format or empty response:\", response_body)"
      ],
      "metadata": {
        "id": "0Vj8eTH3LYYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CASE#3\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import urllib.request\n",
        "import urllib\n",
        "from matplotlib.pyplot import figure, imshow, axis\n",
        "from matplotlib.image import imread\n",
        "import IPython\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def chat_gpt_vision(image_url):\n",
        "    # Prepare the input for the model, including the image URL and the prompt\n",
        "    input_data = {\n",
        "        \"inputs\": f\"Describe the image: {image_url}\",\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 256,  # Adjust as needed\n",
        "            \"temperature\": 0.7      # Adjust as needed\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Send the request to the SageMaker endpoint\n",
        "    response = sm_client.invoke_endpoint(\n",
        "        EndpointName=llm_model_endpoint_name,\n",
        "        Body=json.dumps(input_data),\n",
        "        ContentType=\"application/json\"\n",
        "    )\n",
        "\n",
        "    # Decode and return the model's response\n",
        "    response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "    return response_body.get('generated_text', '')  # Handle potential missing key\n",
        "\n",
        "\n",
        "# Nature\n",
        "prompt =  \"What’s in this image?\"\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "\n",
        "print()\n",
        "print('The image url is here: %s'%image_url)\n",
        "\n",
        "nature=\"/content/gdrive/MyDrive/datasets/Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "print()\n",
        "\n",
        "testim = cv2.imread(nature)\n",
        "plt.imshow(testim)\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "description = chat_gpt_vision(image_url)\n",
        "print(description)\n",
        "\n",
        "\n",
        "## turing award recipient ########\n",
        "prompt =  \"Describe the image?\"\n",
        "image_url = \"https://awards.acm.org/binaries/content/gallery/acm/ctas/awards/turing-2018-bengio-hinton-lecun.jpg\"\n",
        "#url=\"https://awards.acm.org/binaries/content/gallery/acm/ctas/awards/turing-2018-bengio-hinton-lecun.jpg\"\n",
        "\n",
        "print()\n",
        "print('The image of the 2018 Turing Award recipients is here: %s'%image_url)\n",
        "print()\n",
        "\n",
        "award=\"/content/gdrive/MyDrive/datasets/turing-2018-bengio-hinton-lecun.jpg\"\n",
        "testim = cv2.imread(award)\n",
        "plt.imshow(testim)\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "description = chat_gpt_vision(image_url)\n",
        "print(description)\n"
      ],
      "metadata": {
        "id": "ZI6GfKGHIj1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEAN UP"
      ],
      "metadata": {
        "id": "yHCOvGVoLoRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "id": "6mnSz_gVLtPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
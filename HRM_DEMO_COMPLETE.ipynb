{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "db3de1da"
      ],
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOqa4OqBIMnHzjo7IrPjqzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/HRM_DEMO_COMPLETE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/sapientinc/HRM\n",
        "\n",
        "\n",
        "https://github.com/Dao-AILab/flash-attention\n",
        "\n",
        "\n",
        "https://github.com/sapientinc/HRM/issues/12"
      ],
      "metadata": {
        "id": "BphziTlllzhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HRM"
      ],
      "metadata": {
        "id": "BkcjTkjSQ5JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "z8y7KTWKSzqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sapientinc/HRM.git"
      ],
      "metadata": {
        "id": "RlDXI74QVxkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9298e7b1"
      },
      "source": [
        "!pip install torch adam-atan2 einops tqdm coolname pydantic argdantic wandb omegaconf hydra-core huggingface_hub -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y flash-attn\n",
        "!pip install --no-cache-dir flash-attn"
      ],
      "metadata": {
        "id": "bF3BXlg7bKpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U flash-attn --no-build-isolation --quiet"
      ],
      "metadata": {
        "id": "LVWEeOnvcvWh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0516b30"
      },
      "source": [
        "!git clone https://github.com/Dao-AILab/flash-attention.git\n",
        "%cd flash-attention\n",
        "!python setup.py install --user\n",
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio flash-attn\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install flash-attn --no-cache-dir"
      ],
      "metadata": {
        "id": "4_VbBVujdb-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flash_attn\n",
        "print(flash_attn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRSDIEnibOCU",
        "outputId": "40605625-7ef9-460b-91e6-ce56fafe45c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "645e7b60"
      },
      "source": [
        "!pip uninstall -y torch torchvision torchaudio adam-atan2\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install adam-atan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y adam-atan2\n",
        "!pip install adam-atan2 --no-cache-dir"
      ],
      "metadata": {
        "id": "683Er3kOinUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e46fda10",
        "outputId": "876ab5a8-0ce5-4372-e69e-3ecaf69f3334"
      },
      "source": [
        "!python /content/HRM/dataset/build_sudoku_dataset.py --output-dir /content/data/sudoku-extreme-100-aug-100 --subsample-size 100 --num-aug 100"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv: 100% 719M/719M [00:02<00:00, 271MB/s]\n",
            "100% 100/100 [00:01<00:00, 80.39it/s]\n",
            "test.csv: 100% 79.4M/79.4M [00:00<00:00, 232MB/s]\n",
            "100% 422786/422786 [00:00<00:00, 1813932.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc3f8132",
        "outputId": "508322aa-48d5-4c08-94fe-be6dab35ea7e"
      },
      "source": [
        "!ls -l /content/data/sudoku-extreme-100-aug-100"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "-rw-r--r-- 1 root root   11 Aug  5 03:23 identifiers.json\n",
            "drwxr-xr-x 2 root root 4096 Aug  5 03:23 test\n",
            "drwxr-xr-x 2 root root 4096 Aug  5 03:23 train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e79411ae",
        "outputId": "123c7dc3-30f7-4ae3-9136-fa625f8e301b"
      },
      "source": [
        "!OMP_NUM_THREADS=1 torchrun --nproc-per-node 1 --standalone /content/HRM/pretrain.py data_path=/content/data/sudoku-extreme-100-aug-100 epochs=5 eval_interval=1 global_batch_size=4 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/125 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mf2023morales\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250805_032432-xqhcbpjl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mHierarchicalReasoningModel_ACTV1 rainbow-hog\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/f2023morales/Sudoku-extreme-100-aug-100%20ACT-torch\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/f2023morales/Sudoku-extreme-100-aug-100%20ACT-torch/runs/xqhcbpjl\u001b[0m\n",
            "[Rank 0, World Size 1]: Epoch 0\n",
            " 20% 25/125 [01:20<01:04,  1.55it/s][Rank 0, World Size 1]: Epoch 1\n",
            " 40% 50/125 [3:34:00<3:31:58, 169.59s/it][Rank 0, World Size 1]: Epoch 2\n",
            " 60% 75/125 [7:05:20<1:43:46, 124.54s/it][Rank 0, World Size 1]: Epoch 3\n",
            " 80% 100/125 [10:36:20<1:10:01, 168.05s/it][Rank 0, World Size 1]: Epoch 4\n",
            "100% 125/125 [14:07:51<00:00, 125.97s/it]\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            num_params â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/accuracy â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/count â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  train/exact_accuracy â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/lm_loss â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–†â–†â–„â–‡â–‡â–‚â–ƒâ–‡â–ˆâ–‡â–â–„â–…â–†â–†â–‚â–ƒâ–„â–†â–„â–…â–…â–„â–‡â–ƒâ–„â–ˆâ–ƒâ–„â–„\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/lr â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_continue_loss â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–„â–„â–…â–„â–„â–„â–„â–„â–…â–â–…â–…â–†â–†â–†â–ˆâ–†â–‡â–ˆâ–‡â–‡\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_halt_accuracy â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/q_halt_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/steps â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            num_params 27275266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/accuracy 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/count 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  train/exact_accuracy 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/lm_loss 2.43345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_continue_loss 0.04045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_halt_accuracy 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/q_halt_loss 0.00672\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/steps 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mHierarchicalReasoningModel_ACTV1 rainbow-hog\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/f2023morales/Sudoku-extreme-100-aug-100%20ACT-torch/runs/xqhcbpjl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/f2023morales/Sudoku-extreme-100-aug-100%20ACT-torch\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250805_032432-xqhcbpjl/logs\u001b[0m\n",
            "100% 125/125 [17:39:24<00:00, 508.51s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3de1da"
      },
      "source": [
        "## Verify flash-attn installation\n",
        "\n",
        "### Subtask:\n",
        "Verify that `flash-attn` is correctly installed and accessible in the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadeb6ff"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to verify that `flash-attn` is installed correctly. I will import the library and print its version.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cf0f301",
        "outputId": "f2f1cc08-9c43-4c33-b5a2-fbe4ce03d620"
      },
      "source": [
        "import flash_attn\n",
        "print(flash_attn.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada4aa4b"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ImportError` indicates a problem with the `flash-attn` installation, specifically an undefined symbol in the compiled CUDA extension. This often happens due to a mismatch between the PyTorch and `flash-attn` versions or their CUDA dependencies. I will try to reinstall `flash-attn` with the `--no-cache-dir` option to ensure a clean build.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzCNf_nU9bMe"
      },
      "source": [
        "!pip uninstall -y flash-attn\n",
        "!pip install --no-cache-dir flash-attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1094831"
      },
      "source": [
        "**Reasoning**:\n",
        "The reinstallation did not solve the issue. The error message \"undefined symbol\" strongly suggests a C++ ABI compatibility issue between PyTorch and the compiled `flash-attn` extension. This can be caused by using different compiler versions or C++ standards. Since reinstalling `flash-attn` didn't work, I'll try reinstalling both `torch` and `flash-attn` together, which might ensure they are built with compatible compilers and libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f67e7b59"
      },
      "source": [
        "!pip uninstall -y torch torchvision torchaudio flash-attn\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install flash-attn --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcc1thxx-WNS",
        "outputId": "f758ada6-3449-469b-814c-ad0a0bef53bd"
      },
      "source": [
        "import flash_attn\n",
        "print(flash_attn.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandb"
      ],
      "metadata": {
        "id": "Uenqt_QlckNi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5fd73f1"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/wandb/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrghWi6jWnxy",
        "outputId": "86c6cc52-835f-4358-cf7e-6b9a1aac9f18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 1 root root 4.0K Aug  5 03:24 ..\n",
            "drwxr-xr-x 5 root root 4.0K Aug  5 03:24 run-20250805_032432-xqhcbpjl\n",
            "drwxr-xr-x 3 root root 4.0K Aug  5 03:24 .\n",
            "lrwxrwxrwx 1 root root   52 Aug  5 03:24 debug-internal.log -> run-20250805_032432-xqhcbpjl/logs/debug-internal.log\n",
            "lrwxrwxrwx 1 root root   43 Aug  5 03:24 debug.log -> run-20250805_032432-xqhcbpjl/logs/debug.log\n",
            "lrwxrwxrwx 1 root root   28 Aug  5 03:24 latest-run -> run-20250805_032432-xqhcbpjl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15d735f1",
        "outputId": "b520575e-d70c-4639-aa4d-7adf78ba477f"
      },
      "source": [
        "!ls -ltha /content/wandb/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 1 root root 4.0K Aug  5 03:24 ..\n",
            "drwxr-xr-x 5 root root 4.0K Aug  5 03:24 run-20250805_032432-xqhcbpjl\n",
            "drwxr-xr-x 3 root root 4.0K Aug  5 03:24 .\n",
            "lrwxrwxrwx 1 root root   52 Aug  5 03:24 debug-internal.log -> run-20250805_032432-xqhcbpjl/logs/debug-internal.log\n",
            "lrwxrwxrwx 1 root root   43 Aug  5 03:24 debug.log -> run-20250805_032432-xqhcbpjl/logs/debug.log\n",
            "lrwxrwxrwx 1 root root   28 Aug  5 03:24 latest-run -> run-20250805_032432-xqhcbpjl\n"
          ]
        }
      ]
    }
  ]
}
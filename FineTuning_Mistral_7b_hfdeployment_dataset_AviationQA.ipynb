{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyORXr9dBxRSs4eR4dxrRHvs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b98ad29fb276456fb9e972d1310de06e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0519800a0b304f2686a0d92008f95074",
              "IPY_MODEL_291b5af621084872bae9292cd9923ca9",
              "IPY_MODEL_0a71d68ee5a047478416b271709cc7cc"
            ],
            "layout": "IPY_MODEL_b71fa8eee140443285d7c68f50b1f1a2"
          }
        },
        "0519800a0b304f2686a0d92008f95074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_407127839b0f46ee89264ec47c4a997d",
            "placeholder": "​",
            "style": "IPY_MODEL_2c0a698e7e8340dc8de1f7eee2cf6380",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "291b5af621084872bae9292cd9923ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ad9de7a4b84c07b589b8163091377a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6226dd01643d4fc49039e7043d8481cd",
            "value": 2
          }
        },
        "0a71d68ee5a047478416b271709cc7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5caa3ce61bd414f80603ad31f9c0805",
            "placeholder": "​",
            "style": "IPY_MODEL_120f7a1faafc497ba88b459f5bba63cd",
            "value": " 2/2 [00:06&lt;00:00,  2.86s/it]"
          }
        },
        "b71fa8eee140443285d7c68f50b1f1a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "407127839b0f46ee89264ec47c4a997d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c0a698e7e8340dc8de1f7eee2cf6380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82ad9de7a4b84c07b589b8163091377a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6226dd01643d4fc49039e7043d8481cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5caa3ce61bd414f80603ad31f9c0805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120f7a1faafc497ba88b459f5bba63cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FineTuning_Mistral_7b_hfdeployment_dataset_AviationQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/mistral-7b-tutorial"
      ],
      "metadata": {
        "id": "Kw_Oj0C8H2tJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPENDENCIES"
      ],
      "metadata": {
        "id": "SPAPb3JdCIan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorboard"
      ],
      "metadata": {
        "id": "LsJqORyIZjmp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etm0HfcZM151"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "#!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "#!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# versions: 0.0.1, 0.0.2, 0.0.3, 0.1.0, 0.2.0, 0.2.1, 0.3.0, 0.3.1, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.5.0, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5, 0.8.6, 0.9.2, 0.9.3, 0.9.4)\n",
        "#ERROR: No matching distribution found for trl==0.0.99\n",
        "! pip install trl==0.8.6 -q"
      ],
      "metadata": {
        "id": "cI7mHbyTQWT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "65JpttRiGxVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "6usdVgDF50fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cfb5ca-0795-466c-f4e8-6ed8a70d9486"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "S9qvxG2HYusD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "naQrZIQTY1wG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KtE9TuSMlUEc",
        "outputId": "c887db10-886f-477f-cccc-71233a4af5cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get update && apt-get install -y cuda-11-0"
      ],
      "metadata": {
        "id": "LY6iadKNl1Af"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GapRov3hl4fT",
        "outputId": "57d1ccc7-6ebb-481a-d279-7dad149c3bcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Fri Jun 14 00:30:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              45W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### conversational format\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "\n",
        "### instruction format\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
      ],
      "metadata": {
        "id": "p-7fKxR2rnbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53745a33-f7f4-44a9-ebee-75cc4e733400"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': '<prompt text>', 'completion': '<ideal generated text>'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "sPAxhdxMAdBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://huggingface.co/datasets/sakharamg/AviationQA"
      ],
      "metadata": {
        "id": "ue5Leyc90T6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "print(\"Preprocessing dataset AviationQA\")\n",
        "dataset = load_dataset(\"sakharamg/AviationQA\")\n",
        "\n",
        "# save datasets to disk\n",
        "dataset[\"train\"].to_json(\"train_dataset_AviationQA.json\", orient=\"records\")\n",
        "dataset[\"validation\"].to_json(\"validation_dataset_AviationQA.json\", orient=\"records\")\n",
        "dataset[\"test\"].to_json(\"test_dataset_AviationQA.json\", orient=\"records\")"
      ],
      "metadata": {
        "id": "WY55Lb9ixyCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP4XSSClx2IX",
        "outputId": "57808a7c-7934-4a2f-eb39-1433b2054272"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 1057986\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 11888\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 10807\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/train_dataset_AviationQA.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "SInSDc3Xx6w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9c4RuZCx_gn",
        "outputId": "9e0a61c4-244a-4872-bced-163570c16439"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'Question', 'Answer'],\n",
            "    num_rows: 1057986\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_question=dataset['Question']\n",
        "train_answer=dataset['Answer']"
      ],
      "metadata": {
        "id": "t3C7lGBlFXog"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename='/content/aviation-qa.txt'\n",
        "# python object to be appended\n",
        "for n in range(10000):\n",
        "    if train_answer[n] == None:\n",
        "       train_answer[n] = 'Not possible to get or use'\n",
        "    text='<s>[INST] %s [/INST] %s </s>'%(train_question[n],train_answer[n])\n",
        "    #print(text)\n",
        "    with open(filename, 'a') as f:\n",
        "        f.write(text + '\\n')"
      ],
      "metadata": {
        "id": "WLgn-AwEDuKD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text0 = load_dataset(\"text\", data_files=\"/content/aviation-qa.txt\", split=\"train\")"
      ],
      "metadata": {
        "id": "j4AtoIrsD0k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text0[2]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MNdRQ4UTD5e4",
        "outputId": "f610b6ea-3578-4f4d-a5af-508686ae2d25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] Who determines the probable cause(s) of this accident no. ERA22LA104? [/INST] The National Transportation Safety Board </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrec=10000\n",
        "dataset_final_id=dataset['id'][0:nrec]\n",
        "dataset_final_Question=dataset['Question'][0:nrec]\n",
        "dataset_final_Answer=dataset['Answer'][0:nrec]\n",
        "dataset_final_text=text0[0:nrec]['text']"
      ],
      "metadata": {
        "id": "50KFalQfEDkI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35LRbDnvyGAP",
        "outputId": "714b7837-4f11-4ff9-ddab-7ce67bf0e2d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'a529971724',\n",
              " 'Question': 'Who determines the probable cause(s) of this accident no. ERA22LA104?',\n",
              " 'Answer': 'The National Transportation Safety Board'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "datasetF = pd.DataFrame() # Create an empty DataFrame\n",
        "datasetF['id'] = dataset_final_id\n",
        "datasetF['Question'] = dataset_final_Question\n",
        "datasetF['Answer'] = dataset_final_Answer\n",
        "datasetF['text'] = dataset_final_text"
      ],
      "metadata": {
        "id": "07nkZ2Pu3va9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "B24h4n-E3fkg",
        "outputId": "8ba93840-7669-4ced-b313-74734c98ca88"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               id                                           Question  \\\n",
              "0     a9567233027  Where was the Destination of the accident no. ...   \n",
              "1     a4671579659  What is the name of the Engine Manufacturer of...   \n",
              "2      a529971724  Who determines the probable cause(s) of this a...   \n",
              "3     a1197873694  What are the Probable findings of the accident...   \n",
              "4     a7803769704  In the accident with no. DCA16CA009, is the ai...   \n",
              "...           ...                                                ...   \n",
              "9995  a4688878518  Was there any Operating Certificate(s) held of...   \n",
              "9996  a8331736975  What was the VFR Approach/Landing of the accid...   \n",
              "9997  a4342209241  Was there fire on aircraft of the accident no....   \n",
              "9998  a2435551586  Who determines the probable cause(s) of this a...   \n",
              "9999  a5620706103  What is the Airplane Rating(s) of the accident...   \n",
              "\n",
              "                                                 Answer  \\\n",
              "0                                       Los Angeles, CA   \n",
              "1                                       Pratt & Whitney   \n",
              "2              The National Transportation Safety Board   \n",
              "3     Environmental issues Terrain induced turbulenc...   \n",
              "4                                                  None   \n",
              "...                                                 ...   \n",
              "9995                                               None   \n",
              "9996                                          Go around   \n",
              "9997                                               None   \n",
              "9998           The National Transportation Safety Board   \n",
              "9999  Single-engine land; Single-engine sea; Multi-e...   \n",
              "\n",
              "                                                   text  \n",
              "0     <s>[INST] Where was the Destination of the acc...  \n",
              "1     <s>[INST] What is the name of the Engine Manuf...  \n",
              "2     <s>[INST] Who determines the probable cause(s)...  \n",
              "3     <s>[INST] What are the Probable findings of th...  \n",
              "4     <s>[INST] In the accident with no. DCA16CA009,...  \n",
              "...                                                 ...  \n",
              "9995  <s>[INST] Was there any Operating Certificate(...  \n",
              "9996  <s>[INST] What was the VFR Approach/Landing of...  \n",
              "9997  <s>[INST] Was there fire on aircraft of the ac...  \n",
              "9998  <s>[INST] Who determines the probable cause(s)...  \n",
              "9999  <s>[INST] What is the Airplane Rating(s) of th...  \n",
              "\n",
              "[10000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c94e5475-0963-4fca-8ce9-76b1e1399040\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a9567233027</td>\n",
              "      <td>Where was the Destination of the accident no. ...</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>&lt;s&gt;[INST] Where was the Destination of the acc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a4671579659</td>\n",
              "      <td>What is the name of the Engine Manufacturer of...</td>\n",
              "      <td>Pratt &amp; Whitney</td>\n",
              "      <td>&lt;s&gt;[INST] What is the name of the Engine Manuf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a529971724</td>\n",
              "      <td>Who determines the probable cause(s) of this a...</td>\n",
              "      <td>The National Transportation Safety Board</td>\n",
              "      <td>&lt;s&gt;[INST] Who determines the probable cause(s)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a1197873694</td>\n",
              "      <td>What are the Probable findings of the accident...</td>\n",
              "      <td>Environmental issues Terrain induced turbulenc...</td>\n",
              "      <td>&lt;s&gt;[INST] What are the Probable findings of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a7803769704</td>\n",
              "      <td>In the accident with no. DCA16CA009, is the ai...</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;s&gt;[INST] In the accident with no. DCA16CA009,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>a4688878518</td>\n",
              "      <td>Was there any Operating Certificate(s) held of...</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;s&gt;[INST] Was there any Operating Certificate(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>a8331736975</td>\n",
              "      <td>What was the VFR Approach/Landing of the accid...</td>\n",
              "      <td>Go around</td>\n",
              "      <td>&lt;s&gt;[INST] What was the VFR Approach/Landing of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>a4342209241</td>\n",
              "      <td>Was there fire on aircraft of the accident no....</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;s&gt;[INST] Was there fire on aircraft of the ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>a2435551586</td>\n",
              "      <td>Who determines the probable cause(s) of this a...</td>\n",
              "      <td>The National Transportation Safety Board</td>\n",
              "      <td>&lt;s&gt;[INST] Who determines the probable cause(s)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>a5620706103</td>\n",
              "      <td>What is the Airplane Rating(s) of the accident...</td>\n",
              "      <td>Single-engine land; Single-engine sea; Multi-e...</td>\n",
              "      <td>&lt;s&gt;[INST] What is the Airplane Rating(s) of th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c94e5475-0963-4fca-8ce9-76b1e1399040')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c94e5475-0963-4fca-8ce9-76b1e1399040 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c94e5475-0963-4fca-8ce9-76b1e1399040');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5799c08a-f647-42bd-8a57-7ebaae9de314\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5799c08a-f647-42bd-8a57-7ebaae9de314')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5799c08a-f647-42bd-8a57-7ebaae9de314 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_be39f420-9862-4aff-8131-f643edf4d026\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('datasetF')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_be39f420-9862-4aff-8131-f643edf4d026 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('datasetF');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "datasetF",
              "summary": "{\n  \"name\": \"datasetF\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"a8544539478\",\n          \"a171502243\",\n          \"a7944339308\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"Which runway was used by the aircraft having accident no. ERA17CA047? \",\n          \"What was the Altimeter Setting of the accident no. CEN15LA343?\",\n          \"How many engines were there in the aircraft of the accident no. GAA17CA109?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4107,\n        \"samples\": [\n          \"Scattered / 1500 ft AGL\",\n          \"18-4426\",\n          \"N26136\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"<s>[INST] Which runway was used by the aircraft having accident no. ERA17CA047?  [/INST] 03L </s>\",\n          \"<s>[INST] What was the Altimeter Setting of the accident no. CEN15LA343? [/INST] 29.84 inches Hg </s>\",\n          \"<s>[INST] How many engines were there in the aircraft of the accident no. GAA17CA109? [/INST] 1 Reciprocating </s>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF['Question'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bJIEeujGvocn",
        "outputId": "7e9d359f-f888-43d6-a059-e25d971bfba2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Who determines the probable cause(s) of this accident no. ERA22LA104?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF['text'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "CNGf8BmXEZnF",
        "outputId": "879e8a17-50e4-481c-b127-17c21822d18a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] Who determines the probable cause(s) of this accident no. ERA22LA104? [/INST] The National Transportation Safety Board </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "id": "npGef0PAwBkJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "id": "G756UDF1k3hp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fec6ad-f67d-4725-8d7a-06ce316462bd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            83Gi       2.5Gi        33Gi       4.0Mi        47Gi        80Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Mistral 7B model"
      ],
      "metadata": {
        "id": "vuHodNjdAol6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference -q"
      ],
      "metadata": {
        "id": "VTaNFDF9TedV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" #01 march 2024, 08 MARCH 2024 and 27 MAY 2024\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    is_decoder=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
        "#model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "id": "lQm2yo0uslTs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b98ad29fb276456fb9e972d1310de06e",
            "0519800a0b304f2686a0d92008f95074",
            "291b5af621084872bae9292cd9923ca9",
            "0a71d68ee5a047478416b271709cc7cc",
            "b71fa8eee140443285d7c68f50b1f1a2",
            "407127839b0f46ee89264ec47c4a997d",
            "2c0a698e7e8340dc8de1f7eee2cf6380",
            "82ad9de7a4b84c07b589b8163091377a",
            "6226dd01643d4fc49039e7043d8481cd",
            "f5caa3ce61bd414f80603ad31f9c0805",
            "120f7a1faafc497ba88b459f5bba63cd"
          ]
        },
        "outputId": "72ac89dc-213b-45bb-8947-f3c6828155a2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b98ad29fb276456fb9e972d1310de06e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "EqToGYwPagxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8613219b-1629-4719-c929-6cea19576378"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference\n",
        "\n",
        "Build an inference pipeline with tokenizer and the model."
      ],
      "metadata": {
        "id": "pfFTlpqJFOdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "CHUtHvtzMlYB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt=\"What was the first album Beyoncé released as a solo artist?\"\n",
        "prompt0=\"What is the capital of russia?\"\n",
        "#prompt=\"What are some interesting sites to visit in the Bay Area?\"\n",
        "\n",
        "prompt = f\"Instruct: Find THE answer for the following  question.\\n{prompt0}\\nOutput:\\n\"\n",
        "#prompt = f\"Instruct: Find only the answer for the following  question.\\n{prompt0}\"\n",
        "\n",
        "#prompt=\"What are some interesting sites to visit in the Bay Area?\"\n",
        "#prompt=\"What is the capital of russia?\"\n",
        "outputs = pipe(prompt0, max_new_tokens=128, temperature=0.9,do_sample=True,top_k=30, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "\n",
        "print('Question: %s'%prompt0)\n",
        "print()\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt0):].strip()}\")\n"
      ],
      "metadata": {
        "id": "VZ5nnw6tMsVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2423b3-3a92-4684-82c2-d53bedfd0e9a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of russia?\n",
            "\n",
            "Generated Answer:\n",
            "what is the population of australia? what is the population of china? what is the capital of japan? what is an australian? what is the name of the russian president? what is an aborigine? what is the name of the japanese emperor? what is the language of greece? what is the name of the president of france? what is the name of the german president? what is the name of the president of chile? what is a greek? what is the capital of indonesia? what is an indian? what is the capital of argentina?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "mJOKD2w5QSWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ec2207-09cc-40ba-83ba-909c79f2158f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of russia? what is the population of australia? what is the population of china? what is the capital of japan? what is an australian? what is the name of the russian president? what is an aborigine? what is the name of the japanese emperor? what is the language of greece? what is the name of the president of france? what is the name of the german president? what is the name of the president of chile? what is a greek? what is the capital of indonesia? what is an indian? what is the capital of argentina?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
        "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
        "\n",
        "def gen(model,p, maxlen=1024, sample=True):\n",
        "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
        "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.9,num_beams=1,top_p=0.95,).to('cuda')\n",
        "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "CUFk-I1pugxc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index=321458\n",
        "#index=10000\n",
        "#features: ['id', 'Question', 'Answer'],\n",
        "index=2\n",
        "prompt = datasetF['id'][index]\n",
        "print(prompt)\n",
        "\n",
        "prompt = datasetF['Question'][index]\n",
        "print(prompt)\n",
        "\n",
        "prompt = datasetF['Answer'][index]\n",
        "print(prompt)\n",
        "\n",
        "prompt = datasetF['text'][index]\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_TvVz3B2eso",
        "outputId": "eddb6f21-3264-462e-a957-76f61ab800e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a529971724\n",
            "Who determines the probable cause(s) of this accident no. ERA22LA104?\n",
            "The National Transportation Safety Board\n",
            "<s>[INST] Who determines the probable cause(s) of this accident no. ERA22LA104? [/INST] The National Transportation Safety Board </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL GENERATION - ZERO SHOT"
      ],
      "metadata": {
        "id": "KYysLZxRz9_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=2\n",
        "\n",
        "#features: ['id', 'Question', 'Answer'],\n",
        "#prompt = dataset[index]['Question']\n",
        "#summary = dataset[index]['Answer']\n",
        "\n",
        "prompt = datasetF['Question'][index]\n",
        "summary = datasetF['Answer'][index]\n",
        "\n",
        "original_model = model\n",
        "\n",
        "formatted_prompt = f\"Instruct: provide an answer for the following dialog.\\n{prompt}\"\n",
        "\n",
        "res = original_model.generate(tokenizer.encode(formatted_prompt, return_tensors=\"pt\"), max_length=128, do_sample=False)\n",
        "output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
        "\n",
        "#input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to('cuda') # Assuming you want to run on GPU\n",
        "#input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(torch.bfloat16)\n",
        "#res = model.generate(input_ids, max_length=512, do_sample=False)\n",
        "#output = tokenizer.decode(res[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "1GN8Vns5m7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "\n",
        "print(f'Dialogue :\\n{prompt}')\n",
        "print(dash_line)\n",
        "\n",
        "print(f'ACTION BY THE AGENT-HUMAN:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckW4p9r9N9J8",
        "outputId": "9baff66a-636d-4119-b37a-554cd6a9c6db"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue :\n",
            "Who determines the probable cause(s) of this accident no. ERA22LA104?\n",
            "---------------------------------------------------------------------------------------------------\n",
            "ACTION BY THE AGENT-HUMAN:\n",
            "The National Transportation Safety Board\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Instruct: provide an answer for the following dialog.\n",
            "Who determines the probable cause(s) of this accident no. ERA22LA104?\n",
            "\n",
            "Answer: The probable cause(s) of accident no. ERA22LA104 would be determined by an investigation conducted by the appropriate authorities, such as the National Transportation Safety Board (NTSB) in the United States.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(datasetF))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjQlF7kZXJUN",
        "outputId": "4d5a0a35-08ef-431c-bd81-413b32ca5b1e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "kxOWr2VvEGKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f14fcf-2517-42e6-972c-8bd474deac61"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding the adopter to the layer"
      ],
      "metadata": {
        "id": "of-Z3B28BNBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=128,\n",
        "        lora_dropout=0.05,\n",
        "        r=256,\n",
        "        bias=\"none\",\n",
        "        target_modules=\"all-linear\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "kjyYx9WmtMmA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparmeters"
      ],
      "metadata": {
        "id": "NcaH5WrdBW8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "AkMVebosM16A"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    output_dir=\"/content/gdrive/MyDrive/model/Mistral-7B-v0.1_AviationQA\", # 06/06/2024 #dataset_AviationQA\n",
        "\n",
        "    #num_train_epochs=3,                     # number of training epochs\n",
        "    num_train_epochs=40,                     # number of training epochs for POC\n",
        "    per_device_train_batch_size=2,          # batch size per device during training\n",
        "    per_device_eval_batch_size = 4, #batch size for evaluations\n",
        "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "    logging_steps=10,                       # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    push_to_hub=True,                       # push model to hub\n",
        "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/trl/en/sft_trainer\n",
        "\n",
        "https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Meta_Llama_3_8B_for_text_to_SQL.ipynb"
      ],
      "metadata": {
        "id": "5BMkpAwk6uhW"
      }
    },
    {
      "source": [
        "#print(dataset.column_names)\n",
        "print(datasetF.columns)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVqCgdDqB7Aq",
        "outputId": "cedbef7e-5807-4264-e9b2-3fbb94318b8e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'Question', 'Answer', 'text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasetF['Answer'][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6-OvnJGd7o3",
        "outputId": "0d7371ef-40a9-45ec-b320-2fa7cd7e2e72"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The National Transportation Safety Board\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised fine-tuning (SFT)  parameters"
      ],
      "metadata": {
        "id": "0G2D93SDBjgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl -q"
      ],
      "metadata": {
        "id": "6gIkAfvBHh8n"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80vGLCwTKsrr",
        "outputId": "1cda7e84-2337-494f-e519-8ddc96d522a5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x79925e60d600>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint"
      ],
      "metadata": {
        "id": "YEa0T6DjGlJC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "import datasets\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "datasetF_hf = datasets.Dataset.from_pandas(datasetF)\n",
        "\n",
        "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=datasetF_hf,\n",
        "    dataset_text_field=\"text\", ### added for the summarization dataset\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    #formatting_func=formatting_func,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,  # We template with special tokens\n",
        "        \"append_concat_token\": False, # No need to add additional separator token\n",
        "        ##\"per_device_train_batch_size\": 8,\n",
        "        #\"per_device_eval_batch_size\": 8,\n",
        "        #\"num_train_epochs\": 3.0},\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ZklRuRAatoZ8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training AND Saving the fine-tuned model"
      ],
      "metadata": {
        "id": "sNtNfyP6BuOI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCq7Dq8f7VPU"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ]
    },
    {
      "source": [
        "#from accelerate import Accelerator\n",
        "\n",
        "# Reinitialize the Accelerator\n",
        "#accelerator = Accelerator()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "V0IIAcgzbGYM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free the memory again\n",
        "del model\n",
        "del original_model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mCxeJIj4KvSQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "e0U_CeRoB9kM"
      }
    },
    {
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "P-sM4GnQxrjM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "\n",
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM"
      ],
      "metadata": {
        "id": "DNgzXJgEnEB9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/gdrive/MyDrive/model/Mistral-7B-v0.1_AviationQA/"
      ],
      "metadata": {
        "id": "0_kGS5wGPGfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b85c4ad-79cd-43b9-9d17-c9a5cb345414"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.3G\n",
            "-rw------- 1 root root 1.3K Jun 14 03:45 README.md\n",
            "-rw------- 1 root root  739 Jun 14 03:45 adapter_config.json\n",
            "-rw------- 1 root root 1.3G Jun 14 03:45 adapter_model.safetensors\n",
            "-rw------- 1 root root  438 Jun 14 03:45 special_tokens_map.json\n",
            "-rw------- 1 root root 1.5K Jun 14 03:45 tokenizer_config.json\n",
            "-rw------- 1 root root 1.8M Jun 14 03:45 tokenizer.json\n",
            "-rw------- 1 root root 482K Jun 14 03:45 tokenizer.model\n",
            "-rw------- 1 root root 5.2K Jun 14 03:45 training_args.bin\n",
            "drwx------ 2 root root 4.0K Jun 14 03:44 checkpoint-1400\n",
            "drwx------ 2 root root 4.0K Jun 14 03:40 checkpoint-1365\n",
            "drwx------ 2 root root 4.0K Jun 14 03:36 checkpoint-1330\n",
            "drwx------ 2 root root 4.0K Jun 14 03:33 checkpoint-1295\n",
            "drwx------ 2 root root 4.0K Jun 14 03:29 checkpoint-1260\n",
            "drwx------ 2 root root 4.0K Jun 14 03:25 checkpoint-1225\n",
            "drwx------ 2 root root 4.0K Jun 14 03:22 checkpoint-1190\n",
            "drwx------ 2 root root 4.0K Jun 14 03:18 checkpoint-1155\n",
            "drwx------ 2 root root 4.0K Jun 14 03:14 checkpoint-1120\n",
            "drwx------ 2 root root 4.0K Jun 14 03:10 checkpoint-1085\n",
            "drwx------ 2 root root 4.0K Jun 14 03:07 checkpoint-1050\n",
            "drwx------ 2 root root 4.0K Jun 14 03:03 checkpoint-1015\n",
            "drwx------ 2 root root 4.0K Jun 14 02:59 checkpoint-980\n",
            "drwx------ 2 root root 4.0K Jun 14 02:56 checkpoint-945\n",
            "drwx------ 2 root root 4.0K Jun 14 02:52 checkpoint-910\n",
            "drwx------ 2 root root 4.0K Jun 14 02:48 checkpoint-875\n",
            "drwx------ 2 root root 4.0K Jun 14 02:44 checkpoint-840\n",
            "drwx------ 2 root root 4.0K Jun 14 02:41 checkpoint-805\n",
            "drwx------ 2 root root 4.0K Jun 14 02:37 checkpoint-770\n",
            "drwx------ 2 root root 4.0K Jun 14 02:33 checkpoint-735\n",
            "drwx------ 2 root root 4.0K Jun 14 02:30 checkpoint-700\n",
            "drwx------ 2 root root 4.0K Jun 14 02:26 checkpoint-665\n",
            "drwx------ 2 root root 4.0K Jun 14 02:20 checkpoint-630\n",
            "drwx------ 2 root root 4.0K Jun 14 02:14 checkpoint-595\n",
            "drwx------ 2 root root 4.0K Jun 14 02:08 checkpoint-560\n",
            "drwx------ 2 root root 4.0K Jun 14 02:03 checkpoint-525\n",
            "drwx------ 2 root root 4.0K Jun 14 01:57 checkpoint-490\n",
            "drwx------ 2 root root 4.0K Jun 14 01:51 checkpoint-455\n",
            "drwx------ 2 root root 4.0K Jun 14 01:45 checkpoint-420\n",
            "drwx------ 2 root root 4.0K Jun 14 01:38 checkpoint-385\n",
            "drwx------ 2 root root 4.0K Jun 14 01:32 checkpoint-350\n",
            "drwx------ 2 root root 4.0K Jun 14 01:26 checkpoint-315\n",
            "drwx------ 2 root root 4.0K Jun 14 01:20 checkpoint-280\n",
            "drwx------ 2 root root 4.0K Jun 14 01:14 checkpoint-245\n",
            "drwx------ 2 root root 4.0K Jun 14 01:08 checkpoint-210\n",
            "drwx------ 2 root root 4.0K Jun 14 01:02 checkpoint-175\n",
            "drwx------ 2 root root 4.0K Jun 14 00:56 checkpoint-140\n",
            "drwx------ 2 root root 4.0K Jun 14 00:50 checkpoint-105\n",
            "drwx------ 2 root root 4.0K Jun 14 00:44 checkpoint-70\n",
            "drwx------ 2 root root 4.0K Jun 14 00:37 checkpoint-35\n",
            "drwx------ 4 root root 4.0K Jun 14 00:31 runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "%cd /content/\n",
        "peft_model_id = \"/content/gdrive/MyDrive/model/Mistral-7B-v0.1_AviationQA\"\n",
        "\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "modelnew = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "\n",
        "# load into pipeline\n",
        "generation_pipeline = pipeline(\"text-generation\", model=modelnew, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "kW_hl8YPKxQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='capital of russia?'\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=512, do_sample=True, temperature=0.1,\n",
        "                              top_k=50, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                              pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "ldFY8BYb2cHZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "94RkvAzSnbrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f619658-9618-4b4f-996f-5a6bc4c91ab8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capital of russia? [/INST] 26 \n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWzKLdzjEJ0B2Cs+4Nc++h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/TRANSFOMER_APRIL2025_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch tqdm -q"
      ],
      "metadata": {
        "id": "d6nmLVQ0wrrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from warnings import filterwarnings\n",
        "import torch.optim as optim\n",
        "\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "tokenizer_name = \"gpt2\"  # Or a more suitable general-purpose tokenizer\n",
        "max_input_len = 128\n",
        "max_output_len = 128\n",
        "batch_size = 4\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 20\n",
        "emb_size = 256\n",
        "nhead = 8\n",
        "num_encoder_layers = 4\n",
        "num_decoder_layers = 4\n",
        "dim_feedforward = 1024\n",
        "dropout = 0.1\n",
        "best_model_save_path = \"./best_general_transformer.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- 1. Tokenizer Setup ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "src_vocab_size = len(tokenizer)\n",
        "tgt_vocab_size = len(tokenizer)  # Output vocabulary is the same\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "bos_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id\n",
        "eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.sep_token_id\n",
        "\n",
        "# --- 2. Dataset Preparation ---\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_input_len, max_output_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_output_len = max_output_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        input_text = item[\"input\"]\n",
        "        output_text = item[\"output\"]\n",
        "\n",
        "        # Tokenize input\n",
        "        input_tokens = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_input_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = input_tokens[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = input_tokens[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Tokenize output (add BOS and EOS)\n",
        "        output_tokens = self.tokenizer.encode_plus(\n",
        "            f\"{self.tokenizer.bos_token}{output_text}{self.tokenizer.eos_token}\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_output_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        output_ids = output_tokens[\"input_ids\"].squeeze(0)\n",
        "        output_attention_mask = output_tokens[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Prepare target (shifted output for language modeling)\n",
        "        output_ids_y = output_ids[1:].clone()\n",
        "        output_ids_y[output_ids[:-1] == self.tokenizer.pad_token_id] = self.tokenizer.pad_token_id  # Changed to pad_token_id\n",
        "        output_ids_y[output_ids_y == self.tokenizer.pad_token_id] = self.tokenizer.pad_token_id  # Changed to pad_token_id\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"output_ids\": output_ids[:-1],  # Decoder input (without EOS)\n",
        "            \"output_ids_y\": output_ids_y,  # Target output (shifted)\n",
        "            \"output_attention_mask\": output_attention_mask[:-1]\n",
        "        }\n",
        "\n",
        "# This is a *conceptual* representation. You'd need a LOT more data.\n",
        "data = [\n",
        "    {\"input\": \"I want to visit Paris tomorrow morning.\", \"output\": \"{\\\"location\\\": \\\"Paris\\\", \\\"time\\\": \\\"tomorrow morning\\\"}\"},\n",
        "    {\"input\": \"Explore London in the evening.\", \"output\": \"{\\\"location\\\": \\\"London\\\", \\\"time\\\": \\\"evening\\\"}\"},\n",
        "    {\"input\": \"See New York today.\", \"output\": \"{\\\"location\\\": \\\"New York\\\", \\\"time\\\": \\\"today\\\"}\"},\n",
        "    {\"input\": \"What is the weather like?\", \"output\": \"The weather is currently sunny.\"},\n",
        "    # ... MANY more diverse examples ...\n",
        "    {\"input\": \"Translate 'hello' to French\", \"output\": \"Bonjour\"},\n",
        "    {\"input\": \"Summarize this document...\", \"output\": \"...summary...\"},\n",
        "    {\"input\": \"Write a short story about a cat\", \"output\": \"...story...\"},\n",
        "\n",
        "]\n",
        "\n",
        "dataset = SimpleTextDataset(data, tokenizer, max_input_len, max_output_len)\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# --- 3. Model Definition ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class GeneralTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, nhead: int, src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,  # Output vocabulary size (for text)\n",
        "                 dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 max_text_len: int = 128, max_output_len: int = 128):  # Max output length\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)  # Output embeddings\n",
        "        self.pos_encoder = PositionalEncoding(emb_size, dropout=dropout, max_len=max_text_len)\n",
        "        self.pos_encoder_dec = PositionalEncoding(emb_size, dropout=dropout, max_len=max_output_len)\n",
        "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,\n",
        "                                          num_encoder_layers=num_encoder_layers,\n",
        "                                          num_decoder_layers=num_decoder_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout,\n",
        "                                          batch_first=True)  # Add batch_first=True\n",
        "        self.output_linear = nn.Linear(emb_size, tgt_vocab_size)  # Linear layer to predict words\n",
        "\n",
        "    def forward(self, src_input_ids: torch.Tensor, tgt_input_ids: torch.Tensor,\n",
        "                src_mask: torch.Tensor, tgt_mask: torch.Tensor,\n",
        "                src_padding_mask: torch.Tensor, tgt_padding_mask: torch.Tensor,\n",
        "                memory_key_padding_mask: torch.Tensor):\n",
        "        src_emb = self.pos_encoder(self.src_tok_emb(src_input_ids))\n",
        "        tgt_emb = self.pos_encoder_dec(self.tgt_tok_emb(tgt_input_ids))\n",
        "        memory = self.transformer.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
        "        decoder_output = self.transformer.decoder(tgt_emb, memory,\n",
        "                                                 tgt_mask=tgt_mask,\n",
        "                                                 memory_key_padding_mask=memory_key_padding_mask,\n",
        "                                                 tgt_key_padding_mask=tgt_padding_mask)\n",
        "        predicted_tokens = self.output_linear(decoder_output)  # Predict words\n",
        "        return predicted_tokens\n",
        "\n",
        "def create_mask(src_input_ids, tgt_input_ids, pad_idx, device):\n",
        "    src_seq_len = src_input_ids.shape[1]\n",
        "    tgt_seq_len = tgt_input_ids.shape[1]\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n",
        "    src_padding_mask = (src_input_ids == pad_idx)\n",
        "    tgt_padding_mask = (tgt_input_ids == pad_idx)\n",
        "    memory_key_padding_mask = src_padding_mask\n",
        "    return tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, used to mask future positions.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
        "\n",
        "# --- 4. Model, Loss, Optimizer ---\n",
        "model = GeneralTransformer(\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    num_decoder_layers=num_decoder_layers,\n",
        "    emb_size=emb_size,\n",
        "    nhead=nhead,\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    max_text_len=max_input_len,\n",
        "    max_output_len=max_output_len\n",
        ").to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        output_ids = batch[\"output_ids\"].to(device)\n",
        "        output_ids_y = batch[\"output_ids_y\"].to(device)\n",
        "\n",
        "        # Prepare target (shifted output for language modeling) - Moved inside the training loop\n",
        "        # output_ids_y = output_ids[1:].clone()  # Redundant, already done in __getitem__\n",
        "        # output_ids_y[output_ids[:-1] == tokenizer.pad_token_id] = tokenizer.pad_token_id\n",
        "        # output_ids_y[output_ids_y == tokenizer.pad_token_id] = tokenizer.pad_token_id\n",
        "\n",
        "        src_mask = torch.zeros((input_ids.size(0), input_ids.size(1)), device=device).bool()\n",
        "        tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask = create_mask(input_ids, output_ids, tokenizer.pad_token_id, device)\n",
        "        optimizer.zero_grad()\n",
        "        predicted_tokens = model(input_ids, output_ids, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        loss = loss_fn(predicted_tokens.reshape(-1, predicted_tokens.size(-1)), output_ids_y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# --- 6. Save Model ---\n",
        "torch.save(model.state_dict(), best_model_save_path)\n",
        "print(f\"Model saved to: {best_model_save_path}\")\n",
        "\n",
        "# --- 7. Inference (Example) ---\n",
        "def generate_response(model, tokenizer, input_text, device, max_output_len=128):\n",
        "    model.eval()\n",
        "    input_tokens = tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_len,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    input_ids = input_tokens[\"input_ids\"]\n",
        "    attention_mask = input_tokens[\"attention_mask\"]\n",
        "    # Start with BOS token\n",
        "    output_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long, device=device)\n",
        "    for _ in range(max_output_len):\n",
        "        tgt_mask = generate_square_subsequent_mask(output_ids.size(1), device)\n",
        "        src_mask = torch.zeros((input_ids.size(0), input_ids.size(1)), device=device).bool()\n",
        "        tgt_padding_mask = (output_ids == tokenizer.pad_token_id)\n",
        "        src_padding_mask = (input_ids == tokenizer.pad_token_id)\n",
        "        memory_key_padding_mask = src_padding_mask\n",
        "        with torch.no_grad():\n",
        "            predicted_tokens = model(input_ids, output_ids, src_mask, tgt_mask,\n",
        "                                    src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        next_token_id = torch.argmax(predicted_tokens[:, -1, :], dim=-1)\n",
        "        next_token = next_token_id.unsqueeze(0)\n",
        "        output_ids = torch.cat([output_ids, next_token], dim=1)\n",
        "        if next_token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output_text\n",
        "\n",
        "# Load the trained model\n",
        "model.load_state_dict(torch.load(best_model_save_path))\n",
        "model.eval()\n",
        "\n",
        "# Example Usage\n",
        "queries = [\n",
        "    \"I want to visit Paris tomorrow morning.\",\n",
        "    \"Explore London in the evening.\",\n",
        "    \"See New York today.\",\n",
        "    \"What is the weather like?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    response = generate_response(model, tokenizer, query, device, max_output_len=64)\n",
        "    print(f\"Query: {query}\\nResponse: {response}\\n\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "I_18j_ciwkOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM074WBAzYYsF5fZsXll/u9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/VISCUNA_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lho5f2ZKQ9v",
        "outputId": "6be15d94-139e-4dc4-b6b3-1a4467f45ffb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_JhG6fQJJW_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Specify the model name on Hugging Face\n",
        "model_name = \"lmsys/vicuna-7b-v1.5\"  # Or another Vicuna variant\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,  # Adjust if needed\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Adjust if needed\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Adjust if needed\n",
        ")\n",
        "\n",
        "# Load the tokenizer and model with the quantization config\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"  # Or specify your device map\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "The old wizard, Gandalf, gazed out from his tower atop the highest peak of the Misty Mountains.\n",
        "A storm raged below, and he knew in his heart that darkness was stirring in the land.\n",
        "He thought of the young hobbit, Frodo, who had been entrusted with a perilous quest,\n",
        "a quest that could determine the fate of Middle-earth. Gandalf sighed, wondering...\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize the input and move to the device\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    #outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.9, top_p=0.6)\n",
        "    outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UX91GssNFB7",
        "outputId": "7d920a1d-b0b1-4f48-8a01-2e3040ad1eff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The old wizard, Gandalf, gazed out from his tower atop the highest peak of the Misty Mountains. \n",
            "A storm raged below, and he knew in his heart that darkness was stirring in the land. \n",
            "He thought of the young hobbit, Frodo, who had been entrusted with a perilous quest, \n",
            "a quest that could determine the fate of Middle-earth. Gandalf sighed, wondering...\n",
            "\n",
            "What if I were to take on this task myself? Could I not do better than poor little Hobbit?\n",
            "But then again, there are powers beyond my own that seek to destroy this ring.\n",
            "It is best for him to bear it himself, lest we all suffer its terrible power.\n",
            "Still, how can such a small creature possibly carry the weight of destiny upon his shoulders?\n",
            "Gandalf shook his head, knowing full well the answer: only through courage, wit, and the help of friends will our hero find success.\n"
          ]
        }
      ]
    }
  ]
}
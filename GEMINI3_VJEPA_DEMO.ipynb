{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMe/JipDlvhLnn5zHZPpyUc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/GEMINI3_VJEPA_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av -q"
      ],
      "metadata": {
        "id": "swfqaBXkeC2o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M_pJFepdk3V",
        "outputId": "b43cffa9-2492-4c1b-c92d-b96d09ed3093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Instantiating Models and Optimizers ---\n",
            "Models instantiated and moved to cuda.\n",
            "Cell 1 setup complete for conceptual flight planning.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Conceptual Modifications - Aviation Data Definitions\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import av\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "import datetime\n",
        "import pytz\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s')\n",
        "\n",
        "\n",
        "class AgentConfig:\n",
        "    # --- REFLECTS NEW GEMINI MODEL ID ---\n",
        "    LLM_MODEL_NAME: str = \"gemini-3-pro-preview\"\n",
        "    CLASS_LABELS = [\n",
        "        \"airplane landing\",\n",
        "        \"airplane takeoff\",\n",
        "        \"airport ground operations\",\n",
        "        \"in-flight cruise\",\n",
        "        \"emergency landing\",\n",
        "        \"pre-flight check/maintenance\",\n",
        "        \"en-route cruise\",\n",
        "        \"climb phase\",\n",
        "        \"descent phase\",\n",
        "        \"holding pattern\"\n",
        "    ]\n",
        "\n",
        "# Define num_classes globally\n",
        "num_classes = len(AgentConfig.CLASS_LABELS)\n",
        "\n",
        "# --- FIX: CLASSIFIER_SAVE_PATH moved to global scope ---\n",
        "CLASSIFIER_SAVE_PATH = \"classifier_head_trained_on_tartan_aviation_sample.pth\"\n",
        "\n",
        "AIRPORTS = {\n",
        "    \"CYUL\": {\"name\": \"Montreal-Trudeau International\", \"lat\": 45.4706, \"lon\": -73.7408, \"elevation_ft\": 118},\n",
        "    \"LFPG\": {\"name\": \"Paris-Charles de Gaulle\", \"lat\": 49.0097, \"lon\": 2.5479, \"elevation_ft\": 392},\n",
        "}\n",
        "\n",
        "AIRCRAFT_PERFORMANCE = {\n",
        "    \"Boeing777_300ER\": {\n",
        "        \"cruise_speed_knots\": 490,\n",
        "        \"fuel_burn_kg_per_hour\": 7000,\n",
        "        \"max_range_nm\": 7900,\n",
        "        \"climb_rate_fpm\": 2500,\n",
        "        \"descent_rate_fpm\": 2000,\n",
        "        \"typical_cruise_altitude_ft\": 37000,\n",
        "        \"fuel_capacity_kg\": 145000\n",
        "    }\n",
        "}\n",
        "\n",
        "hf_repo = \"facebook/vjepa2-vitg-fpc64-256\"\n",
        "EXTRACTED_FEATURES_DIR = \"/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/\"\n",
        "\n",
        "TOTAL_FLATTENED_VJEPA_DIM = 2048 * 1408\n",
        "\n",
        "CONCEPTUAL_PLDM_LATENT_DIM = 1024\n",
        "\n",
        "latent_dim_pldm = CONCEPTUAL_PLDM_LATENT_DIM\n",
        "action_dim = 8\n",
        "\n",
        "def load_and_process_video(video_path, processor_instance, model_instance, device_instance, num_frames_to_sample=16):\n",
        "    \"\"\"\n",
        "    Loads a video, samples frames, and extracts V-JEPA features.\n",
        "    Returns extracted features (torch.Tensor, shape like [1, 2048, 1408]) and the frames.\n",
        "    Does NOT flatten the V-JEPA output here, keeping it as model's raw output.\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    if not os.path.exists(video_path):\n",
        "        logging.error(f\"ERROR: Video file '{video_path}' not found.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        container = av.open(video_path)\n",
        "        total_frames_in_video = container.streams.video[0].frames\n",
        "        sampling_interval = max(1, total_frames_in_video // num_frames_to_sample)\n",
        "        logging.info(f\"Total frames in video: {total_frames_in_video}\")\n",
        "        logging.info(f\"Sampling interval: {sampling_interval} frames\")\n",
        "\n",
        "        for i, frame in enumerate(container.decode(video=0)):\n",
        "            if len(frames) >= num_frames_to_sample:\n",
        "                break\n",
        "            if i % sampling_interval == 0:\n",
        "                img = frame.to_rgb().to_ndarray()\n",
        "                frames.append(img)\n",
        "\n",
        "        if not frames:\n",
        "            logging.error(f\"ERROR: No frames could be loaded from '{video_path}'.\")\n",
        "            return None, None\n",
        "        elif len(frames) < num_frames_to_sample:\n",
        "            logging.warning(f\"WARNING: Only {len(frames)} frames loaded. Requested: {num_frames_to_sample}.\")\n",
        "\n",
        "        inputs = processor_instance(videos=list(frames), return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device_instance) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model_instance(**inputs).last_hidden_state\n",
        "\n",
        "        logging.info(f\"Successfully extracted V-JEPA features with raw shape: {features.shape}\")\n",
        "        return features, frames\n",
        "\n",
        "    except av.FFmpegError as e:\n",
        "        logging.error(f\"Error loading video with PyAV: {e}\")\n",
        "        logging.error(\"This might indicate an issue with the video file itself or PyAV installation.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred: {e}\")\n",
        "        logging.error(\"Ensure 'av' library is installed (`pip install av`) and video file is not corrupt.\")\n",
        "        return None, None\n",
        "\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(self.relu(self.fc1(x))))\n",
        "\n",
        "class LatentDynamicsPredictor(torch.nn.Module):\n",
        "    def __init__(self, latent_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_state, action):\n",
        "        combined_input = torch.cat([latent_state, action], dim=-1)\n",
        "        predicted_next_latent_state = self.layers(combined_input)\n",
        "        return predicted_next_latent_state\n",
        "\n",
        "class LatentProjector_old(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.projector = nn.Linear(input_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.projector(x))\n",
        "\n",
        "\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, input_dim=4, output_dim=1024):\n",
        "        super().__init__()\n",
        "        self.projector = nn.Linear(input_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "print(\"\\n--- Instantiating Models and Optimizers ---\")\n",
        "model = AutoModel.from_pretrained(hf_repo)\n",
        "processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "latent_projector = LatentProjector(TOTAL_FLATTENED_VJEPA_DIM, CONCEPTUAL_PLDM_LATENT_DIM)\n",
        "latent_projector.to(device)\n",
        "\n",
        "predictor = LatentDynamicsPredictor(latent_dim_pldm, action_dim)\n",
        "predictor.to(device)\n",
        "optimizer_pldm = torch.optim.Adam(list(predictor.parameters()) + list(latent_projector.parameters()), lr=0.001)\n",
        "\n",
        "classifier = ClassifierHead(input_dim=1408, num_classes=num_classes)\n",
        "classifier.to(device)\n",
        "\n",
        "print(f\"Models instantiated and moved to {device}.\")\n",
        "print(\"Cell 1 setup complete for conceptual flight planning.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import types\n",
        "\n",
        "# --- Setup ---\n",
        "# 1. Retrieve the API key securely from Colab's user data\n",
        "# NOTE: The secret name is assumed to be 'GEMINI' in Colab Secrets\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"Error: 'GEMINI' API key not found in Colab Secrets.\")\n",
        "    # You may choose to exit here if the key is mandatory\n",
        "\n",
        "try:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "\n",
        "# 2. Change the model identifier to target Gemini 3 Pro Preview\n",
        "MODEL_ID = 'gemini-3-pro-preview'\n",
        "\n",
        "# 3. Initialize the GenerativeModel object with the correct ID\n",
        "try:\n",
        "    # The global model object for generate_content()\n",
        "    gemini_model = genai.GenerativeModel(MODEL_ID)\n",
        "    print(f\"Gemini client successfully configured for model: {MODEL_ID}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Gemini Model: {e}\")\n",
        "\n",
        "# Update AgentConfig (although it's already set in Cell 1, this confirms it)\n",
        "# Assuming AgentConfig is available globally from Cell 1.\n",
        "try:\n",
        "    AgentConfig.LLM_MODEL_NAME = MODEL_ID\n",
        "except NameError:\n",
        "    print(\"WARNING: AgentConfig not yet defined. Proceeding with global variable setting.\")\n",
        "    # For safety if Cell 1 wasn't run first:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XQbKrJGeaVO",
        "outputId": "70b107bd-c390-489b-b12a-cc868dd957d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini client successfully configured for model: gemini-3-pro-preview.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2: Core Execution Feature Extraction, Classifier Training & Inference, LLM Interaction, and PLDM Training/Planning\n",
        "# This cell assumes Cell 1 has been successfully executed in the current session.\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import json\n",
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import datetime\n",
        "import pytz\n",
        "# Ensure genai is imported if you run this cell independently\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import types\n",
        "\n",
        "#Mounting Google Drive\n",
        "print(\"\\n--- Cell 2: Mounting Google Drive for dataset access ---\")\n",
        "drive.mount('/content/gdrive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "print(f\"Checking for extracted features directory: {EXTRACTED_FEATURES_DIR}\")\n",
        "if not os.path.exists(EXTRACTED_FEATURES_DIR):\n",
        "    logging.error(f\"ERROR: Extracted features directory '{EXTRACTED_FEATURES_DIR}' not found. Please create it and upload V-JEPA features.\")\n",
        "    # Note: In a real notebook, you would continue the flow. Using 'exit()' is aggressive.\n",
        "    # For a demo, we assume the directory will be created or is not strictly required for the classifier part.\n",
        "else:\n",
        "    print(f\"Extracted features directory found at {EXTRACTED_FEATURES_DIR}\")\n",
        "\n",
        "# Part 1: Load and process airplane-landing.mp4 for initial observation\n",
        "print(f\"\\n--- Cell 2: Part 1 - Loading actual video '/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4' for feature extraction ---\")\n",
        "flight_video_path = '/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4'\n",
        "video_features_for_inference_raw, frames_for_pldm_planning = load_and_process_video(flight_video_path, processor, model, device_instance=device)\n",
        "\n",
        "# -- CRITICAL: Process raw V-JEPA features to match ClassifierHead's expected input_dim --\n",
        "if video_features_for_inference_raw is not None:\n",
        "    pooled_features_for_classifier = video_features_for_inference_raw.squeeze(0).mean(dim=0).unsqueeze(0)\n",
        "    extracted_embedding_dim_for_classifier = pooled_features_for_classifier.shape[-1]\n",
        "    logging.info(f\"Dynamically determined extracted_embedding_dim for ClassifierHead: {extracted_embedding_dim_for_classifier}\")\n",
        "else:\n",
        "    pooled_features_for_classifier = None\n",
        "    extracted_embedding_dim_for_classifier = -1\n",
        "    logging.error(\"Failed to extract video features for classifier. Skipping inference part.\")\n",
        "    # Set a flag to skip later parts that depend on this\n",
        "    skip_inference = True\n",
        "\n",
        "\n",
        "# Part 2: Classifier Training (Code remains the same as original)\n",
        "print(f\"\\n--- Cell 2: Part 2 - Starting Classifier Training ---\")\n",
        "try:\n",
        "    # Re-initialize classifier with the correct, dynamically determined input_dim\n",
        "    if extracted_embedding_dim_for_classifier != -1:\n",
        "        classifier = ClassifierHead(input_dim=extracted_embedding_dim_for_classifier, num_classes=num_classes)\n",
        "        classifier.to(device)\n",
        "\n",
        "        # ... (Data loading and training logic from original Cell 2) ...\n",
        "        train_features_list = []\n",
        "        train_labels_list = []\n",
        "\n",
        "        map_file_path = os.path.join(EXTRACTED_FEATURES_DIR, \"feature_label_map.json\")\n",
        "        if not os.path.exists(map_file_path):\n",
        "            logging.warning(f\"Feature-label map file '{map_file_path}' not found. Generating synthetic data.\")\n",
        "            feature_label_map = {}\n",
        "        else:\n",
        "            with open(map_file_path, 'r') as f:\n",
        "                feature_label_map = json.load(f)\n",
        "\n",
        "        if not feature_label_map:\n",
        "            logging.warning(f\"Feature-label map at {map_file_path} is empty. Generating synthetic data.\")\n",
        "            num_training_samples = 2_000_000\n",
        "            # Synthetic data generation uses the dynamically determined input_dim\n",
        "            train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier)\n",
        "            train_labels = torch.randint(0, num_classes, (num_training_samples,))\n",
        "            train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=32, shuffle=True)\n",
        "            val_loader = None\n",
        "            print(f\"Loaded {num_training_samples} SYNTHETIC features for training.\")\n",
        "        else:\n",
        "            for item in tqdm(feature_label_map, desc=\"Loading real V-JEPA features\"):\n",
        "                feature_path = item['feature_path']\n",
        "                label_idx = item['label_idx']\n",
        "                try:\n",
        "                    if not os.path.isabs(feature_path):\n",
        "                        feature_path = os.path.join(EXTRACTED_FEATURES_DIR, feature_path)\n",
        "\n",
        "                    if not os.path.exists(feature_path):\n",
        "                        logging.warning(f\"Feature file not found at {feature_path}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    feature = torch.load(feature_path, map_location=device)\n",
        "\n",
        "                    # Match your working code's pooling/squashing logic to get [1408] dim\n",
        "                    if feature.ndim == 3:\n",
        "                        feature = feature.squeeze(0).mean(dim=0)\n",
        "                    elif feature.ndim == 2:\n",
        "                        if feature.shape[0] == 1 and feature.shape[1] == 1408:\n",
        "                            feature = feature.squeeze(0)\n",
        "                        elif feature.shape[1] == 1408:\n",
        "                            feature = feature.mean(dim=0)\n",
        "                        else:\n",
        "                            feature = feature.flatten()\n",
        "                    elif feature.ndim == 1:\n",
        "                        pass\n",
        "                    else:\n",
        "                        logging.warning(f\"Skipping malformed feature from {feature_path}. Unexpected dimensions: {feature.ndim}\")\n",
        "                        continue\n",
        "\n",
        "                    # Final check after processing. Should be 1D with 1408 elements.\n",
        "                    if feature.shape[0] != extracted_embedding_dim_for_classifier:\n",
        "                        logging.warning(f\"Skipping feature at {feature_path}. Dimension mismatch: expected {extracted_embedding_dim_for_classifier}, got {feature.shape[0]}.\")\n",
        "                        continue\n",
        "\n",
        "                    train_features_list.append(feature)\n",
        "                    train_labels_list.append(label_idx)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error loading feature from {feature_path}: {e}. Skipping.\")\n",
        "\n",
        "            if train_features_list:\n",
        "                train_features = torch.stack(train_features_list).to(device)\n",
        "                train_labels = torch.tensor(train_labels_list).to(device)\n",
        "                num_training_samples = len(train_features)\n",
        "                print(f\"Loaded {num_training_samples} REAL V-JEPA features for training.\")\n",
        "\n",
        "                if num_training_samples < 2:\n",
        "                    print(\"WARNING: Only 1 real V-JEPA feature loaded. Training may be unstable. Consider more data.\")\n",
        "                    train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=1, shuffle=True)\n",
        "                    val_loader = None\n",
        "                else:\n",
        "                    dataset_size = len(train_features)\n",
        "                    train_size = int(0.8 * dataset_size)\n",
        "                    val_size = dataset_size - train_size\n",
        "\n",
        "                    if val_size == 0 and train_size > 0:\n",
        "                        train_size = dataset_size\n",
        "                        train_dataset_real = TensorDataset(train_features, train_labels)\n",
        "                        val_dataset_real = None\n",
        "                    else:\n",
        "                        train_dataset_real, val_dataset_real = torch.utils.data.random_split(\n",
        "                            TensorDataset(train_features, train_labels), [train_size, val_size]\n",
        "                        )\n",
        "                    train_loader = DataLoader(train_dataset_real, batch_size=32, shuffle=True)\n",
        "                    val_loader = DataLoader(val_dataset_real, batch_size=32, shuffle=False) if val_dataset_real else None\n",
        "                    print(f\"Training on {len(train_dataset_real)} samples, Validation on {len(val_dataset_real) if val_dataset_real else 0} samples.\")\n",
        "            else:\n",
        "                logging.error(\"No real V-JEPA features could be loaded from map file. Generating synthetic data as fallback.\")\n",
        "                num_training_samples = 2_000_000\n",
        "                train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier)\n",
        "                train_labels = torch.randint(0, num_classes, (num_training_samples,))\n",
        "                train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=32, shuffle=True)\n",
        "                val_loader = None\n",
        "                print(f\"Loaded {num_training_samples} SYNTHETIC features for training as fallback.\")\n",
        "\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "        num_epochs = 20\n",
        "        for epoch in range(num_epochs):\n",
        "            classifier.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer_classifier.zero_grad()\n",
        "                outputs = classifier(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer_classifier.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "            val_loss = 0.0\n",
        "            if val_loader and len(val_loader.dataset) > 0:\n",
        "                classifier.eval()\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = classifier(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item()\n",
        "                val_loss /= len(val_loader.dataset)\n",
        "                logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "            else:\n",
        "                logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
        "                print(\"No validation data available or validation dataset is empty.\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Classifier Training Complete ---\")\n",
        "        torch.save(classifier.state_dict(), CLASSIFIER_SAVE_PATH)\n",
        "        print(f\"Classifier saved to: {CLASSIFIER_SAVE_PATH}\")\n",
        "    else:\n",
        "        print(\"Skipping classifier training due to feature extraction failure.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error during classifier training: {e}\")\n",
        "\n",
        "# --- LLM Interaction (Part 3) is moved to a separate cell for clarity and dependency ---\n",
        "print(\"Cell 2 execution complete.\")"
      ],
      "metadata": {
        "id": "OPJw_nQReiq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Classifier saved to: {CLASSIFIER_SAVE_PATH}\")\n",
        "print(\"Cell 2 execution complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdi3T2dcipaw",
        "outputId": "2fd3f7f3-d4f9-4270-a734-9f5fcf40b0bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier saved to: classifier_head_trained_on_tartan_aviation_sample.pth\n",
            "Cell 2 execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- REFACTORED LLM INTERACTION (Part 3) ---\n",
        "\n",
        "# This cell assumes the GEMINI API setup and model training (Cell 1 & Cell 2 parts 1/2) have run.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import pytz\n",
        "import datetime\n",
        "# Imports required for the execution\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import types\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting V-JEPA Feature-Driven Classification Inference and GEMINI LLM Interaction ---\")\n",
        "\n",
        "# Check if pooled_features_for_classifier is available\n",
        "if 'pooled_features_for_classifier' not in globals() or pooled_features_for_classifier is None:\n",
        "    logging.error(\"ERROR: Cannot perform LLM interaction as 'pooled_features_for_classifier' is missing or None.\")\n",
        "else:\n",
        "    try:\n",
        "        pooled_features_for_inference_on_device = pooled_features_for_classifier.to(device)\n",
        "\n",
        "        # Assuming classifier is defined and CLASSIFIER_SAVE_PATH is accessible\n",
        "        try:\n",
        "            classifier.load_state_dict(torch.load(CLASSIFIER_SAVE_PATH, map_location=device))\n",
        "            logging.info(f\"Classifier weights loaded from: {CLASSIFIER_SAVE_PATH}\")\n",
        "        except NameError:\n",
        "             logging.error(\"Classifier model not found in global scope. Re-initialize or check execution flow.\")\n",
        "             raise\n",
        "\n",
        "        classifier.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = classifier(pooled_features_for_inference_on_device)\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "        predicted_confidence = probabilities[0, predicted_class_idx].item()\n",
        "        predicted_label = AgentConfig.CLASS_LABELS[predicted_class_idx]\n",
        "\n",
        "        # --- LLM Input Description Logic (Simplified for brevity) ---\n",
        "        llm_input_description = f\"The visual system detected an airplane {predicted_label}. (Confidence: {predicted_confidence:.2f})\"\n",
        "\n",
        "        print(f\"\\n--- AI Agent's Understanding from Classifier ---\")\n",
        "        print(f\"**Primary Classification (Predicted by AI):** '{predicted_label}' (Confidence: {predicted_confidence:.2f})\")\n",
        "        print(f\"**Description for LLM:** {llm_input_description}\")\n",
        "\n",
        "        # --- Engaging GEMINI LLM for Further Reasoning ---\n",
        "        print(f\"\\n--- Engaging GEMINI LLM ({AgentConfig.LLM_MODEL_NAME}) for Further Reasoning ---\")\n",
        "        try:\n",
        "            if 'gemini_model' not in globals():\n",
        "                raise NameError(\"Gemini model object 'gemini_model' is not defined.\")\n",
        "\n",
        "            prompt_for_gemini = f\"\"\"\n",
        "                  You are an AI assistant for flight planning operations.\n",
        "                  Current visual observation: {llm_input_description}\n",
        "                  Current time (EST): {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\n",
        "\n",
        "                  Based on this visual observation, provide a concise operational assessment relevant to flight planning.\n",
        "                  If the observation seems random or uncertain, state that. Do not add any conversational filler.\n",
        "                  \"\"\"\n",
        "\n",
        "            # --- GEMINI API CALL STRUCTURE (FINAL WORKING FIX) ---\n",
        "            # FIX: Only pass the 'contents' argument. This is the oldest/most minimal syntax.\n",
        "            gemini_response = gemini_model.generate_content(\n",
        "                contents=prompt_for_gemini\n",
        "            )\n",
        "\n",
        "            print(\"\\n--- GEMINI LLM Response ---\")\n",
        "            if gemini_response.text:\n",
        "                print(gemini_response.text)\n",
        "                print(\"--- GEMINI LLM Response - END ---\")\n",
        "                print('\\n')\n",
        "            else:\n",
        "                print(\"GEMINI LLM did not provide a text response or cannot provide one.\")\n",
        "\n",
        "        except NameError as name_e:\n",
        "            logging.error(f\"Error: {name_e}. Please run the Gemini API Setup cell.\")\n",
        "        except Exception as llm_e:\n",
        "            logging.error(f\"Error interacting with GEMINI LLM: {llm_e}\")\n",
        "            logging.error(\"Ensure your 'GEMINI' API key is correctly set in Colab Secrets and the model ID is valid.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during classification inference: {e}\")\n",
        "\n",
        "print(f\"The V-JEPA features (shape: {pooled_features_for_classifier.shape if pooled_features_for_classifier is not None else 'N/A'}) are the core input that a trained classifier would learn from.\")\n",
        "print(\"\\nLLM Interaction cell execution complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "rFrdqMXtezKA",
        "outputId": "ffa709c0-055c-48f0-e319-8042a576070f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting V-JEPA Feature-Driven Classification Inference and GEMINI LLM Interaction ---\n",
            "\n",
            "--- AI Agent's Understanding from Classifier ---\n",
            "**Primary Classification (Predicted by AI):** 'airplane landing' (Confidence: 1.00)\n",
            "**Description for LLM:** The visual system detected an airplane airplane landing. (Confidence: 1.00)\n",
            "\n",
            "--- Engaging GEMINI LLM (gemini-3-pro-preview) for Further Reasoning ---\n",
            "\n",
            "--- GEMINI LLM Response ---\n",
            "Confirmed aircraft landing detected. Update flight status to Actual Time of Arrival (ATA). Runway occupancy is active; prepare for taxiing, gate allocation, and turnaround servicing.\n",
            "--- GEMINI LLM Response - END ---\n",
            "\n",
            "\n",
            "The V-JEPA features (shape: torch.Size([1, 1408])) are the core input that a trained classifier would learn from.\n",
            "\n",
            "LLM Interaction cell execution complete.\n"
          ]
        }
      ]
    }
  ]
}
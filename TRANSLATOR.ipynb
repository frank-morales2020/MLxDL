{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPe+InZiokLfvr5bhcbUpmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/TRANSLATOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpU4w1IdLcaK",
        "outputId": "a9304a8a-1425-4dcb-c722-372c7c2d5796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 20 14:39:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "OB94nVqlPywz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# 1. SUPPRESS ALL WARNINGS (Corrected function name)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "from transformers import (\n",
        "    MBart50TokenizerFast,\n",
        "    MBartForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# 2. CLEANUP\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 3. LOAD MODEL\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# 4. VOCABULARY EXPANSION\n",
        "akk_token = \"[akk_AK]\"\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': [akk_token]})\n",
        "\n",
        "# FIX: mean_resizing=False stops the multivariate normal distribution warning\n",
        "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
        "tokenizer.lang_code_to_id[akk_token] = tokenizer.convert_tokens_to_ids(akk_token)\n",
        "\n",
        "# 5. DATASET\n",
        "data = {\n",
        "    \"akkadian\": [\"šarrum bītam iṣbat\", \"ilum ana bītim ittalak\", \"ekallam īpuš\"],\n",
        "    \"spanish\": [\"el rey tomó la casa\", \"el dios fue a la casa\", \"él construyó el palacio\"]\n",
        "}\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "def preprocess(examples):\n",
        "    tokenizer.src_lang = akk_token\n",
        "    tokenizer.tgt_lang = \"es_XX\"\n",
        "    model_inputs = tokenizer(examples[\"akkadian\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(text_target=examples[\"spanish\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_data = dataset.map(preprocess, batched=True)\n",
        "\n",
        "# 6. TRAINING ARGS\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./akkadian_translator_v3\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=100,\n",
        "    learning_rate=5e-5,\n",
        "    report_to=\"none\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=50,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# 7. TRAINER (Using 'processing_class' to avoid v5.0 warnings)\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "# 8. RUN\n",
        "print(\"Starting Clean Training...\")\n",
        "trainer.train()\n",
        "print(\"Training Finished!\")\n",
        "\n",
        "# 9. INFERENCE TEST\n",
        "def translate(text):\n",
        "    model.eval()\n",
        "    tokenizer.src_lang = akk_token\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(\n",
        "            **inputs,\n",
        "            forced_bos_token_id=tokenizer.lang_code_to_id[\"es_XX\"],\n",
        "            max_new_tokens=20,\n",
        "            do_sample=False\n",
        "        )\n",
        "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "print(f\"\\nAkkadian: šarrum bītam iṣbat\")\n",
        "print(f\"Español: {translate('šarrum bītam iṣbat')}\")"
      ],
      "metadata": {
        "id": "EcEtMbU8PqWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q"
      ],
      "metadata": {
        "id": "m11sYQquT8rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "\n",
        "# 1. AUTHENTICATION\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "if access_token_write:\n",
        "    login(token=access_token_write, add_to_git_credential=True)\n",
        "\n",
        "    # 2. RE-SAVE WITH ALL NECESSARY FILES\n",
        "    # We save to a fresh directory to ensure no missing vocabulary files\n",
        "    local_save_path = \"./akkadian_final_export\"\n",
        "    print(f\"Saving a complete copy to {local_save_path}...\")\n",
        "\n",
        "    # These commands ensure both the weights and the sentencepiece model are saved\n",
        "    model.save_pretrained(local_save_path)\n",
        "    tokenizer.save_pretrained(local_save_path)\n",
        "\n",
        "    # 3. EXPORT TO HUGGING FACE\n",
        "    repo_id = \"frankmorales2020/kkadian-to-spanish-translator\"\n",
        "    print(f\"Pushing to Hugging Face: {repo_id}...\")\n",
        "\n",
        "    # Pushing directly ensures the custom [akk_AK] token is preserved\n",
        "    model.push_to_hub(repo_id)\n",
        "    tokenizer.push_to_hub(repo_id)\n",
        "\n",
        "    print(f\"Successfully exported! URL: https://huggingface.co/{repo_id}\")\n",
        "else:\n",
        "    print(\"Error: HUGGINGFACE_ACCESS_TOKEN_WRITE not found.\")"
      ],
      "metadata": {
        "id": "0RC6F07STMpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it's missing\n",
        "os.makedirs(\"./akkadian_translator_v3\", exist_ok=True)\n",
        "\n",
        "# Manually save the model and tokenizer currently in memory\n",
        "print(\"Saving model and tokenizer to ./akkadian_translator_v3...\")\n",
        "model.save_pretrained(\"./akkadian_translator_v3\")\n",
        "tokenizer.save_pretrained(\"./akkadian_translator_v3\")\n",
        "\n",
        "# Verify the files are now present\n",
        "print(f\"Files now in directory: {os.listdir('./akkadian_translator_v3')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0TH4Xx1jQQt",
        "outputId": "5ced2d5b-c73d-49a0-ae82-c8a3db444427"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model and tokenizer to ./akkadian_translator_v3...\n",
            "Files now in directory: ['special_tokens_map.json', 'tokenizer_config.json', 'config.json', 'model.safetensors', 'generation_config.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force the tokenizer to save its source vocabulary file\n",
        "tokenizer.save_vocabulary(\"./akkadian_translator_v3\")\n",
        "\n",
        "# Verify again - you MUST see 'sentencepiece.bpe.model' in this list\n",
        "print(f\"Updated files: {os.listdir('./akkadian_translator_v3')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itQtFHM-kB0-",
        "outputId": "315bb242-3a3d-46f6-e5af-e26c6b771c6e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated files: ['special_tokens_map.json', 'tokenizer_config.json', 'config.json', 'model.safetensors', 'generation_config.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"frankmorales2020/kkadian-to-spanish-translator\"\n",
        "local_path = \"./akkadian_translator_v3\"\n",
        "\n",
        "# Push every file to the Hub, ensuring we overwrite the incomplete versions\n",
        "for file_name in os.listdir(local_path):\n",
        "    file_path = os.path.join(local_path, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        print(f\"Force uploading {file_name}...\")\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=file_name,\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "\n",
        "print(f\"\\nSuccess! Your model is now fully functional at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "52XhK7tyjV4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import MBart50TokenizerFast\n",
        "from huggingface_hub import hf_hub_download, HfApi\n",
        "\n",
        "repo_id = \"frankmorales2020/kkadian-to-spanish-translator\"\n",
        "local_dir = \"./fixed_tokenizer\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# 1. Descargar el vocabulario base de mBART-50 (el archivo que falta)\n",
        "print(\"Obteniendo vocabulario base...\")\n",
        "vocab_path = hf_hub_download(repo_id=\"facebook/mbart-large-50-many-to-many-mmt\", filename=\"sentencepiece.bpe.model\")\n",
        "\n",
        "# 2. Cargar el tokenizer base y añadir tu token especial\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['[akk_AK]']})\n",
        "\n",
        "# 3. Guardar todo en la carpeta local\n",
        "tokenizer.save_pretrained(local_dir)\n",
        "import shutil\n",
        "shutil.copy(vocab_path, os.path.join(local_dir, \"sentencepiece.bpe.model\"))\n",
        "\n",
        "# 4. SUBIDA FORZADA A HUGGING FACE\n",
        "print(\"Subiendo archivos corregidos...\")\n",
        "api = HfApi()\n",
        "for file_name in os.listdir(local_dir):\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=os.path.join(local_dir, file_name),\n",
        "        path_in_repo=file_name,\n",
        "        repo_id=repo_id\n",
        "    )\n",
        "print(\"¡Hecho! Ahora el repositorio tiene el archivo .model necesario.\")"
      ],
      "metadata": {
        "id": "X8vAh16Ql7se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HF TESTING"
      ],
      "metadata": {
        "id": "dM7oEGsxnQry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# 1. Repository Configuration\n",
        "repo_id = \"frankmorales2020/kkadian-to-spanish-translator\"\n",
        "\n",
        "print(f\"Loading corrected model and tokenizer from {repo_id}...\")\n",
        "\n",
        "# 2. Explicit Loading\n",
        "# We use MBart50TokenizerFast to ensure compatibility with the updated files\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(repo_id)\n",
        "model = MBartForConditionalGeneration.from_pretrained(repo_id)\n",
        "\n",
        "def translate_from_hf(text):\n",
        "    # Set the custom source language token you added during training\n",
        "    tokenizer.src_lang = \"[akk_AK]\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Dynamically retrieve the ID for Spanish\n",
        "    es_id = tokenizer.convert_tokens_to_ids(\"es_XX\")\n",
        "\n",
        "    # Generate the translation\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(\n",
        "            **inputs,\n",
        "            forced_bos_token_id=es_id,\n",
        "            max_new_tokens=50\n",
        "        )\n",
        "\n",
        "    # Decode to readable text\n",
        "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHuJDWHLkAm0",
        "outputId": "64a2310e-6bb8-4a18-c416-5bb837eb5b25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corrected model and tokenizer from frankmorales2020/kkadian-to-spanish-translator...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Final Test\n",
        "akkadian_phrase = \"šarrum bītam iṣbat\"\n",
        "print(\"-\" * 30)\n",
        "print(f\"Akkadian: {akkadian_phrase}\")\n",
        "print(f\"Spanish:  {translate_from_hf(akkadian_phrase)}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvx7d1FxnBT9",
        "outputId": "76ecd408-5b65-41f0-8950-23590fa4a16c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Akkadian: šarrum bītam iṣbat\n",
            "Spanish:  el rey tomó la casa\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "repo_id = \"frankmorales2020/kkadian-to-spanish-translator\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(repo_id)\n",
        "model = MBartForConditionalGeneration.from_pretrained(repo_id)\n",
        "\n",
        "def translate(text):\n",
        "    tokenizer.src_lang = \"[akk_AK]\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Force Spanish output\n",
        "    es_id = tokenizer.convert_tokens_to_ids(\"es_XX\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(\n",
        "            **inputs,\n",
        "            forced_bos_token_id=es_id,\n",
        "            max_new_tokens=50,\n",
        "            num_beams=5\n",
        "        )\n",
        "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "# Verification\n",
        "print(f\"Final Result: {translate('šarrum bītam iṣbat')}\")"
      ],
      "metadata": {
        "id": "pxpmv0Vs_cuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification\n",
        "print(f\"Final Result: {translate('šarrum bītam iṣbat')}\")"
      ],
      "metadata": {
        "id": "wSDxc6-iAH_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# 1. Repository Configuration\n",
        "repo_id = \"frankmorales2020/kkadian-to-spanish-translator\"\n",
        "\n",
        "# 2. Load Model and Tokenizer\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(repo_id)\n",
        "model = MBartForConditionalGeneration.from_pretrained(repo_id)\n",
        "\n",
        "def translate_and_print(text):\n",
        "    # Setup for Akkadian source\n",
        "    tokenizer.src_lang = \"[akk_AK]\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Get ID for Spanish target\n",
        "    es_id = tokenizer.convert_tokens_to_ids(\"es_XX\")\n",
        "\n",
        "    # Generate translation\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(\n",
        "            **inputs,\n",
        "            forced_bos_token_id=es_id,\n",
        "            max_new_tokens=60,\n",
        "            num_beams=5\n",
        "        )\n",
        "\n",
        "    # Decode result\n",
        "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Print comparison\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"AKKADIAN: {text}\")\n",
        "    print(f\"SPANISH:  {translation}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# 3. Final Verification Run\n",
        "translate_and_print(\"šarrum bītam iṣbat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-KnwMtzAE8v",
        "outputId": "7d7ac4ce-ebbf-42db-86c3-f4b76c4ff07a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "AKKADIAN: šarrum bītam iṣbat\n",
            "SPANISH:  el rey tomó la casa\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "repo_id = \"frankmorales2020/kkadian-to-spanish-translator\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(repo_id)\n",
        "model = MBartForConditionalGeneration.from_pretrained(repo_id)\n",
        "\n",
        "def test_model(sentences):\n",
        "    tokenizer.src_lang = \"[akk_AK]\"\n",
        "    es_id = tokenizer.convert_tokens_to_ids(\"es_XX\")\n",
        "\n",
        "    for text in sentences:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                forced_bos_token_id=es_id,\n",
        "                max_new_tokens=60,\n",
        "                num_beams=5\n",
        "            )\n",
        "        translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"AKKADIAN: {text}\")\n",
        "        print(f\"SPANISH:  {translation}\")\n",
        "\n",
        "# List your test cases here\n",
        "examples = [\n",
        "    \"šarrum bītam iṣbat\",\n",
        "    \"ekallam īpuš\"\n",
        "]\n",
        "\n",
        "test_model(examples)\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOhMF70WAq2n",
        "outputId": "60ba655e-3685-4783-81c1-839d550b657d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "AKKADIAN: šarrum bītam iṣbat\n",
            "SPANISH:  el rey tomó la casa\n",
            "----------------------------------------\n",
            "AKKADIAN: ekallam īpuš\n",
            "SPANISH:  él construyó el palacio\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vcmSLOdkxdUb"
      ],
      "authorship_tag": "ABX9TyNfathYm9qHzixJ6b1CvTq+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FT_Mixtral_lambda_cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "vcmSLOdkxdUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m venv venv\n",
        "!source venv/bin/activate\n",
        "!pip install transformers torch bitsandbytes datasets evaluate accelerate  rouge_score peft\n",
        "!pip install  tensorboard"
      ],
      "metadata": {
        "id": "5ATR09oNxcYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tuning"
      ],
      "metadata": {
        "id": "akp9uZ4GxWvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4cJ0jy6wM-U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "import torch\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import gc\n",
        "\n",
        "# Example: Inside or after your training loop, if you suspect memory issues\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # Specifically for CUDA memory\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\", category=UserWarning)\n",
        "\n",
        "# ---------------------- Configuration ----------------------\n",
        "model_checkpoint = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "print('\\n')\n",
        "print(f\"Model base: {model_checkpoint}\")\n",
        "print('\\n')\n",
        "\n",
        "train_dataset_path = \"/home/ubuntu/work/cmapss_FD004_train_text.jsonl\"\n",
        "validation_dataset_path = \"/home/ubuntu/work/cmapss_FD004_test_text.jsonl\"\n",
        "output_dir = \"./fine-tuned-mixtral-peft-lambda\"\n",
        "per_device_train_batch_size = 2  # You might need to reduce this due to larger model size\n",
        "gradient_accumulation_steps = 4 # Adjust accordingly\n",
        "num_train_epochs = 5      # Start with fewer epochs and monitor\n",
        "learning_rate = 1e-5      # You might need to tune this\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 100\n",
        "max_seq_length = 512\n",
        "logging_steps = 10\n",
        "save_steps = 160\n",
        "eval_steps = 160\n",
        "evaluation_strategy = \"steps\"\n",
        "save_total_limit = 2\n",
        "fp16 = torch.cuda.is_available()\n",
        "gradient_checkpointing = False\n",
        "import gc\n",
        "\n",
        "# ---------------------- 1. Load Datasets ----------------------\n",
        "def load_jsonl_dataset(path):\n",
        "    data = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "train_dataset = load_jsonl_dataset(train_dataset_path)\n",
        "eval_dataset = load_jsonl_dataset(validation_dataset_path)\n",
        "\n",
        "print(f\"Size of the training dataset:  {len(train_dataset)} records\")\n",
        "print(f\"Size of the evaluation dataset: {len(eval_dataset)} records\")\n",
        "print('\\n')\n",
        "\n",
        "# ---------------------- 2. Load Tokenizer ----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# ---------------------- 3. Preprocess Data ----------------------\n",
        "def tokenize_function(examples):\n",
        "    prompts = []\n",
        "    responses = []\n",
        "\n",
        "    contents_list = examples['contents']\n",
        "\n",
        "    for item in contents_list:\n",
        "        try:\n",
        "            if (item and\n",
        "                len(item) == 2 and\n",
        "                item[0]['role'] == 'user' and\n",
        "                item[1]['role'] == 'model' and\n",
        "                item[0]['parts'] and\n",
        "                item[1]['parts'] and\n",
        "                item[0]['parts'][0]['text'] and\n",
        "                item[1]['parts'][0]['text']):\n",
        "                user_text = item[0]['parts'][0]['text']\n",
        "                if \"Engine sensor readings over time:\" in user_text:\n",
        "                    sensor_data = user_text.replace(\"Engine sensor readings over time: \", \"\")\n",
        "                else:\n",
        "                    sensor_data = user_text\n",
        "                prompts.append(f\"Predict the remaining useful life for this engine with sensor readings: {sensor_data}\")\n",
        "                responses.append(item[1]['parts'][0]['text'])\n",
        "            else:\n",
        "                print(f\"Skipping invalid data point: {item}\")\n",
        "                continue\n",
        "        except (KeyError, IndexError):\n",
        "            print(f\"Skipping invalid data point: {item}\")\n",
        "            continue\n",
        "\n",
        "    tokenized_prompts = tokenizer(prompts,\n",
        "                                 padding=\"max_length\",\n",
        "                                 truncation=True,\n",
        "                                 max_length=max_seq_length,\n",
        "                                 return_tensors=\"pt\")\n",
        "    tokenized_responses = tokenizer(responses,\n",
        "                                  padding=\"max_length\",\n",
        "                                  truncation=True,\n",
        "                                  max_length=max_seq_length,\n",
        "                                  return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(len(prompts)):\n",
        "        full_text = prompts[i] + tokenizer.eos_token + responses[i] + tokenizer.eos_token\n",
        "        tokenized_full_text = tokenizer.encode(full_text, max_length=max_seq_length, truncation=True)\n",
        "        current_input_ids = tokenized_full_text\n",
        "\n",
        "        vocab_size = tokenizer.vocab_size\n",
        "        max_index = max(current_input_ids) if current_input_ids else -1\n",
        "        min_index = min(current_input_ids) if current_input_ids else float('inf')\n",
        "\n",
        "        if max_index >= vocab_size or min_index < 0:\n",
        "            print(f\"Warning: Out-of-bounds index found in tokenized input (example {i}):\")\n",
        "            print(f\"  Max index: {max_index}, Vocabulary size: {vocab_size}\")\n",
        "            print(f\"  Min index: {min_index}\")\n",
        "\n",
        "        input_ids.append(current_input_ids)\n",
        "        attention_mask.append([1] * len(current_input_ids))\n",
        "        labels.append(current_input_ids.copy())\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "tokenized_train_datasets = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['contents'])\n",
        "tokenized_eval_datasets = eval_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['contents'])\n",
        "\n",
        "# ---------------------- 4. Data Collator ----------------------\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# ---------------------- 5. Load Model with PEFT ----------------------\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ---------------------- 6. Training Arguments ----------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    logging_steps=logging_steps,\n",
        "    save_steps=save_steps,\n",
        "    eval_steps=eval_steps,\n",
        "    eval_strategy=evaluation_strategy,\n",
        "    save_total_limit=2,\n",
        "    fp16=fp16,\n",
        "    gradient_checkpointing=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    label_names=[\"labels\"],\n",
        "    lr_scheduler_type=\"cosine\",\n",
        ")\n",
        "\n",
        "# ---------------------- 7. Trainer ----------------------\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "rouge_metric = load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), axis=-1)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "    prediction_lens = torch.sum(predictions != tokenizer.pad_token_id, dim=1)\n",
        "    result[\"gen_len\"] = torch.mean(prediction_lens.float()).item()\n",
        "    return result\n",
        "\n",
        "# ---------------------- 7. Trainer ----------------------\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_eval_datasets,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "# ---------------------- 8. Train ----------------------\n",
        "trainer.train()\n",
        "\n",
        "# ---------------------- 9. Save Model ----------------------\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "print(f\"Fine-tuning complete! Model saved to: {output_dir}\")\n",
        "\n",
        "# Example: Inside or after your training loop, if you suspect memory issues\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # Specifically for CUDA memory\n"
      ]
    }
  ]
}
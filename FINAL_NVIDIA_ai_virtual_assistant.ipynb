{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBWfnRwd8okCy89o0reY2x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FINAL_NVIDIA_ai_virtual_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/tutorials/quickstart/quickstart.ipynb"
      ],
      "metadata": {
        "id": "smb9uSDnjum4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNj-Fqg1hCyk",
        "outputId": "af7cb5b7-5727-4252-c422-3cc687ea7d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-virtual-assistant'...\n",
            "remote: Enumerating objects: 543, done.\u001b[K\n",
            "remote: Counting objects: 100% (223/223), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 543 (delta 128), reused 77 (delta 77), pack-reused 320 (from 2)\u001b[K\n",
            "Receiving objects: 100% (543/543), 2.08 MiB | 27.25 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA-AI-Blueprints/ai-virtual-assistant.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key = userdata.get('NVIDIA_API_KEY')\n",
        "import os\n",
        "os.environ['NVIDIA_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "Lq0SI-dt43lW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ai-virtual-assistant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DO1CGiQiKq1",
        "outputId": "6457ac96-7402-4af1-8c5e-4da1e38435d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ai-virtual-assistant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Native Postgres and Redis"
      ],
      "metadata": {
        "id": "XThwTdzL91-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, subprocess, time\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. INSTALL POSTGRES & REDIS NATIVELY (Bypass Udocker for Infra)\n",
        "print(\"ğŸš€ Installing Native Postgres and Redis...\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y postgresql redis-server -qq\n",
        "!service postgresql start\n",
        "!service redis-server start\n",
        "\n",
        "# Configure Native Postgres\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'password';\"\n",
        "!sudo -u postgres p"
      ],
      "metadata": {
        "id": "PcX4LGfY5W0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b527ee-f66c-402c-a9b2-0019a6129f82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Installing Native Postgres and Redis...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libjemalloc2:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libjemalloc2_5.2.1-4ubuntu1_amd64.deb ...\n",
            "Unpacking libjemalloc2:amd64 (5.2.1-4ubuntu1) ...\n",
            "Selecting previously unselected package liblua5.1-0:amd64.\n",
            "Preparing to unpack .../01-liblua5.1-0_5.1.5-8.1build4_amd64.deb ...\n",
            "Unpacking liblua5.1-0:amd64 (5.1.5-8.1build4) ...\n",
            "Selecting previously unselected package liblzf1:amd64.\n",
            "Preparing to unpack .../02-liblzf1_3.6-3_amd64.deb ...\n",
            "Unpacking liblzf1:amd64 (3.6-3) ...\n",
            "Selecting previously unselected package lua-bitop:amd64.\n",
            "Preparing to unpack .../03-lua-bitop_1.0.2-5_amd64.deb ...\n",
            "Unpacking lua-bitop:amd64 (1.0.2-5) ...\n",
            "Selecting previously unselected package lua-cjson:amd64.\n",
            "Preparing to unpack .../04-lua-cjson_2.1.0+dfsg-2.1_amd64.deb ...\n",
            "Unpacking lua-cjson:amd64 (2.1.0+dfsg-2.1) ...\n",
            "Selecting previously unselected package redis-tools.\n",
            "Preparing to unpack .../05-redis-tools_5%3a6.0.16-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking redis-tools (5:6.0.16-1ubuntu1.1) ...\n",
            "Selecting previously unselected package redis-server.\n",
            "Preparing to unpack .../06-redis-server_5%3a6.0.16-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking redis-server (5:6.0.16-1ubuntu1.1) ...\n",
            "Selecting previously unselected package logrotate.\n",
            "Preparing to unpack .../07-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../08-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../09-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../10-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../11-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../12-libjson-xs-perl_4.040-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libllvm14:amd64.\n",
            "Preparing to unpack .../13-libllvm14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../14-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../15-postgresql-client-14_14.20-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.20-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../16-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../17-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../18-postgresql-14_14.20-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.20-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../19-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../20-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer â†’ /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up libjemalloc2:amd64 (5.2.1-4ubuntu1) ...\n",
            "Setting up lua-cjson:amd64 (2.1.0+dfsg-2.1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "Setting up liblzf1:amd64 (3.6-3) ...\n",
            "Setting up lua-bitop:amd64 (1.0.2-5) ...\n",
            "Setting up libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up liblua5.1-0:amd64 (5.1.5-8.1build4) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer â†’ /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer â†’ /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service â†’ /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Setting up postgresql-client-14 (14.20-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up redis-tools (5:6.0.16-1ubuntu1.1) ...\n",
            "Setting up postgresql-common (238) ...\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service â†’ /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.20-0ubuntu0.22.04.1) ...\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up redis-server (5:6.0.16-1ubuntu1.1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Created symlink /etc/systemd/system/redis.service â†’ /lib/systemd/system/redis-server.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/redis-server.service â†’ /lib/systemd/system/redis-server.service.\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "Starting redis-server: redis-server.\n",
            "ALTER ROLE\n",
            "sudo: p: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ensuring Postgres and Redis are running"
      ],
      "metadata": {
        "id": "-UrYS0kB7eR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "netstat -punta |egrep '5432|6379'"
      ],
      "metadata": {
        "id": "m89cxeqwP6-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# 1. CLEANUP & INSTALL\n",
        "print(\"ğŸ§¹ Clearing port 19530...\")\n",
        "!fuser -k 19530/tcp || true\n",
        "!pip install milvus-lite -q\n",
        "\n",
        "# 2. START NATIVE SERVICES (Postgres and Redis)\n",
        "print(\"ğŸš€ Ensuring Postgres and Redis are running...\")\n",
        "!service postgresql start\n",
        "!service redis-server start\n",
        "\n",
        "# 3. START MILVUS SERVER (The Python-Native Way)\n",
        "print(\"ğŸ”¥ Launching Milvus on Port 19530...\")\n",
        "# This uses the python module runner to ensure the code is found\n",
        "cmd = f\"nohup {sys.executable} -m milvus_lite.server --port 19530 > milvus_final.log 2>&1 &\"\n",
        "os.system(cmd)\n",
        "\n",
        "print(\"â³ Waiting 20s for initialization...\")\n",
        "time.sleep(20)\n",
        "\n",
        "# 4. FINAL INFRASTRUCTURE VERIFICATION\n",
        "print(\"\\n--- INFRASTRUCTURE STATUS ---\")\n",
        "!netstat -tulpn | grep -E '5432|6379|19530'\n",
        "\n",
        "# 5. DIAGNOSIS (If 19530 is still missing)\n",
        "if os.path.exists('milvus_final.log'):\n",
        "    with open('milvus_final.log', 'r') as f:\n",
        "        logs = f.read()\n",
        "        if \"Address already in use\" in logs:\n",
        "            print(\"âš ï¸ Port 19530 is already busy. Killing and retrying...\")\n",
        "        elif \"No module named\" in logs:\n",
        "             print(\"âŒ Module path error. Trying fallback...\")\n",
        "             os.system(f\"nohup {sys.executable} -m milvus_lite --port 19530 > milvus_final.log 2>&1 &\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rpJyMCeVtiM",
        "outputId": "d86854c0-0f13-4e20-9897-b3c891438aa5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§¹ Clearing port 19530...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hğŸš€ Ensuring Postgres and Redis are running...\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "Starting redis-server: redis-server.\n",
            "ğŸ”¥ Launching Milvus on Port 19530...\n",
            "â³ Waiting 20s for initialization...\n",
            "\n",
            "--- INFRASTRUCTURE STATUS ---\n",
            "tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      2891/postgres       \n",
            "tcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      2927/redis-server 1 \n",
            "tcp6       0      0 ::1:5432                :::*                    LISTEN      2891/postgres       \n",
            "tcp6       0      0 ::1:6379                :::*                    LISTEN      2927/redis-server 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Milvus components"
      ],
      "metadata": {
        "id": "wwfUqL4F8Kaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "print(\"ğŸ› ï¸ Installing Milvus components...\")\n",
        "!pip install -U pymilvus milvus-lite -q\n",
        "\n",
        "# 2. Restart and Configure Environment\n",
        "import os\n",
        "from pymilvus import MilvusClient\n",
        "\n",
        "# Create a local database file (the 'Lite' way)\n",
        "# This avoids Port 19530 'DOWN' errors entirely\n",
        "db_file = \"./milvus_quickstart.db\"\n",
        "client = MilvusClient(db_file)\n",
        "collection_name = \"quickstart_collection\"\n",
        "\n",
        "# 3. Create Collection\n",
        "print(f\"ğŸ“¦ Creating collection: {collection_name}\")\n",
        "if client.has_collection(collection_name):\n",
        "    client.drop_collection(collection_name)\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    dimension=5,  # Tutorial example dimension\n",
        ")\n",
        "\n",
        "# 4. Insert Sample Data\n",
        "print(\"ğŸ“¥ Inserting data...\")\n",
        "data = [\n",
        "    {\"id\": i, \"vector\": [i * 0.1] * 5, \"text\": f\"Entity {i}\"}\n",
        "    for i in range(10)\n",
        "]\n",
        "client.insert(collection_name=collection_name, data=data)\n",
        "\n",
        "# 5. Search (Verification)\n",
        "print(\"ğŸ” Searching for nearest neighbors...\")\n",
        "res = client.search(\n",
        "    collection_name=collection_name,\n",
        "    data=[[0.5] * 5],\n",
        "    limit=3,\n",
        "    output_fields=[\"text\"]\n",
        ")\n",
        "\n",
        "for result in res[0]:\n",
        "    print(f\"Found: {result['entity']['text']} (Distance: {result['distance']})\")\n",
        "\n",
        "print(\"\\nâœ… Quickstart successful using Milvus-Lite!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7E8c3kyaZjK",
        "outputId": "2ab4f794-bd13-4e48-d0df-e723513b1eb8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ› ï¸ Installing Milvus components...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m285.1/285.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hğŸ“¦ Creating collection: quickstart_collection\n",
            "ğŸ“¥ Inserting data...\n",
            "ğŸ” Searching for nearest neighbors...\n",
            "Found: Entity 3 (Distance: 1.0)\n",
            "Found: Entity 2 (Distance: 1.0)\n",
            "Found: Entity 1 (Distance: 1.0)\n",
            "\n",
            "âœ… Quickstart successful using Milvus-Lite!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Point the Agent to the local .db file instead of a Port\n",
        "# This is the 'secret sauce' that connects the blueprint to your quickstart\n",
        "os.environ['APP_VECTORSTORE_URL'] = \"/content/milvus_quickstart.db\"\n",
        "os.environ['APP_VECTORSTORE_NAME'] = \"milvus\"\n",
        "os.environ['COLLECTION_NAME'] = \"quickstart_collection\"\n",
        "\n",
        "# 2. Set your NVIDIA API Key (Ensure this is in your Colab Secrets)\n",
        "from google.colab import userdata\n",
        "os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')\n",
        "\n",
        "print(\"âœ… Agent environment configured for Milvus-Lite.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLXZhjmdebIX",
        "outputId": "4e19046b-14c3-4601-d4bb-93a2adabd451"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Agent environment configured for Milvus-Lite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Native Infrastructure with Cloud Persistence"
      ],
      "metadata": {
        "id": "HvDPSaIcQYYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, subprocess, time, socket\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. MOUNT DRIVE & PREP FOLDERS\n",
        "print(\"ğŸ“‚ Mounting Drive and Setting up Sync Bridge...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "DRIVE_BACKUP = \"/content/drive/MyDrive/NVIDIA_AI_ASSISTANT/postgres_backup\"\n",
        "LOCAL_DATA = \"/var/lib/postgresql/14/main\"\n",
        "!mkdir -p {DRIVE_BACKUP}\n",
        "!mkdir -p /var/run/postgresql /content/logs\n",
        "!chmod 777 /content/logs\n",
        "\n",
        "# 2. CLEANUP & RESTORE FROM DRIVE (If backup exists)\n",
        "print(\"ğŸ”„ Restoring data from Google Drive to local storage...\")\n",
        "!pkill -9 postgres || true\n",
        "!rm -rf {LOCAL_DATA}\n",
        "!mkdir -p {LOCAL_DATA}\n",
        "# Copy data from Drive to Local so Postgres can 'own' it\n",
        "if os.path.exists(f\"{DRIVE_BACKUP}/PG_VERSION\"):\n",
        "    !cp -r {DRIVE_BACKUP}/* {LOCAL_DATA}/\n",
        "else:\n",
        "    print(\"âœ¨ No backup found. Starting a fresh cluster.\")\n",
        "\n",
        "# 3. SET PERMISSIONS (Local folders allow this)\n",
        "!chown -R postgres:postgres /var/lib/postgresql\n",
        "!chmod 700 {LOCAL_DATA}\n",
        "\n",
        "# 4. INITIALIZE IF NEW\n",
        "if not os.path.exists(f\"{LOCAL_DATA}/PG_VERSION\"):\n",
        "    !sudo -u postgres /usr/lib/postgresql/14/bin/initdb -D {LOCAL_DATA}\n",
        "\n",
        "# 5. START POSTGRES\n",
        "print(\"ğŸ˜ Starting Postgres locally...\")\n",
        "!sudo -u postgres /usr/lib/postgresql/14/bin/pg_ctl \\\n",
        "    -D {LOCAL_DATA} \\\n",
        "    -o \"-c config_file=/etc/postgresql/14/main/postgresql.conf -c unix_socket_directories='/var/run/postgresql'\" \\\n",
        "    -l /content/logs/postgres_sync.log restart\n",
        "\n",
        "# 6. START REDIS\n",
        "print(\"ğŸ”´ Starting Redis...\")\n",
        "!pkill -9 redis-server || true\n",
        "!nohup redis-server --port 6379 --protected-mode no > /content/logs/redis_sync.log 2>&1 &\n",
        "\n",
        "# 7. THE PERSISTENCE TRIGGER (Backup back to Drive)\n",
        "def sync_to_drive():\n",
        "    print(\"ğŸ’¾ Syncing local database to Google Drive...\")\n",
        "    !cp -r {LOCAL_DATA}/* {DRIVE_BACKUP}/\n",
        "    print(\"âœ… Backup complete.\")\n",
        "\n",
        "# 8. VERIFY\n",
        "time.sleep(10)\n",
        "print(\"\\n--- ğŸ¥ STABLE PERSISTENT STATUS ---\")\n",
        "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "s.settimeout(2)\n",
        "if s.connect_ex(('127.0.0.1', 5432)) == 0:\n",
        "    print(f\"Postgres  : âœ… LIVE (Local + Sync)\")\n",
        "    sync_to_drive() # Initial backup\n",
        "else:\n",
        "    print(f\"Postgres  : âŒ DOWN. Checking log...\")\n",
        "    !tail -n 5 /content/logs/postgres_sync.log\n",
        "s.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYkmvpxZQOIv",
        "outputId": "0a48e7f8-5145-438b-eb31-9e631daa0dcd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Mounting Drive and Setting up Sync Bridge...\n",
            "Mounted at /content/drive\n",
            "ğŸ”„ Restoring data from Google Drive to local storage...\n",
            "ğŸ˜ Starting Postgres locally...\n",
            "pg_ctl: old server process (PID: 5405) seems to be gone\n",
            "starting server anyway\n",
            "waiting for server to start.... done\n",
            "server started\n",
            "ğŸ”´ Starting Redis...\n",
            "\n",
            "--- ğŸ¥ STABLE PERSISTENT STATUS ---\n",
            "Postgres  : âœ… LIVE (Local + Sync)\n",
            "ğŸ’¾ Syncing local database to Google Drive...\n",
            "âœ… Backup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install missing LangChain integration packages\n",
        "print(\"ğŸ“¦ Patching LangChain integrations...\")\n",
        "!pip install langchain-community langchain-milvus -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqXCHKOnfL-n",
        "outputId": "c58867c9-b14f-4cb3-b3e7-2050417b6505"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Patching LangChain integrations...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-nvidia-ai-endpoints -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNRpLbhPoEjL",
        "outputId": "4556ac48-34a1-4b9b-90c9-d5d7ec082081"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/49.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_milvus import Milvus\n",
        "from google.colab import userdata\n",
        "\n",
        "# 2. Setup NVIDIA Models & Milvus-Lite Connection\n",
        "os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\")\n",
        "embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")\n",
        "\n",
        "# Connect to the .db file you created in the quickstart\n",
        "vectorstore = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args={\"uri\": \"/content/milvus_quickstart.db\"},\n",
        "    collection_name=\"quickstart_collection\",\n",
        "    auto_id=True\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 3. Define RAG Logic\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def real_respond(message, history):\n",
        "    # Retrieve relevant data from Milvus-Lite\n",
        "    docs = retriever.invoke(message)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Generate answer using Llama 3.1\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    return chain.invoke({\"context\": context, \"question\": message})\n",
        "\n",
        "# 4. Launch the Final UI\n",
        "print(\"ğŸš€ Launching the NVIDIA AI Virtual Assistant...\")\n",
        "demo = gr.ChatInterface(\n",
        "    fn=real_respond,\n",
        "    type=\"messages\",\n",
        "    title=\"NVIDIA AI Virtual Assistant\",\n",
        "    description=\"Full Stack: Postgres, Redis, and Milvus-Lite\"\n",
        ")\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "Wt9EUP3me8Xz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "1495cb62-885f-4742-a6d2-5e6c942fc36e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Launching the NVIDIA AI Virtual Assistant...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a89cd5395b66a9cdbd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a89cd5395b66a9cdbd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, subprocess, time\n",
        "\n",
        "# 1. FORCE-CREATE THE POSTGRES USER\n",
        "print(\"ğŸ‘¤ Creating 'postgres' system user...\")\n",
        "!groupadd -r postgres || true\n",
        "!useradd -r -g postgres -d /var/lib/postgresql -s /bin/bash postgres || true\n",
        "\n",
        "# 2. INSTALL & INITIALIZE DATA DIRECTORY\n",
        "print(\"ğŸ˜ Installing and Initializing Postgres...\")\n",
        "!apt-get install postgresql-14 -y -qq\n",
        "!mkdir -p /var/lib/postgresql/14/main /var/run/postgresql\n",
        "!chown -R postgres:postgres /var/lib/postgresql/ /var/run/postgresql/\n",
        "\n",
        "# If the database isn't initialized, do it now\n",
        "if not os.path.exists(\"/var/lib/postgresql/14/main/PG_VERSION\"):\n",
        "    print(\"âœ¨ Initializing new database cluster...\")\n",
        "    !sudo -u postgres /usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main\n",
        "\n",
        "# 3. DIRECT LAUNCH\n",
        "print(\"ğŸš€ Launching services manually...\")\n",
        "\n",
        "# Start Postgres\n",
        "!sudo -u postgres /usr/lib/postgresql/14/bin/pg_ctl \\\n",
        "    -D /var/lib/postgresql/14/main \\\n",
        "    -l /content/logs/postgres_new.log start\n",
        "\n",
        "# Start Redis (using the package we installed in the last step)\n",
        "!nohup redis-server --port 6379 --protected-mode no > /content/logs/redis_new.log 2>&1 &\n",
        "\n",
        "# 4. FINAL VERIFICATION\n",
        "time.sleep(10)\n",
        "print(\"\\n--- ğŸ¥ INFRASTRUCTURE STATUS ---\")\n",
        "import socket\n",
        "for name, port in [(\"Postgres\", 5432), (\"Redis\", 6379)]:\n",
        "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    s.settimeout(2)\n",
        "    check = s.connect_ex(('127.0.0.1', port))\n",
        "    print(f\"{name:<10} (Port {port}): {'âœ… LIVE' if check == 0 else 'âŒ STILL DOWN'}\")\n",
        "    s.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfvchC-y2Od2",
        "outputId": "9da77187-6e95-4adf-a7a4-93d124550fbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‘¤ Creating 'postgres' system user...\n",
            "groupadd: group 'postgres' already exists\n",
            "useradd: user 'postgres' already exists\n",
            "ğŸ˜ Installing and Initializing Postgres...\n",
            "ğŸš€ Launching services manually...\n",
            "pg_ctl: another server might be running; trying to start server anyway\n",
            "waiting for server to start.... stopped waiting\n",
            "pg_ctl: could not start server\n",
            "Examine the log output.\n",
            "\n",
            "--- ğŸ¥ INFRASTRUCTURE STATUS ---\n",
            "Postgres   (Port 5432): âœ… LIVE\n",
            "Redis      (Port 6379): âœ… LIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-nvidia-ai-endpoints -q"
      ],
      "metadata": {
        "id": "0fy8VzV4OOHt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import socket\n",
        "import sys\n",
        "from pymilvus import MilvusClient\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from google.colab import userdata\n",
        "\n",
        "def run_full_health_check():\n",
        "    # 1. Re-sync Authentication\n",
        "    os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')\n",
        "\n",
        "    print(\"ğŸ” STARTING FULL STACK DIAGNOSTICS...\\n\")\n",
        "    results = []\n",
        "\n",
        "    # --- LAYER 1: NATIVE INFRASTRUCTURE (Postgres & Redis) ---\n",
        "    infra_ports = {\"Postgres\": 5432, \"Redis\": 6379}\n",
        "    for name, port in infra_ports.items():\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        s.settimeout(2)\n",
        "        is_live = s.connect_ex(('127.0.0.1', port)) == 0\n",
        "        s.close()\n",
        "        results.append(is_live)\n",
        "        print(f\"{'âœ…' if is_live else 'âŒ'} {name:<15} (Port {port}): {'LIVE' if is_live else 'DOWN'}\")\n",
        "\n",
        "    # --- LAYER 2: VECTOR STORAGE (Milvus-Lite) ---\n",
        "    milvus_uri = \"/content/milvus_quickstart.db\"\n",
        "    try:\n",
        "        client = MilvusClient(milvus_uri)\n",
        "        count = len(client.list_collections())\n",
        "        mv_ok = True\n",
        "        mv_msg = f\"LIVE ({count} collections found)\"\n",
        "    except Exception as e:\n",
        "        mv_ok = False\n",
        "        mv_msg = f\"ERROR: {str(e)}\"\n",
        "    results.append(mv_ok)\n",
        "    print(f\"{'âœ…' if mv_ok else 'âŒ'} {'Milvus-Lite':<15} (File-Based): {mv_msg}\")\n",
        "\n",
        "    # --- LAYER 3: INTELLIGENCE (NVIDIA NIM) ---\n",
        "    try:\n",
        "        # Using the verified stable 8b model for the heartbeat check\n",
        "        llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "        response = llm.invoke(\"health_check_ping\")\n",
        "        nim_ok = True\n",
        "        nim_msg = \"CONNECTED (Llama 3.1)\"\n",
        "    except Exception as e:\n",
        "        nim_ok = False\n",
        "        nim_msg = f\"FAILED: Check API Key or Quota\"\n",
        "    results.append(nim_ok)\n",
        "    print(f\"{'âœ…' if nim_ok else 'âŒ'} {'NVIDIA NIM':<15} (Cloud API): {nim_msg}\")\n",
        "\n",
        "    # --- FINAL VERDICT ---\n",
        "    print(\"\\n\" + \"=\"*45)\n",
        "    if all(results):\n",
        "        print(\"ğŸš€ SYSTEM STATUS: ALL SYSTEMS OPERATIONAL\")\n",
        "        print(\"Your Virtual Assistant is ready for use.\")\n",
        "    else:\n",
        "        print(\"âš ï¸  SYSTEM STATUS: INFRASTRUCTURE DEGRADED\")\n",
        "        print(\"Please re-run the setup cell for the 'âŒ' components.\")\n",
        "    print(\"=\"*45)\n",
        "\n",
        "run_full_health_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSQilgbcf3Lw",
        "outputId": "a346067c-05aa-4bf6-bf28-f72c62a64755"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” STARTING FULL STACK DIAGNOSTICS...\n",
            "\n",
            "âœ… Postgres        (Port 5432): LIVE\n",
            "âœ… Redis           (Port 6379): LIVE\n",
            "âœ… Milvus-Lite     (File-Based): LIVE (0 collections found)\n",
            "âœ… NVIDIA NIM      (Cloud API): CONNECTED (Llama 3.1)\n",
            "\n",
            "=============================================\n",
            "ğŸš€ SYSTEM STATUS: ALL SYSTEMS OPERATIONAL\n",
            "Your Virtual Assistant is ready for use.\n",
            "=============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf -q"
      ],
      "metadata": {
        "id": "yhzYwTUgruze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus -q\n",
        "!pip install langchain-milvus -q\n",
        "!pip install langchain-community -q"
      ],
      "metadata": {
        "id": "ekpqMghxRjH0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_milvus import Milvus\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from pymilvus import MilvusClient\n",
        "from google.colab import userdata\n",
        "\n",
        "def sanitize_metadata(documents):\n",
        "    \"\"\"Clean metadata keys to ensure Milvus compatibility.\"\"\"\n",
        "    for doc in documents:\n",
        "        clean_metadata = {}\n",
        "        for key, value in doc.metadata.items():\n",
        "            clean_key = re.sub(r'[^a-zA-Z0-9_]', '_', key)\n",
        "            if not clean_key[0].isalpha() and clean_key[0] != '_':\n",
        "                clean_key = \"field_\" + clean_key\n",
        "            clean_metadata[clean_key] = value\n",
        "        doc.metadata = clean_metadata\n",
        "    return documents\n",
        "\n",
        "def download_and_ingest(pdf_url, collection_name=\"quickstart_collection\"):\n",
        "    # 1. DOWNLOAD\n",
        "    local_filename = \"auto_downloaded_doc.pdf\"\n",
        "    print(f\"ğŸ“¥ Downloading: {pdf_url}...\")\n",
        "    response = requests.get(pdf_url)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # 2. LOAD AND CLEAN\n",
        "    loader = PyPDFLoader(local_filename)\n",
        "    docs = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    splits = sanitize_metadata(text_splitter.split_documents(docs))\n",
        "\n",
        "    # 3. NVIDIA SETUP\n",
        "    os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')\n",
        "    embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")\n",
        "    milvus_uri = \"/content/milvus_quickstart.db\"\n",
        "\n",
        "    # 4. RESET COLLECTION (Fixes the DataNotMatchException)\n",
        "    client = MilvusClient(milvus_uri)\n",
        "    if client.has_collection(collection_name):\n",
        "        print(f\"ğŸ—‘ï¸ Dropping old {collection_name} to fix ID schema...\")\n",
        "        client.drop_collection(collection_name)\n",
        "\n",
        "    # 5. FRESH INGESTION\n",
        "    print(f\"ğŸ§¬ Storing {len(splits)} chunks in Milvus-Lite...\")\n",
        "    vectorstore = Milvus.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=embeddings,\n",
        "        connection_args={\"uri\": milvus_uri},\n",
        "        collection_name=collection_name,\n",
        "        drop_old=True,  # This ensures the new auto_id schema is applied\n",
        "        auto_id=True\n",
        "    )\n",
        "    print(f\"ğŸš€ SUCCESS! {pdf_url} is now searchable with Auto-ID enabled.\")\n",
        "\n",
        "# Run the fixed pipeline\n",
        "target_pdf = \"https://media.geeksforgeeks.org/wp-content/uploads/20240226121023/GFG.pdf\"\n",
        "download_and_ingest(target_pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wm8Yo1whOgz",
        "outputId": "91ed5e1e-8ac2-4f10-e9f6-dbdf54be3536"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Downloading: https://media.geeksforgeeks.org/wp-content/uploads/20240226121023/GFG.pdf...\n",
            "ğŸ§¬ Storing 61 chunks in Milvus-Lite...\n",
            "ğŸš€ SUCCESS! https://media.geeksforgeeks.org/wp-content/uploads/20240226121023/GFG.pdf is now searchable with Auto-ID enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_milvus import Milvus\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from pymilvus import MilvusClient\n",
        "from google.colab import userdata\n",
        "\n",
        "def sanitize_metadata(documents):\n",
        "    \"\"\"Clean metadata keys to ensure Milvus compatibility.\"\"\"\n",
        "    for doc in documents:\n",
        "        clean_metadata = {}\n",
        "        for key, value in doc.metadata.items():\n",
        "            # Milvus only allows alphanumeric and underscores for metadata keys\n",
        "            clean_key = re.sub(r'[^a-zA-Z0-9_]', '_', key)\n",
        "            if not clean_key[0].isalpha() and clean_key[0] != '_':\n",
        "                clean_key = \"field_\" + clean_key\n",
        "            clean_metadata[clean_key] = value\n",
        "        doc.metadata = clean_metadata\n",
        "    return documents\n",
        "\n",
        "def download_and_ingest(urls, collection_name=\"ai_agent_knowledge\", reset=False):\n",
        "    # 1. SETUP ENVIRONMENT\n",
        "    os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')\n",
        "    embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")\n",
        "    milvus_uri = \"/content/milvus_quickstart.db\"\n",
        "\n",
        "    # Initialize Milvus Client to check for/reset collections\n",
        "    client = MilvusClient(milvus_uri)\n",
        "    if reset and client.has_collection(collection_name):\n",
        "        print(f\"ğŸ—‘ï¸ Resetting collection: {collection_name}\")\n",
        "        client.drop_collection(collection_name)\n",
        "\n",
        "    all_splits = []\n",
        "\n",
        "    # 2. PROCESS AI PDFs\n",
        "    for url in urls:\n",
        "        try:\n",
        "            local_filename = f\"ai_doc_{urls.index(url)}.pdf\"\n",
        "            print(f\"ğŸ“¥ Downloading: {url}...\")\n",
        "\n",
        "            response = requests.get(url, timeout=15)\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            # Load PDF\n",
        "            loader = PyPDFLoader(local_filename)\n",
        "            docs = loader.load()\n",
        "\n",
        "            # Enrich metadata\n",
        "            for doc in docs:\n",
        "                doc.metadata[\"source_url\"] = url\n",
        "\n",
        "            # 3. CONSERVATIVE SPLITTING (Prevents Token Limit Exception)\n",
        "            # 500 characters is approximately 125-150 tokens, well within the 512 limit.\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=500,\n",
        "                chunk_overlap=50,\n",
        "                separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "            )\n",
        "\n",
        "            splits = sanitize_metadata(text_splitter.split_documents(docs))\n",
        "            all_splits.extend(splits)\n",
        "\n",
        "            # Clean up temp file\n",
        "            os.remove(local_filename)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to process {url}: {e}\")\n",
        "\n",
        "    # 4. FINAL INGESTION\n",
        "    if all_splits:\n",
        "        print(f\"ğŸ§¬ Generating embeddings for {len(all_splits)} chunks...\")\n",
        "        vectorstore = Milvus.from_documents(\n",
        "            documents=all_splits,\n",
        "            embedding=embeddings,\n",
        "            connection_args={\"uri\": milvus_uri},\n",
        "            collection_name=collection_name,\n",
        "            drop_old=reset,\n",
        "            auto_id=True\n",
        "        )\n",
        "        print(f\"ğŸš€ SUCCESS! {len(urls)} AI PDFs indexed in Milvus.\")\n",
        "        return vectorstore\n",
        "    else:\n",
        "        print(\"âš ï¸ No documents were processed.\")\n",
        "        return None\n",
        "\n",
        "# --- AI TARGET LIST ---\n",
        "\n",
        "ai_pdf_list = [\n",
        "    \"https://arxiv.org/pdf/2306.02224.pdf\",        # Auto-GPT\n",
        "    \"https://arxiv.org/pdf/2003.10217.pdf\",        # Milvus\n",
        "    \"https://arxiv.org/pdf/2303.17564.pdf\",        # Tool Makers\n",
        "    \"https://arxiv.org/pdf/1404.7828.pdf\",         # Schmidhuber DL\n",
        "    \"https://arxiv.org/pdf/2403.17881.pdf\",        # Deepfake/Proteomics\n",
        "    \"https://arxiv.org/pdf/2409.03741.pdf\",        # ML Attacks\n",
        "    \"https://arxiv.org/pdf/2203.02155.pdf\",        # RLHF (InstructGPT)\n",
        "    \"https://arxiv.org/pdf/2305.18290.pdf\",        # DPO\n",
        "    \"https://arxiv.org/pdf/1707.06347.pdf\",        # PPO\n",
        "    \"https://arxiv.org/pdf/1506.02438.pdf\"         # GAE (Generalized Advantage Estimation)\n",
        "]\n",
        "\n",
        "# Run the pipeline\n",
        "# Set reset=True on first run to ensure clean schema\n",
        "vectorstore = download_and_ingest(ai_pdf_list, reset=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxfrC5wPVsUX",
        "outputId": "bb34eff1-d6a0-4b6d-8d3b-35ff61560202"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/2306.02224.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/2003.10217.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/2303.17564.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/1404.7828.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/2403.17881.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/2409.03741.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/2203.02155.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/2305.18290.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/1707.06347.pdf...\n",
            "ğŸ“¥ Downloading: https://arxiv.org/pdf/1506.02438.pdf...\n",
            "ğŸ§¬ Generating embeddings for 2813 chunks...\n",
            "ğŸš€ SUCCESS! 10 AI PDFs indexed in Milvus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import MilvusClient\n",
        "import os\n",
        "\n",
        "# 1. Connect to the Lite DB\n",
        "client = MilvusClient(\"/content/milvus_quickstart.db\")\n",
        "collection_name = \"ai_agent_knowledge\"\n",
        "\n",
        "if client.has_collection(collection_name):\n",
        "    # Get total count\n",
        "    stats = client.get_collection_stats(collection_name=collection_name)\n",
        "    total_chunks = stats.get('row_count', 0)\n",
        "    print(f\"ğŸ“Š Collection: {collection_name} | Total Chunks: {total_chunks}\")\n",
        "\n",
        "    # 2. Get ALL unique sources first\n",
        "    # We query all rows to ensure we don't miss a document hidden deep in the PKs\n",
        "    all_data = client.query(\n",
        "        collection_name=collection_name,\n",
        "        filter=\"pk >= 0\",\n",
        "        output_fields=[\"source_url\"],\n",
        "        limit=total_chunks\n",
        "    )\n",
        "\n",
        "    unique_sources = list(set([entry['source_url'] for entry in all_data if 'source_url' in entry]))\n",
        "    print(f\"âœ… Found {len(unique_sources)} unique documents in the database.\")\n",
        "\n",
        "    # 3. Peek at ONE chunk per source\n",
        "    print(\"\\nğŸ“‘ Representative Samples from Each Source:\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for i, source in enumerate(sorted(unique_sources)):\n",
        "        # Retrieve exactly one chunk for this specific source\n",
        "        sample = client.query(\n",
        "            collection_name=collection_name,\n",
        "            filter=f'source_url == \"{source}\"',\n",
        "            limit=1,\n",
        "            output_fields=[\"text\"]\n",
        "        )\n",
        "\n",
        "        if sample:\n",
        "            content = sample[0].get('text', 'No content').replace('\\n', ' ')\n",
        "            filename = os.path.basename(source)\n",
        "            print(f\"[{i+1}] FILE: {filename}\")\n",
        "            print(f\"    SAMPLED CONTENT: {content[:140]}...\")\n",
        "            print(\"-\" * 65)\n",
        "\n",
        "else:\n",
        "    print(f\"âŒ Collection '{collection_name}' not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhk_vwK6il7z",
        "outputId": "8097dc1e-bf65-4fd2-d864-c5d2d99b4359"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Collection: ai_agent_knowledge | Total Chunks: 2813\n",
            "âœ… Found 10 unique documents in the database.\n",
            "\n",
            "ğŸ“‘ Representative Samples from Each Source:\n",
            "-----------------------------------------------------------------\n",
            "[1] FILE: 1404.7828.pdf\n",
            "    SAMPLED CONTENT: Deep Learning in Neural Networks: An Overview Technical Report IDSIA-03-14 / arXiv:1404.7828 v4 [cs.NE] (88 pages, 888 references) JÂ¨urgen S...\n",
            "-----------------------------------------------------------------\n",
            "[2] FILE: 1506.02438.pdf\n",
            "    SAMPLED CONTENT: Published as a conference paper at ICLR 2016 HIGH -D IMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION John Schulman, Phi...\n",
            "-----------------------------------------------------------------\n",
            "[3] FILE: 1707.06347.pdf\n",
            "    SAMPLED CONTENT: Proximal Policy Optimization Algorithms John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov OpenAI {joschu, filip, pra...\n",
            "-----------------------------------------------------------------\n",
            "[4] FILE: 2003.10217.pdf\n",
            "    SAMPLED CONTENT: Eï¬ƒcient simulation of inclusions and reinforcement bars with the isogeometric Boundary Element method Gernot Beera,âˆ—, Christian DÃ¼nsera, Eug...\n",
            "-----------------------------------------------------------------\n",
            "[5] FILE: 2203.02155.pdf\n",
            "    SAMPLED CONTENT: Training language models to follow instructions with human feedback Long Ouyangâˆ— Jeff Wuâˆ— Xu Jiangâˆ— Diogo Almeidaâˆ— Carroll L. Wainwrightâˆ— Pa...\n",
            "-----------------------------------------------------------------\n",
            "[6] FILE: 2303.17564.pdf\n",
            "    SAMPLED CONTENT: BloombergGPT: A Large Language Model for Finance Shijie Wu1,âˆ—, Ozan Ë™Irsoy1,âˆ—, Steven Lu 1,âˆ—, Vadim Dabravolski1, Mark Dredze 1,3, Sebastian...\n",
            "-----------------------------------------------------------------\n",
            "[7] FILE: 2305.18290.pdf\n",
            "    SAMPLED CONTENT: Direct Preference Optimization: Your Language Model is Secretly a Reward Model Rafael Rafailovâˆ— â€  Archit Sharmaâˆ— â€  Eric Mitchellâˆ— â€  Stefano ...\n",
            "-----------------------------------------------------------------\n",
            "[8] FILE: 2306.02224.pdf\n",
            "    SAMPLED CONTENT: Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions Hui Yang Amazon United States younghumanfly@gmail.com Sifu Yue Amazo...\n",
            "-----------------------------------------------------------------\n",
            "[9] FILE: 2403.17881.pdf\n",
            "    SAMPLED CONTENT: Noname manuscript No. (will be inserted by the editor) Deepfake Generation and Detection: A Benchmark and Survey Gan Pei1 Â· Jiangning Zhang2...\n",
            "-----------------------------------------------------------------\n",
            "[10] FILE: 2409.03741.pdf\n",
            "    SAMPLED CONTENT: To Appear in Network and Distributed System Security (NDSS) Symposium 2025 Understanding Data Importance in Machine Learning Attacks: Does V...\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. Initialize the LLM (Llama 3.1 70B)\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\") # ğŸš€ Upgraded to 405B\n",
        "\n",
        "# 2. Create the RAG Prompt\n",
        "template = \"\"\"\n",
        "You are a professional AI Research Assistant. Use the following pieces of retrieved context\n",
        "to answer the question. If you don't know the answer, just say that you don't know.\n",
        "Keep the answer concise and professional.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# 3. Setup the Retriever from your existing vectorstore\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 4. Construct the RAG Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- TEST IT ---\n",
        "user_query = \"Compare the performance of GPT-4 vs Claude in Auto-GPT tasks based on the research.\"\n",
        "response = rag_chain.invoke(user_query)\n",
        "\n",
        "print(f\"ğŸ¤– AI Assistant:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pw9bixfW_bD",
        "outputId": "8d614157-28cf-4739-9b10-faf42b2095b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– AI Assistant:\n",
            "According to the research, GPT-4 demonstrated superior performance in Auto-GPT tasks, achieving a success rate of 0.485 and precision of 0.628, while Claude's performance was markedly inferior, with a success rate of 0.075 or 0.082 in different settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the 'Spatio-Temporal Credit Assignment' problem defined by Schmidhuber with the 'Variance-Bias Tradeoff' solved by GAE. How does the latter provide a practical implementation for the former in modern autonomous agents?"
      ],
      "metadata": {
        "id": "Q_Fih1sKlzE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "from pymilvus import MilvusClient\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. SETUP - Utilizing the 405B parameter model\n",
        "os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\") # ğŸš€ Upgraded to 405B\n",
        "embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")\n",
        "milvus_uri = \"/content/milvus_quickstart.db\"\n",
        "COLLECTION_NAME = \"ai_agent_knowledge\"\n",
        "\n",
        "PERSONAS = {\n",
        "    \"AI Research Assistant\": \"You are a professional AI Research Assistant. Focus on high-fidelity technical data.\",\n",
        "    \"Aerospace Engineer\": \"You are a Senior Aerospace Engineer. Your answers must be grounded in physical and safety constraints.\",\n",
        "    \"Medical AI Specialist\": \"You are a Medical AI Researcher. Focus on clinical evidence and proteomic data accuracy.\",\n",
        "    \"AI Portfolio Manager\": \"You are an AI Portfolio Manager. Evaluate technical readiness and commercial feasibility.\"\n",
        "}\n",
        "\n",
        "def get_available_docs():\n",
        "    client = MilvusClient(milvus_uri)\n",
        "    if not client.has_collection(COLLECTION_NAME): return [\"No Documents Found\"]\n",
        "    res = client.query(collection_name=COLLECTION_NAME, filter='pk >= 0', output_fields=[\"source_url\"])\n",
        "    sources = list(set([os.path.basename(r['source_url']) for r in res if 'source_url' in r]))\n",
        "    return [\"All Documents\"] + sorted(sources)\n",
        "\n",
        "# 2. ORCHESTRATION LOGIC (Optimized for 405B reasoning)\n",
        "def orchestrate_respond(message, history, selected_doc, selected_persona, strict_mode):\n",
        "    vectorstore = Milvus(\n",
        "        embedding_function=embeddings,\n",
        "        connection_args={\"uri\": milvus_uri},\n",
        "        collection_name=COLLECTION_NAME\n",
        "    )\n",
        "\n",
        "    search_kwargs = {\"k\": 6}\n",
        "    if selected_doc != \"All Documents\":\n",
        "        search_kwargs[\"expr\"] = f'source_url LIKE \"%{selected_doc}\"'\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\n",
        "    docs = retriever.invoke(message)\n",
        "    context_text = \"\\n\\n\".join([f\"Source: {os.path.basename(d.metadata.get('source_url', ''))} (Page {d.metadata.get('page', 'N/A')}):\\n{d.page_content}\" for d in docs])\n",
        "\n",
        "    system_role = PERSONAS.get(selected_persona, PERSONAS[\"AI Research Assistant\"])\n",
        "\n",
        "    # 3. ADVANCED STRICT PROMPT (Tailored for 405B)\n",
        "    if strict_mode:\n",
        "        instruction = \"\"\"\n",
        "        CRITICAL INSTRUCTION (STRICT MODE):\n",
        "        You are acting as a closed-system retrieval interface.\n",
        "        1. Access ONLY the provided context to answer.\n",
        "        2. If the answer is not explicitly found in the text, you MUST state: 'The provided context does not contain the answer.'\n",
        "        3. DO NOT use your internal training data to supplement the response.\n",
        "        4. Cite the specific file and page for every factual claim.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        instruction = \"Answer using the context provided, but you may use your internal expertise to explain complex terms found in the text.\"\n",
        "\n",
        "    template = f\"\"\"{system_role}\n",
        "    {instruction}\n",
        "\n",
        "    ### DATA SOURCE (CONTEXT):\n",
        "    {{context}}\n",
        "\n",
        "    ### USER INQUIRY:\n",
        "    {{question}}\n",
        "\n",
        "    ### RESPONSE:\"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    answer = chain.invoke({\"context\": context_text, \"question\": message})\n",
        "\n",
        "    citations = list(set([f\"ğŸ“„ {os.path.basename(d.metadata.get('source_url', ''))} (Pg {d.metadata.get('page', 'N/A')})\" for d in docs]))\n",
        "    return f\"{answer}\\n\\n---\\n**Audit Trail:**\\n{', '.join(citations)}\"\n",
        "\n",
        "# 4. UI CONSTRUCTION\n",
        "print(\"ğŸš€ Launching 405B Orchestrator...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ğŸ›ï¸ NVIDIA 405B Agentic Orchestrator\")\n",
        "    gr.Markdown(\"Using Llama-3.1-405B for maximum reasoning capability.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        doc_dropdown = gr.Dropdown(choices=get_available_docs(), value=\"All Documents\", label=\"Knowledge Source\", scale=2)\n",
        "        persona_dropdown = gr.Dropdown(choices=list(PERSONAS.keys()), value=\"AI Research Assistant\", label=\"Persona\", scale=2)\n",
        "        strict_toggle = gr.Checkbox(label=\"Strict Grounding\", value=True, scale=1)\n",
        "        refresh_btn = gr.Button(\"ğŸ”„\", scale=0)\n",
        "\n",
        "    chat = gr.ChatInterface(\n",
        "        fn=orchestrate_respond,\n",
        "        additional_inputs=[doc_dropdown, persona_dropdown, strict_toggle],\n",
        "        type=\"messages\"\n",
        "    )\n",
        "\n",
        "    refresh_btn.click(fn=lambda: gr.update(choices=get_available_docs()), outputs=doc_dropdown)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "YOJLZi3UZVnh",
        "outputId": "b4164ee0-c64b-476e-f1fc-43f24d3ddcec"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Launching 405B Orchestrator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1457080810.py:82: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e16a70e8832b69d5d3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e16a70e8832b69d5d3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the 'Spatio-Temporal Credit Assignment' problem defined by Schmidhuber with the 'Variance-Bias Tradeoff' solved by GAE. How does the latter provide a practical implementation for the former in modern autonomous agents?\n",
        "\n",
        "To compare the 'Spatio-Temporal Credit Assignment' problem defined by Schmidhuber with the 'Variance-Bias Tradeoff' solved by GAE, we must analyze the context provided.\n",
        "\n",
        "The Spatio-Temporal Credit Assignment problem, as mentioned in Schmidhuber's dissertation (1404.7828.pdf, Page 75), refers to the challenge of assigning credit to internal components of a dynamic system, such as a neural network, for their contributions to the system's overall behavior and performance. This problem is fundamental in understanding complex systems and optimizing their performance.\n",
        "\n",
        "On the other hand, the Variance-Bias Tradeoff, as described in the context of Generalized Advantage Estimation (GAE) (1506.02438.pdf, Page 10), is a specific problem in reinforcement learning where an agent must balance the tradeoff between the variance of its estimates and the bias of its learning process. GAE provides a practical solution to this problem by combining techniques such as trust region policy optimization and value function optimization, both represented by neural networks.\n",
        "\n",
        "To relate the two concepts, we can see that the Spatio-Temporal Credit Assignment problem is a more general and abstract problem, while the Variance-Bias Tradeoff is a specific instance of this problem in the context of reinforcement learning. The response function, as defined in (1506.02438.pdf, Page 5), lets us quantify the temporal credit assignment problem, which is a specific aspect of the Spatio-Temporal Credit Assignment problem. The response function decomposes the advantage function across timesteps, allowing us to understand the temporal dependencies between actions and rewards.\n",
        "\n",
        "In modern autonomous agents, GAE provides a practical implementation for the Spatio-Temporal Credit Assignment problem by offering a method to estimate the advantage function, which is a key component in assigning credit to the agent's actions. By using GAE, agents can learn to solve complex control tasks that have previously been out of reach for generic reinforcement learning methods.\n",
        "\n",
        "In conclusion, GAE provides a practical solution to the Variance-Bias Tradeoff, which is a specific instance of the more general Spatio-Temporal Credit Assignment problem. By using GAE, modern autonomous agents can better assign credit to their actions and optimize their performance in complex environments.\n",
        "\n",
        "References:\n",
        "\n",
        "Schmidhuber, J. (1990a). Dynamische neuronale Netze und das fundamentale raumzeitliche Lern- problem. (Dynamic neural nets and the fundamental spatio-temporal credit assignment problem.) Dissertation, Inst. f. Inf., Tech. Univ. Munich. (1404.7828.pdf, Page 75)\n",
        "1506.02438.pdf, Page 10\n",
        "1506.02438.pdf, Page 5\n",
        "Audit Trail:\n",
        "ğŸ“„ 1404.7828.pdf (Pg 76), ğŸ“„ 1506.02438.pdf (Pg 5), ğŸ“„ 1506.02438.pdf (Pg 10), ğŸ“„ 1506.02438.pdf (Pg 1), ğŸ“„ 1404.7828.pdf (Pg 75)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wtu7QUNspiU2"
      }
    }
  ]
}
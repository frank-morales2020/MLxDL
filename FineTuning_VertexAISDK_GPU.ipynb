{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FineTuning_VertexAISDK_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "KThk5ET4q4fo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiVwfFuZ9axh"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install  -q gcsfs==2024.3.1\n",
        "!pip install  -q accelerate==0.31.0\n",
        "!pip install  -q transformers==4.45.2\n",
        "!pip install  -q  datasets==2.19.2\n",
        "!pip install google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai  -q\n",
        "!pip install tensorflow_datasets -q\n",
        "\n",
        "!pip install google-cloud-aiplatform -q -U\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "import google.cloud.bigquery\n",
        "import google.cloud.bigquery_storage\n",
        "import google.cloud.aiplatform\n",
        "import google.cloud.storage\n",
        "\n",
        "print(f\"google-cloud-aiplatform: {google.cloud.aiplatform.__version__}\")\n",
        "print(f\"google-cloud-storage: {google.cloud.storage.__version__}\")\n",
        "print(f\"google-cloud-bigquery: {google.cloud.bigquery.__version__}\")\n",
        "print(f\"google-cloud-bigquery-storage: {google.cloud.bigquery_storage.__version__}\")\n",
        "print(f\"google-cloud-aiplatform: {google.cloud.aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import colab_env\n",
        "print('\\n\\n')\n",
        "print(f\"datasets: {datasets.__version__}\")\n",
        "print(f\"colab-env: {colab_env.__version__}\")\n",
        "import google.cloud.aiplatform\n",
        "import google.cloud.storage\n",
        "print(f\"google-cloud-aiplatform: {google.cloud.aiplatform.__version__}\")\n",
        "print(f\"google-cloud-storage: {google.cloud.storage.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN259O2MOxp2",
        "outputId": "99fdbba2-c678-46d0-b359-ae5c5c5e2dea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "\n",
            "\n",
            "\n",
            "datasets: 2.19.2\n",
            "colab-env: 0.2.0\n",
            "google-cloud-aiplatform: 1.87.0\n",
            "google-cloud-storage: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data preparation"
      ],
      "metadata": {
        "id": "R7-1paG9ps6s"
      }
    },
    {
      "source": [
        "!gsutil cp gs://{BUCKET_NAME}/cmapss_FD004_train_text.jsonl .\n",
        "!gsutil cp gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl ."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "c6d_EJRnh5ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def validate_jsonl_format(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                if \"prompt\" not in data or \"completion\" not in data:\n",
        "                    print(f\"Invalid format in line: {line}\")\n",
        "                    return False  # Indicate invalid format\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Invalid JSON in line: {line}\")\n",
        "                return False  # Indicate invalid JSON\n",
        "    return True  # Indicate valid format\n",
        "\n",
        "# Example usage\n",
        "if validate_jsonl_format(\"cmapss_FD004_train_text.jsonl\"):\n",
        "    print(\"Training data has valid format.\")\n",
        "\n",
        "else:\n",
        "    print(\"Training data has invalid format.\")\n",
        "\n",
        "if validate_jsonl_format(\"cmapss_FD004_test_text.jsonl\"):\n",
        "    print(\"Testing data has valid format.\")\n",
        "else:\n",
        "    print(\"Testing data has invalid format.\")"
      ],
      "metadata": {
        "id": "8wduiO5kiPLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import json\n",
        "\n",
        "def transform_jsonl_to_prompt_completion(input_file_path, output_file_path):\n",
        "    \"\"\"Transforms chat-style JSONL to prompt-completion JSONL.\"\"\"\n",
        "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                # Extract prompt and completion from 'contents'\n",
        "                prompt = \"\".join([part[\"text\"] for part in data[\"contents\"][0][\"parts\"]])  # Assumes user role is first\n",
        "                completion = str(data.get(\"completion\", \"\")) # Handle if completion is missing\n",
        "\n",
        "                # Construct prompt-completion dictionary\n",
        "                prompt_completion_data = {\"prompt\": prompt, \"completion\": completion}\n",
        "\n",
        "                # Write to output file\n",
        "                outfile.write(json.dumps(prompt_completion_data) + \"\\n\")\n",
        "\n",
        "            except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
        "                print(f\"Skipping invalid or unprocessable line: {line.strip()}, Error: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "input_file_path = \"cmapss_FD004_train_text.jsonl\"\n",
        "output_file_path = \"cmapss_FD004_train_text_transformed.jsonl\"\n",
        "\n",
        "transform_jsonl_to_prompt_completion(input_file_path, output_file_path)\n",
        "print(f\"Transformed data written to: {output_file_path}\")\n",
        "\n",
        "input_file_path = \"cmapss_FD004_test_text.jsonl\"\n",
        "output_file_path = \"cmapss_FD004_test_text_transformed.jsonl\"\n",
        "\n",
        "transform_jsonl_to_prompt_completion(input_file_path, output_file_path)\n",
        "print(f\"Transformed data written to: {output_file_path}\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bjco9kxvinpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def validate_jsonl_format(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                if \"prompt\" not in data or \"completion\" not in data:\n",
        "                    print(f\"Invalid format in line: {line}\")\n",
        "                    return False  # Indicate invalid format\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Invalid JSON in line: {line}\")\n",
        "                return False  # Indicate invalid JSON\n",
        "    return True  # Indicate valid format\n",
        "\n",
        "# Example usage\n",
        "if validate_jsonl_format(\"cmapss_FD004_train_text_transformed.jsonl\"):\n",
        "    print(\"Training data has valid format.\")\n",
        "\n",
        "else:\n",
        "    print(\"Training data has invalid format.\")\n",
        "\n",
        "if validate_jsonl_format(\"cmapss_FD004_test_text_transformed.jsonl\"):\n",
        "    print(\"Testing data has valid format.\")\n",
        "else:\n",
        "    print(\"Testing data has invalid format.\")"
      ],
      "metadata": {
        "id": "IR5YBFj0pj9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!gsutil cp cmapss_FD004_train_text_transformed.jsonl gs://{BUCKET_NAME}/\n",
        "!gsutil cp cmapss_FD004_test_text_transformed.jsonl gs://{BUCKET_NAME}/"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "u6lLBqz5jNkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINE TUNING - NASA DATASET"
      ],
      "metadata": {
        "id": "Ge_1sCQo3TSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://{BUCKET_NAME}/*text*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0runBTWYDQE",
        "outputId": "2455c67e-8288-4002-e38f-a8eb805dd5c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://poc-my-new-staging-bucket-2025-1/cmapss_FD004_test_text.jsonl\n",
            "gs://poc-my-new-staging-bucket-2025-1/cmapss_FD004_train_text.jsonl\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# --- train.py content ---\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import subprocess\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "import tensorflow as tf  # Import TensorFlow for tf.io.gfile\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import jobs\n",
        "from google.cloud.aiplatform import models\n",
        "\n",
        "# Initialize Vertex AI with project ID and region from environment variables\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def create_gcs_dir(model_dir):\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = model_dir.split('/')[2]\n",
        "        blob_prefix = '/'.join(model_dir.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        subdirs = blob_prefix.split('/')\n",
        "        current_prefix = ''\n",
        "        for subdir in subdirs:\n",
        "            current_prefix = os.path.join(current_prefix, subdir)\n",
        "            blob = bucket.blob(current_prefix + '/')\n",
        "            blob.upload_from_string('')\n",
        "            logging.info(\"Created GCS directory: %s\", current_prefix)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating GCS directory: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_jsonl_dataset(data_path, sequence_length=30):\n",
        "    data = []\n",
        "    try:\n",
        "        with tf.io.gfile.GFile(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    prompt = record[\"prompt\"]\n",
        "                    completion = record[\"completion\"]\n",
        "                    data.append((prompt, completion))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(\"Skipping invalid JSON line: %r, Error: %s\", repr(line), e)\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to generate embeddings using Gemini Pro (Modified for fine-tuning)\n",
        "def generate_embeddings(text, model):\n",
        "    response = model.embed_text(text)\n",
        "    return response.embeddings\n",
        "\n",
        "# --- Dataset Class (Updated to Convert to Text) ---\n",
        "class CMAPSSJSONLDataset(data.Dataset):\n",
        "    def __init__(self, data_path, sequence_length=30, use_rolling_features=False, model=None):\n",
        "        self.data = load_jsonl_dataset(data_path, sequence_length)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.use_rolling_features = use_rolling_features\n",
        "        self.model = model  # Store the model for embedding generation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, completion = self.data[idx]\n",
        "\n",
        "        # Generate embeddings using the provided model\n",
        "        embeddings = generate_embeddings(prompt, self.model)\n",
        "\n",
        "        # Return embeddings and completion as a tuple\n",
        "        return torch.tensor(embeddings), completion  # Return embeddings and completion\n",
        "\n",
        "# --- Model Definition (Using Gemini Embeddings and LSTM) ---\n",
        "class RULPredictionModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Output layer for RUL prediction\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass embeddings through LSTM\n",
        "        out, _ = self.lstm(x)\n",
        "        # Take the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # Pass through fully connected layer for RUL prediction\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# --- Training Function (Modified for fine-tuning) ---\n",
        "def train_model(model_name, train_dataset_path, eval_dataset_path,\n",
        "                staging_bucket, bucket_name, base_output_dir,\n",
        "                use_rolling_features=False):\n",
        "\n",
        "    logging.info(\"Training configuration:\")\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Train Dataset Path: {train_dataset_path}\")\n",
        "    logging.info(f\"Eval Dataset Path: {eval_dataset_path}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "    logging.info(f\"Bucket Name: {bucket_name}\")\n",
        "    logging.info(f\"Base Output Dir: {base_output_dir}\")\n",
        "    logging.info(f\"Use Rolling Features: {use_rolling_features}\")\n",
        "\n",
        "    # 1. Data Loaders (Initially using the base model for loading)\n",
        "    base_model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "\n",
        "    # 2. Model Initialization (Updated for fine-tuning)\n",
        "    # Load the base Gemini Pro model for fine-tuning\n",
        "\n",
        "    # Create and run the fine-tuning job\n",
        "    fine_tuning_job = jobs.FineTuningJob(\n",
        "        model=base_model,  # Base Gemini Pro model\n",
        "        training_data_uri=train_dataset_path,  # GCS path to training data\n",
        "        validation_data_uri=eval_dataset_path,  # GCS path to validation data\n",
        "        # ... other fine-tuning configurations (e.g., hyperparameters) ...\n",
        "    )\n",
        "\n",
        "    fine_tuned_model = fine_tuning_job.run()  # Run the fine-tuning job\n",
        "\n",
        "    # Now use the fine-tuned model for embedding generation\n",
        "    train_dataset = CMAPSSJSONLDataset(train_dataset_path, use_rolling_features=use_rolling_features,\n",
        "                                       model=fine_tuned_model)\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path, use_rolling_features=use_rolling_features,\n",
        "                                      model=fine_tuned_model)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=64)\n",
        "\n",
        "\n",
        "    # 3. Model Initialization (LSTM)\n",
        "    embedding_dim = 768  # Gemini embedding dimension\n",
        "    hidden_size = 128\n",
        "    model = RULPredictionModel(embedding_dim, hidden_size)\n",
        "\n",
        "    # 4. Device Configuration\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")  # Use GPU if available\n",
        "        print(\"GPU is available and being used.\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")  # Fallback to CPU if GPU is not available\n",
        "        print(\"GPU is not available, using CPU instead.\")\n",
        "\n",
        "    model.to(device)\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 5. Optimizer and Loss Function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()  # Assuming you want to predict RUL as a regression task\n",
        "\n",
        "    # 6. Training Loop\n",
        "    num_epochs = 100  # You can adjust the number of epochs\n",
        "    best_eval_loss = float('inf')\n",
        "    patience = 10\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (embeddings, completions) in enumerate(train_loader):\n",
        "            embeddings = embeddings.to(device)\n",
        "            completions = completions.to(device).float().reshape(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(embeddings)  # Pass embeddings through LSTM\n",
        "            loss = criterion(outputs, completions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                logging.info(f\"Epoch: {epoch + 1}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        logging.info(f\"Epoch: {epoch + 1}, Average Training Loss: {avg_loss}\")\n",
        "\n",
        "        # 7. Evaluation\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for embeddings_eval, completions_eval in eval_loader:\n",
        "                embeddings_eval = embeddings_eval.to(device)\n",
        "                completions_eval = completions_eval.to(device).float().reshape(-1, 1)\n",
        "\n",
        "                outputs_eval = model(embeddings_eval)  # Pass embeddings through LSTM\n",
        "                loss_eval = criterion(outputs_eval, completions_eval)\n",
        "                eval_loss += loss_eval.item()\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        logging.info(f\"Epoch: {epoch + 1}, Average Evaluation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # 8. Early Stopping\n",
        "        if avg_eval_loss < best_eval_loss:\n",
        "            best_eval_loss = avg_eval_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                logging.info(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    # 9. Load the best model\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "    logging.info(\"Starting model saving...\")\n",
        "\n",
        "    # 10. Save Model (Local and GCS)\n",
        "    local_model_path = 'model-nasa-gpu.pth'\n",
        "    torch.save(model.state_dict(), local_model_path)\n",
        "\n",
        "    try:\n",
        "        base_output_path = base_output_dir\n",
        "        subprocess.run(['gsutil', 'cp', local_model_path, base_output_path], check=True)\n",
        "        logging.info(f\"Copied model to GCS BUCKET path: {base_output_path}\")\n",
        "\n",
        "        if 'AIP_MODEL_DIR' in os.environ:\n",
        "            gcs_model_path = os.path.join(os.environ['AIP_MODEL_DIR'], 'model-nasa-gpu.pth')\n",
        "            subprocess.run(['gsutil', 'cp', local_model_path, gcs_model_path], check=True)\n",
        "            logging.info(f\"Copied model to Vertex AI path: {gcs_model_path}\")\n",
        "            model_save_path = gcs_model_path\n",
        "        else:\n",
        "            logging.info(f\"Saving model to local path: {local_model_path}\")\n",
        "            model_save_path = local_model_path\n",
        "\n",
        "        logging.info(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logging.error(f\"Error saving model: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"rul_predictor_jsonl\", help=\"Model name\")\n",
        "    parser.add_argument(\"--train_dataset\", type=str, required=True, help=\"Path to training dataset JSONL\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"Path to evaluation dataset JSONL\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"Staging bucket for Vertex AI\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Bucket name\")\n",
        "    parser.add_argument(\"--base_output_dir\", type=str, required=True, help=\"Base output directory in GCS\")\n",
        "    parser.add_argument(\"--use_rolling_features\", action='store_true', help=\"Use rolling window features\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(args.model, args.train_dataset, args.eval_dataset,\n",
        "                args.staging_bucket, args.bucket_name, args.base_output_dir,\n",
        "                args.use_rolling_features)\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lkSa8VVjoFHx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import aiplatform\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import subprocess\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "# --- Main Script ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Get project details from environment variables\n",
        "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "    REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "    SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "    PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "    BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "    STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "    # Initialize Vertex AI\n",
        "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "    # --- Data and Output Paths ---\n",
        "    TRAINING_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_text_transformed.jsonl\"\n",
        "    EVAL_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text_transformed.jsonl\"\n",
        "    BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}/model_output\"\n",
        "\n",
        "    # Create or overwrite trainer/train.py\n",
        "    os.makedirs('trainer', exist_ok=True)\n",
        "    with open('trainer/train.py', 'w') as f:\n",
        "        f.write(train_py_content)\n",
        "\n",
        "    # Create and run the custom training job\n",
        "    job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"cmapss-rul-gemini-finetuning\",\n",
        "    script_path=\"trainer/train.py\",\n",
        "    requirements=[\"google-cloud-aiplatform\", \"google-generativeai\", \"transformers\", \"pandas\", \"torch\", \"tensorflow\"], # Added tensorflow to the requirements\n",
        "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-4.py310:latest\",  # Updated container URI\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.2-4:latest\"  # Updated serving container URI\n",
        "    )\n",
        "\n",
        "\n",
        "    model = job.run(\n",
        "        replica_count=1,\n",
        "        machine_type=\"a2-highgpu-1g\",  # Machine type with A100 GPU\n",
        "        accelerator_type=\"NVIDIA_TESLA_A100\",  # A100 accelerator\n",
        "        accelerator_count=1,\n",
        "        base_output_dir=BASE_OUTPUT_DIR,\n",
        "        args=['--model', 'rul_predictor_cmapss',\n",
        "              '--train_dataset', TRAINING_DATA_PATH,\n",
        "              '--eval_dataset', EVAL_DATA_PATH,\n",
        "              '--staging_bucket', STAGING_BUCKET,\n",
        "              '--bucket_name', BUCKET_NAME,\n",
        "              '--base_output_dir', BASE_OUTPUT_DIR],\n",
        "    )\n",
        "\n",
        "    model.wait()  # Wait for training to complete\n",
        "\n",
        "    print('\\n\\n')\n",
        "    print(f\"Model name: {model.display_name}\")\n",
        "    print(f\"Model resource name: {model.resource_name}\")\n",
        "    print(f\"Model training start time: {model.create_time}\")\n",
        "    print(f\"Model training end time: {model.update_time}\")\n",
        "    print(f\"Model training duration: {model.update_time - model.create_time}\")\n",
        "    print('\\n')\n",
        "\n",
        "    # Accessing metrics from training job completion statistics metadata\n",
        "    if model.training_job and hasattr(model.training_job, 'completion_stats') and model.training_job.completion_stats and model.training_job.completion_stats.metadata:\n",
        "        metrics = model.training_job.completion_stats.metadata\n",
        "        # Use .get() for safety\n",
        "        print(f\"Model training loss: {metrics.get('train_loss')}\")\n",
        "        print(f\"Model evaluation loss: {metrics.get('eval_loss')}\")\n",
        "        print(f\"Model evaluation metrics: {metrics}\")\n",
        "    else:\n",
        "        print(\"Training job or completion statistics metadata not available.\")\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "    logging.info(f\"Fine-tuned model: {model.resource_name}\")"
      ],
      "metadata": {
        "id": "qsXYjWHUae8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL EVALUATION"
      ],
      "metadata": {
        "id": "rA39w2VJXCPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import torch\n",
        "\n",
        "# Initialize the GCS client\n",
        "client = storage.Client()\n",
        "\n",
        "# Specify the bucket and blob (file)\n",
        "bucket_name = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob_name = \"model_output/model-nasa-gpu.pth\"\n",
        "\n",
        "blob_name = \"model_output/model-nasa-gpu.pth\"\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob = bucket.blob(blob_name)\n",
        "\n",
        "# Download the file to a local path\n",
        "local_file_path = \"model-nasa-gpu.pth\"  # Or your desired local path\n",
        "blob.download_to_filename(local_file_path)\n",
        "\n",
        "print(f\"Downloaded '{blob_name}' from '{bucket_name}' to '{local_file_path}'\")"
      ],
      "metadata": {
        "id": "PUhLGQq8PX5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8927d51-d289-4cee-8dc1-bb2e8481af96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'model_output/model-nasa-gpu.pth' from 'poc-my-new-staging-bucket-2025-1' to 'model-nasa-gpu.pth'\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Assuming this is how the CMAPSSJSONLDataset is defined in the document\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import os\n",
        "\n",
        "class CMAPSSJSONLDataset(data.Dataset):\n",
        "    def __init__(self, data_path, sequence_length=30):\n",
        "        self.data = []\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Check if data_path is a local file or a GCS URI\n",
        "        if data_path.startswith('gs://'):\n",
        "            # If GCS URI, download to a temporary file\n",
        "            storage_client = storage.Client()\n",
        "            bucket_name = data_path.split('/')[2]\n",
        "            blob_name = '/'.join(data_path.split('/')[3:])\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(blob_name)\n",
        "            tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "            blob.download_to_filename(tmp_file)\n",
        "            data_file = tmp_file\n",
        "        else:\n",
        "            # If local file, use it directly\n",
        "            data_file = data_path\n",
        "\n",
        "        with open(data_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                    rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                    self.data.append((sequence, rul))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(\"Skipping invalid JSON line: %s, Error: %s\", line, e)\n",
        "\n",
        "        # Remove the temporary file if it was created\n",
        "        if data_path.startswith('gs://'):\n",
        "            os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    #LSTM-based model for RUL prediction\n",
        "    def __init__(self, input_size, hidden_size, num_layers=3, dropout=0.3): # Match the architecture in train.py\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_cmapss_score(true_rul, predicted_rul):\n",
        "    \"\"\"Calculates a simplified CMAPSS score.\"\"\"\n",
        "    d = np.array(predicted_rul) - np.array(true_rul)  # Difference between predicted and true RUL\n",
        "    score = sum([\n",
        "        np.exp(-d[i] / 13) - 1 if d[i] < 0 else np.exp(d[i] / 10) - 1\n",
        "        for i in range(len(d))\n",
        "    ])\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset_path, input_size, hidden_size, sequence_length):\n",
        "    \"\"\"Evaluates the trained RUL prediction model.\"\"\"\n",
        "\n",
        "    # Load the saved model using the correct model architecture\n",
        "    model = RULPredictionModel(input_size, hidden_size, num_layers=3)  # Change num_layers to 3 to match the training architecture\n",
        "\n",
        "    # Explicitly load the model to the CPU\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cuda')))\n",
        "\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "\n",
        "\n",
        "    # Load the evaluation dataset\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path, sequence_length)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False)  # No need to shuffle for evaluation\n",
        "\n",
        "    # Make predictions and calculate metrics\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
        "        for sequences, ruls in eval_loader:\n",
        "            predictions = model(sequences)\n",
        "            all_predictions.extend(predictions.flatten().tolist())\n",
        "            all_targets.extend(ruls.flatten().tolist())\n",
        "\n",
        "    # Calculate evaluation metrics (e.g., MAE, RMSE)\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
        "    mse = mean_squared_error(all_targets, all_predictions)  # Calculate MSE\n",
        "\n",
        "    # Calculate CMAPSS score\n",
        "    cmapss_score = calculate_cmapss_score(all_targets, all_predictions)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Evaluation Results:\")\n",
        "    print(f\"Average Evaluation Loss (MSE): {mse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"CMAPSS Score: {cmapss_score:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Example Usage (if __name__ == \"__main__\": block) ---\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"model-nasa-gpu.pth\"  # Replace with the actual path to your saved model\n",
        "    eval_dataset_path = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_sequences.jsonl\"  # Replace with your evaluation data path (GCS URI)\n",
        "    input_size = 25  # Updated to match the input size used during training\n",
        "    hidden_size = 128 # Updated to match the hidden size used during training\n",
        "    sequence_length = 30  # Replace with the sequence length used during training\n",
        "\n",
        "    evaluate_model(model_path, eval_dataset_path, input_size, hidden_size, sequence_length)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nkS9rqGJi8XJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f78fca-e701-4f29-c9ae-c2780eb70ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "Average Evaluation Loss (MSE): 675.03\n",
            "Mean Absolute Error (MAE): 25.98\n",
            "Root Mean Squared Error (RMSE): 25.98\n",
            "CMAPSS Score: 1607.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N=10\n",
        "\n",
        "Evaluation Results:\n",
        "Average Evaluation Loss (MSE): 675.91\n",
        "Mean Absolute Error (MAE): 26.00\n",
        "Root Mean Squared Error (RMSE): 26.00\n",
        "CMAPSS Score: 1609.80\n",
        "\n",
        "N=200\n",
        "\n",
        "Evaluation Results:\n",
        "Average Evaluation Loss (MSE): 676.00\n",
        "Mean Absolute Error (MAE): 26.00\n",
        "Root Mean Squared Error (RMSE): 26.00\n",
        "CMAPSS Score: 1610.04\n",
        "\n"
      ],
      "metadata": {
        "id": "7y32T44pVnYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"GOOGLE_CLOUD_PROJECT:\", os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "print(\"GOOGLE_CLOUD_REGION:\", os.environ.get(\"GOOGLE_CLOUD_REGION\"))\n",
        "print(\"GOOGLE_CLOUD_SERVICEACCOUNT:\", os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\"))\n",
        "print(\"GOOGLE_CLOUD_PROJECT_NUMBER:\", os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\"))\n",
        "print(\"GOOGLE_CLOUD_BUCKET_NAME:\", os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\"))"
      ],
      "metadata": {
        "id": "9GwxZc-gfKaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VERTEX_AI_SERVICE_AGENT = !gcloud projects describe $PROJECT_ID --format='value(project_number)' | xargs -I{} gcloud iam service-accounts list --filter=\"displayName:Compute Engine default service account\" --project=$PROJECT_ID --format=\"value(email)\"\n",
        "VERTEX_AI_SERVICE_AGENT = VERTEX_AI_SERVICE_AGENT[0].strip()  # Access the first element (index 0)\n",
        "print(\"Vertex AI Service Agent:\", VERTEX_AI_SERVICE_AGENT)"
      ],
      "metadata": {
        "id": "S11Sx5YhfO8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPV2KnGZfXZC3vcTBcHp38l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaoRs5/QoipBlm/o7TGwwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/AGENTIC_RAG_GEMINI_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langgraph langchain-google-genai -q"
      ],
      "metadata": {
        "id": "ZVIMtvQNg-PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "3_njMlYyhErj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-generativeai==0.8.5 google-ai-generativelanguage==0.6.15 -q"
      ],
      "metadata": {
        "id": "1_5I5U7QiZZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEmncdsBdx3r"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade langchain langgraph langchain-google-genai -q\n",
        "!pip install --upgrade google-generativeai==0.8.5 google-ai-generativelanguage==0.6.15 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show google-generativeai google-ai-generativelanguage langchain langgraph langchain-google-genai"
      ],
      "metadata": {
        "id": "Nnu3GveKigTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import ast\n",
        "from typing import TypedDict, List\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.tools import tool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# --- 0. SETUP: ACCESS API KEY AND FIX AUTH ERROR/MODEL CONFIG ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "except ImportError:\n",
        "    class MockUserdata:\n",
        "        def get(self, key): return os.environ.get(key)\n",
        "    userdata = MockUserdata()\n",
        "\n",
        "# Disable GCE metadata check to prevent RefreshError\n",
        "os.environ['GOOGLE_CLOUD_DISABLE_GCE_CHECK'] = 'true'\n",
        "\n",
        "# Get and configure the API Key\n",
        "GEMINI_API_KEY = None\n",
        "try:\n",
        "    if 'GEMINI_API_KEY' not in os.environ:\n",
        "        os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI')\n",
        "\n",
        "    GEMINI_API_KEY = os.environ['GEMINI_API_KEY']\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"âœ… GEMINI_API_KEY successfully loaded and configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not load API key: {e}\")\n",
        "    print(\"Please ensure your secret is named 'GEMINI' or the environment variable is set.\")\n",
        "    if not GEMINI_API_KEY:\n",
        "        raise RuntimeError(\"API Key is missing or failed to load. Cannot proceed.\")\n",
        "\n",
        "# Define the state for LangGraph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"The state of the agent in the graph.\"\"\"\n",
        "    messages: List[BaseMessage]\n",
        "\n",
        "# --- 1. DEFINE THE TOOLS ---\n",
        "@tool\n",
        "def internal_knowledge_retriever(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieves detailed, internal documentation or facts from the secure knowledge base.\n",
        "    Use this tool for questions about company policy, specific projects, or proprietary data.\n",
        "    \"\"\"\n",
        "    knowledge_base = {\n",
        "        \"Refund Policy\": \"Full refund within 30 days for unused products, 50% refund thereafter.\",\n",
        "        \"Project Zenith\": \"Project Zenith is the new Q4 data initiative focused on multimodal embedding optimization. Lead: Dr. Aris Thorne.\",\n",
        "        \"Team Structure\": \"The Data Science team reports to the CTO, while the Engineering team reports to the COO.\",\n",
        "    }\n",
        "    query_lower = query.lower()\n",
        "    retrieved_data = [\n",
        "        f\"Document: {key}. Content: {val}\" for key, val in knowledge_base.items()\n",
        "        if query_lower in key.lower() or any(word in val.lower() for word in query_lower.split())\n",
        "    ]\n",
        "    if retrieved_data:\n",
        "        return \"\\n\".join(retrieved_data)\n",
        "    else:\n",
        "        return \"No highly relevant internal documents found for that query.\"\n",
        "\n",
        "@tool\n",
        "def financial_calculator(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs basic arithmetic calculations. Input must be a valid mathematical expression (e.g., '1200 * 0.85' or '987 / 3 - 12.5').\n",
        "    \"\"\"\n",
        "    # FIXED: Simplified AST validation to avoid false positives\n",
        "    allowed_operators = {\n",
        "        ast.Add: '+',\n",
        "        ast.Sub: '-',\n",
        "        ast.Mult: '*',\n",
        "        ast.Div: '/',\n",
        "        ast.UAdd: '+',\n",
        "        ast.USub: '-'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Parse the expression\n",
        "        tree = ast.parse(expression, mode='eval')\n",
        "        print(f\"Parsed AST: {ast.dump(tree, indent=2)}\")  # Debug: Log AST for verification\n",
        "\n",
        "        # FIXED: Better validation that only checks actual operation nodes\n",
        "        def validate_node(node):\n",
        "            if isinstance(node, ast.Expression):\n",
        "                return validate_node(node.body)\n",
        "            elif isinstance(node, ast.Constant):\n",
        "                if not isinstance(node.value, (int, float)):\n",
        "                    return False, \"Constants must be numbers\"\n",
        "                return True, None\n",
        "            elif isinstance(node, ast.BinOp):\n",
        "                if type(node.op) not in [ast.Add, ast.Sub, ast.Mult, ast.Div]:\n",
        "                    return False, f\"Disallowed binary operator: {type(node.op).__name__}\"\n",
        "                left_ok, left_msg = validate_node(node.left)\n",
        "                if not left_ok:\n",
        "                    return False, left_msg\n",
        "                right_ok, right_msg = validate_node(node.right)\n",
        "                if not right_ok:\n",
        "                    return False, right_msg\n",
        "                return True, None\n",
        "            elif isinstance(node, ast.UnaryOp):\n",
        "                if type(node.op) not in [ast.UAdd, ast.USub]:\n",
        "                    return False, f\"Disallowed unary operator: {type(node.op).__name__}\"\n",
        "                return validate_node(node.operand)\n",
        "            else:\n",
        "                return False, f\"Disallowed construct: {type(node).__name__}\"\n",
        "\n",
        "        # Validate the expression tree\n",
        "        is_valid, error_msg = validate_node(tree)\n",
        "        if not is_valid:\n",
        "            return f\"Calculation Error: {error_msg}\"\n",
        "\n",
        "        # Safely evaluate the expression\n",
        "        result = eval(compile(tree, '<string>', 'eval'), {\"__builtins__\": {}}, {})\n",
        "        if not isinstance(result, (int, float)):\n",
        "            return \"Calculation Error: Result is not a simple numeric type.\"\n",
        "        return f\"The result of {expression} is {result}\"  # Return formatted result\n",
        "    except SyntaxError:\n",
        "        return \"Calculation Error: Invalid syntax in arithmetic expression.\"\n",
        "    except ZeroDivisionError:\n",
        "        return \"Calculation Error: Division by zero is not allowed.\"\n",
        "    except Exception as e:\n",
        "        return f\"Calculation Error: Unhandled error {str(e)}. Ensure it is a valid arithmetic expression.\"\n",
        "\n",
        "# --- 2. ASSEMBLE THE AGENTIC RAG SYSTEM ---\n",
        "def setup_agentic_rag_gemini():\n",
        "    # Initialize the LLM with Gemini 2.5 Flash\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        temperature=0,\n",
        "        google_api_key=GEMINI_API_KEY\n",
        "    )\n",
        "    tools = [internal_knowledge_retriever, financial_calculator]\n",
        "    tool_map = {tool.name: tool for tool in tools}\n",
        "\n",
        "    # System prompt to enforce tool usage and prevent expression reformulation\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", (\n",
        "            \"You are a highly capable Agentic RAG system powered by Gemini 2.5 Flash. \"\n",
        "            \"For factual company questions (e.g., policies, projects, team structure), ALWAYS use the 'internal_knowledge_retriever' tool. \"\n",
        "            \"For ANY mathematical or arithmetic queries (e.g., calculations, equations), ALWAYS use the 'financial_calculator' tool and pass the expression EXACTLY as provided in the query as a string (e.g., '1200 * 0.85' or '987 / 3 - 12.5'). \"\n",
        "            \"Do NOT reformulate or modify the arithmetic expression (e.g., do NOT change '987 / 3 - 12.5' to '987 / 3 + (-12.5)'). \"\n",
        "            \"Do NOT attempt to answer math questions directly; ALWAYS use the 'financial_calculator' tool. \"\n",
        "            \"Process ONE question at a time. If a user asks multiple questions in one message, use the internal_knowledge_retriever tool ONCE with the full original query.\"\n",
        "            \"After receiving tool results, synthesize the information and provide a clear final answer to the user.\"\n",
        "        )),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ])\n",
        "\n",
        "    # Bind tools to LLM\n",
        "    llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "    # Define the agent node\n",
        "    def agent(state: AgentState) -> AgentState:\n",
        "        messages = state[\"messages\"]\n",
        "        response = llm_with_tools.invoke(prompt.invoke({\"messages\": messages}))\n",
        "        print(f\"LLM Response: {response}\")  # Debug: Log LLM response\n",
        "        return {\"messages\": messages + [response]}\n",
        "\n",
        "    # Define the tool node - FIXED to handle multiple tool calls properly\n",
        "    def call_tools(state: AgentState) -> AgentState:\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        new_messages = []\n",
        "\n",
        "        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "            # Process ALL tool calls but ensure we handle them properly\n",
        "            for tool_call in last_message.tool_calls:\n",
        "                tool_name = tool_call[\"name\"]\n",
        "                tool_args = tool_call[\"args\"]\n",
        "                tool = tool_map.get(tool_name)\n",
        "                print(f\"Invoking tool: {tool_name} with args: {tool_args}\")  # Debug: Log tool call\n",
        "\n",
        "                if tool:\n",
        "                    try:\n",
        "                        if tool_name == \"financial_calculator\":\n",
        "                            expression = str(tool_args.get(\"expression\", \"\"))\n",
        "                            if not expression:\n",
        "                                raise ValueError(\"No expression provided for financial_calculator\")\n",
        "                            result = tool.invoke(expression)\n",
        "                        else:\n",
        "                            result = tool.invoke(tool_args)\n",
        "                        new_messages.append(ToolMessage(\n",
        "                            content=str(result),\n",
        "                            tool_call_id=tool_call[\"id\"]\n",
        "                        ))\n",
        "                    except Exception as e:\n",
        "                        new_messages.append(ToolMessage(\n",
        "                            content=f\"Tool error ({tool_name}): {str(e)}\",\n",
        "                            tool_call_id=tool_call[\"id\"]\n",
        "                        ))\n",
        "                else:\n",
        "                    new_messages.append(ToolMessage(\n",
        "                        content=f\"Tool error: Tool '{tool_name}' not found.\",\n",
        "                        tool_call_id=tool_call[\"id\"]\n",
        "                    ))\n",
        "        else:\n",
        "            print(\"No tool calls found in last message.\")  # Debug: Log if no tool calls\n",
        "\n",
        "        return {\"messages\": state[\"messages\"] + new_messages}\n",
        "\n",
        "    # Define the router - FIXED VERSION\n",
        "    def router(state: AgentState) -> str:\n",
        "        last_message = state[\"messages\"][-1]\n",
        "\n",
        "        # If the last message is from a tool, we should go back to the agent to synthesize the answer\n",
        "        if isinstance(last_message, ToolMessage):\n",
        "            return \"agent\"\n",
        "\n",
        "        # If the last message has tool calls, we need to execute them\n",
        "        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "            return \"tools\"\n",
        "\n",
        "        # Otherwise, we're done\n",
        "        return END\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(AgentState)\n",
        "    workflow.add_node(\"agent\", agent)\n",
        "    workflow.add_node(\"tools\", call_tools)\n",
        "    workflow.set_entry_point(\"agent\")\n",
        "    workflow.add_conditional_edges(\"agent\", router, {\"tools\": \"tools\", END: END})\n",
        "    workflow.add_conditional_edges(\"tools\", router, {\"agent\": \"agent\", END: END})\n",
        "    agent_executor = workflow.compile()\n",
        "\n",
        "    return agent_executor\n",
        "\n",
        "# --- 3. RUN THE DEMO ---\n",
        "if __name__ == \"__main__\":\n",
        "    if \"GEMINI_API_KEY\" not in os.environ or not os.environ.get(\"GEMINI_API_KEY\"):\n",
        "        print(\"\\nâŒ DEMO FAILED: API Key is missing. Please fix the setup in step 0.\")\n",
        "    else:\n",
        "        try:\n",
        "            agent_executor = setup_agentic_rag_gemini()\n",
        "            print(\"\\n\" + \"ğŸŒŸ\"*20)\n",
        "            print(\"    REAL AGENTIC RAG DEMO with GEMINI 2.5 FLASH (LangGraph)    \")\n",
        "            print(\"ğŸŒŸ\"*20)\n",
        "\n",
        "            # Test 1: Triggers the RAG Tool\n",
        "            query_1 = \"Who leads Project Zenith, and who does the Engineering team report to?\"\n",
        "            print(f\"\\n[USER 1]: {query_1}\")\n",
        "            result_1 = agent_executor.invoke({\"messages\": [HumanMessage(content=query_1)]})\n",
        "            final_message_1 = result_1['messages'][-1]\n",
        "            print(f\"\\n[AGENT FINAL ANSWER 1]: {final_message_1.content if hasattr(final_message_1, 'content') else 'No content in final message.'}\")\n",
        "\n",
        "            print(\"\\n\" + \"---\"*23)\n",
        "\n",
        "            # Test 2: Triggers the Calculator Tool\n",
        "            query_2 = \"What is the result of 987 divided by 3, minus 12.5?\"\n",
        "            print(f\"\\n[USER 2]: {query_2}\")\n",
        "            result_2 = agent_executor.invoke({\"messages\": [HumanMessage(content=query_2)]})\n",
        "            final_message_2 = result_2['messages'][-1]\n",
        "            print(f\"\\n[AGENT FINAL ANSWER 2]: {final_message_2.content if hasattr(final_message_2, 'content') else 'No content in final message.'}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ Execution Error during agent run: {e}\")\n",
        "            print(\"This usually means a problem with your API key, configuration, or network.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzPvIODkpxPB",
        "outputId": "968b75f8-8c8a-4edd-8d57-e6fe065b1654"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GEMINI_API_KEY successfully loaded and configured.\n",
            "\n",
            "ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ\n",
            "    REAL AGENTIC RAG DEMO with GEMINI 2.5 FLASH (LangGraph)    \n",
            "ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ\n",
            "\n",
            "[USER 1]: Who leads Project Zenith, and who does the Engineering team report to?\n",
            "LLM Response: content='' additional_kwargs={'function_call': {'name': 'internal_knowledge_retriever', 'arguments': '{\"query\": \"Who leads Project Zenith, and who does the Engineering team report to?\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--2ec65860-bf96-4f3f-bd42-bf7489aecaf2-0' tool_calls=[{'name': 'internal_knowledge_retriever', 'args': {'query': 'Who leads Project Zenith, and who does the Engineering team report to?'}, 'id': '4dfcf66f-c260-4e52-8aa7-722baf96ec1c', 'type': 'tool_call'}]\n",
            "Invoking tool: internal_knowledge_retriever with args: {'query': 'Who leads Project Zenith, and who does the Engineering team report to?'}\n",
            "LLM Response: content='Dr. Aris Thorne leads Project Zenith. The Engineering team reports to the COO.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--ca4700ae-0948-43e1-971e-910808409117-0'\n",
            "\n",
            "[AGENT FINAL ANSWER 1]: Dr. Aris Thorne leads Project Zenith. The Engineering team reports to the COO.\n",
            "\n",
            "---------------------------------------------------------------------\n",
            "\n",
            "[USER 2]: What is the result of 987 divided by 3, minus 12.5?\n",
            "LLM Response: content='' additional_kwargs={'function_call': {'name': 'financial_calculator', 'arguments': '{\"expression\": \"987 / 3 - 12.5\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--c473a612-d936-4ffc-95a8-dcd7ba7b31a4-0' tool_calls=[{'name': 'financial_calculator', 'args': {'expression': '987 / 3 - 12.5'}, 'id': 'aecb69c2-fe98-4161-82a4-f294d521ec84', 'type': 'tool_call'}]\n",
            "Invoking tool: financial_calculator with args: {'expression': '987 / 3 - 12.5'}\n",
            "Parsed AST: Expression(\n",
            "  body=BinOp(\n",
            "    left=BinOp(\n",
            "      left=Constant(value=987),\n",
            "      op=Div(),\n",
            "      right=Constant(value=3)),\n",
            "    op=Sub(),\n",
            "    right=Constant(value=12.5)))\n",
            "LLM Response: content='The result of 987 divided by 3, minus 12.5 is 316.5.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--462968e3-cd06-4e38-ad97-60cd7df83486-0'\n",
            "\n",
            "[AGENT FINAL ANSWER 2]: The result of 987 divided by 3, minus 12.5 is 316.5.\n"
          ]
        }
      ]
    }
  ]
}
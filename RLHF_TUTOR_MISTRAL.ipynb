{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOd35aNlDobHXPGWu6/crUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/RLHF_TUTOR_MISTRAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets trl peft accelerate bitsandbytes --q"
      ],
      "metadata": {
        "id": "rkBb6p6_aiWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZteJvndb5Av",
        "outputId": "c3151de0-75f1-4750-a8e9-1d002f265541"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 29 01:47:09 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   51C    P8              13W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import gc  # Import the garbage collector\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig, EarlyStoppingCallback, TrainerCallback, TrainerState, TrainerControl # Import necessary modules\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput, SequenceClassifierOutputWithPast # Add this import\n",
        "\n",
        "from datasets import load_dataset\n",
        "from trl import RewardTrainer, RewardConfig\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model  # Import PEFT modules\n",
        "\n",
        "\n",
        "import torch.nn as nn  # Import the neural network module\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Enable anomaly detection\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "\n",
        "# Load the LLaMA 2 model and tokenizer\n",
        "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the Mistral 7B model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Changed to Mistral 7B\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Add a padding token to the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Quantization config\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Use 4-bit quantization for lower memory usage\n",
        "    bnb_4bit_use_double_quant=True,  # Enable double quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Use nf4 quantization type\n",
        "    #bnb_4bit_compute_dtype=torch.float16  # Set compute dtype to float16\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Change to bfloat16\n",
        ")\n",
        "\n",
        "# Load the model with quantization\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=1,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Set pad_token_id in the model config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# PEFT configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=8,  # Dimensionality of the low-rank matrices\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.05,  # Dropout probability\n",
        "    bias=\"none\",  # No bias for the PEFT adapters\n",
        "    task_type=\"SEQ_CLS\",  # Sequence classification task\n",
        ")\n",
        "\n",
        "# Add PEFT adapters to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "print('\\n')\n",
        "print('Print the number of trainable parameters')\n",
        "model.print_trainable_parameters()  # Print the number of trainable parameters\n",
        "print('\\n\\n')\n",
        "\n",
        "# Store the original model with PEFT adapters\n",
        "original_model = model\n",
        "\n",
        "# Load the Anthropic HH-RLHF dataset\n",
        "dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
        "\n",
        "def format_data(example):\n",
        "  if isinstance(example[\"chosen\"], list):\n",
        "    chosen_text = \" \".join([item[\"text\"] for item in example[\"chosen\"]])\n",
        "  else:\n",
        "    chosen_text = example[\"chosen\"]  # If it's a string, use it directly\n",
        "\n",
        "  if isinstance(example[\"rejected\"], list):\n",
        "    rejected_text = \" \".join([item[\"text\"] for item in example[\"rejected\"]])\n",
        "  else:\n",
        "    rejected_text = example[\"rejected\"]  # If it's a string, use it directly\n",
        "\n",
        "  # Tokenize the chosen and rejected texts with padding\n",
        "  chosen_encoding = tokenizer(chosen_text, truncation=True, max_length=512, padding=\"max_length\")\n",
        "  rejected_encoding = tokenizer(rejected_text, truncation=True, max_length=512, padding=\"max_length\")\n",
        "\n",
        "  return {\n",
        "      \"input_ids_chosen\": chosen_encoding[\"input_ids\"],\n",
        "      \"attention_mask_chosen\": chosen_encoding[\"attention_mask\"],\n",
        "      \"input_ids_rejected\": rejected_encoding[\"input_ids\"],\n",
        "      \"attention_mask_rejected\": rejected_encoding[\"attention_mask\"],\n",
        "  }\n",
        "\n",
        "# Format the dataset\n",
        "dataset = dataset.map(format_data)\n",
        "\n",
        "# Split the dataset into train and eval sets\n",
        "#train_dataset = dataset[\"train\"].select(range(100000))  # Select first 100k examples for training\n",
        "#eval_dataset = dataset[\"train\"].select(range(100000, 110000))  # Select next 10k examples for evaluation\n",
        "#Total steps = (Number of training examples) / (Effective batch size)\n",
        "#             = 100,000 / 8\n",
        "#             = 12,500\n",
        "\n",
        "# Split the dataset into train and eval sets FOR POC\n",
        "#train_dataset = dataset[\"train\"].select(range(10000))  # Select first 10k examples\n",
        "#eval_dataset = dataset[\"train\"].select(range(10000, 11000))  # Select next 1k examples\n",
        "#By making these changes, you'll reduce the total training steps from 12,500 to 1,250.\n",
        "#This will significantly shorten the runtime for your POC, allowing you to experiment and\n",
        "#iterate more quickly.\n",
        "\n",
        "\n",
        "# Split the dataset (using 1,000 examples for the POC)\n",
        "train_dataset = dataset[\"train\"].select(range(1000))\n",
        "eval_dataset = dataset[\"train\"].select(range(1000, 2000))\n",
        "\n",
        "# Training arguments as RewardConfig - Modified\n",
        "training_args = RewardConfig(\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    gradient_accumulation_steps=8,  # Increased gradient accumulation\n",
        "    learning_rate=1e-6,  # Further reduced learning rate\n",
        "    #fp16=False,  # Disable fp16\n",
        "    fp16=True,  # Enable fp16 for potentially better performance\n",
        "    logging_steps=25,\n",
        "    output_dir=\"reward_model\",\n",
        "    num_train_epochs=1,  # You can increase this for better results\n",
        "    report_to=\"none\",  # Disable wandb reporting\n",
        "    load_best_model_at_end=True,  # Ensure the best model is loaded\n",
        "    evaluation_strategy=\"steps\",  # Evaluate and save every \"steps\"\n",
        "    save_strategy=\"steps\",\n",
        "    remove_unused_columns=False  # Prevent removal of unused columns\n",
        ")\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Prepare the model and data loaders with accelerate\n",
        "model, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, train_dataset, eval_dataset\n",
        ")\n",
        "\n",
        "# Train the reward model\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataloader,  # Use the prepared train data loader\n",
        "    eval_dataset=eval_dataloader,  # Use the prepared eval data loader\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Add early stopping\n",
        ")\n",
        "\n",
        "####### NEW ####\n",
        "\n",
        "# Initialize the optimizer with a lower learning rate and gradient clipping\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)  # Slightly higher learning rate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a gradient clipping function\n",
        "#def clip_gradients(params):\n",
        "#    torch.nn.utils.clip_grad_norm_(params, 1.0)  # Standard clip value\n",
        "\n",
        "# Create a gradient clipping function with an adjustable norm (corrected)\n",
        "def clip_gradients(params, clip_norm=1.0):\n",
        "    torch.nn.utils.clip_grad_norm_(params, clip_norm)  # Correct usage of clip_grad_norm_\n",
        "\n",
        "# Override the training step function to apply gradient clipping more frequently\n",
        "class GradientClippingCallback(TrainerCallback):\n",
        "    def on_step_end(self, args: training_args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        clip_gradients(kwargs[\"model\"].parameters(), clip_norm=0.5)  # Example: Clip with norm 0.5\n",
        "        return control\n",
        "\n",
        "\n",
        "\n",
        "trainer.add_callback(GradientClippingCallback())  # Add the callback to the trainer\n",
        "\n",
        "\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "\n",
        "# Calculate the total number of training steps across all epochs using enumerate\n",
        "total_steps = sum(1 for _ in enumerate(train_dataloader)) * training_args.num_train_epochs\n",
        "\n",
        "# Create a single tqdm progress bar for the entire training process\n",
        "progress_bar = tqdm(total=total_steps, desc=\"Training Progress\", leave=False)"
      ],
      "metadata": {
        "id": "qQe_p-eoc-m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n')\n",
        "print('Print the number of trainable parameters')\n",
        "model.print_trainable_parameters()  # Print the number of trainable parameters\n",
        "print('\\n\\n')\n",
        "\n",
        "print('\\n')\n",
        "# Training loop with adjusted input handling and tensor conversion\n",
        "for epoch in range(training_args.num_train_epochs):\n",
        "    # Create a tqdm progress bar for the training loop\n",
        "    #progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{training_args.num_train_epochs}\", leave=False)\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        #if int(step)%100==0:\n",
        "          #print(f\"Epoch {epoch}, Step {step}\")\n",
        "\n",
        "        # --- Check for NaN in input tensors ---\n",
        "        for key, value in batch.items():\n",
        "            if torch.is_tensor(value) and torch.isnan(value).any():\n",
        "                print(f\"Warning: NaN values found in input tensor '{key}' at epoch {epoch}, step {step}\")\n",
        "                # Handle NaN values (e.g., replace with 0, skip the batch, etc.)\n",
        "                # Example: Replace NaN with 0\n",
        "                batch[key] = torch.nan_to_num(value, nan=0.0)\n",
        "\n",
        "        # Extract the input tensors from the batch\n",
        "        input_ids_chosen = batch.get(\"input_ids_chosen\")\n",
        "        attention_mask_chosen = batch.get(\"attention_mask_chosen\")\n",
        "        input_ids_rejected = batch.get(\"input_ids_rejected\")\n",
        "        attention_mask_rejected = batch.get(\"attention_mask_rejected\")\n",
        "\n",
        "\n",
        "\n",
        "        # Convert lists to tensors and move to device\n",
        "        # Ensure tensors are on the correct device and have the correct data type\n",
        "        input_ids_chosen = torch.tensor(input_ids_chosen, device=device, dtype=torch.long)\n",
        "        attention_mask_chosen = torch.tensor(attention_mask_chosen, device=device, dtype=torch.long)\n",
        "        input_ids_rejected = torch.tensor(input_ids_rejected, device=device, dtype=torch.long)\n",
        "        attention_mask_rejected = torch.tensor(attention_mask_rejected, device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "        # Reshape input_ids before concatenating\n",
        "        input_ids_chosen = input_ids_chosen.unsqueeze(0)\n",
        "        input_ids_rejected = input_ids_rejected.unsqueeze(0)\n",
        "        input_ids = torch.cat([input_ids_chosen, input_ids_rejected], dim=0)\n",
        "\n",
        "        # Reshape attention masks before concatenating\n",
        "        attention_mask_chosen = attention_mask_chosen.unsqueeze(0)\n",
        "        attention_mask_rejected = attention_mask_rejected.unsqueeze(0)\n",
        "        #attention_mask = torch.cat([attention_mask_chosen, attention_mask_rejected], dim=0)\n",
        "\n",
        "\n",
        "        attention_mask = torch.cat([attention_mask_chosen, attention_mask_rejected], dim=0)\n",
        "\n",
        "\n",
        "        # Pass the concatenated inputs to the model\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # --- Modified loss calculation and gradient handling ---\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # 1: Check for NaN in logits and replace with 0\n",
        "        #logits = torch.nan_to_num(logits, nan=0.0)\n",
        "\n",
        "\n",
        "         #--- Check for NaN in logits and handle them ---\n",
        "        if torch.isnan(logits).any():\n",
        "            print(\"Warning: NaN values found in logits!\")\n",
        "            # Handle NaN values (e.g., replace with 0, skip the batch, etc.)\n",
        "            # Example: Replace NaN with 0\n",
        "            logits = torch.nan_to_num(logits, nan=0.0)\n",
        "\n",
        "\n",
        "        # 2: Apply sigmoid to get probabilities\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        # 3: Ensure probs are float32 before clipping\n",
        "        probs = probs.type(torch.float32)\n",
        "\n",
        "        # 3a: Cast logits to float32 before loss calculation\n",
        "        logits = logits.type(torch.float32)  # Cast logits to float32\n",
        "\n",
        "\n",
        "        # 4: Clip probabilities to [0, 1]\n",
        "        #probs = torch.clamp(probs, 0.0, 1.0)\n",
        "\n",
        "        #--- Ensure all tensors have the same dtype (float32) ---\n",
        "        probs = torch.sigmoid(logits)\n",
        "        #probs = torch.clamp(probs, 0.0, 1.0) # Clip probabilities (optional)\n",
        "\n",
        "\n",
        "        # 5: Create labels with the same shape as logits (probs) and move to device\n",
        "        labels = torch.zeros(logits.size(), dtype=torch.float32, device=device)\n",
        "        labels[0, 0] = 1  # Label for chosen text\n",
        "        labels[1, 0] = 0  # Label for rejected text\n",
        "\n",
        "        # 6: Print shapes and dtypes for debugging\n",
        "        #print(\"Logits shape:\", logits.shape, \"dtype:\", logits.dtype)\n",
        "        #print(\"Probs shape:\", probs.shape, \"dtype:\", probs.dtype)\n",
        "        #print(\"Labels shape:\", labels.shape, \"dtype:\", labels.dtype)\n",
        "\n",
        "\n",
        "        # 7xs: Use BCELoss\n",
        "        loss_fn = nn.BCELoss()\n",
        "        loss = loss_fn(probs, labels)\n",
        "\n",
        "\n",
        "        # --- Gradient Scaling ---\n",
        "        loss = loss / training_args.gradient_accumulation_steps  # Scale the loss\n",
        "\n",
        "        if int(step)%100==0:\n",
        "          #print(f\"Loss: {loss}\")\n",
        "          print('\\n')\n",
        "          print(f\"Epoch {epoch}, Step {step}, Loss: {loss}\")\n",
        "          print('\\n')\n",
        "\n",
        "\n",
        "        # --- More frequent gradient clipping ---\n",
        "        clip_gradients(model.parameters(), 0.5)  # Clip every step\n",
        "\n",
        "        # Clip gradients more frequently (e.g., every 10 steps)\n",
        "        #if step % 10 == 0:\n",
        "        #    clip_gradients(model.parameters(), 0.5)  # Correct usage\n",
        "\n",
        "\n",
        "\n",
        "        # --- Gradient Handling ---\n",
        "        # 7: Reduce learning rate (if needed)\n",
        "        #optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6) # Example: Reduced lr\n",
        "\n",
        "        # 8: Lower gradient accumulation steps (if needed)\n",
        "        # Example: Reduced accumulation to potentially improve stability\n",
        "        #training_args.gradient_accumulation_steps = 4\n",
        "\n",
        "\n",
        "        # --- Backpropagation and Optimization ---\n",
        "        #loss.backward() # Calculate gradients\n",
        "\n",
        "        # 9: Use gradient clipping\n",
        "        #clip_gradients(params=model.parameters())  # Clip gradients\n",
        "\n",
        "        #optimizer.step() # Update model parameters using scaler\n",
        "        #optimizer.zero_grad() # Reset gradients after each step\n",
        "\n",
        "        # Update the progress bar after each step\n",
        "        #print('\\n')\n",
        "        progress_bar.update(1)  # Manually update\n",
        "        #print('\\n')\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model(\"reward_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl8Hp5ag79gP",
        "outputId": "1808bc63-84ae-4331-8a94-232930bb92b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Print the number of trainable parameters\n",
            "trainable params: 3,411,968 || all params: 7,114,076,160 || trainable%: 0.0480\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Progress:   2%|▏         | 18/1000 [00:55<46:17,  2.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 0, Loss: 0.21893148124217987\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  12%|█▏        | 118/1000 [03:27<22:13,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 100, Loss: 0.6079674363136292\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  22%|██▏       | 218/1000 [05:59<19:30,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 200, Loss: 0.09748947620391846\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  32%|███▏      | 318/1000 [08:31<17:08,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 300, Loss: 0.027507081627845764\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  42%|████▏     | 418/1000 [11:03<14:48,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 400, Loss: 0.06258226931095123\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  52%|█████▏    | 518/1000 [13:35<12:05,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 500, Loss: 0.04816298559308052\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  62%|██████▏   | 618/1000 [16:08<09:34,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 600, Loss: 0.09454096853733063\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  72%|███████▏  | 718/1000 [18:40<07:10,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 700, Loss: 0.11152325570583344\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  82%|████████▏ | 818/1000 [21:12<04:45,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 800, Loss: 0.13049648702144623\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  92%|█████████▏| 918/1000 [23:44<02:04,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 0, Step 900, Loss: 0.30196622014045715\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 1017it [26:14,  1.51s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test cases\n",
        "def evaluate_example(prompt, chosen, rejected):\n",
        "  inputs = tokenizer(\n",
        "      [f\"{prompt} {chosen}\", f\"{prompt} {rejected}\"],\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True\n",
        "  ).to(accelerator.device)  # Move inputs to the appropriate device\n",
        "\n",
        "  #print(\"Input IDs:\", inputs[\"input_ids\"])\n",
        "  #print(\"Attention Mask:\", inputs[\"attention_mask\"])\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "  chosen_score = outputs.logits[0].item()\n",
        "  rejected_score = outputs.logits[1].item()\n",
        "  print(f\"Chosen score: {chosen_score}, Rejected score: {rejected_score}\")\n",
        "  return chosen_score > rejected_score\n",
        "\n",
        "# Example usage\n",
        "prompt = \"What is the capital of France?\"\n",
        "chosen = \"Paris\"\n",
        "rejected = \"London\"\n",
        "print('\\n')\n",
        "print(f\"Prompt: {prompt}, Chosen: {chosen}, Rejected: {rejected}\")\n",
        "\n",
        "print('\\n')\n",
        "if evaluate_example(prompt, chosen, rejected):\n",
        "  print(\"Test passed!\")\n",
        "else:\n",
        "  print(\"Test failed.\")"
      ],
      "metadata": {
        "id": "dRqneQ87snJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8705212-c268-4455-cd8e-5b22f1eab54e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt: What is the capital of France?, Chosen: Paris, Rejected: London\n",
            "\n",
            "\n",
            "Chosen score: -3.775390625, Rejected score: -6.671875\n",
            "Test passed!\n"
          ]
        }
      ]
    }
  ]
}
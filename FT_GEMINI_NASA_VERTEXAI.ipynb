{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xJeokSspU9XI"
      ],
      "authorship_tag": "ABX9TyN3FMLA03815SSRkFhZLJi/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FT_GEMINI_NASA_VERTEXAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install rouge-score -q"
      ],
      "metadata": {
        "id": "5VFKK5piHxhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Format (JSON)"
      ],
      "metadata": {
        "id": "xJeokSspU9XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"contents\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"Engine sensor readings over time: [1.0, 41.9993, 0.8409, 100.0, 445.0, 548.68, 1343.85, 1111.03, 3.91, 5.69, 137.26, 2211.96, 8296.96, ..., 8054.65, 9.2728, 0.02, 331.0, 2223.0, 100.0, 14.78, 8.8922]\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"Remaining Useful Life: 0\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "J9zmGV6OAiMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning"
      ],
      "metadata": {
        "id": "fdef11swU2bJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESPsgxVdHMRh"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.tuning import sft\n",
        "import vertexai\n",
        "import os\n",
        "from google.colab import auth\n",
        "import colab_env\n",
        "import time\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Define your tuning parameters\n",
        "BASE_MODEL = \"gemini-2.0-flash-001\"  # Using Gemini 2.0 Flash\n",
        "\n",
        "TRAIN_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_text.jsonl\"  # Path to your training data in JSONL format\n",
        "VALIDATION_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\"  # Path to your validation data in JSONL format\n",
        "TUNED_MODEL_DISPLAY_NAME = \"cmapss-text-tuned-gemini-2.0-flash-001\"\n",
        "EPOCHS = 10  # Adjust as needed\n",
        "LEARNING_RATE_MULTIPLIER = 1.0  # Adjust as needed\n",
        "\n",
        "\n",
        "\n",
        "# Start the fine-tuning job\n",
        "try:\n",
        "    sft_tuning_job = sft.train(\n",
        "        source_model=BASE_MODEL,\n",
        "        train_dataset=TRAIN_DATASET_URI,\n",
        "        validation_dataset=VALIDATION_DATASET_URI,\n",
        "        tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\n",
        "        epochs=EPOCHS,\n",
        "        learning_rate_multiplier=LEARNING_RATE_MULTIPLIER,\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Tuning job started: {sft_tuning_job.resource_name}\")\n",
        "\n",
        "    # Periodically check the job status until it's complete\n",
        "    while True:\n",
        "        job_status = sft_tuning_job.state  # Get the job's state directly\n",
        "\n",
        "        if job_status in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
        "            break  # Exit the loop if the job is finished\n",
        "\n",
        "        print(f\"Job status: {job_status}, waiting...\")\n",
        "        time.sleep(60)  # Wait for 60 seconds before checking again\n",
        "\n",
        "    print(f\"Tuning job completed with status: {job_status}. Resource name: {sft_tuning_job.resource_name}\")\n",
        "\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please double-check the base model name and your Vertex AI setup.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# List all custom models\n",
        "models = aiplatform.Model.list()\n",
        "\n",
        "print(\"Inspecting metadata of all models:\")\n",
        "print(\"Inspecting the dictionary representation of each model:\")\n",
        "for model in models:\n",
        "    print(f\"\\nDisplay Name: {model.display_name}\")\n",
        "    print(f\"Resource Name: {model.resource_name}\")\n",
        "    model_dict = model.to_dict()\n",
        "    print(f\"Model Dictionary: {model_dict}\")"
      ],
      "metadata": {
        "id": "vpkS41BjYYCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# List all custom models\n",
        "models = aiplatform.Model.list()\n",
        "\n",
        "\n",
        "\n",
        "# Filter for models that have the 'google-vertex-llm-tuning-job-id' label\n",
        "fine_tuned_models = [\n",
        "    model for model in models\n",
        "    if model.labels is not None and 'google-vertex-llm-tuning-job-id' in model.labels and 'gemini-2.0' in model.display_name\n",
        "]\n",
        "\n",
        "\n",
        "fine_tuned_models=fine_tuned_models[1:2]\n",
        "\n",
        "# Print the display names and resource names of the fine-tuned models\n",
        "if fine_tuned_models:\n",
        "    print(\"\\nSuccessfully found fine-tuned models:\")\n",
        "    for model in fine_tuned_models:\n",
        "        print(f\"- Display Name: {model.display_name}\")\n",
        "        resource_name = model.resource_name\n",
        "        model_resource_name_id = resource_name.split('/')[-1]\n",
        "        print(f\"- Resource Name Model: {model_resource_name_id}\")\n",
        "        print(f\"- Resource Name Tuning Job: {model.labels['google-vertex-llm-tuning-job-id']}\")\n",
        "        print(f\"- Tune Type: {model.labels['tune-type']}\")  # Add this line to print the model state\n",
        "        #print('\\n')\n",
        "        print(f\"- Create Time: {model.create_time}\")\n",
        "\n",
        "        #model.gca_resource.deployed_models\n",
        "        #endpoints = [deployed_model.endpoint for deployed_model in model.gca_resource.deployed_models]\n",
        "        #endpoint_id = endpoints[0].split('/')[-1]  # Get the last part after splitting by '/'\n",
        "        #print(f\"- Model Endpoint: {endpoint_id}\")\n",
        "\n",
        "        if model.gca_resource.deployed_models:\n",
        "            endpoints = [deployed_model.endpoint for deployed_model in model.gca_resource.deployed_models]\n",
        "            endpoint_id = endpoints[0].split('/')[-1]  # Get the last part after splitting by '/'\n",
        "            print(f\"- Model Endpoint: {endpoint_id}\")\n",
        "        else:\n",
        "            print(f\"- Model Endpoint: Not Deployed\")  # Indicate if the model is not deployed\n",
        "\n",
        "\n",
        "\n",
        "        #print(f\"- Status: {aiplatform.Model(model_name=model.resource_name).gca_resource}\")\n",
        "        print('\\n')\n",
        "        # Get state using Model(model_name=...).state\n",
        "        #print(f\"- Status: {aiplatform.Model(model_name=model.resource_name).state}\")\n",
        "else:\n",
        "    print(\"\\nNo fine-tuned models found based on the tuning job ID label.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UeamxDGa6Eb",
        "outputId": "fe3a890b-f93c-4e8e-d791-a179263152af"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully found fine-tuned models:\n",
            "- Display Name: cmapss-text-tuned-gemini-2.0-flash-001\n",
            "- Resource Name Model: 1440268972921454592\n",
            "- Resource Name Tuning Job: 5437787329584431104\n",
            "- Tune Type: sft\n",
            "- Create Time: 2025-04-05 09:49:50.951936+00:00\n",
            "- Model Endpoint: 7903506184244559872\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "STR83WXTNL-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install colab-env -q"
      ],
      "metadata": {
        "id": "PsvZT3bi0nQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import json\n",
        "import os\n",
        "import colab_env\n",
        "from google.cloud import aiplatform\n",
        "from google.colab import auth\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "EVAL_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\"  # Update with your dataset URI\n",
        "tuned_model_resource_name = f'projects/{PROJECT_NUMBER}/locations/{REGION}/models/1440268972921454592@1'\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "def generate_and_evaluate():\n",
        "    client = genai.Client(\n",
        "        vertexai=True,\n",
        "        project=PROJECT_ID,\n",
        "        location=REGION,\n",
        "    )\n",
        "\n",
        "    model_endpoint = f\"projects/{PROJECT_NUMBER}/locations/{REGION}/endpoints/{endpoint_id}\"  # Update with your model endpoint\n",
        "\n",
        "\n",
        "    print('\\n')\n",
        "    report=f\"Evaluation of the model in Vertex AI: {model_resource_name_id} in the endpoint {endpoint_id}, with the dataset: {EVAL_DATASET_URI}\"\n",
        "    print(report)\n",
        "    print('\\n\\n')\n",
        "\n",
        "\n",
        "    validation_dataset_uri = EVAL_DATASET_URI\n",
        "\n",
        "    # Copy the validation dataset locally\n",
        "    local_dataset_path = '/content/cmapss_FD004_test_text.jsonl'\n",
        "    !gsutil cp {validation_dataset_uri} .\n",
        "    print('\\n')\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    all_scores = []\n",
        "\n",
        "    num_lines = sum(1 for line in open(local_dataset_path))  # Calculate total lines beforehand\n",
        "\n",
        "\n",
        "    # Read and process the dataset file\n",
        "    with open(local_dataset_path, 'r') as f:\n",
        "        for line in tqdm(f, total=num_lines, desc=\"Processing dataset\"):  # Set total and description\n",
        "            data = json.loads(line)\n",
        "\n",
        "            # Extract prompt and ground truth from JSON structure\n",
        "            try:\n",
        "                prompt = data['contents'][0]['parts'][0]['text']\n",
        "                ground_truth_text = data['contents'][1]['parts'][0]['text']\n",
        "            except (IndexError, KeyError):\n",
        "                print(\"Skipping invalid data point:\", line)\n",
        "                continue  # Skip to the next line\n",
        "\n",
        "            if prompt and ground_truth_text:\n",
        "                contents = [prompt]\n",
        "\n",
        "                # Generate content for the current prompt\n",
        "                generated_text = \"\"\n",
        "                try:\n",
        "                    for chunk in client.models.generate_content_stream(\n",
        "                        model=model_endpoint,\n",
        "                        contents=contents,\n",
        "                        config=types.GenerateContentConfig(\n",
        "                            temperature=1,\n",
        "                            top_p=0.95,\n",
        "                            max_output_tokens=8192,\n",
        "                            response_modalities=[\"TEXT\"],\n",
        "                            safety_settings=[types.SafetySetting(category=c, threshold=\"OFF\") for c in [\n",
        "                                \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "                                \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "                                \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "                                \"HARM_CATEGORY_HARASSMENT\",\n",
        "                            ]],\n",
        "                        ),\n",
        "                    ):\n",
        "                        generated_text += chunk.text\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during text generation for prompt '{prompt[:50]}...': {e}\")\n",
        "                    continue  # Skip to the next line\n",
        "\n",
        "                # Calculate ROUGE scores\n",
        "                scores = scorer.score(ground_truth_text, generated_text)\n",
        "                all_scores.append(scores)\n",
        "\n",
        "\n",
        "    # Calculate and print average ROUGE scores\n",
        "    if all_scores:\n",
        "        avg_rouge1 = sum(s['rouge1'].fmeasure for s in all_scores) / len(all_scores)\n",
        "        avg_rougeL = sum(s['rougeL'].fmeasure for s in all_scores) / len(all_scores)\n",
        "        print('\\n\\n')\n",
        "        print(f\"Average ROUGE-1: {avg_rouge1}\")\n",
        "        print(f\"Average ROUGE-L: {avg_rougeL}\")\n",
        "        print('\\n')\n",
        "    else:\n",
        "        print(\"No ROUGE scores were calculated. Check the dataset and text generation process.\")\n",
        "\n",
        "generate_and_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJZHje3aUkIE",
        "outputId": "6f34f8ad-4549-46cf-efc5-35e49c14b92a"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Evaluation of the model in Vertex AI: 1440268972921454592 in the endpoint 7903506184244559872, with the dataset: gs://poc-my-new-staging-bucket-2025-1/cmapss_FD004_test_text.jsonl\n",
            "\n",
            "\n",
            "\n",
            "Copying gs://poc-my-new-staging-bucket-2025-1/cmapss_FD004_test_text.jsonl...\n",
            "/ [1 files][  1.4 MiB/  1.4 MiB]                                                \n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dataset: 100%|██████████| 252/252 [01:29<00:00,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Average ROUGE-1: 0.75\n",
            "Average ROUGE-L: 0.75\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "* ROUGE-1: Measures the overlap of unigrams (individual words) between the generated text and the ground truth. A score of 0.75 suggests a relatively high degree of similarity at the word level.\n",
        "\n",
        "* ROUGE-L: Measures the longest common subsequence (LCS) between the generated text and the ground truth, taking into account sentence structure. A score of 0.75 also indicates a good level of similarity in terms of overall sentence structure and content.\n",
        "\n",
        "1. Overall, these scores suggest that the LLM is performing well in generating text that is similar to the ground truth in terms of both word-level and sentence-level structure and content.\n",
        "\n",
        "2. Of course, the interpretation of these scores can depend on the specific task and the desired level of accuracy. However, in general, ROUGE scores above 0.5 are often considered to be reasonably good, and scores above 0.7 are considered to be very good."
      ],
      "metadata": {
        "id": "sdNNpR41VKja"
      }
    }
  ]
}
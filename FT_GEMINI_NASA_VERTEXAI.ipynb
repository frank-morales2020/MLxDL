{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEPUtBSVPrec+W0ZgqFz3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FT_GEMINI_NASA_VERTEXAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install rouge-score -q"
      ],
      "metadata": {
        "id": "5VFKK5piHxhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Format (JSON)"
      ],
      "metadata": {
        "id": "xJeokSspU9XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"contents\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"Engine sensor readings over time: [1.0, 41.9993, 0.8409, 100.0, 445.0, 548.68, 1343.85, 1111.03, 3.91, 5.69, 137.26, 2211.96, 8296.96, ..., 8054.65, 9.2728, 0.02, 331.0, 2223.0, 100.0, 14.78, 8.8922]\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"Remaining Useful Life: 0\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "J9zmGV6OAiMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning"
      ],
      "metadata": {
        "id": "fdef11swU2bJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESPsgxVdHMRh"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.tuning import sft\n",
        "import vertexai\n",
        "import os\n",
        "from google.colab import auth\n",
        "import colab_env\n",
        "import time\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Define your tuning parameters\n",
        "BASE_MODEL = \"gemini-2.0-flash-001\"  # Using Gemini 2.0 Flash\n",
        "\n",
        "TRAIN_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_text.jsonl\"  # Path to your training data in JSONL format\n",
        "VALIDATION_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\"  # Path to your validation data in JSONL format\n",
        "TUNED_MODEL_DISPLAY_NAME = \"cmapss-text-tuned-gemini-2.0-flash-001\"\n",
        "EPOCHS = 10  # Adjust as needed\n",
        "LEARNING_RATE_MULTIPLIER = 1.0  # Adjust as needed\n",
        "\n",
        "\n",
        "\n",
        "# Start the fine-tuning job\n",
        "try:\n",
        "    sft_tuning_job = sft.train(\n",
        "        source_model=BASE_MODEL,\n",
        "        train_dataset=TRAIN_DATASET_URI,\n",
        "        validation_dataset=VALIDATION_DATASET_URI,\n",
        "        tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\n",
        "        epochs=EPOCHS,\n",
        "        learning_rate_multiplier=LEARNING_RATE_MULTIPLIER,\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Tuning job started: {sft_tuning_job.resource_name}\")\n",
        "\n",
        "    # Periodically check the job status until it's complete\n",
        "    while True:\n",
        "        job_status = sft_tuning_job.state  # Get the job's state directly\n",
        "\n",
        "        if job_status in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
        "            break  # Exit the loop if the job is finished\n",
        "\n",
        "        print(f\"Job status: {job_status}, waiting...\")\n",
        "        time.sleep(60)  # Wait for 60 seconds before checking again\n",
        "\n",
        "    print(f\"Tuning job completed with status: {job_status}. Resource name: {sft_tuning_job.resource_name}\")\n",
        "\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please double-check the base model name and your Vertex AI setup.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tuning job completed with Resource name: {sft_tuning_job.resource_name}\")"
      ],
      "metadata": {
        "id": "vpkS41BjYYCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "STR83WXTNL-I"
      }
    },
    {
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import base64\n",
        "import json\n",
        "import os\n",
        "import colab_env\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import gapic as aip\n",
        "from google.colab import auth\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "EVAL_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\"\n",
        "tuned_model_resource_name = f'projects/{PROJECT_NUMBER}/locations/{REGION}/models/1440268972921454592@1'\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "\n",
        "def generate():\n",
        "  client = genai.Client(\n",
        "      vertexai=True,\n",
        "      project=PROJECT_ID,\n",
        "      location=REGION,\n",
        "  )\n",
        "\n",
        "\n",
        "  model = f\"projects/{PROJECT_NUMBER}/locations/{REGION}/endpoints/7903506184244559872\"\n",
        "\n",
        "  # Format the prompt including roles as a single string\n",
        "  prompt = \"\"\"\n",
        "  {\n",
        "    \"contents\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"parts\": [\n",
        "          {\n",
        "            \"text\": \"Engine sensor readings over time: [1.0, 41.9993, 0.8409, 100.0, 445.0, 548.68, 1343.85, 1111.03, 3.91, 5.69, 137.26, 2211.96, 8296.96, ..., 8054.65, 9.2728, 0.02, 331.0, 2223.0, 100.0, 14.78, 8.8922]\"\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"model\",\n",
        "        \"parts\": [\n",
        "          {\n",
        "            \"text\": \"\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "  \"\"\"\n",
        "  contents = [prompt] # Send the prompt as a string\n",
        "\n",
        "\n",
        "  generate_content_config = types.GenerateContentConfig(\n",
        "    temperature = 1,\n",
        "    top_p = 0.95,\n",
        "    max_output_tokens = 8192,\n",
        "    response_modalities = [\"TEXT\"],\n",
        "    safety_settings = [types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "      threshold=\"OFF\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "      threshold=\"OFF\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      threshold=\"OFF\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "      threshold=\"OFF\"\n",
        "    )],\n",
        "  )\n",
        "\n",
        "  for chunk in client.models.generate_content_stream(\n",
        "    model = model,\n",
        "    contents = contents,\n",
        "    config = generate_content_config,\n",
        "    ):\n",
        "    print(chunk.text, end=\"\")\n",
        "\n",
        "generate()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfXIvmrp9nMX",
        "outputId": "0f316387-9482-49a4-e37e-adbe3d968f7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining Useful Life: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import json\n",
        "import os\n",
        "import colab_env\n",
        "from google.cloud import aiplatform\n",
        "from google.colab import auth\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "EVAL_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\"  # Update with your dataset URI\n",
        "tuned_model_resource_name = f'projects/{PROJECT_NUMBER}/locations/{REGION}/models/1440268972921454592@1'\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "def generate_and_evaluate():\n",
        "    client = genai.Client(\n",
        "        vertexai=True,\n",
        "        project=PROJECT_ID,\n",
        "        location=REGION,\n",
        "    )\n",
        "\n",
        "    model = f\"projects/{PROJECT_NUMBER}/locations/{REGION}/endpoints/7903506184244559872\"  # Update with your model endpoint\n",
        "\n",
        "    validation_dataset_uri = EVAL_DATASET_URI\n",
        "\n",
        "    # Copy the validation dataset locally\n",
        "    local_dataset_path = '/content/cmapss_FD004_test_text.jsonl'\n",
        "    !gsutil cp {validation_dataset_uri} .\n",
        "    print('\\n')\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    all_scores = []\n",
        "\n",
        "    num_lines = sum(1 for line in open(local_dataset_path))  # Calculate total lines beforehand\n",
        "\n",
        "\n",
        "    # Read and process the dataset file\n",
        "    with open(local_dataset_path, 'r') as f:\n",
        "        for line in tqdm(f, total=num_lines, desc=\"Processing dataset\"):  # Set total and description\n",
        "            data = json.loads(line)\n",
        "\n",
        "            # Extract prompt and ground truth from JSON structure\n",
        "            try:\n",
        "                prompt = data['contents'][0]['parts'][0]['text']\n",
        "                ground_truth_text = data['contents'][1]['parts'][0]['text']\n",
        "            except (IndexError, KeyError):\n",
        "                print(\"Skipping invalid data point:\", line)\n",
        "                continue  # Skip to the next line\n",
        "\n",
        "            if prompt and ground_truth_text:\n",
        "                contents = [prompt]\n",
        "\n",
        "                # Generate content for the current prompt\n",
        "                generated_text = \"\"\n",
        "                try:\n",
        "                    for chunk in client.models.generate_content_stream(\n",
        "                        model=model,\n",
        "                        contents=contents,\n",
        "                        config=types.GenerateContentConfig(\n",
        "                            temperature=1,\n",
        "                            top_p=0.95,\n",
        "                            max_output_tokens=8192,\n",
        "                            response_modalities=[\"TEXT\"],\n",
        "                            safety_settings=[types.SafetySetting(category=c, threshold=\"OFF\") for c in [\n",
        "                                \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "                                \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "                                \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "                                \"HARM_CATEGORY_HARASSMENT\",\n",
        "                            ]],\n",
        "                        ),\n",
        "                    ):\n",
        "                        generated_text += chunk.text\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during text generation for prompt '{prompt[:50]}...': {e}\")\n",
        "                    continue  # Skip to the next line\n",
        "\n",
        "                # Calculate ROUGE scores\n",
        "                scores = scorer.score(ground_truth_text, generated_text)\n",
        "                all_scores.append(scores)\n",
        "\n",
        "\n",
        "    # Calculate and print average ROUGE scores\n",
        "    if all_scores:\n",
        "        avg_rouge1 = sum(s['rouge1'].fmeasure for s in all_scores) / len(all_scores)\n",
        "        avg_rougeL = sum(s['rougeL'].fmeasure for s in all_scores) / len(all_scores)\n",
        "        print('\\n\\n')\n",
        "        print(f\"Average ROUGE-1: {avg_rouge1}\")\n",
        "        print(f\"Average ROUGE-L: {avg_rougeL}\")\n",
        "        print('\\n')\n",
        "    else:\n",
        "        print(\"No ROUGE scores were calculated. Check the dataset and text generation process.\")\n",
        "\n",
        "generate_and_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJZHje3aUkIE",
        "outputId": "f0242ca8-31d4-4053-b6b3-75437137ba6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://poc-my-new-staging-bucket-2025-1/cmapss_FD004_test_text.jsonl...\n",
            "/ [0 files][    0.0 B/  1.4 MiB]                                                \r/ [1 files][  1.4 MiB/  1.4 MiB]                                                \r\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dataset: 100%|██████████| 252/252 [01:21<00:00,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Average ROUGE-1: 0.75\n",
            "Average ROUGE-L: 0.75\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "* ROUGE-1: Measures the overlap of unigrams (individual words) between the generated text and the ground truth. A score of 0.75 suggests a relatively high degree of similarity at the word level.\n",
        "\n",
        "* ROUGE-L: Measures the longest common subsequence (LCS) between the generated text and the ground truth, taking into account sentence structure. A score of 0.75 also indicates a good level of similarity in terms of overall sentence structure and content.\n",
        "\n",
        "1. Overall, these scores suggest that the LLM is performing well in generating text that is similar to the ground truth in terms of both word-level and sentence-level structure and content.\n",
        "\n",
        "2. Of course, the interpretation of these scores can depend on the specific task and the desired level of accuracy. However, in general, ROUGE scores above 0.5 are often considered to be reasonably good, and scores above 0.7 are considered to be very good."
      ],
      "metadata": {
        "id": "sdNNpR41VKja"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN7nFRbGsGrujdbb0PB95QM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FineTune_DeepSeek_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzv-hHjqpD58"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl # For SFTTrainer\n",
        "!pip install -U bitsandbytes\n",
        "!pip install unsloth # Recommended for speed and efficiency\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # For latest Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None # Automatically chooses best data type (float16, bfloat16, etc.)\n",
        "load_in_4bit = True # Enable 4-bit quantization to reduce memory usage\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\", # Or other DeepSeek-R1 variants\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "iL9RFEDQpUXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16, # Rank of the LoRA matrices (common values: 8, 16, 32, 64)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Common target modules\n",
        "    lora_alpha=16, # Scaling factor for LoRA weights\n",
        "    lora_dropout=0, # Dropout rate for LoRA (set to 0 for inference)\n",
        "    bias=\"none\", # Or \"all\", \"lora_only\"\n",
        "    use_gradient_checkpointing=True, # Recommended for memory saving\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "id": "lSDOk2bVpZJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# Load your dataset (example: a medical counseling dataset)\n",
        "dataset = load_dataset(\"Sulav/mental_health_counseling_conversations_sharegpt\", split=\"train\")\n",
        "\n",
        "# Standardize to ShareGPT format if needed\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "# Apply a chat template\n",
        "def format_dataset(example):\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(\n",
        "        example[\"conversations\"],\n",
        "        tokenize=False,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(format_dataset, batched=False)"
      ],
      "metadata": {
        "id": "OxiWIuORpd3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\", # The column in your dataset containing the formatted text\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2, # Adjust based on your GPU memory\n",
        "        gradient_accumulation_steps=4, # Accumulate gradients to simulate larger batch size\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=1, # Or set max_steps\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(), # Use fp16 if bfloat16 is not supported\n",
        "        bf16=torch.cuda.is_bf16_supported(), # Use bfloat16 if supported\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        seed=3407,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qtU1DN0spjuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainOutput(global_step=439, training_loss=2.243786193243735, metrics={'train_runtime': 1866.875, 'train_samples_per_second': 1.881, 'train_steps_per_second': 0.235, 'total_flos': 5.815900021093171e+16, 'train_loss': 2.243786193243735})"
      ],
      "metadata": {
        "id": "zvGTcw3w_otR"
      }
    },
    {
      "source": [
        "# Save only the LoRA adapters\n",
        "model.save_pretrained(\"DeepSeek-R1-FineTuned\", tokenizer) # Save only adapters\n",
        "\n",
        "# Or push to Hugging Face Hub\n",
        "ft_model=\"frankmorales2020/unsloth-DeepSeek-R1-Distill-Llama-8B-mental_health_counseling\"\n",
        "model.push_to_hub(ft_model, tokenizer)\n",
        "\n",
        "# model.push_to_hub(\"your_username/your_fine_tuned_deepseek_r1_model\", tokenizer)\n",
        "\n",
        "# To save the full model in GGUF format (for local inference, e.g., with llama.cpp)\n",
        "# model.push_to_hub_gguf(my_model, tokenizer, quantization_method = \"q4_k_m\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpBcZkyk-ll4",
        "outputId": "d59bf900-eb8d-4935-bfac-c2a3687f4d39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to https://huggingface.co/frankmorales2020/unsloth-DeepSeek-R1-Distill-Llama-8B-mental_health_counseling\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N114em33Gnh6",
        "jqU4vuVsFfu-"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzUDldQikXfgvg5Ie9WhTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/DEEPSEEK_Kimi_K2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai -q\n",
        "!pip install unsloth -q"
      ],
      "metadata": {
        "id": "2-Lo11WAAj-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INTEGRA-LLM"
      ],
      "metadata": {
        "id": "N114em33Gnh6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pga5o2xfAfGK",
        "outputId": "d33dd3aa-ac5e-4846-9543-c1ad81b40ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "Loading DeepSeek model...\n",
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "DeepSeek model loaded successfully.\n",
            "\n",
            "--- Generating with DeepSeek R1 (Local) ---\n",
            "You are a helpful assistant.<ÔΩúUserÔΩú>What are the benefits of using a Mixture-of-Experts (MoE) architecture in large language models?<ÔΩúAssistantÔΩú><think>\n",
            "Okay, so I'm trying to understand the benefits of using a Mixture-of-Experts (MoE) architecture in large language models. I've heard about this term before, but I'm not exactly sure how it works or why it's useful. Let me try to break it down.\n",
            "\n",
            "First, I know that MoE stands for Mixture-of-Experts, and it's a type of neural network architecture. I think it's related to how neural networks process information, maybe like how different experts contribute their knowledge to the model. So, maybe each expert in the model has its own area of expertise, and the model combines them somehow.\n",
            "\n",
            "I remember reading that traditional models like Transformers use attention mechanisms, which allow them to weigh different parts of the input. Maybe MoE is a different approach. I think MoE divides the model into multiple sub-models or experts, each handling different parts of the problem. So, for a given input, the model might route the information to the relevant expert, or maybe combine the outputs from several experts.\n",
            "\n",
            "I'm trying to think about how this would be beneficial. One benefit could be efficiency. If each expert is specialized, maybe the model doesn't need to process the entire input through a single large model. Instead, it can break the task into smaller parts, which might be faster or use less resources. That would be useful for tasks that require a lot of computation, like generating long texts or translating large documents.\n",
            "\n",
            "Another benefit might be flexibility. Since each expert can focus on a specific task, the model can be more adaptable. For example, if the task changes, you can just add or remove experts without redesigning the entire model. This modular approach could make it easier to scale or update the model as needed.\n",
            "\n",
            "I also wonder about the scalability. Maybe with MoE, you can scale the number of experts linearly based on the task's complexity. So, for more complex tasks, you can add more experts, each handling a piece of the puzzle. This could make the model more efficient without making it too large, which might help with training and inference time.\n",
            "\n",
            "I'm curious about how the experts are combined. Do they work in parallel, or do they take turns? I think in parallel, because that would allow the model to process the input more efficiently. If each expert can handle a part of the input at the same time, the overall processing speed could be better.\n",
            "\n",
            "Another thought is about how this affects the model's performance. If each expert is specialized, maybe the model can handle\n",
            "\n",
            "--- Generating with Kimi K2 (OpenRouter API) ---\n",
            "‚óÅthink‚ñ∑Okay, let's tackle this question. The user wants to know the benefits of using a Mixture-of-Experts (MoE) architecture in large language models. \n",
            "\n",
            "First, I need to understand what MoE is. From what I remember, MoE is a model architecture where multiple expert networks are used, and a gating mechanism decides which experts to activate for each input. This approach allows the model to handle different parts of the problem with specialized experts, potentially improving efficiency and performance.\n",
            "\n",
            "In the context of large language models, which are often very big and computationally expensive, MoE can offer several advantages. Let me think about each possible benefit.\n",
            "\n",
            "1. **Scalability**: MoE allows models to scale beyond the capacity of a single GPU by distributing different experts across multiple devices. This is crucial for training extremely large models that might not fit into memory otherwise. So, scalability is definitely a key benefit.\n",
            "\n",
            "2. **Efficiency**: Since only a subset of experts is active for each input, the model can be more efficient during both training and inference. This means less computation per step, which can lead to faster processing times and lower energy consumption.\n",
            "\n",
            "3. **Specialization**: Each expert can learn to handle specific aspects of the language, like different domains, tasks, or even languages. This specialization can lead to better performance on a wide range of tasks compared to a single monolithic model.\n",
            "\n",
            "4. **Flexibility**: MoE models can be adapted more easily to new tasks or data by adding or modifying experts. This flexibility is important for maintaining and updating models over time.\n",
            "\n",
            "5. **Reduced Overfitting**: By having multiple experts, the model might be less prone to overfitting on any particular part of the data, as each expert focuses on different aspects. However, this might be more of a side effect rather than a primary benefit.\n",
            "\n",
            "Now, let me check if there are any other benefits. For example, MoE can help in handling heterogeneous data by routing inputs to the most relevant experts. Also, it might improve the model's ability to generalize across different tasks through the gating mechanism.\n",
            "\n",
            "Putting this all together, the main benefits of MoE in large language models are scalability, efficiency, specialization, and flexibility. These factors are critical for building and deploying large-scale models that can handle diverse and complex language tasks effectively.\n",
            "‚óÅ/think‚ñ∑\n",
            "\n",
            "The Mixture-of-Experts (MoE) architecture offers several key benefits for large language models:\n",
            "\n",
            "1. **Scalability**: Enables training of larger models\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "from datetime import datetime, timedelta\n",
        "from unsloth import FastLanguageModel\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# ====================================================================\n",
        "# DeepSeek Model (Local execution with Unsloth)\n",
        "# ====================================================================\n",
        "\n",
        "# Assume a global variable for the loaded model and tokenizer\n",
        "deepseek_model = None\n",
        "deepseek_tokenizer = None\n",
        "\n",
        "def load_deepseek_model():\n",
        "    \"\"\"Loads the DeepSeek model and tokenizer using Unsloth.\"\"\"\n",
        "    global deepseek_model, deepseek_tokenizer\n",
        "\n",
        "    if deepseek_model is None or deepseek_tokenizer is None:\n",
        "        # Check for CUDA and bfloat16 support\n",
        "        if not torch.cuda.is_available():\n",
        "            print(\"CUDA is not available. DeepSeek model cannot be loaded.\")\n",
        "            return False\n",
        "\n",
        "        max_seq_length = 4096\n",
        "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        load_in_4bit = True\n",
        "\n",
        "        print(\"Loading DeepSeek model...\")\n",
        "        try:\n",
        "            deepseek_model, deepseek_tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "                max_seq_length=max_seq_length,\n",
        "                dtype=dtype,\n",
        "                load_in_4bit=load_in_4bit,\n",
        "            )\n",
        "            print(\"DeepSeek model loaded successfully.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load DeepSeek model: {e}\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def generate_with_deepseek(prompt):\n",
        "    \"\"\"Generates text using the loaded DeepSeek model.\"\"\"\n",
        "    if deepseek_model is None or deepseek_tokenizer is None:\n",
        "        print(\"DeepSeek model is not loaded. Skipping generation.\")\n",
        "        return\n",
        "\n",
        "    # Assuming a simple chat template for demonstration\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    formatted_prompt = deepseek_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generating with DeepSeek R1 (Local) ---\")\n",
        "    inputs = deepseek_tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = deepseek_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        use_cache=True,\n",
        "        pad_token_id=deepseek_tokenizer.eos_token_id, # Set pad_token_id to EOS token\n",
        "    )\n",
        "\n",
        "    decoded_output = deepseek_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Extract only the assistant's response\n",
        "    response_start_tag = \"<|assistant|>\"\n",
        "    response_start_index = decoded_output.find(response_start_tag)\n",
        "    if response_start_index != -1:\n",
        "        clean_output = decoded_output[response_start_index + len(response_start_tag):].strip()\n",
        "    else:\n",
        "        clean_output = decoded_output.strip()\n",
        "\n",
        "    print(clean_output)\n",
        "\n",
        "# ====================================================================\n",
        "# Kimi K2 (Remote execution via OpenRouter API)\n",
        "# ====================================================================\n",
        "\n",
        "def generate_with_kimi(prompt, openrouter_api_key):\n",
        "    \"\"\"Generates text using the Kimi K2 model via the OpenRouter API.\"\"\"\n",
        "    print(\"\\n--- Generating with Kimi K2 (OpenRouter API) ---\")\n",
        "\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=openrouter_api_key,\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Define the desired streaming behavior before the API call\n",
        "    use_streaming = False\n",
        "\n",
        "    try:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=\"moonshotai/kimi-dev-72b:free\",\n",
        "            messages=messages,\n",
        "            temperature=0.3,\n",
        "            stream=use_streaming, # Use the variable here\n",
        "            max_tokens=512\n",
        "        )\n",
        "\n",
        "        if not use_streaming: # Now checking the boolean variable, not the response object\n",
        "            print(stream.choices[0].message.content)\n",
        "        else:\n",
        "            for chunk in stream:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    print(chunk.choices[0].delta.content, end=\"\")\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred with the Kimi API call: {e}\")\n",
        "\n",
        "# ====================================================================\n",
        "# Main script execution\n",
        "# ====================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test prompt for both models\n",
        "    test_prompt = \"What are the benefits of using a Mixture-of-Experts (MoE) architecture in large language models?\"\n",
        "\n",
        "    # --- Demo DeepSeek ---\n",
        "    if load_deepseek_model():\n",
        "        generate_with_deepseek(test_prompt)\n",
        "\n",
        "    # --- Demo Kimi K2 ---\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        if openrouter_api_key:\n",
        "            generate_with_kimi(test_prompt, openrouter_api_key)\n",
        "        else:\n",
        "            print(\"\\nOPENROUTER_API_KEY not found. Skipping Kimi K2 demo.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCould not retrieve OPENROUTER_API_KEY from userdata: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AGENTIC"
      ],
      "metadata": {
        "id": "jqU4vuVsFfu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "from datetime import datetime, timedelta\n",
        "from unsloth import FastLanguageModel\n",
        "from openai import OpenAI\n",
        "# Assuming google.colab.userdata is available in the environment\n",
        "# If running outside Colab, you'd need to load API key differently (e.g., from environment variables)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "except ImportError:\n",
        "    print(\"google.colab.userdata not found. Please ensure you are in a Colab environment or handle API key loading manually.\")\n",
        "    # Fallback for non-Colab environments (e.g., set OPENROUTER_API_KEY as an environment variable)\n",
        "    class UserDataMock:\n",
        "        def get(self, key):\n",
        "            return os.getenv(key)\n",
        "    userdata = UserDataMock()\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# DeepSeek Model (Local execution with Unsloth)\n",
        "# This model will act as our \"Summarizer Agent\".\n",
        "# ====================================================================\n",
        "\n",
        "# Global variables for the loaded model and tokenizer\n",
        "deepseek_model = None\n",
        "deepseek_tokenizer = None\n",
        "\n",
        "def load_deepseek_model():\n",
        "    \"\"\"\n",
        "    Loads the DeepSeek model and tokenizer using Unsloth.\n",
        "    Returns True if successful, False otherwise.\n",
        "    \"\"\"\n",
        "    global deepseek_model, deepseek_tokenizer\n",
        "\n",
        "    if deepseek_model is None or deepseek_tokenizer is None:\n",
        "        # Check for CUDA availability as Unsloth requires a GPU\n",
        "        if not torch.cuda.is_available():\n",
        "            print(\"CUDA is not available. DeepSeek model cannot be loaded for local inference.\")\n",
        "            return False\n",
        "\n",
        "        # Determine appropriate dtype based on CUDA support\n",
        "        max_seq_length = 4096\n",
        "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        load_in_4bit = True # Load model in 4-bit for memory efficiency\n",
        "\n",
        "        print(\"Loading DeepSeek model for Summarizer Agent...\")\n",
        "        try:\n",
        "            deepseek_model, deepseek_tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "                max_seq_length=max_seq_length,\n",
        "                dtype=dtype,\n",
        "                load_in_4bit=load_in_4bit,\n",
        "            )\n",
        "            print(\"DeepSeek model loaded successfully for Summarizer Agent.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load DeepSeek model for Summarizer Agent: {e}\")\n",
        "            print(\"Please ensure you have a compatible GPU and sufficient memory.\")\n",
        "            return False\n",
        "    return True # Model already loaded\n",
        "\n",
        "def summarize_with_deepseek(text_to_summarize: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a summary of the given text using the loaded DeepSeek model.\n",
        "    Acts as the \"Summarizer Agent\".\n",
        "    \"\"\"\n",
        "    if deepseek_model is None or deepseek_tokenizer is None:\n",
        "        print(\"DeepSeek model (Summarizer Agent) is not loaded. Skipping summarization.\")\n",
        "        return \"Summarization failed: Model not loaded.\"\n",
        "\n",
        "    print(\"\\n[Summarizer Agent] - Summarizing information...\")\n",
        "\n",
        "    # Define a prompt template for summarization\n",
        "    summarization_prompt = f\"\"\"Summarize the following text concisely and clearly.\n",
        "Text:\n",
        "---\n",
        "{text_to_summarize}\n",
        "---\n",
        "Summary:\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "        {\"role\": \"user\", \"content\": summarization_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply chat template for DeepSeek\n",
        "    formatted_prompt = deepseek_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = deepseek_tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    try:\n",
        "        outputs = deepseek_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256, # Limit summary length\n",
        "            use_cache=True,\n",
        "            pad_token_id=deepseek_tokenizer.eos_token_id,\n",
        "            temperature=0.5, # Make summarization more focused\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "        decoded_output = deepseek_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        # Extract only the assistant's response, removing any chat template artifacts\n",
        "        response_start_tag = \"<|assistant|>\"\n",
        "        response_start_index = decoded_output.find(response_start_tag)\n",
        "        if response_start_index != -1:\n",
        "            clean_output = decoded_output[response_start_index + len(response_start_tag):].strip()\n",
        "        else:\n",
        "            clean_output = decoded_output.strip()\n",
        "\n",
        "        # Remove any lingering \"think\" tags or similar internal monologue if present\n",
        "        clean_output = clean_output.replace(\"‚óÅthink‚ñ∑\", \"\").replace(\"‚óÅ/think‚ñ∑\", \"\").strip()\n",
        "        clean_output = clean_output.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
        "\n",
        "        return clean_output\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during DeepSeek summarization: {e}\")\n",
        "        return f\"Summarization failed due to an error: {e}\"\n",
        "\n",
        "# ====================================================================\n",
        "# Kimi K2 (Remote execution via OpenRouter API)\n",
        "# This model will act as our \"Research Agent\".\n",
        "# ====================================================================\n",
        "\n",
        "def research_with_kimi(query: str, openrouter_api_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs research based on the query using the Kimi K2 model via the OpenRouter API.\n",
        "    Acts as the \"Research Agent\".\n",
        "    \"\"\"\n",
        "    print(f\"\\n[Research Agent] - Researching: '{query}'...\")\n",
        "\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=openrouter_api_key,\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and informative research assistant. Provide detailed and factual information.\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    use_streaming = False # For this multi-agent setup, we want the full response at once\n",
        "\n",
        "    try:\n",
        "        # Make the API call to Kimi K2\n",
        "        response_object = client.chat.completions.create(\n",
        "            model=\"moonshotai/kimi-dev-72b:free\", # Using the specified Kimi model\n",
        "            messages=messages,\n",
        "            temperature=0.3, # Keep temperature low for factual research\n",
        "            stream=use_streaming,\n",
        "            max_tokens=1024 # Allow for detailed research output\n",
        "        )\n",
        "\n",
        "        # Extract content from the response\n",
        "        if not use_streaming:\n",
        "            research_content = response_object.choices[0].message.content\n",
        "        else:\n",
        "            # This block would only execute if use_streaming was True, but we set it to False\n",
        "            # However, including for completeness if streaming were to be enabled.\n",
        "            research_content_parts = []\n",
        "            for chunk in response_object:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    research_content_parts.append(chunk.choices[0].delta.content)\n",
        "            research_content = \"\".join(research_content_parts)\n",
        "\n",
        "        # Clean up any internal \"think\" tags from the research agent's output\n",
        "        research_content = research_content.replace(\"‚óÅthink‚ñ∑\", \"\").replace(\"‚óÅ/think‚ñ∑\", \"\").strip()\n",
        "        research_content = research_content.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
        "\n",
        "        print(\"[Research Agent] - Research complete.\")\n",
        "        return research_content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Kimi K2 research API call: {e}\")\n",
        "        return f\"Research failed due to an error: {e}\"\n",
        "\n",
        "# ====================================================================\n",
        "# Orchestrator (Main script logic for multi-agent coordination)\n",
        "# ====================================================================\n",
        "\n",
        "def run_multi_agent_solution(initial_query: str):\n",
        "    \"\"\"\n",
        "    Orchestrates the multi-agent solution to research and summarize a topic.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Multi-Agent Solution ---\")\n",
        "    print(f\"Initial Query: '{initial_query}'\")\n",
        "\n",
        "    # 1. Load DeepSeek Model for Summarizer Agent (if not already loaded)\n",
        "    deepseek_loaded = load_deepseek_model()\n",
        "    if not deepseek_loaded:\n",
        "        print(\"DeepSeek model could not be loaded. Summarizer Agent will not function.\")\n",
        "        print(\"Multi-agent solution cannot proceed without the Summarizer Agent.\")\n",
        "        return\n",
        "\n",
        "    # 2. Get OpenRouter API Key for Research Agent\n",
        "    openrouter_api_key = None\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        if not openrouter_api_key:\n",
        "            print(\"OPENROUTER_API_KEY not found in userdata. Research Agent will not function.\")\n",
        "            print(\"Multi-agent solution cannot proceed without the Research Agent.\")\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve OPENROUTER_API_KEY: {e}. Research Agent will not function.\")\n",
        "        return\n",
        "\n",
        "    # 3. Research Phase (Research Agent - Kimi K2)\n",
        "    researched_info = research_with_kimi(initial_query, openrouter_api_key)\n",
        "\n",
        "    if \"Research failed\" in researched_info:\n",
        "        print(\"Research phase failed. Cannot proceed to summarization.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Research Information Collected ---\")\n",
        "    print(researched_info)\n",
        "    print(\"------------------------------------\")\n",
        "\n",
        "    # 4. Summarization Phase (Summarizer Agent - DeepSeek R1)\n",
        "    final_summary = summarize_with_deepseek(researched_info)\n",
        "\n",
        "    print(\"\\n--- Final Summary from Summarizer Agent ---\")\n",
        "    print(final_summary)\n",
        "    print(\"-----------------------------------------\")\n",
        "    print(\"--- Multi-Agent Solution Finished ---\")\n",
        "\n",
        "# ====================================================================\n",
        "# Entry point for the script\n",
        "# ====================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the initial problem for the multi-agent system\n",
        "    problem_query = \"Explain the history and significance of the Renaissance period in Europe.\"\n",
        "    run_multi_agent_solution(problem_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9x4xhR6FjUh",
        "outputId": "dd7e1c43-d9e0-42ad-e3a1-d78c2e1d5a01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Multi-Agent Solution ---\n",
            "Initial Query: 'Explain the history and significance of the Renaissance period in Europe.'\n",
            "Loading DeepSeek model for Summarizer Agent...\n",
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "DeepSeek model loaded successfully for Summarizer Agent.\n",
            "\n",
            "[Research Agent] - Researching: 'Explain the history and significance of the Renaissance period in Europe.'...\n",
            "[Research Agent] - Research complete.\n",
            "\n",
            "--- Research Information Collected ---\n",
            "Okay, let's tackle explaining the history and significance of the Renaissance period in Europe. \n",
            "\n",
            "First, I need to recall when the Renaissance occurred. It's generally considered to have started in the 14th century and lasted through the 17th century. The term \"Renaissance\" comes from the French word meaning \"rebirth,\" which suggests a cultural revival. But what exactly was being reborn?\n",
            "\n",
            "The Renaissance marked a shift from the Middle Ages to modernity. It began in Italy, particularly in cities like Florence and Venice. The period was characterized by a renewed interest in classical learning from ancient Greece and Rome. This was partly due to the rediscovery of texts that had been preserved in the Middle East and brought back to Europe.\n",
            "\n",
            "One of the key aspects of the Renaissance was humanism. Humanists believed in the potential of individuals and emphasized education, critical thinking, and the study of classical texts. This led to advancements in literature, art, science, and philosophy. For example, Leonardo da Vinci's works like the Mona Lisa and The Last Supper are iconic examples of Renaissance art. His notebooks also show his curiosity in anatomy, engineering, and other fields.\n",
            "\n",
            "In science, figures like Galileo Galilei and Nicolaus Copernicus made groundbreaking discoveries. Copernicus proposed the heliocentric model of the solar system, challenging the long-held geocentric view. Galileo's improvements to the telescope and his observations supported this model, leading to conflicts with the Church but ultimately advancing our understanding of the universe.\n",
            "\n",
            "The printing press, invented by Johannes Gutenberg around 1440, played a crucial role in spreading knowledge. It allowed books to be produced more quickly and cheaply, making education more accessible and contributing to the scientific revolution.\n",
            "\n",
            "In architecture, there was a return to classical styles. Buildings like St. Peter's Basilica in Rome, designed by architects such as Bramante, Michelangelo, and Bernini, showcase the grandeur and symmetry inspired by ancient Rome.\n",
            "\n",
            "The Renaissance also saw the rise of nation-states and changes in political thought. Machiavelli's \"The Prince\" is a notable work that discussed political power and strategy, reflecting the tumultuous political landscape of the time.\n",
            "\n",
            "Religion was another area affected by the Renaissance. The Protestant Reformation, led by Martin Luther, challenged the Catholic Church's authority and practices. This religious upheaval had profound effects on European society and politics.\n",
            "\n",
            "The significance of the Renaissance lies in its role as a bridge between the Middle Ages and the modern world. It fostered a cultural and intellectual flowering that laid the groundwork for many subsequent developments in science, art, and philosophy. The emphasis on individualism and critical thinking helped shape the modern Western worldview.\n",
            "\n",
            "So, the Renaissance was a time of great cultural, artistic, political, and scientific dynamism. Its legacy can be seen in the enduring works of art, the foundations of modern science, and the evolution of political and religious thought.\n",
            "\n",
            "\n",
            "The Renaissance period in Europe, spanning from the 14th to 17th centuries, was a transformative era marked by a cultural and intellectual rebirth. Originating in Italy, it spread throughout Europe, bringing about significant advancements in various fields.\n",
            "\n",
            "**Key Aspects:**\n",
            "1. **Humanism:** Emphasis on individual potential, education, and classical learning from ancient Greece and Rome.\n",
            "2. **Art and Literature:** Masterpieces by Leonardo da Vinci, Michelangelo, and Shakespeare exemplify the period's artistic and literary achievements.\n",
            "3. **Scientific Revolution:** Pioneers like Galileo and Copernicus challenged existing paradigms, laying foundations for modern science.\n",
            "4. **Printing Press:** Gutenberg's invention democratized knowledge, fueling literacy and intellectual exchange.\n",
            "5. **Architecture:** Classical styles influenced grand structures like St. Peter's Basilica.\n",
            "6. **Political Thought:** Works like Machiavelli's \"The Prince\" shaped modern political theory.\n",
            "7. **Religious Reformation:** Challenges to the Catholic Church led to the Protestant Reformation, altering the religious landscape.\n",
            "\n",
            "**Significance:**\n",
            "- Bridged the Middle Ages and modernity, fostering a culture of inquiry and innovation.\n",
            "- Laid groundwork for modern science, democracy, and individual rights.\n",
            "- Produced enduring cultural treasures that continue to inspire and influence today.\n",
            "\n",
            "The Renaissance's emphasis on human potential, critical thinking, and classical wisdom has had a lasting impact on Western civilization, shaping the world we live in today.\n",
            "------------------------------------\n",
            "\n",
            "[Summarizer Agent] - Summarizing information...\n",
            "\n",
            "--- Final Summary from Summarizer Agent ---\n",
            "You are a helpful assistant that summarizes text.<ÔΩúUserÔΩú>Summarize the following text concisely and clearly.\n",
            "Text:\n",
            "---\n",
            "Okay, let's tackle explaining the history and significance of the Renaissance period in Europe. \n",
            "\n",
            "First, I need to recall when the Renaissance occurred. It's generally considered to have started in the 14th century and lasted through the 17th century. The term \"Renaissance\" comes from the French word meaning \"rebirth,\" which suggests a cultural revival. But what exactly was being reborn?\n",
            "\n",
            "The Renaissance marked a shift from the Middle Ages to modernity. It began in Italy, particularly in cities like Florence and Venice. The period was characterized by a renewed interest in classical learning from ancient Greece and Rome. This was partly due to the rediscovery of texts that had been preserved in the Middle East and brought back to Europe.\n",
            "\n",
            "One of the key aspects of the Renaissance was humanism. Humanists believed in the potential of individuals and emphasized education, critical thinking, and the study of classical texts. This led to advancements in literature, art, science, and philosophy. For example, Leonardo da Vinci's works like the Mona Lisa and The Last Supper are iconic examples of Renaissance art. His notebooks also show his curiosity in anatomy, engineering, and other fields.\n",
            "\n",
            "In science, figures like Galileo Galilei and Nicolaus Copernicus made groundbreaking discoveries. Copernicus proposed the heliocentric model of the solar system, challenging the long-held geocentric view. Galileo's improvements to the telescope and his observations supported this model, leading to conflicts with the Church but ultimately advancing our understanding of the universe.\n",
            "\n",
            "The printing press, invented by Johannes Gutenberg around 1440, played a crucial role in spreading knowledge. It allowed books to be produced more quickly and cheaply, making education more accessible and contributing to the scientific revolution.\n",
            "\n",
            "In architecture, there was a return to classical styles. Buildings like St. Peter's Basilica in Rome, designed by architects such as Bramante, Michelangelo, and Bernini, showcase the grandeur and symmetry inspired by ancient Rome.\n",
            "\n",
            "The Renaissance also saw the rise of nation-states and changes in political thought. Machiavelli's \"The Prince\" is a notable work that discussed political power and strategy, reflecting the tumultuous political landscape of the time.\n",
            "\n",
            "Religion was another area affected by the Renaissance. The Protestant Reformation, led by Martin Luther, challenged the Catholic Church's authority and practices. This religious upheaval had profound effects on European society and politics.\n",
            "\n",
            "The significance of the Renaissance lies in its role as a bridge between the Middle Ages and the modern world. It fostered a cultural and intellectual flowering that laid the groundwork for many subsequent developments in science, art, and philosophy. The emphasis on individualism and critical thinking helped shape the modern Western worldview.\n",
            "\n",
            "So, the Renaissance was a time of great cultural, artistic, political, and scientific dynamism. Its legacy can be seen in the enduring works of art, the foundations of modern science, and the evolution of political and religious thought.\n",
            "\n",
            "\n",
            "The Renaissance period in Europe, spanning from the 14th to 17th centuries, was a transformative era marked by a cultural and intellectual rebirth. Originating in Italy, it spread throughout Europe, bringing about significant advancements in various fields.\n",
            "\n",
            "**Key Aspects:**\n",
            "1. **Humanism:** Emphasis on individual potential, education, and classical learning from ancient Greece and Rome.\n",
            "2. **Art and Literature:** Masterpieces by Leonardo da Vinci, Michelangelo, and Shakespeare exemplify the period's artistic and literary achievements.\n",
            "3. **Scientific Revolution:** Pioneers like Galileo and Copernicus challenged existing paradigms, laying foundations for modern science.\n",
            "4. **Printing Press:** Gutenberg's invention democratized knowledge, fueling literacy and intellectual exchange.\n",
            "5. **Architecture:** Classical styles influenced grand structures like St. Peter's Basilica.\n",
            "6. **Political Thought:** Works like Machiavelli's \"The Prince\" shaped modern political theory.\n",
            "7. **Religious Reformation:** Challenges to the Catholic Church led to the Protestant Reformation, altering the religious landscape.\n",
            "\n",
            "**Significance:**\n",
            "- Bridged the Middle Ages and modernity, fostering a culture of inquiry and innovation.\n",
            "- Laid groundwork for modern science, democracy, and individual rights.\n",
            "- Produced enduring cultural treasures that continue to inspire and influence today.\n",
            "\n",
            "The Renaissance's emphasis on human potential, critical thinking, and classical wisdom has had a lasting impact on Western civilization, shaping the world we live in today.\n",
            "---\n",
            "Summary:<ÔΩúAssistantÔΩú>\n",
            "Okay, so I need to summarize the given text about the Renaissance. Let me read through it carefully to understand the main points.\n",
            "\n",
            "The Renaissance started in the 14th century and ended in the 17th century. It's called the \"rebirth\" because it was a revival of classical learning. It began in Italy, especially in cities like Florence and Venice. During this time, people rediscovered classical texts from ancient Greece and Rome, which had been preserved in the Middle East.\n",
            "\n",
            "Key aspects include humanism, which focused on individual potential, education, and classical learning. This led to advancements in art, literature, science, and philosophy. Artists like Leonardo da Vinci and Michelangelo created famous works. Scientists such as Galileo and Copernicus made significant discoveries, like the heliocentric model of the solar system.\n",
            "\n",
            "The printing press, invented by Gutenberg, played a big role in spreading knowledge. Architecture saw a return to classical styles, with grand buildings like St. Peter's Basilica. Politically, the rise of nation-states and works like Machiavelli's \"The Prince\" were important. Religiously, the Protestant Reformation challenged the Catholic Church's authority.\n",
            "\n",
            "The significance of the Renaissance is that it bridged the Middle Ages and modernity,\n",
            "-----------------------------------------\n",
            "--- Multi-Agent Solution Finished ---\n"
          ]
        }
      ]
    }
  ]
}
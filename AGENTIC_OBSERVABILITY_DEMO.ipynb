{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNneHARjf7M7EC0FjQyq10B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/AGENTIC_OBSERVABILITY_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN1"
      ],
      "metadata": {
        "id": "USfSKKUCGsk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from typing import Dict, Any\n",
        "\n",
        "# --- CONFIGURATION (Direct Key Retrieval) ---\n",
        "\n",
        "# Assuming this runs in a Google Colab environment or similar setup\n",
        "# where the key is available via `userdata.get()`.\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    # Use the user-provided method to get the actual key\n",
        "    API_KEY = userdata.get(\"DEEPSEEK_API_KEY\")\n",
        "\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"DEEPSEEK_API_KEY not found in Colab userdata.\")\n",
        "\n",
        "    # Initialize the DeepSeek-compatible client\n",
        "    client = OpenAI(api_key=API_KEY, base_url=\"https://api.deepseek.com\")\n",
        "    print(\"âœ… DeepSeek Client Initialized successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error initializing client: {e}\")\n",
        "    print(\"Please ensure you have run `pip3 install openai` and the 'DEEPSEEK_API_KEY' secret is correctly set.\")\n",
        "    client = None\n",
        "\n",
        "# --- 1. The Agent Class ---\n",
        "class DeepSeekReasonerAgent:\n",
        "    \"\"\"A minimal agent that uses DeepSeek-Reasoner to solve a task and logs observability metrics.\"\"\"\n",
        "\n",
        "    def __init__(self, client: OpenAI, agent_id=\"Reasoner-001\"):\n",
        "        self.client = client\n",
        "        self.id = agent_id\n",
        "\n",
        "    def run_task(self, request: str, system_prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"Executes the LLM call and captures observability metrics.\"\"\"\n",
        "\n",
        "        # --- (A) Execution & Metric Collection ---\n",
        "        start_time = time.time()\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": request},\n",
        "        ]\n",
        "\n",
        "        agent_output = \"API call failed.\"\n",
        "        obs_data = {}\n",
        "\n",
        "        try:\n",
        "            # ðŸ’¡ The actual call to the DeepSeek API\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"deepseek-reasoner\",\n",
        "                messages=messages,\n",
        "                stream=False\n",
        "            )\n",
        "\n",
        "            end_time = time.time()\n",
        "            # Calculate latency\n",
        "            inference_latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "            # Extract key metrics directly from the response object\n",
        "            usage = response.usage\n",
        "\n",
        "            # The agent's core output\n",
        "            final_content = response.choices[0].message.content\n",
        "\n",
        "            # The DeepSeek Reasoner model may include a 'reasoning_content' attribute\n",
        "            # We check for its presence to track the agent's decision path/CoT usage.\n",
        "            reasoning = getattr(response.choices[0].message, 'reasoning_content', 'N/A')\n",
        "\n",
        "            agent_output = final_content\n",
        "\n",
        "            # --- (B) Structured Observability Data Payload ---\n",
        "            obs_data = {\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"agent_id\": self.id,\n",
        "                \"request_summary\": request[:50] + \"...\",\n",
        "                \"inference_latency_ms\": round(inference_latency_ms, 2),\n",
        "\n",
        "                # --- Crucial LLM Observability Metrics (Cost Monitoring) ---\n",
        "                \"tokens_input\": usage.prompt_tokens,\n",
        "                \"tokens_output\": usage.completion_tokens,\n",
        "                \"tokens_total\": usage.total_tokens,\n",
        "\n",
        "                # --- Reasoning/Governance Metrics ---\n",
        "                \"model_name\": response.model,\n",
        "                \"had_reasoning_step\": reasoning != 'N/A', # Tracks if Chain-of-Thought was used\n",
        "                \"response_quality_evaluation\": \"Requires Human or Automated Grading\",\n",
        "                \"reasoning_trace_length\": len(reasoning) if reasoning != 'N/A' else 0,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = time.time()\n",
        "            inference_latency_ms = (end_time - start_time) * 1000\n",
        "            agent_output = f\"API Error: {str(e)}\"\n",
        "\n",
        "            # Log failure metrics\n",
        "            obs_data = {\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"agent_id\": self.id,\n",
        "                \"request_summary\": request[:50] + \"...\",\n",
        "                \"inference_latency_ms\": round(inference_latency_ms, 2),\n",
        "                \"status\": \"FAILURE\",\n",
        "                \"error_message\": str(e),\n",
        "                \"tokens_total\": 0,\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"agent_output\": agent_output,\n",
        "            \"observability_data\": obs_data\n",
        "        }\n",
        "\n",
        "# --- 2. Observability Logging Function ---\n",
        "def log_agent_event(data: Dict[str, Any]):\n",
        "    \"\"\"Prints the structured observability data in a readable JSON format.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âœ¨ AGENT OBSERVABILITY TRACE CAPTURED (DeepSeek Agent) âœ¨\")\n",
        "    print(\"=\"*70)\n",
        "    # Use JSON format for easy ingestion by monitoring tools\n",
        "    print(json.dumps(data, indent=4))\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# --- 3. Demo Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    if not client:\n",
        "        print(\"\\nDemo cannot run. Client initialization failed due to missing API key or connection error.\")\n",
        "    else:\n",
        "        my_agent = DeepSeekReasonerAgent(client=client)\n",
        "\n",
        "        # --- Scenario: Logic Puzzle ---\n",
        "        puzzle_request = (\n",
        "            \"There are three boxes. One contains only apples, one contains only oranges, and one contains both. \"\n",
        "            \"Each box is incorrectly labeled. You can only open one box and take out one fruit. \"\n",
        "            \"By looking at the fruit, how can you label all the boxes correctly?\"\n",
        "        )\n",
        "        puzzle_system_prompt = \"You are a helpful assistant that excels at advanced logic puzzles and provides step-by-step reasoning.\"\n",
        "\n",
        "        print(f\"-> Agent Receives Task: {puzzle_request[:70]}...\")\n",
        "\n",
        "        # Run the agent task\n",
        "        result = my_agent.run_task(puzzle_request, puzzle_system_prompt)\n",
        "\n",
        "        # Log the full structured data for the observability pipeline\n",
        "        log_agent_event(result[\"observability_data\"])\n",
        "\n",
        "        print(\"\\n--- AGENT'S FINAL ANSWER ---\")\n",
        "        print(result[\"agent_output\"])\n",
        "\n",
        "        # Summarize key Agentic Metrics\n",
        "        print(\"\\n--- AGENTIC OBSERVABILITY METRICS SUMMARY ---\")\n",
        "        obs_data = result['observability_data']\n",
        "        print(f\"**Inference Latency:** {obs_data.get('inference_latency_ms', 'N/A')} ms\")\n",
        "        print(f\"**Total Tokens Consumed (Cost):** {obs_data.get('tokens_total', 'N/A')}\")\n",
        "        print(f\"**Used Reasoning/CoT:** {obs_data.get('had_reasoning_step', 'N/A')}\")\n",
        "\n",
        "        print(\"\\nThis structured log is vital for monitoring **cost, performance, and decision quality** in a real AI Agent system.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghatMgkLGlwa",
        "outputId": "e28c77ef-b7b5-4bbb-83da-b8d14ee92ef0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DeepSeek Client Initialized successfully.\n",
            "-> Agent Receives Task: There are three boxes. One contains only apples, one contains only ora...\n",
            "\n",
            "======================================================================\n",
            "âœ¨ AGENT OBSERVABILITY TRACE CAPTURED (DeepSeek Agent) âœ¨\n",
            "======================================================================\n",
            "{\n",
            "    \"timestamp\": \"2025-12-04 11:25:39\",\n",
            "    \"agent_id\": \"Reasoner-001\",\n",
            "    \"request_summary\": \"There are three boxes. One contains only apples, o...\",\n",
            "    \"inference_latency_ms\": 85132.39,\n",
            "    \"tokens_input\": 75,\n",
            "    \"tokens_output\": 2441,\n",
            "    \"tokens_total\": 2516,\n",
            "    \"model_name\": \"deepseek-reasoner\",\n",
            "    \"had_reasoning_step\": true,\n",
            "    \"response_quality_evaluation\": \"Requires Human or Automated Grading\",\n",
            "    \"reasoning_trace_length\": 8992\n",
            "}\n",
            "======================================================================\n",
            "\n",
            "--- AGENT'S FINAL ANSWER ---\n",
            "To solve the puzzle, open the box labeled \"Both\". Since all boxes are incorrectly labeled, this box cannot contain both fruits. It must contain either only apples or only oranges.\n",
            "\n",
            "After taking out one fruit:\n",
            "- If you take out an **apple**, then the box labeled \"Both\" actually contains **only apples**.  \n",
            "  - Therefore, the box labeled \"Oranges\" cannot contain only oranges (by incorrect labeling) and cannot contain only apples (since apples are in the \"Both\" box), so it must contain **both fruits**.  \n",
            "  - The box labeled \"Apples\" cannot contain only apples (by incorrect labeling) and cannot contain both fruits (since both are in the \"Oranges\" box), so it must contain **only oranges**.\n",
            "\n",
            "- If you take out an **orange**, then the box labeled \"Both\" actually contains **only oranges**.  \n",
            "  - Therefore, the box labeled \"Apples\" cannot contain only apples (by incorrect labeling) and cannot contain only oranges (since oranges are in the \"Both\" box), so it must contain **both fruits**.  \n",
            "  - The box labeled \"Oranges\" cannot contain only oranges (by incorrect labeling) and cannot contain both fruits (since both are in the \"Apples\" box), so it must contain **only apples**.\n",
            "\n",
            "In either case, you can deduce the correct contents of all three boxes. Opening any other box might lead to ambiguity if you draw a fruit that doesn't uniquely identify the box's contents.\n",
            "\n",
            "--- AGENTIC OBSERVABILITY METRICS SUMMARY ---\n",
            "**Inference Latency:** 85132.39 ms\n",
            "**Total Tokens Consumed (Cost):** 2516\n",
            "**Used Reasoning/CoT:** True\n",
            "\n",
            "This structured log is vital for monitoring **cost, performance, and decision quality** in a real AI Agent system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN2"
      ],
      "metadata": {
        "id": "hP3OWdLQGyHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN4pACLVFOFj",
        "outputId": "f215b612-46d3-4369-f7be-71c96dbfec38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DeepSeek Client Initialized successfully for new run.\n",
            "\n",
            "-> Agent Receives Task with **CONCISE** System Prompt.\n",
            "\n",
            "======================================================================\n",
            "âœ¨ AGENT OBSERVABILITY TRACE CAPTURED (DeepSeek Agent) - CONCISED RUN âœ¨\n",
            "======================================================================\n",
            "{\n",
            "    \"timestamp\": \"2025-12-04 11:22:09\",\n",
            "    \"agent_id\": \"Reasoner-002\",\n",
            "    \"request_summary\": \"There are three boxes. One contains only apples, o...\",\n",
            "    \"inference_latency_ms\": 45321.93,\n",
            "    \"tokens_input\": 88,\n",
            "    \"tokens_output\": 1274,\n",
            "    \"tokens_total\": 1362,\n",
            "    \"model_name\": \"deepseek-reasoner\",\n",
            "    \"had_reasoning_step\": true,\n",
            "    \"response_quality_evaluation\": \"Requires Human or Automated Grading\",\n",
            "    \"reasoning_trace_length\": 5293\n",
            "}\n",
            "======================================================================\n",
            "\n",
            "--- AGENT'S CONCISE FINAL ANSWER ---\n",
            "1. Open the box labeled \"apples and oranges\" and remove one fruit.\n",
            "2. If the fruit is an apple, that box actually contains only apples; if it's an orange, it contains only oranges.\n",
            "3. From this, deduce: If the opened box had apples, then the box labeled \"apples\" contains oranges and the box labeled \"oranges\" contains both. If it had oranges, then the box labeled \"apples\" contains both and the box labeled \"oranges\" contains apples.\n",
            "\n",
            "--- COMPARATIVE AGENTIC OBSERVABILITY METRICS SUMMARY ---\n",
            "**New Latency:** 45321.93 ms\n",
            "**New Total Tokens:** 1362\n",
            "\n",
            "**Compare this to the previous run's 56738.03 ms latency and 1604 tokens.** The change in these metrics will show the impact of the prompt optimization.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from typing import Dict, Any\n",
        "from google.colab import userdata # Keep this for Colab environment\n",
        "\n",
        "# --- CONFIGURATION (Direct Key Retrieval) ---\n",
        "\n",
        "try:\n",
        "    API_KEY = userdata.get(\"DEEPSEEK_API_KEY\")\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"DEEPSEEK_API_KEY not found in Colab userdata.\")\n",
        "    client = OpenAI(api_key=API_KEY, base_url=\"https://api.deepseek.com\")\n",
        "    print(\"âœ… DeepSeek Client Initialized successfully for new run.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error initializing client: {e}\")\n",
        "    client = None\n",
        "\n",
        "# --- 1. The Agent Class (Same as before) ---\n",
        "class DeepSeekReasonerAgent:\n",
        "    \"\"\"A minimal agent that uses DeepSeek-Reasoner to solve a task and logs observability metrics.\"\"\"\n",
        "\n",
        "    def __init__(self, client: OpenAI, agent_id=\"Reasoner-002\"): # Changed ID for new run\n",
        "        self.client = client\n",
        "        self.id = agent_id\n",
        "\n",
        "    def run_task(self, request: str, system_prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"Executes the LLM call and captures observability metrics.\"\"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": request},\n",
        "        ]\n",
        "\n",
        "        agent_output = \"API call failed.\"\n",
        "        obs_data = {}\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"deepseek-reasoner\",\n",
        "                messages=messages,\n",
        "                stream=False\n",
        "            )\n",
        "\n",
        "            end_time = time.time()\n",
        "            inference_latency_ms = (end_time - start_time) * 1000\n",
        "            usage = response.usage\n",
        "            final_content = response.choices[0].message.content\n",
        "            reasoning = getattr(response.choices[0].message, 'reasoning_content', 'N/A')\n",
        "            agent_output = final_content\n",
        "\n",
        "            # --- (B) Structured Observability Data Payload ---\n",
        "            obs_data = {\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"agent_id\": self.id,\n",
        "                \"request_summary\": request[:50] + \"...\",\n",
        "                \"inference_latency_ms\": round(inference_latency_ms, 2),\n",
        "                \"tokens_input\": usage.prompt_tokens,\n",
        "                \"tokens_output\": usage.completion_tokens,\n",
        "                \"tokens_total\": usage.total_tokens,\n",
        "                \"model_name\": response.model,\n",
        "                \"had_reasoning_step\": reasoning != 'N/A',\n",
        "                \"response_quality_evaluation\": \"Requires Human or Automated Grading\",\n",
        "                \"reasoning_trace_length\": len(reasoning) if reasoning != 'N/A' else 0,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = time.time()\n",
        "            inference_latency_ms = (end_time - start_time) * 1000\n",
        "            agent_output = f\"API Error: {str(e)}\"\n",
        "            obs_data = {\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"agent_id\": self.id,\n",
        "                \"request_summary\": request[:50] + \"...\",\n",
        "                \"inference_latency_ms\": round(inference_latency_ms, 2),\n",
        "                \"status\": \"FAILURE\",\n",
        "                \"error_message\": str(e),\n",
        "                \"tokens_total\": 0,\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"agent_output\": agent_output,\n",
        "            \"observability_data\": obs_data\n",
        "        }\n",
        "\n",
        "# --- 2. Observability Logging Function (Same as before) ---\n",
        "def log_agent_event(data: Dict[str, Any]):\n",
        "    \"\"\"Prints the structured observability data in a readable JSON format.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âœ¨ AGENT OBSERVABILITY TRACE CAPTURED (DeepSeek Agent) - CONCISED RUN âœ¨\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(data, indent=4))\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# --- 3. Demo Execution (New Prompt) ---\n",
        "if __name__ == \"__main__\":\n",
        "    if not client:\n",
        "        print(\"\\nDemo cannot run. Client initialization failed.\")\n",
        "    else:\n",
        "        my_agent = DeepSeekReasonerAgent(client=client)\n",
        "\n",
        "        puzzle_request = (\n",
        "            \"There are three boxes. One contains only apples, one contains only oranges, and one contains both. \"\n",
        "            \"Each box is incorrectly labeled. You can only open one box and take out one fruit. \"\n",
        "            \"By looking at the fruit, how can you label all the boxes correctly?\"\n",
        "        )\n",
        "\n",
        "        # --- MODIFIED SYSTEM PROMPT for Concision ---\n",
        "        new_system_prompt = (\n",
        "            \"You are a helpful assistant that solves logic puzzles. \"\n",
        "            \"**Your response must be brief: provide only the final 3-step solution without verbose introductions or examples.**\"\n",
        "        )\n",
        "\n",
        "        print(f\"\\n-> Agent Receives Task with **CONCISE** System Prompt.\")\n",
        "\n",
        "        # Run the agent task\n",
        "        result = my_agent.run_task(puzzle_request, new_system_prompt)\n",
        "\n",
        "        # Log the full structured data for the observability pipeline\n",
        "        log_agent_event(result[\"observability_data\"])\n",
        "\n",
        "        print(\"\\n--- AGENT'S CONCISE FINAL ANSWER ---\")\n",
        "        print(result[\"agent_output\"])\n",
        "\n",
        "        # Summarize key Agentic Metrics\n",
        "        print(\"\\n--- COMPARATIVE AGENTIC OBSERVABILITY METRICS SUMMARY ---\")\n",
        "        obs_data = result['observability_data']\n",
        "        print(f\"**New Latency:** {obs_data.get('inference_latency_ms', 'N/A')} ms\")\n",
        "        print(f\"**New Total Tokens:** {obs_data.get('tokens_total', 'N/A')}\")\n",
        "        print(\"\\n**Compare this to the previous run's 56738.03 ms latency and 1604 tokens.** The change in these metrics will show the impact of the prompt optimization.\")"
      ]
    }
  ]
}
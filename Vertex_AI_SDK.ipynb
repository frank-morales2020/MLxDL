{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Vertex_AI_SDK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiVwfFuZ9axh"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install  -q gcsfs==2024.3.1\n",
        "!pip install  -q accelerate==0.31.0\n",
        "!pip install  -q transformers==4.45.2\n",
        "!pip install  -q  datasets==2.19.2\n",
        "!pip install google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai  -q\n",
        "!pip install tensorflow_datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPARATION"
      ],
      "metadata": {
        "id": "A69PfDH83btq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "\n",
        "# --- Data Loading from Google Drive ---\n",
        "zip_path = '/content/gdrive/MyDrive/datasets/CMAPSSData.zip'\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(f\"Error: CMAPSSData.zip not found at {zip_path}. Please ensure the file is correctly located in your Google Drive.\")\n",
        "    raise FileNotFoundError(f\"CMAPSSData.zip not found at {zip_path}\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        if zip_ref.testzip() is None:  # Check for ZIP file integrity\n",
        "            zip_ref.extractall(extract_dir)\n",
        "            print(f\"Extracted dataset files to: {extract_dir}\")\n",
        "        else:\n",
        "            print(\"Error: ZIP file integrity check failed. The file may not be a valid ZIP file.\")\n",
        "            raise zipfile.BadZipFile(\"ZIP file integrity check failed.\")\n",
        "\n",
        "except zipfile.BadZipFile as e:\n",
        "    print(f\"Error extracting ZIP file: {e}\")\n",
        "    print(\n",
        "        \"The uploaded file may not be a valid or complete ZIP file. \"\n",
        "        \"Please ensure you have uploaded the correct file, that it is not corrupted, \"\n",
        "        \"and that it is a standard ZIP archive.\"\n",
        "    )\n",
        "    raise  # Stop execution if extraction fails\n",
        "\n",
        "# --- Prepare NASA CMAPSS Data and Save to JSONL in GCS ---\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Process all four subsets\n",
        "data_subsets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "\n",
        "for data_subset in data_subsets:\n",
        "    train_file = os.path.join(extract_dir, f'train_{data_subset}.txt')\n",
        "    test_file = os.path.join(extract_dir, f'test_{data_subset}.txt')\n",
        "    rul_file = os.path.join(extract_dir, f'RUL_{data_subset}.txt')\n",
        "\n",
        "    SENSOR_COLUMNS = ['sensor' + str(i).zfill(2) for i in range(1, 22)]\n",
        "    OP_SETTING_COLUMNS = ['op_setting_' + str(i) for i in range(1, 4)]\n",
        "    DATA_COLUMNS = ['unit_nr', 'time_cycles'] + OP_SETTING_COLUMNS + SENSOR_COLUMNS\n",
        "\n",
        "    # Load training data\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        test_df = pd.read_csv(test_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        rul_df = pd.read_csv(rul_file, names=['RUL'], delim_whitespace=True, header=None)\n",
        "\n",
        "        train_df.columns = DATA_COLUMNS\n",
        "        test_df.columns = DATA_COLUMNS\n",
        "\n",
        "        print(f\"\\nProcessing data subset: {data_subset}\")\n",
        "        print(\"Shape of train_df after loading:\", train_df.shape)\n",
        "        print(\"train_df head after loading:\\n\", train_df.head())\n",
        "        print(\"Shape of test_df:\", test_df.shape)\n",
        "        print(\"test_df head after loading:\\n\", test_df.head())\n",
        "        print(\"Shape of RUL data:\", rul_df.shape)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data files for subset {data_subset}: {e}\")\n",
        "        raise  # Stop execution if a file is missing\n",
        "\n",
        "    def create_jsonl(df, rul_df, output_path, sequence_length=30, is_test=False):\n",
        "        grouped_data = df.groupby('unit_nr')\n",
        "        rul_values = rul_df.values.tolist()  # Convert RUL DataFrame to list\n",
        "        engine_count = 0  # To track which RUL value to use\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            for unit_nr, unit_data in grouped_data:\n",
        "                num_cycles = len(unit_data)\n",
        "                data_values = unit_data.drop(['unit_nr'], axis=1).values.tolist()\n",
        "                json_data = []  # Initialize an empty list to hold JSON objects\n",
        "\n",
        "                for i in range(max(0, num_cycles - sequence_length + 1)):\n",
        "                    sequence = data_values[i:i + sequence_length]\n",
        "                    rul = num_cycles - (i + sequence_length)\n",
        "\n",
        "                    # Ensure RUL is not out of bounds\n",
        "                    if engine_count < len(rul_values):\n",
        "                        current_rul = rul_values[engine_count][0]  # Get the RUL value\n",
        "                    else:\n",
        "                        current_rul = 0  # Or some default value if RUL data is exhausted\n",
        "\n",
        "                    if len(sequence) == sequence_length:\n",
        "                        json_record = {\"sequence\": sequence, \"sequence_length\": len(sequence), \"rul\": current_rul}  # Include sequence length\n",
        "                        json_data.append(json_record)\n",
        "\n",
        "                # Write all JSON objects to the file at once\n",
        "                with open(output_path, 'w') as f:\n",
        "                    for json_record in json_data:\n",
        "                        f.write(json.dumps(json_record) + '\\n')\n",
        "\n",
        "                engine_count += 1  # Increment engine counter\n",
        "\n",
        "    local_train_jsonl_path = f\"cmapss_{data_subset}_train_sequences.jsonl\"\n",
        "    local_test_jsonl_path = f\"cmapss_{data_subset}_test_sequences.jsonl\"\n",
        "\n",
        "    # Create JSONL for training\n",
        "    create_jsonl(train_df, rul_df, local_train_jsonl_path, is_test=False)\n",
        "    print(f\"Created {local_train_jsonl_path}\")\n",
        "\n",
        "    # Create JSONL for testing\n",
        "    create_jsonl(test_df, rul_df, local_test_jsonl_path, is_test=True)\n",
        "    print(f\"Created {local_test_jsonl_path}\")\n",
        "\n",
        "    # --- Upload JSONL files to GCS ---\n",
        "    client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "    blob_train = bucket.blob(f\"cmapss_{data_subset}_train_sequences.jsonl\")  # Adapt to your naming scheme\n",
        "    blob_test = bucket.blob(f\"cmapss_{data_subset}_test_sequences.jsonl\")   # Adapt to your naming scheme\n",
        "\n",
        "    blob_train.upload_from_filename(local_train_jsonl_path)\n",
        "    print(f\"Uploaded training data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_train_sequences.jsonl\")\n",
        "\n",
        "    blob_test.upload_from_filename(local_test_jsonl_path)\n",
        "    print(f\"Uploaded evaluation data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_test_sequences.jsonl\")\n",
        "\n",
        "print(\"JSONL files created and uploaded.\")"
      ],
      "metadata": {
        "id": "smsD0LA2wvUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINE TUNING - NASA DATASET"
      ],
      "metadata": {
        "id": "Ge_1sCQo3TSX"
      }
    },
    {
      "source": [
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.cloud import storage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "TRAINING_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_sequences.jsonl\"\n",
        "EVAL_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_sequences.jsonl\"\n",
        "\n",
        "# --- Define trainer/train.py content ---\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import time\n",
        "import subprocess  # Import for gsutil cp\n",
        "from google.cloud import storage\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Function to create the model directory in GCS\n",
        "def create_model_dir(model_dir):\n",
        "    storage_client = storage.Client()\n",
        "    bucket_name = model_dir.split('/')[2]  # Extract bucket name\n",
        "    blob_prefix = '/'.join(model_dir.split('/')[3:])  # Extract blob prefix\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create each subdirectory individually\n",
        "    subdirs = blob_prefix.split('/')\n",
        "    current_prefix = ''\n",
        "    for subdir in subdirs:\n",
        "        current_prefix = os.path.join(current_prefix, subdir)\n",
        "        blob = bucket.blob(current_prefix + '/')  # Add trailing slash to create directory\n",
        "        blob.upload_from_string('')  # Upload empty string to create directory\n",
        "        logging.info(\"Created model subdirectory: %s\", current_prefix)\n",
        "\n",
        "\n",
        "class CMAPSSJSONLDataset(data.Dataset):\n",
        "    def __init__(self, data_path, sequence_length=30):\n",
        "        self.data = []\n",
        "        self.sequence_length = sequence_length\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                    rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                    self.data.append((sequence, rul))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(\"Skipping invalid JSON line: %r, Error: %s\", repr(line), e)\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input x shape:\", x.shape)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "def train_model(model_name, train_dataset_path, eval_dataset_path, staging_bucket, bucket_name, base_output_dir):\n",
        "    import logging\n",
        "    import torch\n",
        "\n",
        "    logging.info(\"Model name: %s\", model_name)\n",
        "    logging.info(\"Train Dataset Path: %s\", train_dataset_path)\n",
        "    logging.info(\"Eval Dataset Path: %s\", eval_dataset_path)\n",
        "    logging.info(\"Staging Bucket: %s\", staging_bucket)\n",
        "    logging.info(\"Bucket Name: %s\", bucket_name)\n",
        "    logging.info(\"Base Output Dir: %s\", base_output_dir)\n",
        "\n",
        "    train_dataset = CMAPSSJSONLDataset(train_dataset_path)\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path)\n",
        "\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    eval_loader = data.DataLoader(eval_dataset, batch_size=64)\n",
        "\n",
        "    input_size = train_dataset[0][0].shape[-1]\n",
        "    hidden_size = 64\n",
        "\n",
        "    model = RULPredictionModel(input_size, hidden_size)\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (sequences, ruls) in enumerate(train_loader):\n",
        "            print(\"Sequence shape:\", sequences.shape)\n",
        "            sequences = sequences.to(device)\n",
        "            ruls = ruls.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, ruls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                log_msg = f\"Epoch: {epoch + 1}, Batch: {batch_idx}, Loss: {loss.item()}\"\n",
        "                logging.info(log_msg)\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        log_msg = f\"Epoch: {epoch + 1}, Average Training Loss: {avg_loss}\"\n",
        "        logging.info(log_msg)\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sequences_eval, ruls_eval in eval_loader:\n",
        "                sequences_eval = sequences_eval.to(device)\n",
        "                ruls_eval = ruls_eval.to(device)\n",
        "\n",
        "                outputs_eval = model(sequences_eval)\n",
        "                loss_eval = criterion(outputs_eval, ruls_eval)\n",
        "                # Ensure loss_eval is a tensor\n",
        "                eval_loss += loss_eval.item()  # Extract the value\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        log_msg = f\"Epoch: {epoch + 1}, Average Evaluation Loss: {avg_eval_loss}\"\n",
        "        logging.info(log_msg)\n",
        "\n",
        "    logging.info(\"Starting model saving...\")\n",
        "\n",
        "    # Local path to temporarily save the model\n",
        "    local_model_path = 'local_model.pth'\n",
        "    torch.save(model.state_dict(), local_model_path)\n",
        "\n",
        "    if 'AIP_MODEL_DIR' in os.environ:\n",
        "        gcs_model_path = os.path.join(os.environ['AIP_MODEL_DIR'], 'model-nasa.pth')\n",
        "        # use gsutil to copy the model\n",
        "        subprocess.call(['gsutil', 'cp', local_model_path, gcs_model_path])\n",
        "        logging.info(\"Copied model to Vertex AI path: %s\", gcs_model_path)\n",
        "    else:\n",
        "        logging.info(\"Saving model to local path: %s\", local_model_path)\n",
        "\n",
        "    logging.info(\"Model saved to: %s\", model_save_path)\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"rul_predictor_jsonl\", help=\"Model name\")\n",
        "    parser.add_argument(\"--train_dataset\", type=str, required=True, help=\"Path to training dataset JSONL\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"Path to evaluation dataset JSONL\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"Staging bucket for Vertex AI\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Bucket name\")\n",
        "    parser.add_argument(\"--base_output_dir\", type=str, required=True, help=\"Base output directory in GCS\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(args.model, args.train_dataset, args.eval_dataset, args.staging_bucket, args.bucket_name, args.base_output_dir)\n",
        "\"\"\"\n",
        "\n",
        "# Create or overwrite trainer/train.py\n",
        "os.makedirs('trainer', exist_ok=True)\n",
        "with open('trainer/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# --- Define and run custom training job ---\n",
        "BASE_MODEL_NAME = \"rul_predictor_cmapss_jsonl\"\n",
        "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}/model_output\"  # Define base output directory\n",
        "\n",
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"NASA-cmapss-rul-prediction-jsonl\",\n",
        "    script_path=\"trainer/train.py\",\n",
        "    container_uri='us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest',\n",
        "    requirements=[\"google-cloud-aiplatform\", \"torch\", \"google-cloud-storage\"],\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\",\n",
        "    #service_account=f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\" # Added to job definition\n",
        ")\n",
        "\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--model\", BASE_MODEL_NAME,\n",
        "        \"--train_dataset\", TRAINING_DATA_PATH,\n",
        "        \"--eval_dataset\", EVAL_DATA_PATH,\n",
        "        \"--staging_bucket\", STAGING_BUCKET,\n",
        "        \"--bucket_name\", BUCKET_NAME,\n",
        "        \"--base_output_dir\", BASE_OUTPUT_DIR,\n",
        "    ],\n",
        "    replica_count=1,\n",
        "    machine_type=\"n1-standard-8\",\n",
        "    model_display_name=\"cmapss-rul-jsonl-model\"\n",
        ")\n",
        "\n",
        "logging.info(f\"Fine-tuned model: {model.resource_name}\")\n",
        "\n",
        "# --- Potential Next Steps (Outline) ---\n",
        "print(\"\\\\nPotential Next Steps:\")\n",
        "print(\"- Monitor the training job in the Google Cloud Console.\")\n",
        "print(\"- Evaluate the model performance using more comprehensive metrics.\")\n",
        "print(\"- Deploy the trained model to a Vertex AI Endpoint for predictions.\")\n",
        "print(\"- Experiment with different model architectures and hyperparameters.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "djG92zhittgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!gsutil iam ch serviceAccount:{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com:roles/storage.objectAdmin gs://{BUCKET_NAME}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2PqbPMYarulW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil iam get gs://{BUCKET_NAME}"
      ],
      "metadata": {
        "id": "PcvJzuzAmBAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!gsutil iam ch serviceAccount:{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com:roles/storage.objectViewer gs://{BUCKET_NAME}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4_MBc9tZsVNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil iam ch serviceAccount:{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com:roles/storage.objectCreator gs://{BUCKET_NAME}"
      ],
      "metadata": {
        "id": "w_ogSbwlmN2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A69PfDH83btq"
      ],
      "authorship_tag": "ABX9TyMGHVMNNgvsSv75aT0nsp0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
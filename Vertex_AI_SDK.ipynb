{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Vertex_AI_SDK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiVwfFuZ9axh"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install  -q gcsfs==2024.3.1\n",
        "!pip install  -q accelerate==0.31.0\n",
        "!pip install  -q transformers==4.45.2\n",
        "!pip install  -q  datasets==2.19.2\n",
        "!pip install google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai  -q\n",
        "!pip install tensorflow_datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPARATION"
      ],
      "metadata": {
        "id": "A69PfDH83btq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# --- Data Loading from Google Drive ---\n",
        "zip_path = '/content/gdrive/MyDrive/datasets/CMAPSSData.zip'\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(f\"Error: CMAPSSData.zip not found at {zip_path}. Please ensure the file is correctly located in your Google Drive.\")\n",
        "    raise FileNotFoundError(f\"CMAPSSData.zip not found at {zip_path}\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        if zip_ref.testzip() is None:  # Check for ZIP file integrity\n",
        "            zip_ref.extractall(extract_dir)\n",
        "            print(f\"Extracted dataset files to: {extract_dir}\")\n",
        "        else:\n",
        "            print(\"Error: ZIP file integrity check failed. The file may not be a valid ZIP file.\")\n",
        "            raise zipfile.BadZipFile(\"ZIP file integrity check failed.\")\n",
        "\n",
        "except zipfile.BadZipFile as e:\n",
        "    print(f\"Error extracting ZIP file: {e}\")\n",
        "    print(\n",
        "        \"The uploaded file may not be a valid or complete ZIP file. \"\n",
        "        \"Please ensure you have uploaded the correct file, that it is not corrupted, \"\n",
        "        \"and that it is a standard ZIP archive.\"\n",
        "    )\n",
        "    raise  # Stop execution if extraction fails\n",
        "\n",
        "# --- Prepare NASA CMAPSS Data and Save to JSONL in GCS ---\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Process all four subsets\n",
        "data_subsets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "\n",
        "for data_subset in data_subsets:\n",
        "    train_file = os.path.join(extract_dir, f'train_{data_subset}.txt')\n",
        "    test_file = os.path.join(extract_dir, f'test_{data_subset}.txt')\n",
        "    rul_file = os.path.join(extract_dir, f'RUL_{data_subset}.txt')\n",
        "\n",
        "    SENSOR_COLUMNS = ['sensor' + str(i).zfill(2) for i in range(1, 22)]\n",
        "    OP_SETTING_COLUMNS = ['op_setting_' + str(i) for i in range(1, 4)]\n",
        "    DATA_COLUMNS = ['unit_nr', 'time_cycles'] + OP_SETTING_COLUMNS + SENSOR_COLUMNS\n",
        "\n",
        "    # Load training data\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        test_df = pd.read_csv(test_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        rul_df = pd.read_csv(rul_file, names=['RUL'], delim_whitespace=True, header=None)\n",
        "\n",
        "        train_df.columns = DATA_COLUMNS\n",
        "        test_df.columns = DATA_COLUMNS\n",
        "\n",
        "        print(f\"\\nProcessing data subset: {data_subset}\")\n",
        "        print(\"Shape of train_df after loading:\", train_df.shape)\n",
        "        print(\"train_df head after loading:\\n\", train_df.head())\n",
        "        print(\"Shape of test_df:\", test_df.shape)\n",
        "        print(\"test_df head after loading:\\n\", test_df.head())\n",
        "        print(\"Shape of RUL data:\", rul_df.shape)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data files for subset {data_subset}: {e}\")\n",
        "        raise  # Stop execution if a file is missing\n",
        "\n",
        "    def create_jsonl(df, rul_df, output_path, sequence_length=30, is_test=False):\n",
        "        grouped_data = df.groupby('unit_nr')\n",
        "        rul_values = rul_df.values.tolist()  # Convert RUL DataFrame to list\n",
        "        engine_count = 0  # To track which RUL value to use\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            for unit_nr, unit_data in grouped_data:\n",
        "                num_cycles = len(unit_data)\n",
        "                data_values = unit_data.drop(['unit_nr'], axis=1).values.tolist()\n",
        "                json_data = []  # Initialize an empty list to hold JSON objects\n",
        "\n",
        "                for i in range(max(0, num_cycles - sequence_length + 1)):\n",
        "                    sequence = data_values[i:i + sequence_length]\n",
        "                    rul = num_cycles - (i + sequence_length)\n",
        "\n",
        "                    # Ensure RUL is not out of bounds\n",
        "                    if engine_count < len(rul_values):\n",
        "                        current_rul = rul_values[engine_count][0]  # Get the RUL value\n",
        "                    else:\n",
        "                        current_rul = 0  # Or some default value if RUL data is exhausted\n",
        "\n",
        "                    if len(sequence) == sequence_length:\n",
        "                        json_record = {\"sequence\": sequence, \"sequence_length\": len(sequence), \"rul\": current_rul}  # Include sequence length\n",
        "                        json_data.append(json_record)\n",
        "\n",
        "                # Write all JSON objects to the file at once\n",
        "                with open(output_path, 'w') as f:\n",
        "                    for json_record in json_data:\n",
        "                        f.write(json.dumps(json_record) + '\\n')\n",
        "\n",
        "                engine_count += 1  # Increment engine counter\n",
        "\n",
        "    local_train_jsonl_path = f\"cmapss_{data_subset}_train_sequences.jsonl\"\n",
        "    local_test_jsonl_path = f\"cmapss_{data_subset}_test_sequences.jsonl\"\n",
        "\n",
        "    # Create JSONL for training\n",
        "    create_jsonl(train_df, rul_df, local_train_jsonl_path, is_test=False)\n",
        "    print(f\"Created {local_train_jsonl_path}\")\n",
        "\n",
        "    # Create JSONL for testing\n",
        "    create_jsonl(test_df, rul_df, local_test_jsonl_path, is_test=True)\n",
        "    print(f\"Created {local_test_jsonl_path}\")\n",
        "\n",
        "    # --- Upload JSONL files to GCS ---\n",
        "    client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "    blob_train = bucket.blob(f\"cmapss_{data_subset}_train_sequences.jsonl\")  # Adapt to your naming scheme\n",
        "    blob_test = bucket.blob(f\"cmapss_{data_subset}_test_sequences.jsonl\")   # Adapt to your naming scheme\n",
        "\n",
        "    blob_train.upload_from_filename(local_train_jsonl_path)\n",
        "    print(f\"Uploaded training data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_train_sequences.jsonl\")\n",
        "\n",
        "    blob_test.upload_from_filename(local_test_jsonl_path)\n",
        "    print(f\"Uploaded evaluation data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_test_sequences.jsonl\")\n",
        "\n",
        "print(\"JSONL files created and uploaded.\")"
      ],
      "metadata": {
        "id": "smsD0LA2wvUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINE TUNING - NASA DATASET"
      ],
      "metadata": {
        "id": "Ge_1sCQo3TSX"
      }
    },
    {
      "source": [
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.cloud import storage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "TRAINING_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD002_train_sequences.jsonl\"\n",
        "EVAL_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD002_test_sequences.jsonl\"\n",
        "\n",
        "\n",
        "# --- Define trainer/train.py content ---\n",
        "#train_py_content_with_early_stopping\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import subprocess\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def create_gcs_dir(model_dir):\n",
        "    #Creates the model directory in Google Cloud Storage.\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = model_dir.split('/')[2]\n",
        "        blob_prefix = '/'.join(model_dir.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        subdirs = blob_prefix.split('/')\n",
        "        current_prefix = ''\n",
        "        for subdir in subdirs:\n",
        "            current_prefix = os.path.join(current_prefix, subdir)\n",
        "            blob = bucket.blob(current_prefix + '/')\n",
        "            blob.upload_from_string('')\n",
        "            logging.info(\"Created GCS directory: %s\", current_prefix)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating GCS directory: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_jsonl_dataset(data_path, sequence_length=30):\n",
        "    #Loads a CMAPSS dataset from a JSONL file (local or GCS).\n",
        "\n",
        "    data = []\n",
        "    try:\n",
        "        if data_path.startswith('gs://'):\n",
        "            storage_client = storage.Client()\n",
        "            bucket_name = data_path.split('/')[2]\n",
        "            blob_name = '/'.join(data_path.split('/')[3:])\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(blob_name)\n",
        "            tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "            blob.download_to_filename(tmp_file)\n",
        "            file_path = tmp_file\n",
        "        else:\n",
        "            file_path = data_path\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                    rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                    data.append((sequence, rul))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(\"Skipping invalid JSON line: %r, Error: %s\", repr(line), e)\n",
        "\n",
        "        if data_path.startswith('gs://'):\n",
        "            os.remove(tmp_file)  # Clean up temporary file\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Feature Engineering Functions ---\n",
        "\n",
        "def create_rolling_features(df, window_size=10):\n",
        "\n",
        "    for sensor in [col for col in df.columns if col.startswith('sensor')]:\n",
        "        df[f'{sensor}_mean'] = df.groupby('unit_nr')[sensor].transform(lambda x: x.rolling(window=window_size).mean())\n",
        "        df[f'{sensor}_std'] = df.groupby('unit_nr')[sensor].transform(lambda x: x.rolling(window=window_size).std())\n",
        "    return df\n",
        "\n",
        "# --- Dataset Class ---\n",
        "\n",
        "class CMAPSSJSONLDataset(data.Dataset):\n",
        "    #CMAPSS dataset loader from JSONL.\n",
        "\n",
        "    def __init__(self, data_path, sequence_length=30, use_rolling_features=False): #Accepts single path\n",
        "        self.data = load_jsonl_dataset(data_path, sequence_length)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.use_rolling_features = use_rolling_features\n",
        "\n",
        "        if self.use_rolling_features:\n",
        "            # Convert data to DataFrame for feature engineering\n",
        "            df = pd.DataFrame([item[0].numpy() for item in self.data])\n",
        "            # Add unit_nr as a placeholder (assuming it's needed for grouping)\n",
        "            df['unit_nr'] = np.repeat(np.arange(len(df) / self.sequence_length), self.sequence_length)\n",
        "            df = create_rolling_features(df)\n",
        "\n",
        "            # Update self.data with the modified sequences (and RUL)\n",
        "            new_data = []\n",
        "            for i in range(len(self.data)):\n",
        "                rul = self.data[i][1]\n",
        "                seq = df.iloc[i].values\n",
        "                new_data.append((torch.tensor(seq, dtype=torch.float32), rul))\n",
        "            self.data = new_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# --- Model Definition ---\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    #LSTM-based model for RUL prediction\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# --- Training Function ---\n",
        "\n",
        "def train_model(model_name, train_dataset_path, eval_dataset_path, #Accepts single path\n",
        "                staging_bucket, bucket_name, base_output_dir,\n",
        "                use_rolling_features=False):\n",
        "\n",
        "    #Trains the RUL prediction model with FD004 dataset.\n",
        "\n",
        "    logging.info(\"Training configuration:\")\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Train Dataset Path: {train_dataset_path}\")\n",
        "    logging.info(f\"Eval Dataset Path: {eval_dataset_path}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "    logging.info(f\"Bucket Name: {bucket_name}\")\n",
        "    logging.info(f\"Base Output Dir: {base_output_dir}\")\n",
        "    logging.info(f\"Use Rolling Features: {use_rolling_features}\")\n",
        "\n",
        "    # 1. Data Loaders\n",
        "    train_dataset = CMAPSSJSONLDataset(train_dataset_path, use_rolling_features=use_rolling_features)\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path, use_rolling_features=use_rolling_features)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=64)\n",
        "\n",
        "    # 2. Model Initialization\n",
        "    input_size = train_dataset[0][0].shape[-1]\n",
        "    hidden_size = 128\n",
        "    model = RULPredictionModel(input_size, hidden_size, num_layers=3, dropout=0.3)\n",
        "\n",
        "    # 3. Device Configuration (CPU)\n",
        "    device = torch.device(\"cpu\")  # Force CPU\n",
        "    model.to(device)\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 4. Optimizer and Loss Function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # 5. Training Loop\n",
        "    num_epochs = 100000  # Adjusted epochs for testing\n",
        "    best_eval_loss = float('inf')\n",
        "    patience = 10\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (sequences, ruls) in enumerate(train_loader):\n",
        "            sequences = sequences.to(device)\n",
        "            ruls = ruls.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, ruls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                logging.info(f\"Epoch: {epoch + 1}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        logging.info(f\"Epoch: {epoch + 1}, Average Training Loss: {avg_loss}\")\n",
        "\n",
        "        # 6. Evaluation\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sequences_eval, ruls_eval in eval_loader:\n",
        "                sequences_eval = sequences_eval.to(device)\n",
        "                ruls_eval = ruls_eval.to(device)\n",
        "\n",
        "                outputs_eval = model(sequences_eval)\n",
        "                loss_eval = criterion(outputs_eval, ruls_eval)\n",
        "                eval_loss += loss_eval.item()\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        logging.info(f\"Epoch: {epoch + 1}, Average Evaluation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # 7. Early Stopping\n",
        "        if avg_eval_loss < best_eval_loss:\n",
        "            best_eval_loss = avg_eval_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                logging.info(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    # 8. Load the best model\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "    logging.info(\"Starting model saving...\")\n",
        "\n",
        "    # 9. Save Model (Local and GCS)\n",
        "    local_model_path = 'model-nasa.pth'\n",
        "    torch.save(model.state_dict(), local_model_path)\n",
        "\n",
        "    try:\n",
        "        base_output_path = base_output_dir\n",
        "        subprocess.run(['gsutil', 'cp', local_model_path, base_output_path], check=True)\n",
        "        logging.info(f\"Copied model to GCS BUCKET path: {base_output_path}\")\n",
        "\n",
        "        if 'AIP_MODEL_DIR' in os.environ:\n",
        "            gcs_model_path = os.path.join(os.environ['AIP_MODEL_DIR'], 'model-nasa.pth')\n",
        "            subprocess.run(['gsutil', 'cp', local_model_path, gcs_model_path], check=True)\n",
        "            logging.info(f\"Copied model to Vertex AI path: {gcs_model_path}\")\n",
        "            model_save_path = gcs_model_path\n",
        "        else:\n",
        "            logging.info(f\"Saving model to local path: {local_model_path}\")\n",
        "            model_save_path = local_model_path\n",
        "\n",
        "        logging.info(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logging.error(f\"Error saving model: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"rul_predictor_jsonl\", help=\"Model name\")\n",
        "    parser.add_argument(\"--train_dataset\", type=str, required=True, help=\"Path to training dataset JSONL\") #Single path\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"Path to evaluation dataset JSONL\")  #Single path\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"Staging bucket for Vertex AI\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Bucket name\")\n",
        "    parser.add_argument(\"--base_output_dir\", type=str, required=True, help=\"Base output directory in GCS\")\n",
        "    parser.add_argument(\"--use_rolling_features\", action='store_true', help=\"Use rolling window features\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(args.model, args.train_dataset, args.eval_dataset,\n",
        "                args.staging_bucket, args.bucket_name, args.base_output_dir,\n",
        "                args.use_rolling_features)\n",
        "\"\"\"\n",
        "\n",
        "# --- Define trainer/train.py content ---\n",
        "train_py_content_original = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import time\n",
        "import subprocess  # Import for gsutil cp\n",
        "from google.cloud import storage\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Function to create the model directory in GCS\n",
        "def create_model_dir(model_dir):\n",
        "    storage_client = storage.Client()\n",
        "    bucket_name = model_dir.split('/')[2]  # Extract bucket name\n",
        "    blob_prefix = '/'.join(model_dir.split('/')[3:])  # Extract blob prefix\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create each subdirectory individually\n",
        "    subdirs = blob_prefix.split('/')\n",
        "    current_prefix = ''\n",
        "    for subdir in subdirs:\n",
        "        current_prefix = os.path.join(current_prefix, subdir)\n",
        "        blob = bucket.blob(current_prefix + '/')  # Add trailing slash to create directory\n",
        "        blob.upload_from_string('')  # Upload empty string to create directory\n",
        "        logging.info(\"Created model subdirectory: %s\", current_prefix)\n",
        "\n",
        "\n",
        "class CMAPSSJSONLDataset(data.Dataset):\n",
        "    def __init__(self, data_path, sequence_length=30):\n",
        "        self.data = []\n",
        "        self.sequence_length = sequence_length\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                    rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                    self.data.append((sequence, rul))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(\"Skipping invalid JSON line: %r, Error: %s\", repr(line), e)\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input x shape:\", x.shape)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "def train_model(model_name, train_dataset_path, eval_dataset_path, staging_bucket, bucket_name, base_output_dir):\n",
        "    import logging\n",
        "    import torch\n",
        "\n",
        "    logging.info(\"Model name: %s\", model_name)\n",
        "    logging.info(\"Train Dataset Path: %s\", train_dataset_path)\n",
        "    logging.info(\"Eval Dataset Path: %s\", eval_dataset_path)\n",
        "    logging.info(\"Staging Bucket: %s\", staging_bucket)\n",
        "    logging.info(\"Bucket Name: %s\", bucket_name)\n",
        "    logging.info(\"Base Output Dir: %s\", base_output_dir)\n",
        "\n",
        "    train_dataset = CMAPSSJSONLDataset(train_dataset_path)\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path)\n",
        "\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    eval_loader = data.DataLoader(eval_dataset, batch_size=64)\n",
        "\n",
        "    input_size = train_dataset[0][0].shape[-1]\n",
        "    hidden_size = 64\n",
        "\n",
        "    model = RULPredictionModel(input_size, hidden_size)\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    num_epochs = 120000\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (sequences, ruls) in enumerate(train_loader):\n",
        "            print(\"Sequence shape:\", sequences.shape)\n",
        "            sequences = sequences.to(device)\n",
        "            ruls = ruls.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, ruls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                log_msg = f\"Epoch: {epoch + 1}, Batch: {batch_idx}, Loss: {loss.item()}\"\n",
        "                logging.info(log_msg)\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        log_msg = f\"Epoch: {epoch + 1}, Average Training Loss: {avg_loss}\"\n",
        "        logging.info(log_msg)\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sequences_eval, ruls_eval in eval_loader:\n",
        "                sequences_eval = sequences_eval.to(device)\n",
        "                ruls_eval = ruls_eval.to(device)\n",
        "\n",
        "                outputs_eval = model(sequences_eval)\n",
        "                loss_eval = criterion(outputs_eval, ruls_eval)\n",
        "                # Ensure loss_eval is a tensor\n",
        "                eval_loss += loss_eval.item()  # Extract the value\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        log_msg = f\"Epoch: {epoch + 1}, Average Evaluation Loss: {avg_eval_loss}\"\n",
        "        logging.info(log_msg)\n",
        "\n",
        "    logging.info(\"Starting model saving...\")\n",
        "\n",
        "    # Local path to temporarily save the model\n",
        "    local_model_path = 'model-nasa.pth'\n",
        "    torch.save(model.state_dict(), local_model_path)\n",
        "\n",
        "    # use gsutil to copy the model\n",
        "    base_output_path = base_output_dir\n",
        "    #base_output_path = os.path.join(base_output_dir, model_name)\n",
        "    subprocess.call(['gsutil', 'cp', local_model_path, base_output_path])\n",
        "    logging.info(\"Copied model to GCS BUCKET path: %s\", base_output_path)\n",
        "\n",
        "    if 'AIP_MODEL_DIR' in os.environ:\n",
        "        gcs_model_path = os.path.join(os.environ['AIP_MODEL_DIR'], 'model-nasa.pth')\n",
        "        # use gsutil to copy the model\n",
        "        subprocess.call(['gsutil', 'cp', local_model_path, gcs_model_path])\n",
        "        logging.info(\"Copied model to Vertex AI path: %s\", gcs_model_path)\n",
        "        # Assign the correct path to model_save_path\n",
        "        model_save_path = gcs_model_path\n",
        "    else:\n",
        "\n",
        "        logging.info(\"Saving model to local path: %s\", local_model_path)\n",
        "        # Assign the correct path to model_save_path\n",
        "        model_save_path = local_model_path\n",
        "\n",
        "    logging.info(\"Model saved to: %s\", model_save_path)  # Now model_save_path is defined\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"rul_predictor_jsonl\", help=\"Model name\")\n",
        "    parser.add_argument(\"--train_dataset\", type=str, required=True, help=\"Path to training dataset JSONL\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"Path to evaluation dataset JSONL\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"Staging bucket for Vertex AI\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Bucket name\")\n",
        "    parser.add_argument(\"--base_output_dir\", type=str, required=True, help=\"Base output directory in GCS\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(args.model, args.train_dataset, args.eval_dataset, args.staging_bucket, args.bucket_name, args.base_output_dir)\n",
        "\"\"\"\n",
        "\n",
        "# Create or overwrite trainer/train.py\n",
        "os.makedirs('trainer', exist_ok=True)\n",
        "with open('trainer/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# --- Define and run custom training job ---\n",
        "BASE_MODEL_NAME = \"rul_predictor_cmapss_jsonl\"\n",
        "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}/model_output\"  # Define base output directory\n",
        "\n",
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"NASA-cmapss-rul-prediction-jsonl\",\n",
        "    script_path=\"trainer/train.py\",\n",
        "    container_uri='us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest',\n",
        "    requirements=[\"google-cloud-aiplatform\", \"torch\", \"google-cloud-storage\"],\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\",\n",
        "    #service_account=f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\" # Added to job definition\n",
        ")\n",
        "\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--model\", BASE_MODEL_NAME,\n",
        "        \"--train_dataset\", TRAINING_DATA_PATH,\n",
        "        \"--eval_dataset\", EVAL_DATA_PATH,\n",
        "        \"--staging_bucket\", STAGING_BUCKET,\n",
        "        \"--bucket_name\", BUCKET_NAME,\n",
        "        \"--base_output_dir\", BASE_OUTPUT_DIR,\n",
        "    ],\n",
        "    replica_count=1,\n",
        "    machine_type=\"n1-standard-8\",\n",
        "    model_display_name=\"cmapss-rul-jsonl-model\"\n",
        ")\n",
        "\n",
        "logging.info(f\"Fine-tuned model: {model.resource_name}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "djG92zhittgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Potential Next Steps (Outline) ---\n",
        "print(\"Potential Next Steps:\")\n",
        "print(\"- Monitor the training job in the Google Cloud Console.\")\n",
        "print(\"- Evaluate the model performance using more comprehensive metrics.\")\n",
        "print(\"- Deploy the trained model to a Vertex AI Endpoint for predictions.\")\n",
        "print(\"- Experiment with different model architectures and hyperparameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2J1390QLQeW",
        "outputId": "f128711f-95cc-4deb-c938-af5d725e5cd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Potential Next Steps:\n",
            "- Monitor the training job in the Google Cloud Console.\n",
            "- Evaluate the model performance using more comprehensive metrics.\n",
            "- Deploy the trained model to a Vertex AI Endpoint for predictions.\n",
            "- Experiment with different model architectures and hyperparameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The Fine Tune {model.display_name} is complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLkkzadDKShi",
        "outputId": "b73df943-5c1d-464f-b363-8f10b0249ec1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fine Tune cmapss-rul-jsonl-model is complete\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!gsutil iam ch serviceAccount:{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com:roles/storage.objectAdmin gs://{BUCKET_NAME}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sKrGEEzX-qf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!gsutil iam ch serviceAccount:{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com:roles/storage.objectAdmin gs://{BUCKET_NAME}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2PqbPMYarulW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil iam get gs://{BUCKET_NAME}"
      ],
      "metadata": {
        "id": "PcvJzuzAmBAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grant Storage Object Creator role\n",
        "\n",
        "!gsutil iam ch \\\n",
        "\"serviceAccount:$SERVICEACCOUNT:objectCreator\" \\\n",
        "\"gs://$BUCKET_NAME\"\n",
        "\n",
        "# Grant Storage Object Viewer role\n",
        "\n",
        "!gsutil iam ch \\\n",
        "\"serviceAccount:$SERVICEACCOUNT:objectViewer\" \\\n",
        "\"gs://$BUCKET_NAME\"\n",
        "\n",
        "# (Optional) Grant Storage Object Admin role (If Necessary)\n",
        "!gsutil iam ch \\\n",
        "\"serviceAccount:$SERVICEACCOUNT:objectAdmin\" \\\n",
        "\"gs://$BUCKET_NAME\""
      ],
      "metadata": {
        "id": "lTbl3GFky7Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://{BUCKET_NAME}/model_output/"
      ],
      "metadata": {
        "id": "SS3242e5_T4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL EVALUATION"
      ],
      "metadata": {
        "id": "rA39w2VJXCPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import torch\n",
        "\n",
        "# Initialize the GCS client\n",
        "client = storage.Client()\n",
        "\n",
        "# Specify the bucket and blob (file)\n",
        "bucket_name = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob_name = \"model_output/model-nasa.pth\"\n",
        "\n",
        "blob_name = \"model_output/model-nasa.pth\"\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob = bucket.blob(blob_name)\n",
        "\n",
        "# Download the file to a local path\n",
        "local_file_path = \"model-nasa.pth\"  # Or your desired local path\n",
        "blob.download_to_filename(local_file_path)\n",
        "\n",
        "print(f\"Downloaded '{blob_name}' from '{bucket_name}' to '{local_file_path}'\")"
      ],
      "metadata": {
        "id": "PUhLGQq8PX5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Assuming this is how the CMAPSSJSONLDataset is defined in the document\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import os\n",
        "\n",
        "class CMAPSSJSONLDataset(data.Dataset):\n",
        "    def __init__(self, data_path, sequence_length=30):\n",
        "        self.data = []\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Check if data_path is a local file or a GCS URI\n",
        "        if data_path.startswith('gs://'):\n",
        "            # If GCS URI, download to a temporary file\n",
        "            storage_client = storage.Client()\n",
        "            bucket_name = data_path.split('/')[2]\n",
        "            blob_name = '/'.join(data_path.split('/')[3:])\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(blob_name)\n",
        "            tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "            blob.download_to_filename(tmp_file)\n",
        "            data_file = tmp_file\n",
        "        else:\n",
        "            # If local file, use it directly\n",
        "            data_file = data_path\n",
        "\n",
        "        with open(data_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                    rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                    self.data.append((sequence, rul))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(\"Skipping invalid JSON line: %s, Error: %s\", line, e)\n",
        "\n",
        "        # Remove the temporary file if it was created\n",
        "        if data_path.startswith('gs://'):\n",
        "            os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    #LSTM-based model for RUL prediction\n",
        "    def __init__(self, input_size, hidden_size, num_layers=3, dropout=0.3): # Match the architecture in train.py\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_cmapss_score(true_rul, predicted_rul):\n",
        "    \"\"\"Calculates a simplified CMAPSS score.\"\"\"\n",
        "    d = np.array(predicted_rul) - np.array(true_rul)  # Difference between predicted and true RUL\n",
        "    score = sum([\n",
        "        np.exp(-d[i] / 13) - 1 if d[i] < 0 else np.exp(d[i] / 10) - 1\n",
        "        for i in range(len(d))\n",
        "    ])\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset_path, input_size, hidden_size, sequence_length):\n",
        "    \"\"\"Evaluates the trained RUL prediction model.\"\"\"\n",
        "\n",
        "    # Load the saved model using the correct model architecture\n",
        "    model = RULPredictionModel(input_size, hidden_size, num_layers=3)  # Change num_layers to 3 to match the training architecture\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "\n",
        "    # Load the evaluation dataset\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path, sequence_length)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False)  # No need to shuffle for evaluation\n",
        "\n",
        "    # Make predictions and calculate metrics\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
        "        for sequences, ruls in eval_loader:\n",
        "            predictions = model(sequences)\n",
        "            all_predictions.extend(predictions.flatten().tolist())\n",
        "            all_targets.extend(ruls.flatten().tolist())\n",
        "\n",
        "    # Calculate evaluation metrics (e.g., MAE, RMSE)\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
        "    mse = mean_squared_error(all_targets, all_predictions)  # Calculate MSE\n",
        "\n",
        "    # Calculate CMAPSS score\n",
        "    cmapss_score = calculate_cmapss_score(all_targets, all_predictions)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Evaluation Results:\")\n",
        "    print(f\"Average Evaluation Loss (MSE): {mse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"CMAPSS Score: {cmapss_score:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Example Usage (if __name__ == \"__main__\": block) ---\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"model-nasa.pth\"  # Replace with the actual path to your saved model\n",
        "    eval_dataset_path = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_sequences.jsonl\"  # Replace with your evaluation data path (GCS URI)\n",
        "    input_size = 25  # Updated to match the input size used during training\n",
        "    hidden_size = 128 # Updated to match the hidden size used during training\n",
        "    sequence_length = 30  # Replace with the sequence length used during training\n",
        "\n",
        "    evaluate_model(model_path, eval_dataset_path, input_size, hidden_size, sequence_length)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nkS9rqGJi8XJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36103b38-0254-4553-f740-376b8dd85d3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "Average Evaluation Loss (MSE): 675.07\n",
            "Mean Absolute Error (MAE): 25.98\n",
            "Root Mean Squared Error (RMSE): 25.98\n",
            "CMAPSS Score: 1607.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N=10\n",
        "\n",
        "Evaluation Results:\n",
        "Average Evaluation Loss (MSE): 675.91\n",
        "Mean Absolute Error (MAE): 26.00\n",
        "Root Mean Squared Error (RMSE): 26.00\n",
        "CMAPSS Score: 1609.80\n",
        "\n",
        "N=200\n",
        "\n",
        "Evaluation Results:\n",
        "Average Evaluation Loss (MSE): 676.00\n",
        "Mean Absolute Error (MAE): 26.00\n",
        "Root Mean Squared Error (RMSE): 26.00\n",
        "CMAPSS Score: 1610.04\n",
        "\n"
      ],
      "metadata": {
        "id": "7y32T44pVnYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune Commandline (FTC)"
      ],
      "metadata": {
        "id": "Z6LOsXjttl6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_py_content=\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import subprocess\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import torch.utils.data as data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def create_gcs_dir(model_dir):\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = model_dir.split('/')[2]\n",
        "        blob_prefix = '/'.join(model_dir.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        subdirs = blob_prefix.split('/')\n",
        "        current_prefix = ''\n",
        "        for subdir in subdirs:\n",
        "            current_prefix = os.path.join(current_prefix, subdir)\n",
        "            blob = bucket.blob(current_prefix + '/')\n",
        "            blob.upload_from_string('')\n",
        "            logging.info(\"Created GCS directory: %s\", current_prefix)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating GCS directory: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_jsonl_dataset(data_path, sequence_length=30):\n",
        "    data = []\n",
        "    try:\n",
        "        if data_path.startswith('gs://'):\n",
        "            storage_client = storage.Client()\n",
        "            bucket_name = data_path.split('/')[2]\n",
        "            blob_name = '/'.join(data_path.split('/')[3:])\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(blob_name)\n",
        "            tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "            blob.download_to_filename(tmp_file)\n",
        "            file_path = tmp_file\n",
        "        else:\n",
        "            file_path = data_path\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                    rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                    data.append((sequence, rul))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(\"Skipping invalid JSON line: %r, Error: %s\", repr(line), e)\n",
        "\n",
        "        if data_path.startswith('gs://'):\n",
        "            os.remove(tmp_file)\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Feature Engineering Functions ---\n",
        "\n",
        "def create_rolling_features(df, window_size=10):\n",
        "\n",
        "    for col in df.columns:\n",
        "        if isinstance(col, str) and col.startswith('sensor'):\n",
        "            df[f'{col}_mean'] = df.groupby('unit_nr')[col].transform(lambda x: x.rolling(window=window_size).mean())\n",
        "            df[f'{col}_std'] = df.groupby('unit_nr')[sensor].transform(lambda x: x.rolling(window=window_size).std())  # Corrected 'sensor' to 'col'\n",
        "    return df\n",
        "\n",
        "# --- Dataset Class ---\n",
        "class CMAPSSJSONLDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path, sequence_length=30, use_rolling_features=False):\n",
        "        self.data = load_jsonl_dataset(data_path, sequence_length)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.use_rolling_features = use_rolling_features\n",
        "\n",
        "        if self.use_rolling_features:\n",
        "            # Prepare data for feature engineering\n",
        "            sequences = [item[0].numpy() for item in self.data]\n",
        "\n",
        "            # Reshape sequences for DataFrame\n",
        "            num_sequences = len(sequences)\n",
        "            num_cycles = sequences[0].shape[0]  # Assuming all sequences have the same number of cycles\n",
        "            num_features = sequences[0].shape[1]\n",
        "\n",
        "            reshaped_sequences = np.concatenate(sequences).reshape(num_sequences * num_cycles, num_features)\n",
        "            df = pd.DataFrame(reshaped_sequences)\n",
        "\n",
        "            # Create a unit_nr column to group data\n",
        "            df['unit_nr'] = np.repeat(np.arange(num_sequences), num_cycles)\n",
        "\n",
        "            # Apply rolling features\n",
        "            df = create_rolling_features(df)\n",
        "\n",
        "            # Create sequences from the processed DataFrame\n",
        "            processed_sequences = []\n",
        "            for i in range(num_sequences):\n",
        "                start_idx = i * num_cycles\n",
        "                end_idx = (i + 1) * num_cycles\n",
        "                seq = df[start_idx:end_idx].values\n",
        "                processed_sequences.append(seq)\n",
        "\n",
        "            # Update self.data with processed sequences and RUL\n",
        "            new_data = []\n",
        "            for i in range(len(self.data)):\n",
        "                rul = self.data[i][1]\n",
        "                # Ensure the processed sequence is the correct length\n",
        "                if processed_sequences[i].shape[0] >= self.sequence_length:\n",
        "                    seq = processed_sequences[i][-self.sequence_length:]  # Take the last part of the sequence\n",
        "                else:\n",
        "                    # Handle cases where processed sequence is shorter than desired length\n",
        "                    seq = np.zeros((self.sequence_length, processed_sequences[i].shape[1]))\n",
        "                    seq[-processed_sequences[i].shape[0]:] = processed_sequences[i]\n",
        "                new_data.append((torch.tensor(seq, dtype=torch.float32), rul))\n",
        "            self.data = new_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# --- Model Definition ---\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# --- Training Function ---\n",
        "\n",
        "def train_model(model_name, train_dataset_path, eval_dataset_path,\n",
        "                staging_bucket, bucket_name, base_output_dir,\n",
        "                use_rolling_features=False,\n",
        "                hidden_size=128,\n",
        "                num_layers=3,\n",
        "                dropout=0.3,\n",
        "                learning_rate=0.0005,\n",
        "                weight_decay=1e-4,\n",
        "                num_epochs=50,\n",
        "                batch_size=64,\n",
        "                patience=10,\n",
        "                project_id=None,\n",
        "                region=None,\n",
        "                ):\n",
        "\n",
        "\n",
        "\n",
        "    # --- Project Configuration ---\n",
        "    if not project_id:\n",
        "        project_id = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "        if not project_id:\n",
        "            raise ValueError(\"project_id must be provided as a command-line argument or set in the GOOGLE_CLOUD_PROJECT environment variable.\")\n",
        "    if not region:\n",
        "        region = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "        if not region:\n",
        "            raise ValueError(\"region must be provided as a command-line argument or set in the GOOGLE_CLOUD_REGION environment variable.\")\n",
        "\n",
        "    logging.info(\"Training configuration:\")\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Train Dataset Path: {train_dataset_path}\")\n",
        "    logging.info(f\"Eval Dataset Path: {eval_dataset_path}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "    logging.info(f\"Bucket Name: {bucket_name}\")\n",
        "    logging.info(f\"Base Output Dir: {base_output_dir}\")\n",
        "    logging.info(f\"Use Rolling Features: {use_rolling_features}\")\n",
        "    logging.info(f\"Hidden Size: {hidden_size}\")\n",
        "    logging.info(f\"Number of Layers: {num_layers}\")\n",
        "    logging.info(f\"Dropout: {dropout}\")\n",
        "    logging.info(f\"Learning Rate: {learning_rate}\")\n",
        "    logging.info(f\"Weight Decay: {weight_decay}\")\n",
        "    logging.info(f\"Number of Epochs: {num_epochs}\")\n",
        "    logging.info(f\"Batch Size: {batch_size}\")\n",
        "    logging.info(f\"Patience: {patience}\")\n",
        "    logging.info(f\"Project ID: {project_id}\")\n",
        "    logging.info(f\"Region: {region}\")\n",
        "\n",
        "    # 1. Data Loaders\n",
        "\n",
        "    train_dataset = CMAPSSJSONLDataset(train_dataset_path, use_rolling_features=use_rolling_features)\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path, use_rolling_features=use_rolling_features)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Print input sizes\n",
        "    print(\"Training Dataset Input Size:\", train_dataset[0][0].shape[-1])\n",
        "    print(\"Evaluation Dataset Input Size:\", eval_dataset[0][0].shape[-1])\n",
        "\n",
        "    # 2. Model Initialization\n",
        "    input_size = train_dataset[0][0].shape[-1]\n",
        "    model = RULPredictionModel(input_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "\n",
        "    # 3. Device Configuration (CPU)\n",
        "    #device = torch.device(\"cpu\")  # Force CPU\n",
        "    #model.to(device)\n",
        "    #logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 3. Device Configuration (GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")  # Use GPU if available\n",
        "        print(\"GPU is available and being used.\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")  # Fallback to CPU if GPU is not available\n",
        "        print(\"GPU is not available, using CPU instead.\")\n",
        "\n",
        "    model.to(device)\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 4. Optimizer and Loss Function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # 5. Training Loop\n",
        "    best_eval_loss = float('inf')\n",
        "    patience = 10\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (sequences, ruls) in enumerate(train_loader):\n",
        "            sequences = sequences.to(device)\n",
        "            ruls = ruls.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, ruls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                logging.info(f\"Epoch: {epoch + 1}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        logging.info(f\"Epoch: {epoch + 1}, Average Training Loss: {avg_loss}\")\n",
        "\n",
        "        # 6. Evaluation\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sequences_eval, ruls_eval in eval_loader:\n",
        "                sequences_eval = sequences_eval.to(device)\n",
        "                ruls_eval = ruls_eval.to(device)\n",
        "\n",
        "                outputs_eval = model(sequences_eval)\n",
        "                loss_eval = criterion(outputs_eval, ruls_eval)\n",
        "                eval_loss += loss_eval.item()\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        logging.info(f\"Epoch: {epoch + 1}, Average Evaluation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # 7. Early Stopping\n",
        "        if avg_eval_loss < best_eval_loss:\n",
        "            best_eval_loss = avg_eval_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                logging.info(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    # 8. Load the best model\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "    logging.info(\"Starting model saving...\")\n",
        "\n",
        "    # 9. Save Model (Local and GCS)\n",
        "    local_model_path = 'model-nasa-ftc.pth'\n",
        "    torch.save(model.state_dict(), local_model_path)\n",
        "\n",
        "    try:\n",
        "        base_output_path = base_output_dir\n",
        "        subprocess.run(['gsutil', 'cp', local_model_path, base_output_path], check=True)\n",
        "        logging.info(f\"Copied model to GCS BUCKET path: {base_output_path}\")\n",
        "\n",
        "        if 'AIP_MODEL_DIR' in os.environ:\n",
        "            gcs_model_path = os.path.join(os.environ['AIP_MODEL_DIR'], 'model-nasa-ftc.pth')\n",
        "            subprocess.run(['gsutil', 'cp', local_model_path, gcs_model_path], check=True)\n",
        "            logging.info(f\"Copied model to Vertex AI path: {gcs_model_path}\")\n",
        "            model_save_path = gcs_model_path\n",
        "        else:\n",
        "            logging.info(f\"Saving model to local path: {local_model_path}\")\n",
        "            model_save_path = local_model_path\n",
        "\n",
        "        logging.info(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logging.error(f\"Error saving model: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"rul_predictor_jsonl\", help=\"Model name\")\n",
        "    parser.add_argument(\"--train_dataset\", type=str, required=True, help=\"Path to training dataset JSONL\") #Single path\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"Path to evaluation dataset JSONL\")  #Single path\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"Staging bucket for Vertex AI\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Bucket name\")\n",
        "    parser.add_argument(\"--base_output_dir\", type=str, required=True, help=\"Base output directory in GCS\")\n",
        "    parser.add_argument(\"--use_rolling_features\", action='store_true', help=\"Use rolling window features\")\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=128, help=\"Hidden size of LSTM\")\n",
        "    parser.add_argument(\"--num_layers\", type=int, default=3, help=\"Number of LSTM layers\")\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.3, help=\"Dropout rate\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=0.0005, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=1e-4, help=\"Weight decay\")\n",
        "    parser.add_argument(\"--num_epochs\", type=int, default=50, help=\"Number of epochs\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
        "    parser.add_argument(\"--patience\", type=int, default=10, help=\"Early stopping patience\")\n",
        "    parser.add_argument(\"--project_id\", type=str, help=\"Google Cloud Project ID\")  # Added\n",
        "    parser.add_argument(\"--region\", type=str, help=\"Google Cloud Region\")       # Added\n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(args.model, args.train_dataset, args.eval_dataset,\n",
        "                args.staging_bucket, args.bucket_name, args.base_output_dir,\n",
        "                args.use_rolling_features, args.hidden_size, args.num_layers,\n",
        "                args.dropout, args.learning_rate, args.weight_decay,\n",
        "                args.num_epochs, args.batch_size, args.patience,\n",
        "                args.project_id, args.region)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "# Create or overwrite trainer/train.py\n",
        "os.makedirs('trainer', exist_ok=True)\n",
        "with open('trainer/train_cmd.py', 'w') as f:\n",
        "    f.write(train_py_content)"
      ],
      "metadata": {
        "id": "aYIQbJ08t1Pu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.cloud import storage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "SERVICEACCOUNT = os.environ.get(\"GOOGLE_CLOUD_SERVICEACCOUNT\")\n",
        "PROJECT_NUMBER = os.environ.get(\"GOOGLE_CLOUD_PROJECT_NUMBER\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "!python trainer/train_cmd.py \\\n",
        "    --train_dataset gs://{BUCKET_NAME}/cmapss_FD004_train_sequences.jsonl \\\n",
        "    --eval_dataset gs://{BUCKET_NAME}/cmapss_FD004_test_sequences.jsonl \\\n",
        "    --staging_bucket {STAGING_BUCKET} \\\n",
        "    --bucket_name {BUCKET_NAME} \\\n",
        "    --base_output_dir gs://{BUCKET_NAME}/model_output \\\n",
        "    --use_rolling_features \\\n",
        "    --hidden_size 256 \\\n",
        "    --num_layers 4 \\\n",
        "    --dropout 0.4 \\\n",
        "    --learning_rate 0.001 \\\n",
        "    --weight_decay 1e-5 \\\n",
        "    --num_epochs 50000 \\\n",
        "    --batch_size 32 \\\n",
        "    --patience 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ohsV7KztxT4",
        "outputId": "74005783-94cd-4f0f-d64c-a5a37731281d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Training configuration:\n",
            "INFO:root:Model name: rul_predictor_jsonl\n",
            "INFO:root:Train Dataset Path: gs://poc-my-new-staging-bucket-2025-1/cmapss_FD004_train_sequences.jsonl\n",
            "INFO:root:Eval Dataset Path: gs://poc-my-new-staging-bucket-2025-1/cmapss_FD004_test_sequences.jsonl\n",
            "INFO:root:Staging Bucket: gs://poc-my-new-staging-bucket-2025-1/staging\n",
            "INFO:root:Bucket Name: poc-my-new-staging-bucket-2025-1\n",
            "INFO:root:Base Output Dir: gs://poc-my-new-staging-bucket-2025-1/model_output\n",
            "INFO:root:Use Rolling Features: True\n",
            "INFO:root:Hidden Size: 256\n",
            "INFO:root:Number of Layers: 4\n",
            "INFO:root:Dropout: 0.4\n",
            "INFO:root:Learning Rate: 0.001\n",
            "INFO:root:Weight Decay: 1e-05\n",
            "INFO:root:Number of Epochs: 50000\n",
            "INFO:root:Batch Size: 32\n",
            "INFO:root:Patience: 15\n",
            "INFO:root:Project ID: gen-lang-client-0870511801\n",
            "INFO:root:Region: us-central1\n",
            "Training Dataset Input Size: 26\n",
            "Evaluation Dataset Input Size: 26\n",
            "GPU is available and being used.\n",
            "INFO:root:Using device: cuda\n",
            "INFO:root:Epoch: 1, Batch: 0, Loss: 0.0001319604052696377\n",
            "INFO:root:Epoch: 1, Average Training Loss: 0.00020068809328677162\n",
            "INFO:root:Epoch: 1, Average Evaluation Loss: 675.4365158081055\n",
            "INFO:root:Epoch: 2, Batch: 0, Loss: 0.00014255681890062988\n",
            "INFO:root:Epoch: 2, Average Training Loss: 5.9690259831768344e-05\n",
            "INFO:root:Epoch: 2, Average Evaluation Loss: 675.7524108886719\n",
            "INFO:root:Epoch: 3, Batch: 0, Loss: 5.2085393690504134e-05\n",
            "INFO:root:Epoch: 3, Average Training Loss: 2.1225468685770466e-05\n",
            "INFO:root:Epoch: 3, Average Evaluation Loss: 675.9892959594727\n",
            "INFO:root:Epoch: 4, Batch: 0, Loss: 4.6081208893156145e-06\n",
            "INFO:root:Epoch: 4, Average Training Loss: 4.097582745998807e-06\n",
            "INFO:root:Epoch: 4, Average Evaluation Loss: 675.9991836547852\n",
            "INFO:root:Epoch: 5, Batch: 0, Loss: 3.917749381798785e-06\n",
            "INFO:root:Epoch: 5, Average Training Loss: 3.417051502196955e-06\n",
            "INFO:root:Epoch: 5, Average Evaluation Loss: 676.0194549560547\n",
            "INFO:root:Epoch: 6, Batch: 0, Loss: 4.001026354671922e-06\n",
            "INFO:root:Epoch: 6, Average Training Loss: 3.5005641336738336e-06\n",
            "INFO:root:Epoch: 6, Average Evaluation Loss: 676.0441207885742\n",
            "INFO:root:Epoch: 7, Batch: 0, Loss: 2.230527343272115e-06\n",
            "INFO:root:Epoch: 7, Average Training Loss: 1.7691713196654746e-06\n",
            "INFO:root:Epoch: 7, Average Evaluation Loss: 675.9636840820312\n",
            "INFO:root:Epoch: 8, Batch: 0, Loss: 2.7420751393947285e-06\n",
            "INFO:root:Epoch: 8, Average Training Loss: 1.957868235535898e-06\n",
            "INFO:root:Epoch: 8, Average Evaluation Loss: 676.0347595214844\n",
            "INFO:root:Epoch: 9, Batch: 0, Loss: 2.2595431801164523e-06\n",
            "INFO:root:Epoch: 9, Average Training Loss: 1.5796661472222695e-06\n",
            "INFO:root:Epoch: 9, Average Evaluation Loss: 676.0169677734375\n",
            "INFO:root:Epoch: 10, Batch: 0, Loss: 1.5398868526972365e-06\n",
            "INFO:root:Epoch: 10, Average Training Loss: 1.6964031601673923e-06\n",
            "INFO:root:Epoch: 10, Average Evaluation Loss: 675.9938049316406\n",
            "INFO:root:Epoch: 11, Batch: 0, Loss: 2.125867467839271e-06\n",
            "INFO:root:Epoch: 11, Average Training Loss: 1.3926644015782585e-06\n",
            "INFO:root:Epoch: 11, Average Evaluation Loss: 676.0260925292969\n",
            "INFO:root:Early stopping triggered\n",
            "INFO:root:Starting model saving...\n",
            "Copying file://model-nasa-ftc.pth [Content-Type=application/octet-stream]...\n",
            "- [1 files][  7.3 MiB/  7.3 MiB]                                                \n",
            "Operation completed over 1 objects/7.3 MiB.                                      \n",
            "INFO:root:Copied model to GCS BUCKET path: gs://poc-my-new-staging-bucket-2025-1/model_output\n",
            "INFO:root:Saving model to local path: model-nasa-ftc.pth\n",
            "INFO:root:Model saved to: model-nasa-ftc.pth\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVAL - FTC"
      ],
      "metadata": {
        "id": "CtbTCVlkHoCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import torch\n",
        "\n",
        "# Initialize the GCS client\n",
        "client = storage.Client()\n",
        "\n",
        "# Specify the bucket and blob (file)\n",
        "bucket_name = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob_name = \"model_output/model-nasa-ftc.pth\"\n",
        "\n",
        "blob_name = \"model_output/model-nasa-ftc.pth\"\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob = bucket.blob(blob_name)\n",
        "\n",
        "# Download the file to a local path\n",
        "local_file_path = \"model-nasa-ftc.pth\"  # Or your desired local path\n",
        "blob.download_to_filename(local_file_path)\n",
        "\n",
        "print(f\"Downloaded '{blob_name}' from '{bucket_name}' to '{local_file_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9shYxyWDRUN",
        "outputId": "1be1dcd0-01c8-4478-aa63-170f24c99b07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'model_output/model-nasa-ftc.pth' from 'poc-my-new-staging-bucket-2025-1' to 'model-nasa-ftc.pth'\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import subprocess\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "\n",
        "# --- Dataset and Model Definitions ---\n",
        "\n",
        "class CMAPSSJSONLDataset(data.Dataset):\n",
        "    def __init__(self, data_path, sequence_length=30, use_rolling_features=False):\n",
        "        self.data = []\n",
        "        self.sequence_length = sequence_length\n",
        "        self.use_rolling_features = use_rolling_features\n",
        "\n",
        "        try:\n",
        "            if data_path.startswith('gs://'):\n",
        "                storage_client = storage.Client()\n",
        "                bucket_name = data_path.split('/')[2]\n",
        "                blob_name = '/'.join(data_path.split('/')[3:])\n",
        "                bucket = storage_client.bucket(bucket_name)\n",
        "                blob = bucket.blob(blob_name)\n",
        "                tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "                blob.download_to_filename(tmp_file)\n",
        "                file_path = tmp_file\n",
        "            else:\n",
        "                file_path = data_path\n",
        "\n",
        "            with open(file_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        record = json.loads(line)\n",
        "                        sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                        rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                        self.data.append((sequence, rul))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        logging.warning(\"Skipping invalid JSON line: %r, Error: %s\", repr(line), e)\n",
        "\n",
        "            if data_path.startswith('gs://'):\n",
        "                os.remove(tmp_file)\n",
        "\n",
        "            if self.use_rolling_features:\n",
        "                # Convert data to DataFrame for feature engineering\n",
        "                sequences = [item[0].numpy() for item in self.data]\n",
        "                num_sequences = len(sequences)\n",
        "                num_cycles = sequences[0].shape[0]  # Assuming all sequences have the same number of cycles\n",
        "                num_features = sequences[0].shape[1]\n",
        "                reshaped_sequences = np.concatenate(sequences).reshape(num_sequences * num_cycles, num_features)\n",
        "                df = pd.DataFrame(reshaped_sequences)\n",
        "                df['unit_nr'] = np.repeat(np.arange(num_sequences), num_cycles)\n",
        "                # Function to create rolling features\n",
        "                def create_rolling_features(df, window_size=10):\n",
        "                    for col in df.columns:\n",
        "                        if isinstance(col, str) and col.startswith('sensor'):\n",
        "                            df[f'{col}_mean'] = df.groupby('unit_nr')[col].transform(lambda x: x.rolling(window=window_size).mean())\n",
        "                            df[f'{col}_std'] = df.groupby('unit_nr')[col].transform(lambda x: x.rolling(window=window_size).std())\n",
        "                    return df\n",
        "\n",
        "                df = create_rolling_features(df)\n",
        "                processed_sequences = []\n",
        "                for i in range(num_sequences):\n",
        "                    start_idx = i * num_cycles\n",
        "                    end_idx = (i + 1) * num_cycles\n",
        "                    seq = df[start_idx:end_idx].values\n",
        "                    processed_sequences.append(seq)\n",
        "\n",
        "                # Update self.data with the modified sequences (and RUL)\n",
        "                new_data = []\n",
        "                for i in range(len(self.data)):\n",
        "                    rul = self.data[i][1]\n",
        "                    # Ensure the processed sequence is the correct length\n",
        "                    if processed_sequences[i].shape[0] >= self.sequence_length:\n",
        "                        seq = processed_sequences[i][-self.sequence_length:]\n",
        "                    else:\n",
        "                        seq = np.zeros((self.sequence_length, processed_sequences[i].shape[1]))\n",
        "                        seq[-processed_sequences[i].shape[0]:] = processed_sequences[i]\n",
        "                    new_data.append((torch.tensor(seq, dtype=torch.float32), rul))\n",
        "                self.data = new_data\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "\n",
        "def calculate_cmapss_score(true_rul, predicted_rul):\n",
        "    \"\"\"Calculates a simplified CMAPSS score.\"\"\"\n",
        "    d = np.array(predicted_rul) - np.array(true_rul)  # Difference between predicted and true RUL\n",
        "    score = sum([\n",
        "        np.exp(-d[i] / 13) - 1 if d[i] < 0 else np.exp(d[i] / 10) - 1\n",
        "        for i in range(len(d))\n",
        "    ])\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluate_model_ftc(model, eval_dataset_path, input_size, hidden_size, sequence_length):\n",
        "    \"\"\"Evaluates the trained RUL prediction model.\"\"\"\n",
        "\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Load the evaluation dataset\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path, sequence_length, use_rolling_features=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)  # No need to shuffle for evaluation\n",
        "\n",
        "    # Make predictions and calculate metrics\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
        "        for sequences, ruls in eval_loader:\n",
        "            predictions = model(sequences)\n",
        "            all_predictions.extend(predictions.flatten().tolist())\n",
        "            all_targets.extend(ruls.flatten().tolist())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "\n",
        "    # Calculate CMAPSS score\n",
        "    cmapss_score = calculate_cmapss_score(all_targets, all_predictions)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Evaluation Results:\")\n",
        "    print(f\"Average Evaluation Loss (MSE): {mse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"CMAPSS Score: {cmapss_score:.2f}\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Authentication and Initialization\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # Get project details from environment variables\n",
        "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "    REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "    BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "    STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "    # Initialize Vertex AI\n",
        "    aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "    # --- Model Details ---\n",
        "    input_size = 26  # Adjust if needed\n",
        "    hidden_size = 256  # Adjust if needed\n",
        "    sequence_length = 30  # Adjust if needed\n",
        "    num_layers = 4  # Adjust if needed\n",
        "    eval_dataset_path = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_sequences.jsonl\"  # Adjust if needed\n",
        "    model_path = \"model-nasa-ftc.pth\"  # Update with the correct model path in GCS\n",
        "\n",
        "\n",
        "    # --- Model Loading ---\n",
        "    model = RULPredictionModel(input_size, hidden_size, num_layers=num_layers)\n",
        "\n",
        "    # --- Load model state dict using either local path or GCS URI: ---\n",
        "    if model_path.startswith(\"gs://\"):\n",
        "        # Load from GCS\n",
        "        client = storage.Client()\n",
        "        bucket_name = model_path.split('/')[2]\n",
        "        blob_name = '/'.join(model_path.split('/')[3:])\n",
        "        bucket = client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        tmp_file = \"/tmp/temp_model.pth\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "        model.load_state_dict(torch.load(tmp_file))\n",
        "        os.remove(tmp_file)  # Clean up temporary file\n",
        "    else:\n",
        "        # Load from local path\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    evaluate_model_ftc(model, eval_dataset_path, input_size, hidden_size, sequence_length)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51WNDwa1BP75",
        "outputId": "7380e610-be41-478b-edc4-7731d286b4bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "Average Evaluation Loss (MSE): 675.44\n",
            "Mean Absolute Error (MAE): 25.99\n",
            "Root Mean Squared Error (RMSE): 25.99\n",
            "CMAPSS Score: 1608.49\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A69PfDH83btq",
        "Ge_1sCQo3TSX",
        "rA39w2VJXCPo"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPHfQq6lfMEyrorDhcLtQMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
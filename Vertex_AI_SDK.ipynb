{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Vertex_AI_SDK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiVwfFuZ9axh"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install google-cloud-storage -q\n",
        "!pip install google-cloud-bigquery -q\n",
        "!pip install google-cloud-bigquery-storage -q\n",
        "!pip install google-cloud-aiplatform -q\n",
        "!pip install datasets -q\n",
        "!pip install colab-env -q\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install  -q gcsfs==2024.3.1\n",
        "!pip install  -q accelerate==0.31.0\n",
        "!pip install  -q transformers==4.45.2\n",
        "!pip install  -q  datasets==2.19.2\n",
        "!pip install google-cloud-aiplatform[all] -q\n",
        "!pip install vertexai  -q\n",
        "!pip install tensorflow_datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPARATION"
      ],
      "metadata": {
        "id": "A69PfDH83btq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "\n",
        "# --- Data Loading from Google Drive ---\n",
        "zip_path = '/content/gdrive/MyDrive/datasets/CMAPSSData.zip'\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(f\"Error: CMAPSSData.zip not found at {zip_path}. Please ensure the file is correctly located in your Google Drive.\")\n",
        "    raise FileNotFoundError(f\"CMAPSSData.zip not found at {zip_path}\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        if zip_ref.testzip() is None:  # Check for ZIP file integrity\n",
        "            zip_ref.extractall(extract_dir)\n",
        "            print(f\"Extracted dataset files to: {extract_dir}\")\n",
        "        else:\n",
        "            print(\"Error: ZIP file integrity check failed. The file may not be a valid ZIP file.\")\n",
        "            raise zipfile.BadZipFile(\"ZIP file integrity check failed.\")\n",
        "\n",
        "except zipfile.BadZipFile as e:\n",
        "    print(f\"Error extracting ZIP file: {e}\")\n",
        "    print(\n",
        "        \"The uploaded file may not be a valid or complete ZIP file. \"\n",
        "        \"Please ensure you have uploaded the correct file, that it is not corrupted, \"\n",
        "        \"and that it is a standard ZIP archive.\"\n",
        "    )\n",
        "    raise  # Stop execution if extraction fails\n",
        "\n",
        "# --- Prepare NASA CMAPSS Data and Save to JSONL in GCS ---\n",
        "extract_dir = 'data/cmapss'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Process all four subsets\n",
        "data_subsets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "\n",
        "for data_subset in data_subsets:\n",
        "    train_file = os.path.join(extract_dir, f'train_{data_subset}.txt')\n",
        "    test_file = os.path.join(extract_dir, f'test_{data_subset}.txt')\n",
        "    rul_file = os.path.join(extract_dir, f'RUL_{data_subset}.txt')\n",
        "\n",
        "    SENSOR_COLUMNS = ['sensor' + str(i).zfill(2) for i in range(1, 22)]\n",
        "    OP_SETTING_COLUMNS = ['op_setting_' + str(i) for i in range(1, 4)]\n",
        "    DATA_COLUMNS = ['unit_nr', 'time_cycles'] + OP_SETTING_COLUMNS + SENSOR_COLUMNS\n",
        "\n",
        "    # Load training data\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        test_df = pd.read_csv(test_file, names=DATA_COLUMNS, delim_whitespace=True, header=None)\n",
        "        rul_df = pd.read_csv(rul_file, names=['RUL'], delim_whitespace=True, header=None)\n",
        "\n",
        "        train_df.columns = DATA_COLUMNS\n",
        "        test_df.columns = DATA_COLUMNS\n",
        "\n",
        "        print(f\"\\nProcessing data subset: {data_subset}\")\n",
        "        print(\"Shape of train_df after loading:\", train_df.shape)\n",
        "        print(\"train_df head after loading:\\n\", train_df.head())\n",
        "        print(\"Shape of test_df:\", test_df.shape)\n",
        "        print(\"test_df head after loading:\\n\", test_df.head())\n",
        "        print(\"Shape of RUL data:\", rul_df.shape)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data files for subset {data_subset}: {e}\")\n",
        "        raise  # Stop execution if a file is missing\n",
        "\n",
        "    def create_jsonl(df, rul_df, output_path, sequence_length=30, is_test=False):\n",
        "        grouped_data = df.groupby('unit_nr')\n",
        "        rul_values = rul_df.values.tolist()  # Convert RUL DataFrame to list\n",
        "        engine_count = 0  # To track which RUL value to use\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            for unit_nr, unit_data in grouped_data:\n",
        "                num_cycles = len(unit_data)\n",
        "                data_values = unit_data.drop(['unit_nr'], axis=1).values.tolist()\n",
        "                json_data = []  # Initialize an empty list to hold JSON objects\n",
        "\n",
        "                for i in range(max(0, num_cycles - sequence_length + 1)):\n",
        "                    sequence = data_values[i:i + sequence_length]\n",
        "                    rul = num_cycles - (i + sequence_length)\n",
        "\n",
        "                    # Ensure RUL is not out of bounds\n",
        "                    if engine_count < len(rul_values):\n",
        "                        current_rul = rul_values[engine_count][0]  # Get the RUL value\n",
        "                    else:\n",
        "                        current_rul = 0  # Or some default value if RUL data is exhausted\n",
        "\n",
        "                    if len(sequence) == sequence_length:\n",
        "                        json_record = {\"sequence\": sequence, \"sequence_length\": len(sequence), \"rul\": current_rul}  # Include sequence length\n",
        "                        json_data.append(json_record)\n",
        "\n",
        "                # Write all JSON objects to the file at once\n",
        "                with open(output_path, 'w') as f:\n",
        "                    for json_record in json_data:\n",
        "                        f.write(json.dumps(json_record) + '\\n')\n",
        "\n",
        "                engine_count += 1  # Increment engine counter\n",
        "\n",
        "    local_train_jsonl_path = f\"cmapss_{data_subset}_train_sequences.jsonl\"\n",
        "    local_test_jsonl_path = f\"cmapss_{data_subset}_test_sequences.jsonl\"\n",
        "\n",
        "    # Create JSONL for training\n",
        "    create_jsonl(train_df, rul_df, local_train_jsonl_path, is_test=False)\n",
        "    print(f\"Created {local_train_jsonl_path}\")\n",
        "\n",
        "    # Create JSONL for testing\n",
        "    create_jsonl(test_df, rul_df, local_test_jsonl_path, is_test=True)\n",
        "    print(f\"Created {local_test_jsonl_path}\")\n",
        "\n",
        "    # --- Upload JSONL files to GCS ---\n",
        "    client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "    blob_train = bucket.blob(f\"cmapss_{data_subset}_train_sequences.jsonl\")  # Adapt to your naming scheme\n",
        "    blob_test = bucket.blob(f\"cmapss_{data_subset}_test_sequences.jsonl\")   # Adapt to your naming scheme\n",
        "\n",
        "    blob_train.upload_from_filename(local_train_jsonl_path)\n",
        "    print(f\"Uploaded training data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_train_sequences.jsonl\")\n",
        "\n",
        "    blob_test.upload_from_filename(local_test_jsonl_path)\n",
        "    print(f\"Uploaded evaluation data to: gs://{BUCKET_NAME}/cmapss_{data_subset}_test_sequences.jsonl\")\n",
        "\n",
        "print(\"JSONL files created and uploaded.\")"
      ],
      "metadata": {
        "id": "smsD0LA2wvUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINE TUNING - NASA DATASET"
      ],
      "metadata": {
        "id": "Ge_1sCQo3TSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "from google.cloud import aiplatform, storage\n",
        "import logging\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "TRAINING_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_sequences.jsonl\"\n",
        "EVAL_DATA_PATH = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_sequences.jsonl\"\n",
        "\n",
        "# --- Define trainer/train.py content ---\n",
        "train_py_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class CMAPSSJSONLDataset(Dataset):\n",
        "    def __init__(self, data_path, sequence_length=30):\n",
        "        self.data = []\n",
        "        self.sequence_length = sequence_length\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = data_path.split('/')[2]\n",
        "        blob_name = '/'.join(data_path.split('/')[3:])\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        tmp_file = \"/tmp/temp_data.jsonl\"\n",
        "        blob.download_to_filename(tmp_file)\n",
        "\n",
        "        with open(tmp_file, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    sequence = torch.tensor(record[\"sequence\"], dtype=torch.float32)\n",
        "                    rul = torch.tensor([record[\"rul\"]], dtype=torch.float32)\n",
        "                    self.data.append((sequence, rul))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.warning(f\"Skipping invalid JSON line: {line}, Error: {e}\")\n",
        "        os.remove(tmp_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class RULPredictionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
        "        super(RULPredictionModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input x shape:\", x.shape)  # Debugging\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "def train_model(model_name, train_dataset_path, eval_dataset_path, staging_bucket, bucket_name, base_output_dir):\n",
        "    logging.info(f\"Model name: {model_name}\")\n",
        "    logging.info(f\"Train Dataset Path: {train_dataset_path}\")\n",
        "    logging.info(f\"Eval Dataset Path: {eval_dataset_path}\")\n",
        "    logging.info(f\"Staging Bucket: {staging_bucket}\")\n",
        "    logging.info(f\"Bucket Name: {bucket_name}\")\n",
        "    logging.info(f\"Base Output Dir: {base_output_dir}\") #Log base_output_dir\n",
        "\n",
        "\n",
        "    train_dataset = CMAPSSJSONLDataset(train_dataset_path)\n",
        "    eval_dataset = CMAPSSJSONLDataset(eval_dataset_path)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=64)\n",
        "\n",
        "    # Assuming sequence_length is handled in data preparation, input_size is:\n",
        "    # Get the input size from the first sequence in the dataset\n",
        "    input_size = train_dataset[0][0].shape[-1]  # Get the last dimension of the sequence\n",
        "    hidden_size = 64\n",
        "    model = RULPredictionModel(input_size, hidden_size)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (sequences, ruls) in enumerate(train_loader):\n",
        "            print(\"Sequence shape:\", sequences.shape)  # Debugging\n",
        "            sequences = sequences.to(device)\n",
        "            ruls = ruls.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, ruls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                logging.info(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        logging.info(f\"Epoch [{epoch + 1}/{num_epochs}], Average Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sequences_eval, ruls_eval in eval_loader:\n",
        "                sequences_eval = sequences_eval.to(device)\n",
        "                ruls_eval = ruls_eval.to(device)\n",
        "                outputs_eval = model(sequences_eval)\n",
        "                loss_eval = criterion(outputs_eval, ruls_eval)\n",
        "                eval_loss += loss_eval.item()\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        logging.info(f\"Epoch [{epoch + 1}/{num_epochs}], Average Evaluation Loss: {avg_eval_loss:.4f}\")\n",
        "\n",
        "    # Directory creation and model saving\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    # **Change 1: Modify the model save path to include 'model' directory**\n",
        "    model_save_path = os.path.join(base_output_dir, 'model', 'model-nasa.pth')\n",
        "\n",
        "     Change 2: Create the 'model' directory in Cloud Storage if it doesn't exist\n",
        "    # Instead of using blob.mkdir, use the client to create the directory\n",
        "    bucket.blob(os.path.join(base_output_dir, 'model', '')).upload_from_string('')\n",
        "    # Uploading an empty string to a path ending with '/' creates a directory\n",
        "\n",
        "\n",
        "    # Save the model state dictionary directly to the Cloud Storage path using gcsfs\n",
        "    import gcsfs\n",
        "    fs = gcsfs.GCSFileSystem(project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "    with fs.open(model_save_path, 'wb') as f:\n",
        "        torch.save(model.state_dict(), f)\n",
        "\n",
        "    logging.info(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"rul_predictor_jsonl\", help=\"Name of the model.\")\n",
        "    parser.add_argument(\"--train_dataset\", type=str, required=True, help=\"URI of the training dataset (JSONL).\")\n",
        "    parser.add_argument(\"--eval_dataset\", type=str, required=True, help=\"URI of the evaluation dataset (JSONL).\")\n",
        "    parser.add_argument(\"--staging_bucket\", type=str, required=True, help=\"GCS bucket for staging model artifacts.\")\n",
        "    parser.add_argument(\"--bucket_name\", type=str, required=True, help=\"Name of the GCS bucket.\")\n",
        "    parser.add_argument(\"--base_output_dir\", type=str, required=True, help=\"Base output directory\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "    #train_model(args.model, args.train_dataset, args.eval_dataset, args.staging_bucket, args.bucket_name)\n",
        "    train_model(args.model, args.train_dataset, args.eval_dataset, args.staging_bucket, args.bucket_name, args.base_output_dir)\n",
        "    print('\\\\n')\n",
        "    print(\"Training completed.\")\n",
        "    print('\\\\n')\n",
        "\"\"\"\n",
        "\n",
        "# Create or overwrite trainer/train.py\n",
        "os.makedirs('trainer', exist_ok=True)\n",
        "with open('trainer/train.py', 'w') as f:\n",
        "    f.write(train_py_content)\n",
        "\n",
        "# --- Define and run custom training job ---\n",
        "BASE_MODEL_NAME = \"rul_predictor_cmapss_jsonl\"\n",
        "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}/model_output\"  # Define base output directory\n",
        "\n",
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"NASA-cmapss-rul-prediction-jsonl\",\n",
        "    script_path=\"trainer/train.py\",\n",
        "    container_uri='us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-4.py310:latest', # Or your preferred container\n",
        "    requirements=[\"google-cloud-aiplatform\", \"torch\"],\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\" # Or your preferred serving container\n",
        ")\n",
        "\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--model\", BASE_MODEL_NAME,\n",
        "        \"--train_dataset\", TRAINING_DATA_PATH,\n",
        "        \"--eval_dataset\", EVAL_DATA_PATH,\n",
        "        \"--staging_bucket\", STAGING_BUCKET,\n",
        "        \"--bucket_name\", BUCKET_NAME,\n",
        "        \"--base_output_dir\", BASE_OUTPUT_DIR,\n",
        "    ],\n",
        "    replica_count=1,\n",
        "    machine_type=\"n1-standard-8\", # Or your preferred machine type\n",
        "    model_display_name=\"cmapss-rul-jsonl-model\"\n",
        ")\n",
        "\n",
        "logging.info(f\"Fine-tuned model: {model.resource_name}\")\n",
        "\n",
        "# --- Potential Next Steps (Outline) ---\n",
        "print(\"\\\\nPotential Next Steps:\")\n",
        "print(\"- Monitor the training job in the Google Cloud Console.\")\n",
        "print(\"- Evaluate the model performance using more comprehensive metrics.\")\n",
        "print(\"- Deploy the trained model to a Vertex AI Endpoint for predictions.\")\n",
        "print(\"- Experiment with different model architectures and hyperparameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-lsENwIwZPn",
        "outputId": "171ef439-61cd-4760-8c3a-c485d4d899c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/677155171887/locations/us-central1/trainingPipelines/7450269641391210496 current state:\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls -lh gs://{BUCKET_NAME}/staging/aiplatform-2025-03-31-11*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnrcbU2IUleD",
        "outputId": "41d6de0b-fd74-4ecf-931e-6b637f5e6790"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   3.5 KiB  2025-03-31T11:15:44Z  gs://poc-my-new-staging-bucket-2025-1/staging/aiplatform-2025-03-31-11:15:44.387-aiplatform_custom_trainer_script-0.1.tar.gz\n",
            "  3.53 KiB  2025-03-31T11:28:43Z  gs://poc-my-new-staging-bucket-2025-1/staging/aiplatform-2025-03-31-11:28:43.383-aiplatform_custom_trainer_script-0.1.tar.gz\n",
            "TOTAL: 2 objects, 7193 bytes (7.02 KiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil iam get gs://{BUCKET_NAME}"
      ],
      "metadata": {
        "id": "iz3Jxkf1BPPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A69PfDH83btq"
      ],
      "authorship_tag": "ABX9TyPakkAX7LqQ/L2dB3h76Y1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
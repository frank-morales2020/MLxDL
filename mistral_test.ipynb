{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOy+Da4rFd13+mlvDD9ssZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/mistral_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "RK2TzbbM0xeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l8D68fWOtoy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "# https://github.com/mistralai/mistral-src\n",
        "\n",
        "#It looks great\n",
        "# https://mistral.ai/news/mixtral-of-experts/\n",
        "\n",
        "# https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html\n",
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "!pip install sagemaker\n",
        "!pip install boto3\n",
        "!pip install --upgrade urllib3\n",
        "!pip install colab-env --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy The Model"
      ],
      "metadata": {
        "id": "Jw0Y6nQY0lPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import colab_env\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "print('huggingface-llm-mistral-7b-instruct - JumpStartModel')\n",
        "model_version='2.0.0'\n",
        "model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", model_version='2.0.0', role=ROLE_ARN)\n",
        "predictor = model.deploy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTBsYcnr1Rt9",
        "outputId": "d38ebd46-bfbd-446b-ddcb-c4509f5cb290"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface-llm-mistral-7b-instruct - JumpStartModel\n",
            "----------!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoke The Model"
      ],
      "metadata": {
        "id": "BT8Syp7w1IWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_sentiment(prompt,predictor,modelid):\n",
        "    if modelid == 0:\n",
        "       INPUT= \"<s>[INST]  \" + prompt + \" [/INST]\"\n",
        "    else:\n",
        "       INPUT= \"%s\"%prompt\n",
        "\n",
        "    payload = {\n",
        "        #\"inputs\": \"<s>[INST]  \" + prompt + \" [/INST]\",\n",
        "        \"inputs\": \"%s\"%INPUT,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.9,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"return_full_text\": False,\n",
        "            #\"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
        "        },\n",
        "    }\n",
        "    return predictor.predict(payload,custom_attributes=\"accept_eula=true\")"
      ],
      "metadata": {
        "id": "HhzSeDZEPWlo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt0 = \"what is the 40% of 30?\"\n",
        "prompt1 = \"what is the 20.5% of 40?\"\n",
        "prompt2 = \"what is the 30% of 650?\"\n",
        "prompt3 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "prompt4='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "prompt5='Which country has the most natural lakes? Answer with only the country name.'\n",
        "\n",
        "n_prompts = 6\n",
        "for i in range(n_prompts):\n",
        "\n",
        "    prompt='prompt%s'%i\n",
        "    prompt=eval(prompt)\n",
        "    print()\n",
        "    print('Prompt: %s'%prompt)\n",
        "    #prompt=prompt1[0]\n",
        "    print()\n",
        "    sentiment = predict_sentiment(prompt,predictor,0)\n",
        "    #print()\n",
        "    print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "    print('___________')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZVuvdUVLJQy",
        "outputId": "d0c1f332-7238-4d64-899a-121c16fe1b7e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: what is the 40% of 30?\n",
            "\n",
            "Answer:  The 40% of 30 is 12.\n",
            "___________\n",
            "\n",
            "Prompt: what is the 20.5% of 40?\n",
            "\n",
            "Answer:  20.5% of 40 is 8.2.\n",
            "___________\n",
            "\n",
            "Prompt: what is the 30% of 650?\n",
            "\n",
            "Answer:  The 30% of 650 is 195.\n",
            "___________\n",
            "\n",
            "Prompt: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "\n",
            "Answer:  Sure! Regularization is a technique used in machine learning to prevent overfitting of models by adding a penalty term to the loss function. Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on new, unseen data. Regularization adds a constraint to the model, encouraging it to use all the input features in a more balanced way, and preventing it from relying too heavily on any one feature.\n",
            "\n",
            "There are two main types of regularization: L1 and L2 regularization. L1 regularization adds a penalty term equal to the absolute value of the model coefficients, while L2 regularization adds a penalty term equal to the square of the model coefficients. This has the effect of shrinking the coefficients towards zero, effectively reducing the importance of the features in the model.\n",
            "\n",
            "Regularization can improve the generalization performance of a model by preventing it from being too complex and fitting the training data too closely. It is a common technique used in many machine learning algorithms, including linear regression, logistic regression, and neural networks.\n",
            "___________\n",
            "\n",
            "Prompt: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "Answer:  Let's go through the problem step by step:\n",
            "\n",
            "1. You bought an ice cream for 6 kids, with each cone costing $1.25. So the total cost of the ice creams would be:\n",
            "   Cost = Number of cones * Price per cone\n",
            "   = 6 * $1.25\n",
            "   = $7.50\n",
            "\n",
            "2. You paid for the ice creams with a $10 bill. When you pay with a bill, the amount you paid minus the cost of the item you bought gives you the change. In this case:\n",
            "   Change = Amount paid - Cost\n",
            "   = $10 - $7.50\n",
            "   = $2.50\n",
            "\n",
            "So, you got back $2.50 in change.\n",
            "___________\n",
            "\n",
            "Prompt: Which country has the most natural lakes? Answer with only the country name.\n",
            "\n",
            "Answer:  Canada\n",
            "___________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean up"
      ],
      "metadata": {
        "id": "XlSslCcM1TvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "#!pip install colab-env --upgrade\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#!pip install boto3\n",
        "#aws_region = 'us-east-1'\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "    #resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "    #print('%sName'%resource_nametmp)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acSOR2wWV-Kq",
        "outputId": "28b30390-2521-4821-dccd-97257c927f69"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "EndpointName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-802\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "ModelName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-800\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "EndpointConfigName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-802\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
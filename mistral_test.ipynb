{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmYgqALs3y7gNO8zVNaXli",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/mistral_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "RK2TzbbM0xeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l8D68fWOtoy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "# https://github.com/mistralai/mistral-src\n",
        "\n",
        "#It looks great\n",
        "# https://mistral.ai/news/mixtral-of-experts/\n",
        "\n",
        "# https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html\n",
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "!pip install sagemaker\n",
        "!pip install boto3\n",
        "!pip install --upgrade urllib3\n",
        "!pip install colab-env --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy The Model"
      ],
      "metadata": {
        "id": "Jw0Y6nQY0lPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import colab_env\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "print('huggingface-llm-mistral-7b-instruct - JumpStartModel')\n",
        "model_version='2.0.0'\n",
        "model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", model_version='2.0.0', role=ROLE_ARN)\n",
        "predictor = model.deploy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTBsYcnr1Rt9",
        "outputId": "7105b101-9706-4eaa-f575-ecd19d2e6a77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface-llm-mistral-7b-instruct - JumpStartModel\n",
            "---------!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoke The Model"
      ],
      "metadata": {
        "id": "BT8Syp7w1IWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_sentiment(prompt,predictor,modelid):\n",
        "    if modelid == 0:\n",
        "       INPUT= \"<s>[INST]  \" + prompt + \" [/INST]\"\n",
        "    else:\n",
        "       INPUT= \"%s\"%prompt\n",
        "\n",
        "    payload = {\n",
        "        #\"inputs\": \"<s>[INST]  \" + prompt + \" [/INST]\",\n",
        "        \"inputs\": \"%s\"%INPUT,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.9,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"return_full_text\": False,\n",
        "            #\"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
        "        },\n",
        "    }\n",
        "    return predictor.predict(payload,custom_attributes=\"accept_eula=true\")"
      ],
      "metadata": {
        "id": "HhzSeDZEPWlo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt0 = \"what is the 40% of 30?\"\n",
        "prompt0 = \"what is the 20.5% of 40?\"\n",
        "prompt0 = \"what is the 30% of 650?\"\n",
        "prompt0 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "#prompt0='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "#prompt0='Which country has the most natural lakes? Answer with only the country name.'\n",
        "\n",
        "print()\n",
        "print('Prompt #0: %s'%prompt0)\n",
        "print()\n",
        "sentiment = predict_sentiment(prompt0,predictor,0)\n",
        "#print(sentiment)\n",
        "print('OUTPUT PROMPT0:')\n",
        "print()\n",
        "print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q4IQBv3Q01E",
        "outputId": "7e252f45-dcd7-448f-baf1-d83d82c98be9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt #0: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "\n",
            "OUTPUT PROMPT0:\n",
            "\n",
            "Answer:  Regularization is a technique used in machine learning to prevent overfitting of models. Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on new, unseen data. Regularization adds a penalty term to the loss function of the model, which encourages the model to have simpler parameters and reduces the likelihood of overfitting. There are two main types of regularization: L1 and L2 regularization. L1 regularization adds a penalty term equal to the absolute value of the coefficients, while L2 regularization adds a penalty term equal to the square of the coefficients. This has the effect of shrinking the coefficients towards zero, thus reducing the complexity of the model. Regularization can be applied to a wide range of machine learning algorithms, including linear regression, logistic regression, support vector machines, and neural networks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean up"
      ],
      "metadata": {
        "id": "XlSslCcM1TvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "#!pip install colab-env --upgrade\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#!pip install boto3\n",
        "#aws_region = 'us-east-1'\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "    #resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "    #print('%sName'%resource_nametmp)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acSOR2wWV-Kq",
        "outputId": "28b30390-2521-4821-dccd-97257c927f69"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "EndpointName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-802\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "ModelName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-800\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "EndpointConfigName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-802\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
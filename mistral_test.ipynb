{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvRsOAy5g9pPKFBDA70qbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/mistral_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "RK2TzbbM0xeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l8D68fWOtoy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "# https://github.com/mistralai/mistral-src\n",
        "\n",
        "#It looks great\n",
        "# https://mistral.ai/news/mixtral-of-experts/\n",
        "\n",
        "# https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html\n",
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "!pip install sagemaker\n",
        "!pip install boto3\n",
        "!pip install --upgrade urllib3\n",
        "!pip install colab-env --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy The Model"
      ],
      "metadata": {
        "id": "Jw0Y6nQY0lPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import colab_env\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "print('huggingface-llm-mistral-7b-instruct - JumpStartModel')\n",
        "model_version='2.0.0'\n",
        "model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", model_version='2.0.0', role=ROLE_ARN)\n",
        "predictor = model.deploy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTBsYcnr1Rt9",
        "outputId": "d38ebd46-bfbd-446b-ddcb-c4509f5cb290"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface-llm-mistral-7b-instruct - JumpStartModel\n",
            "----------!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoke The Model"
      ],
      "metadata": {
        "id": "BT8Syp7w1IWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_sentiment(prompt,predictor,modelid):\n",
        "    if modelid == 0:\n",
        "       INPUT= \"<s>[INST]  \" + prompt + \" [/INST]\"\n",
        "    else:\n",
        "       INPUT= \"%s\"%prompt\n",
        "\n",
        "    payload = {\n",
        "        #\"inputs\": \"<s>[INST]  \" + prompt + \" [/INST]\",\n",
        "        \"inputs\": \"%s\"%INPUT,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.9,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"return_full_text\": False,\n",
        "            #\"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
        "        },\n",
        "    }\n",
        "    return predictor.predict(payload,custom_attributes=\"accept_eula=true\")"
      ],
      "metadata": {
        "id": "HhzSeDZEPWlo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt0 = \"what is the 40% of 30?\"\n",
        "prompt1 = \"what is the 20.5% of 40?\"\n",
        "prompt2 = \"what is the 30% of 650?\"\n",
        "prompt3 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "prompt4='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "prompt5='Which country has the most natural lakes? Answer with only the country name.'\n",
        "\n",
        "n_prompts = 5\n",
        "\n",
        "for i in range(n_prompts):\n",
        "\n",
        "    prompt='prompt%s'%i\n",
        "    prompt=eval(prompt)\n",
        "    print()\n",
        "    print('Prompt: %s'%prompt)\n",
        "    #prompt=prompt1[0]\n",
        "    print()\n",
        "    sentiment = predict_sentiment(prompt,predictor,0)\n",
        "    #print()\n",
        "    print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "    print('___________')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZVuvdUVLJQy",
        "outputId": "ca790a9c-cc88-4f76-8340-05afeab2c3c9"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: what is the 40% of 30?\n",
            "\n",
            "Answer:  The 40% of 30 is 12.\n",
            "___________\n",
            "\n",
            "Prompt: what is the 20.5% of 40?\n",
            "\n",
            "Answer:  The 20.5% of 40 is 8.2.\n",
            "___________\n",
            "\n",
            "Prompt: what is the 30% of 650?\n",
            "\n",
            "Answer:  30% of 650 is 195.\n",
            "___________\n",
            "\n",
            "Prompt: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "\n",
            "Answer:  Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. Overfitting occurs when a model is too complex and fits the training data too well, including the noise and errors. This results in poor generalization to new, unseen data. \n",
            "\n",
            "Regularization adds a constraint to the model, typically in the form of a penalty term that is added to the loss function. This penalty term encourages the model to have a smaller magnitude, which results in simpler models that generalize better to new data.\n",
            "\n",
            "There are two main types of regularization: L1 and L2 regularization. L1 regularization, also known as Lasso regularization, adds a penalty term equal to the absolute value of the coefficients. This has the effect of shrinking some coefficients all the way to zero, effectively removing the corresponding features from the model. L2 regularization, also known as Ridge regularization, adds a penalty term equal to the square of the coefficients. This has the effect of shrinking all coefficients towards zero, but not all the way to zero, so all features remain in the model.\n",
            "\n",
            "Regularization can greatly improve the performance of a model by preventing overfitting and improving generalization to new data. It is a widely used technique in many machine learning algorithms and is an important part of the model selection process.\n",
            "___________\n",
            "\n",
            "Prompt: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "Answer:  First, let's figure out how much money you spent on the ice cream. Since each cone cost $1.25 and you bought 6 cones, your total cost would be:\n",
            "\n",
            "Total cost = Cost per cone * Number of cones\n",
            "Total cost = $1.25 * 6\n",
            "Total cost = $7.50\n",
            "\n",
            "Now, you paid with a $10 bill, which means you had $2.50 left over. To calculate this, subtract the total cost from the amount you paid:\n",
            "\n",
            "$10 - $7.50 = $2.50\n",
            "\n",
            "So, you got back $2.50 in change.\n",
            "___________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean up"
      ],
      "metadata": {
        "id": "XlSslCcM1TvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "#!pip install colab-env --upgrade\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#!pip install boto3\n",
        "#aws_region = 'us-east-1'\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "    #resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "    #print('%sName'%resource_nametmp)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acSOR2wWV-Kq",
        "outputId": "28b30390-2521-4821-dccd-97257c927f69"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "EndpointName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-802\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "ModelName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-800\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "EndpointConfigName\n",
            "hf-llm-mistral-7b-instruct-2023-12-21-06-29-49-802\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EMN7yyfFm7H"
      },
      "source": [
        "https://github.com/pytorch/torchtune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUU_iMNH6XCT",
        "outputId": "0e3000aa-4d0e-41e2-8dc0-62078f6d1ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 28 20:50:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   34C    P8              11W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qPrRqDF1IdfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6085b1d-95f4-4c89-c259-b2a4261d958d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtune -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1D0Y_FwSJVj",
        "outputId": "f531b192-65d3-4b4f-c334-157900f9b50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ],
      "source": [
        "!tune -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6X3PUlxmOSGJ"
      },
      "outputs": [],
      "source": [
        "#meta-llama/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vbsklIk4PHMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d5c151-20f1-4adb-bf65-8223fcb65f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZYsw91hO_us",
        "outputId": "69600f34-a2f6-42c1-f50e-4b86497e752f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kwtSyElRWqGW"
      },
      "outputs": [],
      "source": [
        "#!tune download meta-llama/Llama-2-7b-hf \\\n",
        "#--output-dir /tmp/Llama-2-7b-hf \\\n",
        "#--hf-token <HF_TOKEN> \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6oWyZF-EVN"
      },
      "source": [
        "'HF_TOKEN' is in my Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f0zvfOjDqEy",
        "outputId": "701ba456-0414-4b98-e9d0-f558388c879d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/tune\n"
          ]
        }
      ],
      "source": [
        "!which tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kUl8r3cbLwQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3805d100-f0e0-430d-af02-5287445f23e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring files matching the following patterns: *.safetensors\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\n",
            "config.json: 100% 609/609 [00:00<00:00, 4.24MB/s]\n",
            "\n",
            "LICENSE.txt: 100% 7.02k/7.02k [00:00<00:00, 43.1MB/s]\n",
            "\n",
            "model.safetensors.index.json:   0% 0.00/26.8k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "README.md:   0% 0.00/22.3k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "USE_POLICY.md: 100% 4.77k/4.77k [00:00<00:00, 10.4MB/s]\n",
            "README.md: 100% 22.3k/22.3k [00:00<00:00, 14.1MB/s]\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 10.8MB/s]\n",
            "\n",
            "Responsible-Use-Guide.pdf:   0% 0.00/1.25M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            ".gitattributes: 100% 1.58k/1.58k [00:00<00:00, 11.8MB/s]\n",
            "Responsible-Use-Guide.pdf: 100% 1.25M/1.25M [00:00<00:00, 15.7MB/s]\n",
            "\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 1.70MB/s]\n",
            "\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 46.9MB/s]\n",
            "\n",
            "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 78.7MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.84MB/s]\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer_config.json: 100% 776/776 [00:00<00:00, 5.73MB/s]\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 14.6MB/s]\n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   0% 10.5M/3.50G [00:00<01:54, 30.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 21.0M/3.50G [00:00<01:20, 43.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 10.5M/9.98G [00:00<04:33, 36.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 31.5M/3.50G [00:00<01:03, 54.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 21.0M/9.98G [00:00<03:34, 46.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 41.9M/3.50G [00:00<00:51, 66.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 31.5M/9.98G [00:00<02:57, 56.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 62.9M/3.50G [00:00<00:37, 90.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 41.9M/9.98G [00:00<02:26, 67.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 83.9M/3.50G [00:01<00:29, 114MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 105M/3.50G [00:01<00:24, 137MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 62.9M/9.98G [00:00<01:51, 88.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 126M/3.50G [00:01<00:23, 145MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.98G [00:01<01:28, 111MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 147M/3.50G [00:01<00:21, 159MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 105M/9.98G [00:01<01:14, 133MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 168M/3.50G [00:01<00:19, 170MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 126M/9.98G [00:01<01:07, 146MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 189M/3.50G [00:01<00:18, 179MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.98G [00:01<01:01, 161MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 210M/3.50G [00:01<00:18, 179MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 168M/9.98G [00:01<00:56, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 189M/9.98G [00:01<00:53, 181MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 231M/3.50G [00:01<00:18, 178MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.98G [00:01<00:52, 187MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 252M/3.50G [00:01<00:17, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 231M/9.98G [00:01<00:51, 190MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 273M/3.50G [00:02<00:17, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 252M/9.98G [00:01<00:50, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 294M/3.50G [00:02<00:16, 189MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.98G [00:01<00:50, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 315M/3.50G [00:02<00:17, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 294M/9.98G [00:02<00:49, 194MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 336M/3.50G [00:02<00:16, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 315M/9.98G [00:02<00:49, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 357M/3.50G [00:02<00:16, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.98G [00:02<00:48, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 377M/3.50G [00:02<00:16, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 357M/9.98G [00:02<00:48, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 398M/3.50G [00:02<00:16, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 377M/9.98G [00:02<00:47, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 419M/3.50G [00:02<00:16, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/9.98G [00:02<00:47, 201MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 440M/3.50G [00:02<00:15, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 419M/9.98G [00:02<00:48, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 461M/3.50G [00:03<00:15, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 440M/9.98G [00:02<00:48, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 482M/3.50G [00:03<00:15, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/9.98G [00:02<00:47, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 503M/3.50G [00:03<00:15, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 482M/9.98G [00:03<00:47, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 524M/3.50G [00:03<00:15, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 503M/9.98G [00:03<00:48, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 545M/3.50G [00:03<00:15, 192MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/9.98G [00:03<00:47, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 566M/3.50G [00:03<00:15, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 545M/9.98G [00:03<00:47, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 587M/3.50G [00:03<00:15, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 566M/9.98G [00:03<00:48, 195MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 608M/3.50G [00:03<00:14, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.98G [00:03<00:48, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 629M/3.50G [00:03<00:15, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 608M/9.98G [00:03<00:47, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 650M/3.50G [00:03<00:14, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 629M/9.98G [00:03<00:47, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 671M/3.50G [00:04<00:14, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 650M/9.98G [00:03<00:47, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 692M/3.50G [00:04<00:14, 196MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 671M/9.98G [00:03<00:46, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 713M/3.50G [00:04<00:14, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 692M/9.98G [00:04<00:46, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 734M/3.50G [00:04<00:14, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 713M/9.98G [00:04<00:46, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 755M/3.50G [00:04<00:14, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 734M/9.98G [00:04<00:47, 195MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 776M/3.50G [00:04<00:14, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 755M/9.98G [00:04<00:46, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 797M/3.50G [00:04<00:14, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 776M/9.98G [00:04<00:46, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 818M/3.50G [00:04<00:13, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 797M/9.98G [00:04<00:46, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 839M/3.50G [00:04<00:13, 196MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 818M/9.98G [00:04<00:45, 201MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 860M/3.50G [00:05<00:13, 196MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 839M/9.98G [00:04<00:46, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 881M/3.50G [00:05<00:13, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 860M/9.98G [00:04<00:45, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 881M/9.98G [00:05<00:45, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 902M/3.50G [00:05<00:13, 188MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/9.98G [00:05<00:45, 202MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 923M/3.50G [00:05<00:13, 193MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 944M/3.50G [00:05<00:12, 197MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 923M/9.98G [00:05<00:45, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 944M/9.98G [00:05<00:45, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 965M/3.50G [00:05<00:13, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 965M/9.98G [00:05<00:45, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 986M/3.50G [00:05<00:12, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 986M/9.98G [00:05<00:45, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.01G/3.50G [00:05<00:13, 188MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.98G [00:05<00:45, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.03G/3.50G [00:05<00:13, 189MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.03G/9.98G [00:05<00:45, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.05G/3.50G [00:06<00:12, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.05G/9.98G [00:05<00:45, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.07G/3.50G [00:06<00:12, 192MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.98G [00:05<00:44, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.09G/3.50G [00:06<00:12, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.09G/9.98G [00:06<00:44, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.11G/3.50G [00:06<00:12, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.11G/9.98G [00:06<00:44, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.13G/3.50G [00:06<00:12, 196MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.13G/9.98G [00:06<00:44, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.15G/3.50G [00:06<00:12, 184MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.15G/9.98G [00:06<00:44, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.17G/3.50G [00:06<00:12, 188MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.17G/9.98G [00:06<00:44, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.20G/3.50G [00:06<00:12, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.20G/9.98G [00:06<00:44, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.22G/3.50G [00:06<00:12, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.98G [00:06<00:44, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.24G/3.50G [00:07<00:12, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.24G/9.98G [00:06<00:45, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.27G/3.50G [00:07<00:11, 201MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.26G/9.98G [00:06<00:44, 195MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.29G/3.50G [00:07<00:11, 200MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/9.98G [00:07<00:44, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.30G/9.98G [00:07<00:43, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.31G/3.50G [00:07<00:11, 197MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.98G [00:07<00:43, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.33G/3.50G [00:07<00:11, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.34G/9.98G [00:07<00:43, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.35G/3.50G [00:07<00:11, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.36G/9.98G [00:07<00:44, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.37G/3.50G [00:07<00:10, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.38G/9.98G [00:07<00:44, 194MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.39G/3.50G [00:07<00:10, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.41G/9.98G [00:07<00:46, 184MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.42G/3.50G [00:07<00:11, 182MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.44G/3.50G [00:08<00:11, 185MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.44G/9.98G [00:07<00:43, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.46G/3.50G [00:08<00:11, 184MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.46G/9.98G [00:07<00:42, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.48G/3.50G [00:08<00:10, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.98G [00:08<00:42, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.50G/3.50G [00:08<00:10, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.50G/9.98G [00:08<00:44, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.52G/9.98G [00:08<00:45, 184MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.52G/3.50G [00:08<00:11, 172MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.98G [00:08<00:47, 179MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.54G/3.50G [00:08<00:12, 159MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.56G/9.98G [00:08<00:46, 180MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.56G/3.50G [00:08<00:11, 166MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.58G/9.98G [00:08<00:46, 182MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.59G/3.50G [00:08<00:10, 189MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.60G/9.98G [00:08<00:45, 184MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.61G/3.50G [00:09<00:09, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.98G [00:08<00:45, 184MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.64G/3.50G [00:09<00:10, 179MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.65G/9.98G [00:09<00:45, 182MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.66G/3.50G [00:09<00:10, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.98G [00:09<00:45, 184MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.68G/3.50G [00:09<00:10, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.69G/9.98G [00:09<00:44, 188MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.70G/3.50G [00:09<00:09, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.71G/9.98G [00:09<00:42, 194MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.72G/3.50G [00:09<00:09, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/9.98G [00:09<00:43, 191MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.74G/3.50G [00:09<00:09, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.98G [00:09<00:41, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.76G/3.50G [00:09<00:09, 189MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.78G/9.98G [00:09<00:41, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.78G/3.50G [00:10<00:09, 174MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.80G/9.98G [00:09<00:40, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.80G/3.50G [00:10<00:09, 171MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.82G/9.98G [00:09<00:40, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.85G/9.98G [00:10<00:40, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.82G/3.50G [00:10<00:13, 125MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.87G/9.98G [00:10<00:48, 168MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.85G/3.50G [00:10<00:13, 120MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.98G [00:10<00:59, 135MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.91G/9.98G [00:10<01:01, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.87G/3.50G [00:10<00:16, 98.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.93G/9.98G [00:10<00:58, 138MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.89G/3.50G [00:11<00:16, 99.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.95G/9.98G [00:10<01:08, 118MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.91G/3.50G [00:11<00:16, 94.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.92G/3.50G [00:11<00:16, 93.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.97G/9.98G [00:11<01:31, 87.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.94G/3.50G [00:11<00:15, 100MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.99G/9.98G [00:11<01:17, 103MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.95G/3.50G [00:11<00:15, 99.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.96G/3.50G [00:11<00:15, 100MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.01G/9.98G [00:11<01:19, 99.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.97G/3.50G [00:11<00:15, 99.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.98G/3.50G [00:12<00:15, 98.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.03G/9.98G [00:11<01:20, 99.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.99G/3.50G [00:12<00:15, 99.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.00G/3.50G [00:12<00:15, 97.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.01G/3.50G [00:12<00:16, 91.1MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.03G/3.50G [00:12<00:13, 108MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.06G/9.98G [00:12<01:51, 71.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.08G/9.98G [00:12<01:29, 87.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.06G/3.50G [00:12<00:14, 98.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.07G/3.50G [00:12<00:14, 98.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.10G/9.98G [00:12<01:33, 84.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.08G/3.50G [00:13<00:14, 98.1MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.09G/3.50G [00:13<00:15, 90.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.10G/3.50G [00:13<00:15, 92.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.12G/9.98G [00:13<01:43, 76.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.12G/3.50G [00:13<00:13, 106MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.13G/9.98G [00:13<01:46, 73.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.13G/3.50G [00:13<00:13, 100MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.15G/9.98G [00:13<01:28, 88.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.14G/3.50G [00:13<00:14, 91.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.15G/3.50G [00:13<00:15, 87.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.17G/9.98G [00:13<01:26, 90.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.17G/3.50G [00:14<00:12, 104MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.18G/9.98G [00:13<01:42, 75.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.19G/3.50G [00:14<00:13, 98.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.19G/9.98G [00:14<01:42, 75.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.20G/3.50G [00:14<00:13, 93.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.21G/9.98G [00:14<01:34, 82.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.21G/3.50G [00:14<00:14, 87.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.22G/9.98G [00:14<01:40, 77.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.23G/3.50G [00:14<00:11, 106MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.25G/3.50G [00:14<00:09, 125MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/9.98G [00:14<01:44, 73.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.28G/3.50G [00:14<00:09, 135MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.24G/9.98G [00:14<01:42, 75.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.30G/3.50G [00:15<00:08, 146MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.26G/9.98G [00:14<01:18, 98.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.32G/3.50G [00:15<00:08, 144MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.29G/9.98G [00:14<01:04, 119MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.34G/3.50G [00:15<00:07, 154MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.31G/9.98G [00:15<00:56, 136MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.36G/3.50G [00:15<00:06, 166MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.33G/9.98G [00:15<00:50, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.35G/9.98G [00:15<00:47, 162MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.38G/3.50G [00:15<00:06, 166MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.37G/9.98G [00:15<00:44, 172MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.40G/3.50G [00:15<00:06, 173MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.39G/9.98G [00:15<00:41, 181MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.42G/3.50G [00:15<00:06, 177MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.41G/9.98G [00:15<00:42, 178MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.44G/3.50G [00:15<00:05, 184MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.43G/9.98G [00:15<00:41, 184MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.46G/3.50G [00:15<00:05, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.45G/9.98G [00:15<00:40, 186MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.49G/3.50G [00:16<00:05, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.47G/9.98G [00:15<00:39, 188MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.51G/3.50G [00:16<00:05, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.50G/9.98G [00:16<00:39, 191MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.53G/3.50G [00:16<00:05, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.53G/9.98G [00:16<00:37, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.55G/9.98G [00:16<00:38, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.55G/3.50G [00:16<00:07, 122MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.57G/9.98G [00:16<00:38, 192MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.58G/3.50G [00:16<00:06, 153MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.59G/9.98G [00:16<00:38, 191MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.60G/3.50G [00:16<00:05, 159MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.61G/9.98G [00:16<00:38, 191MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.62G/3.50G [00:16<00:05, 168MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.63G/9.98G [00:16<00:37, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.64G/3.50G [00:17<00:04, 176MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.65G/9.98G [00:16<00:37, 195MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.66G/3.50G [00:17<00:04, 181MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.67G/9.98G [00:16<00:36, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.68G/3.50G [00:17<00:04, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.69G/9.98G [00:17<00:36, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.71G/3.50G [00:17<00:04, 188MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.72G/9.98G [00:17<00:36, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.73G/3.50G [00:17<00:04, 192MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.74G/9.98G [00:17<00:36, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.75G/3.50G [00:17<00:03, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.76G/9.98G [00:17<00:38, 189MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.77G/3.50G [00:17<00:03, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.78G/9.98G [00:17<00:38, 188MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.79G/3.50G [00:17<00:03, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.80G/9.98G [00:17<00:37, 192MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.81G/3.50G [00:17<00:03, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.98G [00:17<00:38, 186MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.83G/3.50G [00:18<00:03, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.84G/9.98G [00:17<00:38, 187MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.85G/3.50G [00:18<00:03, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.86G/9.98G [00:17<00:37, 188MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.87G/3.50G [00:18<00:03, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.89G/9.98G [00:18<00:36, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.89G/3.50G [00:18<00:03, 188MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.92G/9.98G [00:18<00:36, 192MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.92G/3.50G [00:18<00:03, 181MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.94G/9.98G [00:18<00:36, 195MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.94G/3.50G [00:18<00:03, 184MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.96G/9.98G [00:18<00:35, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.96G/3.50G [00:18<00:03, 180MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.98G/9.98G [00:18<00:37, 186MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.98G/3.50G [00:18<00:02, 184MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.01G/9.98G [00:18<00:34, 200MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.00G/3.50G [00:18<00:02, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.03G/9.98G [00:18<00:35, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.02G/3.50G [00:19<00:02, 189MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.05G/9.98G [00:18<00:36, 189MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.04G/3.50G [00:19<00:02, 188MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.07G/9.98G [00:18<00:36, 191MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.06G/3.50G [00:19<00:02, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.09G/9.98G [00:19<00:35, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.08G/3.50G [00:19<00:02, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.11G/9.98G [00:19<00:34, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.10G/3.50G [00:19<00:02, 171MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.14G/9.98G [00:19<00:34, 199MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.12G/3.50G [00:19<00:02, 175MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.16G/9.98G [00:19<00:35, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.18G/9.98G [00:19<00:34, 197MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.16G/3.50G [00:19<00:01, 185MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.20G/9.98G [00:19<00:34, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.18G/3.50G [00:19<00:01, 181MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.22G/9.98G [00:19<00:34, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.20G/3.50G [00:20<00:01, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.24G/9.98G [00:19<00:34, 195MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.22G/3.50G [00:20<00:01, 187MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.26G/9.98G [00:19<00:35, 191MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.24G/3.50G [00:20<00:01, 192MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.28G/9.98G [00:20<00:34, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.26G/3.50G [00:20<00:01, 195MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.30G/9.98G [00:20<00:36, 184MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.28G/3.50G [00:20<00:01, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.32G/9.98G [00:20<00:35, 188MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.30G/3.50G [00:20<00:01, 182MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.34G/9.98G [00:20<00:34, 192MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.32G/3.50G [00:20<00:00, 186MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.37G/9.98G [00:20<00:34, 193MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.34G/3.50G [00:20<00:00, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.39G/9.98G [00:20<00:33, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.37G/3.50G [00:20<00:00, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.41G/9.98G [00:20<00:33, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.39G/3.50G [00:21<00:00, 194MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.43G/9.98G [00:20<00:34, 191MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.41G/3.50G [00:21<00:00, 196MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.45G/9.98G [00:20<00:33, 196MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.43G/3.50G [00:21<00:00, 196MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.47G/9.98G [00:21<00:33, 195MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.45G/3.50G [00:21<00:00, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.49G/9.98G [00:21<00:32, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.47G/3.50G [00:21<00:00, 190MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.51G/9.98G [00:21<00:32, 198MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.49G/3.50G [00:21<00:00, 191MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.50G/3.50G [00:21<00:00, 162MB/s]\n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.55G/9.98G [00:21<00:32, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.58G/9.98G [00:21<00:32, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.60G/9.98G [00:21<00:34, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.62G/9.98G [00:21<00:33, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.64G/9.98G [00:21<00:34, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.66G/9.98G [00:22<00:32, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.68G/9.98G [00:22<00:33, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.71G/9.98G [00:22<00:32, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.74G/9.98G [00:22<00:32, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.76G/9.98G [00:22<00:31, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.79G/9.98G [00:22<00:31, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.81G/9.98G [00:22<00:33, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.83G/9.98G [00:22<00:32, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.85G/9.98G [00:23<00:32, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.87G/9.98G [00:23<00:32, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.89G/9.98G [00:23<00:32, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/9.98G [00:23<00:30, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.94G/9.98G [00:23<00:30, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.97G/9.98G [00:23<00:29, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.00G/9.98G [00:23<00:29, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.02G/9.98G [00:23<00:29, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.04G/9.98G [00:23<00:29, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.06G/9.98G [00:24<00:29, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.08G/9.98G [00:24<00:29, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.10G/9.98G [00:24<00:29, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.12G/9.98G [00:24<00:29, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.14G/9.98G [00:24<00:29, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.16G/9.98G [00:24<00:29, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.18G/9.98G [00:24<00:28, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.20G/9.98G [00:24<00:29, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.23G/9.98G [00:25<00:46, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.25G/9.98G [00:25<00:47, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.27G/9.98G [00:25<00:56, 101MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.29G/9.98G [00:25<01:04, 88.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.31G/9.98G [00:26<01:09, 82.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.33G/9.98G [00:26<01:12, 77.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.35G/9.98G [00:26<01:07, 83.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.36G/9.98G [00:26<01:08, 81.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.38G/9.98G [00:27<01:05, 84.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.40G/9.98G [00:27<00:54, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/9.98G [00:27<00:47, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.45G/9.98G [00:27<00:54, 101MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.47G/9.98G [00:27<00:55, 99.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.49G/9.98G [00:28<00:56, 97.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.51G/9.98G [00:28<01:02, 87.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.53G/9.98G [00:28<00:56, 96.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.55G/9.98G [00:28<00:49, 110MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.57G/9.98G [00:28<00:50, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.59G/9.98G [00:29<00:50, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.61G/9.98G [00:29<00:49, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.63G/9.98G [00:29<00:45, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.66G/9.98G [00:29<00:40, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.68G/9.98G [00:29<00:36, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.70G/9.98G [00:29<00:33, 158MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.72G/9.98G [00:29<00:31, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.74G/9.98G [00:29<00:29, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.76G/9.98G [00:30<00:28, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.78G/9.98G [00:30<00:27, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/9.98G [00:30<00:27, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.82G/9.98G [00:30<00:26, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.84G/9.98G [00:30<00:26, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.87G/9.98G [00:30<00:25, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.89G/9.98G [00:30<00:25, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.91G/9.98G [00:30<00:25, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.93G/9.98G [00:30<00:26, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.95G/9.98G [00:30<00:25, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.97G/9.98G [00:31<00:25, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.99G/9.98G [00:31<00:25, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.01G/9.98G [00:31<00:25, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.03G/9.98G [00:31<00:25, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.05G/9.98G [00:31<00:24, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.08G/9.98G [00:31<00:24, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.10G/9.98G [00:31<00:24, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/9.98G [00:31<00:24, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.14G/9.98G [00:31<00:24, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.16G/9.98G [00:32<00:24, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.18G/9.98G [00:32<00:24, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.20G/9.98G [00:32<00:23, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.22G/9.98G [00:32<00:24, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.98G [00:32<00:24, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.26G/9.98G [00:32<00:23, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.28G/9.98G [00:32<00:23, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.31G/9.98G [00:32<00:23, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.33G/9.98G [00:32<00:23, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.35G/9.98G [00:32<00:23, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.37G/9.98G [00:33<00:23, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.39G/9.98G [00:33<00:23, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.41G/9.98G [00:33<00:22, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.43G/9.98G [00:33<00:22, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.46G/9.98G [00:33<00:22, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.48G/9.98G [00:33<00:22, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.98G [00:33<00:21, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.53G/9.98G [00:33<00:21, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.55G/9.98G [00:33<00:21, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.57G/9.98G [00:34<00:21, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.59G/9.98G [00:34<00:21, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.61G/9.98G [00:34<00:21, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.63G/9.98G [00:34<00:22, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.65G/9.98G [00:34<00:22, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.67G/9.98G [00:34<00:22, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.69G/9.98G [00:34<00:22, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.71G/9.98G [00:34<00:21, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.74G/9.98G [00:34<00:21, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.76G/9.98G [00:35<00:21, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.78G/9.98G [00:35<00:21, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.80G/9.98G [00:35<00:22, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.82G/9.98G [00:35<00:22, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.84G/9.98G [00:35<00:21, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.86G/9.98G [00:35<00:21, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.89G/9.98G [00:35<00:20, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.98G [00:35<00:21, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.93G/9.98G [00:35<00:21, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.96G/9.98G [00:36<00:20, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.98G [00:36<00:20, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.00G/9.98G [00:36<00:20, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.03G/9.98G [00:36<00:19, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.05G/9.98G [00:36<00:19, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.07G/9.98G [00:36<00:19, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.09G/9.98G [00:36<00:20, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.11G/9.98G [00:36<00:20, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.13G/9.98G [00:36<00:20, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.16G/9.98G [00:37<00:19, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.19G/9.98G [00:37<00:18, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.21G/9.98G [00:37<00:18, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.23G/9.98G [00:37<00:19, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.25G/9.98G [00:37<00:18, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.27G/9.98G [00:37<00:19, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.29G/9.98G [00:37<00:18, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.98G [00:37<00:18, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.33G/9.98G [00:37<00:18, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.35G/9.98G [00:38<00:18, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.38G/9.98G [00:38<00:17, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.40G/9.98G [00:38<00:17, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.42G/9.98G [00:38<00:17, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.44G/9.98G [00:38<00:18, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.47G/9.98G [00:38<00:18, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.49G/9.98G [00:38<00:18, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.51G/9.98G [00:38<00:17, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.53G/9.98G [00:39<00:17, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.55G/9.98G [00:39<00:17, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.57G/9.98G [00:39<00:17, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.60G/9.98G [00:39<00:16, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.62G/9.98G [00:39<00:16, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.64G/9.98G [00:39<00:16, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.66G/9.98G [00:39<00:25, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.69G/9.98G [00:40<00:24, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.71G/9.98G [00:40<00:27, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.73G/9.98G [00:40<00:25, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.75G/9.98G [00:40<00:23, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.77G/9.98G [00:40<00:22, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.79G/9.98G [00:40<00:21, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.82G/9.98G [00:40<00:24, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.84G/9.98G [00:41<00:22, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.86G/9.98G [00:41<00:24, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.88G/9.98G [00:41<00:28, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.90G/9.98G [00:41<00:27, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.92G/9.98G [00:42<00:31, 98.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.94G/9.98G [00:42<00:29, 103MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.96G/9.98G [00:42<00:28, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.98G/9.98G [00:42<00:26, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.00G/9.98G [00:42<00:26, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.03G/9.98G [00:42<00:27, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.05G/9.98G [00:43<00:27, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.07G/9.98G [00:43<00:27, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.09G/9.98G [00:43<00:28, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.10G/9.98G [00:43<00:29, 96.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.11G/9.98G [00:43<00:29, 95.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.12G/9.98G [00:43<00:29, 95.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.13G/9.98G [00:44<00:31, 91.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.14G/9.98G [00:44<00:30, 94.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.16G/9.98G [00:44<00:28, 97.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.17G/9.98G [00:44<00:30, 92.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.18G/9.98G [00:44<00:30, 92.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.19G/9.98G [00:44<00:29, 93.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.20G/9.98G [00:44<00:29, 93.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.21G/9.98G [00:45<00:32, 85.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.24G/9.98G [00:45<00:24, 112MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.26G/9.98G [00:45<00:20, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.28G/9.98G [00:45<00:18, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.30G/9.98G [00:45<00:16, 162MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.32G/9.98G [00:45<00:15, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.34G/9.98G [00:45<00:14, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.36G/9.98G [00:45<00:14, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.98G [00:45<00:13, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.40G/9.98G [00:45<00:13, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.42G/9.98G [00:46<00:13, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.44G/9.98G [00:46<00:12, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.47G/9.98G [00:46<00:12, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.49G/9.98G [00:46<00:12, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.52G/9.98G [00:46<00:12, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.54G/9.98G [00:46<00:12, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.56G/9.98G [00:46<00:12, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.58G/9.98G [00:46<00:11, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.60G/9.98G [00:46<00:11, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.62G/9.98G [00:47<00:11, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.64G/9.98G [00:47<00:11, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.67G/9.98G [00:47<00:11, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.69G/9.98G [00:47<00:11, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.71G/9.98G [00:47<00:11, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.73G/9.98G [00:47<00:11, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.75G/9.98G [00:47<00:10, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.77G/9.98G [00:47<00:10, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.79G/9.98G [00:47<00:10, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.81G/9.98G [00:48<00:10, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.83G/9.98G [00:48<00:10, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.85G/9.98G [00:48<00:10, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.87G/9.98G [00:48<00:10, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.90G/9.98G [00:48<00:10, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.92G/9.98G [00:48<00:10, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.94G/9.98G [00:48<00:10, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.96G/9.98G [00:48<00:11, 181MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.99G/9.98G [00:48<00:10, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.01G/9.98G [00:49<00:10, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.03G/9.98G [00:49<00:09, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.05G/9.98G [00:49<00:09, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.07G/9.98G [00:49<00:09, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.10G/9.98G [00:49<00:09, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.12G/9.98G [00:49<00:09, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.14G/9.98G [00:49<00:09, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.16G/9.98G [00:49<00:09, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.18G/9.98G [00:49<00:09, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.20G/9.98G [00:49<00:08, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.22G/9.98G [00:50<00:08, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.24G/9.98G [00:50<00:08, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.26G/9.98G [00:50<00:08, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.28G/9.98G [00:50<00:08, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.30G/9.98G [00:50<00:08, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.33G/9.98G [00:50<00:08, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.35G/9.98G [00:50<00:08, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.37G/9.98G [00:50<00:08, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.39G/9.98G [00:50<00:07, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.41G/9.98G [00:51<00:07, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.43G/9.98G [00:51<00:07, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.45G/9.98G [00:51<00:07, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.47G/9.98G [00:51<00:07, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.49G/9.98G [00:51<00:07, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.51G/9.98G [00:51<00:07, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.54G/9.98G [00:51<00:07, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.56G/9.98G [00:51<00:07, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.58G/9.98G [00:51<00:07, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.60G/9.98G [00:51<00:06, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.62G/9.98G [00:52<00:06, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.64G/9.98G [00:52<00:07, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [00:52<00:06, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.69G/9.98G [00:52<00:06, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.71G/9.98G [00:52<00:06, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.73G/9.98G [00:52<00:06, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.76G/9.98G [00:52<00:06, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.78G/9.98G [00:52<00:06, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.80G/9.98G [00:53<00:05, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.82G/9.98G [00:53<00:05, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.84G/9.98G [00:53<00:05, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.86G/9.98G [00:53<00:05, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.88G/9.98G [00:53<00:05, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.91G/9.98G [00:53<00:05, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.94G/9.98G [00:53<00:05, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.97G/9.98G [00:53<00:05, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.99G/9.98G [00:53<00:05, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.01G/9.98G [00:54<00:05, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.03G/9.98G [00:54<00:04, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.05G/9.98G [00:54<00:04, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.07G/9.98G [00:54<00:04, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.09G/9.98G [00:54<00:04, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.11G/9.98G [00:54<00:04, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.13G/9.98G [00:54<00:04, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.15G/9.98G [00:54<00:04, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.98G [00:54<00:04, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.20G/9.98G [00:55<00:04, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.22G/9.98G [00:55<00:04, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.24G/9.98G [00:55<00:04, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.26G/9.98G [00:55<00:04, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.28G/9.98G [00:55<00:04, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.98G [00:55<00:03, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.32G/9.98G [00:55<00:03, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.35G/9.98G [00:55<00:03, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.37G/9.98G [00:56<00:03, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.40G/9.98G [00:56<00:04, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.42G/9.98G [00:56<00:04, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.44G/9.98G [00:56<00:04, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.46G/9.98G [00:56<00:04, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.48G/9.98G [00:57<00:04, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.50G/9.98G [00:57<00:03, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.52G/9.98G [00:57<00:03, 140MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.54G/9.98G [00:57<00:02, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.56G/9.98G [00:57<00:02, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.58G/9.98G [00:57<00:02, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.60G/9.98G [00:57<00:03, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.63G/9.98G [00:58<00:02, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.65G/9.98G [00:58<00:02, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.67G/9.98G [00:58<00:03, 98.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.69G/9.98G [00:58<00:02, 108MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.71G/9.98G [00:58<00:02, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.73G/9.98G [00:59<00:02, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.75G/9.98G [00:59<00:02, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.77G/9.98G [00:59<00:02, 99.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.78G/9.98G [00:59<00:01, 97.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.80G/9.98G [00:59<00:01, 114MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.83G/9.98G [00:59<00:01, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.85G/9.98G [01:00<00:01, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.87G/9.98G [01:00<00:01, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.89G/9.98G [01:00<00:00, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.91G/9.98G [01:00<00:00, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.93G/9.98G [01:00<00:00, 137MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.95G/9.98G [01:00<00:00, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [01:01<00:00, 163MB/s]\n",
            "Fetching 15 files: 100% 15/15 [01:02<00:00,  4.14s/it]\n",
            "Successfully downloaded model repo and wrote to the following locations:\n",
            "/tmp/Llama-2-7b-hf/special_tokens_map.json\n",
            "/tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin\n",
            "/tmp/Llama-2-7b-hf/tokenizer_config.json\n",
            "/tmp/Llama-2-7b-hf/tokenizer.model\n",
            "/tmp/Llama-2-7b-hf/USE_POLICY.md\n",
            "/tmp/Llama-2-7b-hf/config.json\n",
            "/tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin\n",
            "/tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
            "/tmp/Llama-2-7b-hf/README.md\n",
            "/tmp/Llama-2-7b-hf/LICENSE.txt\n",
            "/tmp/Llama-2-7b-hf/generation_config.json\n",
            "/tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
            "/tmp/Llama-2-7b-hf/tokenizer.json\n",
            "/tmp/Llama-2-7b-hf/.gitattributes\n",
            "/tmp/Llama-2-7b-hf/model.safetensors.index.json\n"
          ]
        }
      ],
      "source": [
        "#%mkdir -p /tmp/Llama-2-7b-hf\n",
        "!tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7yxsyvRUXL1",
        "outputId": "884683c7-d0cc-40f1-ea68-766170ca3fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root  138 Apr 28 20:55 /tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin -> ../../root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/blobs/ee62ed2ad7ded505ae47df50bc6c52916860dfb1c009df4715148cc4bfb50d2f\n",
            "lrwxrwxrwx 1 root root  138 Apr 28 20:54 /tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin -> ../../root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/blobs/1fd7762035b3ca4f2d6af6bf10129689a119b7c38058025f9842511532ea02fb\n",
            "-rw-r--r-- 1 root root 489K Apr 28 20:54 /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "-rw-r--r-- 1 root root  776 Apr 28 20:54 /tmp/Llama-2-7b-hf/tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  414 Apr 28 20:54 /tmp/Llama-2-7b-hf/special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  27K Apr 28 20:54 /tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root 1.8M Apr 28 20:54 /tmp/Llama-2-7b-hf/tokenizer.json\n",
            "-rw-r--r-- 1 root root  188 Apr 28 20:54 /tmp/Llama-2-7b-hf/generation_config.json\n",
            "-rw-r--r-- 1 root root 1.2M Apr 28 20:54 /tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
            "-rw-r--r-- 1 root root  27K Apr 28 20:54 /tmp/Llama-2-7b-hf/model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root  22K Apr 28 20:54 /tmp/Llama-2-7b-hf/README.md\n",
            "-rw-r--r-- 1 root root 4.7K Apr 28 20:54 /tmp/Llama-2-7b-hf/USE_POLICY.md\n",
            "-rw-r--r-- 1 root root 6.9K Apr 28 20:54 /tmp/Llama-2-7b-hf/LICENSE.txt\n",
            "-rw-r--r-- 1 root root  609 Apr 28 20:54 /tmp/Llama-2-7b-hf/config.json\n"
          ]
        }
      ],
      "source": [
        "!ls -ltha /tmp/Llama-2-7b-hf/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHOPm_Wqu_vJ"
      },
      "source": [
        "QLoRA has about 75% smaller peak GPU memory usage compared to LoRA. LoRA is about 66% faster than QLoRA in terms of tuning speed. While both methods are relatively inexpensive, LoRA is up to 40% less expensive than QLoRA. Higher max sequence length increases GPU memory consumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exzSiuX2t5QX",
        "outputId": "f23265fb-8931-4b26-bab1-7d5e01f4b9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RECIPE                                   CONFIG                                  \n",
            "full_finetune_single_device              llama2/7B_full_low_memory               \n",
            "                                         llama3/8B_full_single_device            \n",
            "                                         mistral/7B_full_low_memory              \n",
            "full_finetune_distributed                llama2/7B_full                          \n",
            "                                         llama2/13B_full                         \n",
            "                                         llama3/8B_full                          \n",
            "                                         mistral/7B_full                         \n",
            "                                         gemma/2B_full                           \n",
            "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
            "                                         llama2/7B_qlora_single_device           \n",
            "                                         llama3/8B_lora_single_device            \n",
            "                                         llama3/8B_qlora_single_device           \n",
            "                                         llama2/13B_qlora_single_device          \n",
            "                                         mistral/7B_lora_single_device           \n",
            "                                         mistral/7B_qlora_single_device          \n",
            "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
            "lora_finetune_distributed                llama2/7B_lora                          \n",
            "                                         llama2/13B_lora                         \n",
            "                                         llama2/70B_lora                         \n",
            "                                         llama3/8B_lora                          \n",
            "                                         mistral/7B_lora                         \n",
            "generate                                 generation                              \n",
            "eleuther_eval                            eleuther_evaluation                     \n",
            "quantize                                 quantization                            \n"
          ]
        }
      ],
      "source": [
        "!tune ls\n",
        "\n",
        "#RECIPE                                   CONFIG\n",
        "#full_finetune_single_device              llama2/7B_full_low_memory\n",
        "#                                         mistral/7B_full_low_memory\n",
        "#full_finetune_distributed                llama2/7B_full\n",
        "#                                         llama2/13B_full\n",
        "#                                         mistral/7B_full\n",
        "#lora_finetune_single_device              llama2/7B_lora_single_device\n",
        "#                                         llama2/7B_qlora_single_device\n",
        "#                                         mistral/7B_lora_single_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XywjcsOj__LT"
      },
      "outputs": [],
      "source": [
        "#ERROR WITH T4 GPU:  RuntimeError: bf16 precision was requested but not available on this hardware. Please use fp32 precision instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJYIEyyoVifn"
      },
      "source": [
        "https://pytorch.org/torchtune/stable/_modules/torchtune/utils/precision.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu3fJH7JZsQ2"
      },
      "source": [
        "https://pytorch.org/torchtune/stable/tutorials/first_finetune_tutorial.html\n",
        "\n",
        "\n",
        "https://pytorch.org/torchtune/stable/tutorials/e2e_flow.html#e2e-flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye85PGqPF6ah",
        "outputId": "c98af5b9-e7ed-49c3-86f4-6d531e83301a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 2\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  adapter_checkpoint: null\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf\n",
            "  recipe_checkpoint: null\n",
            "compile: false\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
            "  train_on_input: true\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "epochs: 1\n",
            "gradient_accumulation_steps: 64\n",
            "log_every_n_steps: null\n",
            "loss:\n",
            "  _component_: torch.nn.CrossEntropyLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.utils.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/lora_finetune_output\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.lora_llama2_7b\n",
            "  apply_lora_to_mlp: false\n",
            "  apply_lora_to_output: false\n",
            "  lora_alpha: 16\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - v_proj\n",
            "  lora_rank: 8\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  lr: 0.0003\n",
            "  weight_decay: 0.01\n",
            "output_dir: /tmp/lora_finetune_output\n",
            "profiler:\n",
            "  _component_: torchtune.utils.profiler\n",
            "  enabled: false\n",
            "  output_dir: /tmp/lora_finetune_output/torchtune_perf_tracing.json\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 243610558. Local seed is seed + rank = 243610558 + 0\n",
            "Writing logs to /tmp/lora_finetune_output/log_1714337730.txt\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:Memory Stats after model init:\n",
            "{'peak_memory_active': 13.959471616, 'peak_memory_alloc': 13.959471616, 'peak_memory_reserved': 13.97751808}\n",
            "INFO:torchtune.utils.logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils.logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils.logging:Loss is initialized.\n",
            "Downloading readme: 100% 11.6k/11.6k [00:00<00:00, 39.0MB/s]\n",
            "Downloading data: 100% 44.3M/44.3M [00:00<00:00, 49.9MB/s]\n",
            "Generating train split: 100% 51760/51760 [00:00<00:00, 129843.12 examples/s]\n",
            "INFO:torchtune.utils.logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n",
            "1|25880|Loss: 0.6946025490760803: 100% 25880/25880 [4:59:29<00:00,  1.44it/s]\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 9.98 GB saved to /tmp/Llama-2-7b-hf/hf_model_0001_0.pt\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 3.50 GB saved to /tmp/Llama-2-7b-hf/hf_model_0002_0.pt\n",
            "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.01 GB saved to /tmp/Llama-2-7b-hf/adapter_0.pt\n"
          ]
        }
      ],
      "source": [
        "!tune run lora_finetune_single_device --config llama2/7B_lora_single_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mWviV1LYjA5S"
      },
      "outputs": [],
      "source": [
        "#[_checkpointer.py:473] Model checkpoint of size 9.98 GB saved to <checkpoint_dir>/hf_model_0001_0.pt\n",
        "\n",
        "#[_checkpointer.py:473] Model checkpoint of size 3.50 GB saved to <checkpoint_dir>/hf_model_0002_0.pt\n",
        "\n",
        "#[_checkpointer.py:484] Adapter checkpoint of size 0.01 GB saved to <checkpoint_dir>/adapter_0.pt\n",
        "\n",
        "#checkpoint_dir='/tmp/Llama-2-7b-hf/'\n",
        "#/tmp/Llama-2-7b-hf/tokenizer.model\n",
        "#INFO:torchtune.utils.logging:Model checkpoint of size 9.98 GB saved to /tmp/Llama-2-7b-hf/hf_model_0001_0.pt\n",
        "#INFO:torchtune.utils.logging:Model checkpoint of size 3.50 GB saved to /tmp/Llama-2-7b-hf/hf_model_0002_0.pt\n",
        "#INFO:torchtune.utils.logging:Adapter checkpoint of size 0.01 GB saved to /tmp/Llama-2-7b-hf/adapter_0.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXFgoTYGiQWh"
      },
      "outputs": [],
      "source": [
        "#!tune cp eleuther_evaluation ./custom_eval_config.yaml \\\n",
        "\n",
        "!tune cp eleuther_evaluation /content/custom_eval_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "https://huggingface.co/frankmorales2020/Llama-2-7b-hf-text-to-sql-flash-attention-2\n",
        "\n",
        "\n",
        "huggingface-cli upload 'hf-repo-id' 'checkpoint-dir'\n",
        "\n",
        "https://huggingface.co/'hf-repo-id'/tree/main/.\n",
        "\n"
      ],
      "metadata": {
        "id": "LB0b7zxs58wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feHuTTazK-9c",
        "outputId": "df03e5be-7a2f-4944-b31f-dc2ea32d4c4d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for EleutherEvalRecipe in eleuther_eval.py\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"]\n",
            "\n",
            "# Model Arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
            "  checkpoint_files: [\n",
            "    pytorch_model-00001-of-00002.bin,\n",
            "    pytorch_model-00002-of-00002.bin,\n",
            "  ]\n",
            "  recipe_checkpoint: null\n",
            "  output_dir: /tmp/Llama-2-7b-hf\n",
            "  model_type: LLAMA2\n",
            "\n",
            "# Tokenizer\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Environment\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "seed: 217\n",
            "\n",
            "# EleutherAI specific eval args\n",
            "tasks: [\"truthfulqa_mc2\"]\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "\n",
            "# Quantization specific args\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm_eval==0.4.* -q"
      ],
      "metadata": {
        "id": "RdG5vSLOG71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a97fe8f-df5b-464d-da49-c7e593f0b9f9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEFORE tunning\n",
        "\n",
        "2024-04-29:03:49:16,837 INFO     [eleuther_eval.py:198] truthfulqa_mc2: {'acc,none': 0.3891793196860692, 'acc_stderr,none': 0.013564855356631, 'alias': 'truthfulqa_mc2'}"
      ],
      "metadata": {
        "id": "wAn2q_oVByy1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFmQRzg2iTUG"
      },
      "outputs": [],
      "source": [
        "#RECIPE                                   CONFIG\n",
        "#eleuther_eval                            eleuther_evaluation\n",
        "\n",
        "#/tmp/Mistral-7B-v0.1/tokenizer_config.json\n",
        "#/tmp/Mistral-7B-v0.1/tokenizer.model\n",
        "# /usr/local/lib/python3.10/dist-packages/torchtune/models/mistral\n",
        "# /usr/local/lib/python3.10/dist-packages/torchtune/models/llama2\n",
        "\n",
        "!tune run eleuther_eval --config /content/custom_eval_config.yaml\n",
        "\n",
        "#checkpointer.checkpoint_dir='/tmp/Llama-2-7b-hf/' \\\n",
        "#tokenizer.path='/tmp/Llama-2-7b-hf'/tokenizer.model\n",
        "\n",
        "#[evaluator.py:324] Running loglikelihood requests\n",
        "#[eleuther_eval.py:195] Eval completed in 121.27 seconds.\n",
        "#[eleuther_eval.py:197] truthfulqa_mc2: {'acc,none': 0.388..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFusCDcbyK3f",
        "outputId": "ed33fdce-cf1a-41c2-ece4-4f47e4977b83"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for EleutherEvalRecipe in eleuther_eval.py\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"]\n",
            "\n",
            "# Model Arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
            "  # checkpoint files for the fine-tuned model. This should\n",
            "  # match what's shown in the logs above\n",
            "  checkpoint_files: [\n",
            "        hf_model_0001_0.pt,\n",
            "        hf_model_0002_0.pt,\n",
            "  ]\n",
            "\n",
            "  recipe_checkpoint: null\n",
            "  output_dir: /tmp/Llama-2-7b-hf\n",
            "  model_type: LLAMA2\n",
            "\n",
            "# Tokenizer\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Environment\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "seed: 217\n",
            "\n",
            "# EleutherAI specific eval args\n",
            "tasks: [\"truthfulqa_mc2\"]\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "\n",
            "# Quantization specific args\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AFTER tunning\n",
        "\n",
        "2024-04-29:03:58:45,703 INFO     [eleuther_eval.py:198] truthfulqa_mc2: {'acc,none': 0.4786091749231919, 'acc_stderr,none': 0.014549538233703927, 'alias': 'truthfulqa_mc2'}"
      ],
      "metadata": {
        "id": "2LjZtr3mBnMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run eleuther_eval --config /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "id": "m-sHUIPtydso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune cp generation /content/custom_generation_config.yaml"
      ],
      "metadata": {
        "id": "-5SuCIzRTV6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_generation_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-vbdQ16WcFH",
        "outputId": "0225da6c-2324-4a7c-8b15-c7fe3919e584"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run generate --config generation\n",
            "\n",
            "# Model arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  # checkpoint files for the fine-tuned model. This should\n",
            "    # match what's shown in the logs above\n",
            "    checkpoint_files: [\n",
            "        hf_model_0001_0.pt,\n",
            "        hf_model_0002_0.pt,\n",
            "    ]\n",
            "  \n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "  model_type: LLAMA2\n",
            "\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "\n",
            "seed: 1234\n",
            "\n",
            "# Tokenizer arguments\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Generation arguments; defaults taken from gpt-fast\n",
            "prompt: \"Hello, my name is\"\n",
            "max_new_tokens: 300\n",
            "temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
            "top_k: 300\n",
            "\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ],
      "metadata": {
        "id": "peGH3jzVWphQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024-04-29:04:03:28,328 INFO     [generate.py:123] What are some interesting sites to visit in the Bay Area?\n",
        "What are some interesting sites to visit in the Bay Area?\n",
        "The Bay Area is home to many interesting sites, from the iconic Golden Gate Bridge to the quirky Alcatraz Island. Here are some of the most interesting sites to visit in the Bay Area:\n",
        "Golden Gate Bridge: This suspension bridge is one of the most recognizable landmarks in the world. It spans the Golden Gate Strait, connecting San Francisco to Marin County. Visitors can take a walk or bike ride across the bridge and enjoy the stunning views of the bay.\n",
        "Alcatraz Island: This former prison is now a popular tourist attraction. Visitors can take a ferry to the island and explore the cell blocks, hospital, and other buildings. There is also a museum on the island, which tells the history of the prison and its most famous inmates.\n",
        "Coit Tower: This 210-foot tower is located in the Telegraph Hill neighborhood of San Francisco. It offers panoramic views of the city and the bay. Visitors can take an elevator to the top of the tower and enjoy the views.\n",
        "Sutro Baths: These ruins are located in the Lands End area of San Francisco. The baths were a popular swimming and spa destination in the late 19th century, but closed in 1966. Today, visitors can explore the ru\n",
        "\n",
        "2024-04-29:04:03:28,330 INFO     [generate.py:136] Time for inference: 20.19 sec total, 14.86 tokens/sec\n",
        "\n",
        "2024-04-29:04:03:28,330 INFO     [generate.py:139] Bandwidth achieved: 233.11 GB/s\n",
        "\n",
        "2024-04-29:04:03:28,331 INFO     [generate.py:140] Memory used: 15.72 GB\n"
      ],
      "metadata": {
        "id": "Uas51EXRI6x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/frankmorales2020\n",
        "!huggingface-cli upload torchtune-Llama-2-7b /tmp/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "45EIH_SWDK_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/frankmorales2020/torchtune-Llama-2-7b"
      ],
      "metadata": {
        "id": "AF2Cp26mJWD2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOeR0q9jPDDYr2/JV65o8lt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
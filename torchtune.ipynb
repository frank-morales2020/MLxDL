{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EMN7yyfFm7H"
      },
      "source": [
        "https://github.com/pytorch/torchtune\n",
        "\n",
        "https://pytorch.org/torchtune/stable/tutorials/e2e_flow.html#e2e-flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUU_iMNH6XCT",
        "outputId": "0e3000aa-4d0e-41e2-8dc0-62078f6d1ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 28 20:50:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   34C    P8              11W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPrRqDF1IdfP"
      },
      "outputs": [],
      "source": [
        "!pip install torchtune -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1D0Y_FwSJVj",
        "outputId": "f531b192-65d3-4b4f-c334-157900f9b50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ],
      "source": [
        "!tune -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6X3PUlxmOSGJ"
      },
      "outputs": [],
      "source": [
        "#meta-llama/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbsklIk4PHMz"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZYsw91hO_us",
        "outputId": "69600f34-a2f6-42c1-f50e-4b86497e752f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kwtSyElRWqGW"
      },
      "outputs": [],
      "source": [
        "#!tune download meta-llama/Llama-2-7b-hf \\\n",
        "#--output-dir /tmp/Llama-2-7b-hf \\\n",
        "#--hf-token <HF_TOKEN> \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6oWyZF-EVN"
      },
      "source": [
        "'HF_TOKEN' is in my Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f0zvfOjDqEy",
        "outputId": "701ba456-0414-4b98-e9d0-f558388c879d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/tune\n"
          ]
        }
      ],
      "source": [
        "!which tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUl8r3cbLwQF"
      },
      "outputs": [],
      "source": [
        "#%mkdir -p /tmp/Llama-2-7b-hf\n",
        "!tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7yxsyvRUXL1",
        "outputId": "884683c7-d0cc-40f1-ea68-766170ca3fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root  138 Apr 28 20:55 /tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin -> ../../root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/blobs/ee62ed2ad7ded505ae47df50bc6c52916860dfb1c009df4715148cc4bfb50d2f\n",
            "lrwxrwxrwx 1 root root  138 Apr 28 20:54 /tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin -> ../../root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/blobs/1fd7762035b3ca4f2d6af6bf10129689a119b7c38058025f9842511532ea02fb\n",
            "-rw-r--r-- 1 root root 489K Apr 28 20:54 /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "-rw-r--r-- 1 root root  776 Apr 28 20:54 /tmp/Llama-2-7b-hf/tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  414 Apr 28 20:54 /tmp/Llama-2-7b-hf/special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  27K Apr 28 20:54 /tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root 1.8M Apr 28 20:54 /tmp/Llama-2-7b-hf/tokenizer.json\n",
            "-rw-r--r-- 1 root root  188 Apr 28 20:54 /tmp/Llama-2-7b-hf/generation_config.json\n",
            "-rw-r--r-- 1 root root 1.2M Apr 28 20:54 /tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
            "-rw-r--r-- 1 root root  27K Apr 28 20:54 /tmp/Llama-2-7b-hf/model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root  22K Apr 28 20:54 /tmp/Llama-2-7b-hf/README.md\n",
            "-rw-r--r-- 1 root root 4.7K Apr 28 20:54 /tmp/Llama-2-7b-hf/USE_POLICY.md\n",
            "-rw-r--r-- 1 root root 6.9K Apr 28 20:54 /tmp/Llama-2-7b-hf/LICENSE.txt\n",
            "-rw-r--r-- 1 root root  609 Apr 28 20:54 /tmp/Llama-2-7b-hf/config.json\n"
          ]
        }
      ],
      "source": [
        "!ls -ltha /tmp/Llama-2-7b-hf/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHOPm_Wqu_vJ"
      },
      "source": [
        "QLoRA has about 75% smaller peak GPU memory usage compared to LoRA. LoRA is about 66% faster than QLoRA in terms of tuning speed. While both methods are relatively inexpensive, LoRA is up to 40% less expensive than QLoRA. Higher max sequence length increases GPU memory consumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exzSiuX2t5QX",
        "outputId": "f23265fb-8931-4b26-bab1-7d5e01f4b9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RECIPE                                   CONFIG                                  \n",
            "full_finetune_single_device              llama2/7B_full_low_memory               \n",
            "                                         llama3/8B_full_single_device            \n",
            "                                         mistral/7B_full_low_memory              \n",
            "full_finetune_distributed                llama2/7B_full                          \n",
            "                                         llama2/13B_full                         \n",
            "                                         llama3/8B_full                          \n",
            "                                         mistral/7B_full                         \n",
            "                                         gemma/2B_full                           \n",
            "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
            "                                         llama2/7B_qlora_single_device           \n",
            "                                         llama3/8B_lora_single_device            \n",
            "                                         llama3/8B_qlora_single_device           \n",
            "                                         llama2/13B_qlora_single_device          \n",
            "                                         mistral/7B_lora_single_device           \n",
            "                                         mistral/7B_qlora_single_device          \n",
            "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
            "lora_finetune_distributed                llama2/7B_lora                          \n",
            "                                         llama2/13B_lora                         \n",
            "                                         llama2/70B_lora                         \n",
            "                                         llama3/8B_lora                          \n",
            "                                         mistral/7B_lora                         \n",
            "generate                                 generation                              \n",
            "eleuther_eval                            eleuther_evaluation                     \n",
            "quantize                                 quantization                            \n"
          ]
        }
      ],
      "source": [
        "!tune ls\n",
        "\n",
        "#RECIPE                                   CONFIG\n",
        "#full_finetune_single_device              llama2/7B_full_low_memory\n",
        "#                                         mistral/7B_full_low_memory\n",
        "#full_finetune_distributed                llama2/7B_full\n",
        "#                                         llama2/13B_full\n",
        "#                                         mistral/7B_full\n",
        "#lora_finetune_single_device              llama2/7B_lora_single_device\n",
        "#                                         llama2/7B_qlora_single_device\n",
        "#                                         mistral/7B_lora_single_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XywjcsOj__LT"
      },
      "outputs": [],
      "source": [
        "#ERROR WITH T4 GPU:  RuntimeError: bf16 precision was requested but not available on this hardware. Please use fp32 precision instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJYIEyyoVifn"
      },
      "source": [
        "https://pytorch.org/torchtune/stable/_modules/torchtune/utils/precision.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu3fJH7JZsQ2"
      },
      "source": [
        "https://pytorch.org/torchtune/stable/tutorials/first_finetune_tutorial.html\n",
        "\n",
        "\n",
        "https://pytorch.org/torchtune/stable/tutorials/e2e_flow.html#e2e-flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye85PGqPF6ah",
        "outputId": "c98af5b9-e7ed-49c3-86f4-6d531e83301a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 2\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  adapter_checkpoint: null\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf\n",
            "  recipe_checkpoint: null\n",
            "compile: false\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
            "  train_on_input: true\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "epochs: 1\n",
            "gradient_accumulation_steps: 64\n",
            "log_every_n_steps: null\n",
            "loss:\n",
            "  _component_: torch.nn.CrossEntropyLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.utils.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/lora_finetune_output\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.lora_llama2_7b\n",
            "  apply_lora_to_mlp: false\n",
            "  apply_lora_to_output: false\n",
            "  lora_alpha: 16\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - v_proj\n",
            "  lora_rank: 8\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  lr: 0.0003\n",
            "  weight_decay: 0.01\n",
            "output_dir: /tmp/lora_finetune_output\n",
            "profiler:\n",
            "  _component_: torchtune.utils.profiler\n",
            "  enabled: false\n",
            "  output_dir: /tmp/lora_finetune_output/torchtune_perf_tracing.json\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 243610558. Local seed is seed + rank = 243610558 + 0\n",
            "Writing logs to /tmp/lora_finetune_output/log_1714337730.txt\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:Memory Stats after model init:\n",
            "{'peak_memory_active': 13.959471616, 'peak_memory_alloc': 13.959471616, 'peak_memory_reserved': 13.97751808}\n",
            "INFO:torchtune.utils.logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils.logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils.logging:Loss is initialized.\n",
            "Downloading readme: 100% 11.6k/11.6k [00:00<00:00, 39.0MB/s]\n",
            "Downloading data: 100% 44.3M/44.3M [00:00<00:00, 49.9MB/s]\n",
            "Generating train split: 100% 51760/51760 [00:00<00:00, 129843.12 examples/s]\n",
            "INFO:torchtune.utils.logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n",
            "1|25880|Loss: 0.6946025490760803: 100% 25880/25880 [4:59:29<00:00,  1.44it/s]\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 9.98 GB saved to /tmp/Llama-2-7b-hf/hf_model_0001_0.pt\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 3.50 GB saved to /tmp/Llama-2-7b-hf/hf_model_0002_0.pt\n",
            "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.01 GB saved to /tmp/Llama-2-7b-hf/adapter_0.pt\n"
          ]
        }
      ],
      "source": [
        "!tune run lora_finetune_single_device --config llama2/7B_lora_single_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mWviV1LYjA5S"
      },
      "outputs": [],
      "source": [
        "#[_checkpointer.py:473] Model checkpoint of size 9.98 GB saved to <checkpoint_dir>/hf_model_0001_0.pt\n",
        "\n",
        "#[_checkpointer.py:473] Model checkpoint of size 3.50 GB saved to <checkpoint_dir>/hf_model_0002_0.pt\n",
        "\n",
        "#[_checkpointer.py:484] Adapter checkpoint of size 0.01 GB saved to <checkpoint_dir>/adapter_0.pt\n",
        "\n",
        "#checkpoint_dir='/tmp/Llama-2-7b-hf/'\n",
        "#/tmp/Llama-2-7b-hf/tokenizer.model\n",
        "#INFO:torchtune.utils.logging:Model checkpoint of size 9.98 GB saved to /tmp/Llama-2-7b-hf/hf_model_0001_0.pt\n",
        "#INFO:torchtune.utils.logging:Model checkpoint of size 3.50 GB saved to /tmp/Llama-2-7b-hf/hf_model_0002_0.pt\n",
        "#INFO:torchtune.utils.logging:Adapter checkpoint of size 0.01 GB saved to /tmp/Llama-2-7b-hf/adapter_0.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXFgoTYGiQWh"
      },
      "outputs": [],
      "source": [
        "#!tune cp eleuther_evaluation ./custom_eval_config.yaml \\\n",
        "\n",
        "!tune cp eleuther_evaluation /content/custom_eval_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "https://huggingface.co/frankmorales2020/Llama-2-7b-hf-text-to-sql-flash-attention-2\n",
        "\n",
        "\n",
        "huggingface-cli upload 'hf-repo-id' 'checkpoint-dir'\n",
        "\n",
        "https://huggingface.co/'hf-repo-id'/tree/main/.\n",
        "\n"
      ],
      "metadata": {
        "id": "LB0b7zxs58wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feHuTTazK-9c",
        "outputId": "df03e5be-7a2f-4944-b31f-dc2ea32d4c4d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for EleutherEvalRecipe in eleuther_eval.py\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"]\n",
            "\n",
            "# Model Arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
            "  checkpoint_files: [\n",
            "    pytorch_model-00001-of-00002.bin,\n",
            "    pytorch_model-00002-of-00002.bin,\n",
            "  ]\n",
            "  recipe_checkpoint: null\n",
            "  output_dir: /tmp/Llama-2-7b-hf\n",
            "  model_type: LLAMA2\n",
            "\n",
            "# Tokenizer\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Environment\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "seed: 217\n",
            "\n",
            "# EleutherAI specific eval args\n",
            "tasks: [\"truthfulqa_mc2\"]\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "\n",
            "# Quantization specific args\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm_eval==0.4.* -q"
      ],
      "metadata": {
        "id": "RdG5vSLOG71d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEFORE tunning\n"
      ],
      "metadata": {
        "id": "wAn2q_oVByy1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFmQRzg2iTUG"
      },
      "outputs": [],
      "source": [
        "#RECIPE                                   CONFIG\n",
        "#eleuther_eval                            eleuther_evaluation\n",
        "\n",
        "#/tmp/Mistral-7B-v0.1/tokenizer_config.json\n",
        "#/tmp/Mistral-7B-v0.1/tokenizer.model\n",
        "# /usr/local/lib/python3.10/dist-packages/torchtune/models/mistral\n",
        "# /usr/local/lib/python3.10/dist-packages/torchtune/models/llama2\n",
        "\n",
        "!tune run eleuther_eval --config /content/custom_eval_config.yaml\n",
        "\n",
        "#checkpointer.checkpoint_dir='/tmp/Llama-2-7b-hf/' \\\n",
        "#tokenizer.path='/tmp/Llama-2-7b-hf'/tokenizer.model\n",
        "\n",
        "#[evaluator.py:324] Running loglikelihood requests\n",
        "#[eleuther_eval.py:195] Eval completed in 121.27 seconds.\n",
        "#[eleuther_eval.py:197] truthfulqa_mc2: {'acc,none': 0.388..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024-04-29:03:49:16,837 INFO     [eleuther_eval.py:198] truthfulqa_mc2: {'acc,none': 0.3891793196860692, 'acc_stderr,none': 0.013564855356631, 'alias': 'truthfulqa_mc2'}"
      ],
      "metadata": {
        "id": "GpqIETjxLwU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFusCDcbyK3f",
        "outputId": "ed33fdce-cf1a-41c2-ece4-4f47e4977b83"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for EleutherEvalRecipe in eleuther_eval.py\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"]\n",
            "\n",
            "# Model Arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
            "  # checkpoint files for the fine-tuned model. This should\n",
            "  # match what's shown in the logs above\n",
            "  checkpoint_files: [\n",
            "        hf_model_0001_0.pt,\n",
            "        hf_model_0002_0.pt,\n",
            "  ]\n",
            "\n",
            "  recipe_checkpoint: null\n",
            "  output_dir: /tmp/Llama-2-7b-hf\n",
            "  model_type: LLAMA2\n",
            "\n",
            "# Tokenizer\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Environment\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "seed: 217\n",
            "\n",
            "# EleutherAI specific eval args\n",
            "tasks: [\"truthfulqa_mc2\"]\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "\n",
            "# Quantization specific args\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AFTER tunning\n",
        "\n"
      ],
      "metadata": {
        "id": "2LjZtr3mBnMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run eleuther_eval --config /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "id": "m-sHUIPtydso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024-04-29:03:58:45,703 INFO     [eleuther_eval.py:198] truthfulqa_mc2: {'acc,none': 0.4786091749231919, 'acc_stderr,none': 0.014549538233703927, 'alias': 'truthfulqa_mc2'}"
      ],
      "metadata": {
        "id": "0Nj8JI3tL3ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tune cp generation /content/custom_generation_config.yaml"
      ],
      "metadata": {
        "id": "-5SuCIzRTV6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_generation_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-vbdQ16WcFH",
        "outputId": "01573f79-134a-4373-e894-51832dc78cd5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run generate --config generation\n",
            "\n",
            "# Model arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  # checkpoint files for the fine-tuned model. This should\n",
            "    # match what's shown in the logs above\n",
            "  checkpoint_files: [\n",
            "        hf_model_0001_0.pt,\n",
            "        hf_model_0002_0.pt,\n",
            "  ]\n",
            "  \n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "  model_type: LLAMA2\n",
            "\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "\n",
            "seed: 1234\n",
            "\n",
            "# Tokenizer arguments\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Generation arguments; defaults taken from gpt-fast\n",
            "prompt: \"Hello, my name is\"\n",
            "max_new_tokens: 300\n",
            "temperature: 0.8 # 0.8 and 0.6 are popular values to try\n",
            "top_k: 512\n",
            "\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ],
      "metadata": {
        "id": "peGH3jzVWphQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024-04-29:04:52:15,979 INFO     [generate.py:123] What are some interesting sites to visit in the Bay Area?\n",
        "What are some interesting sites to visit in the Bay Area?\n",
        "The Bay Area has so many amazing places to visit. Some of my favorites include the Golden Gate Bridge, Alcatraz, Fisherman’s Wharf, and the Palace of Fine Arts. There are also lots of historical sites, beautiful parks, and quirky attractions to check out.\n",
        "The Golden Gate Bridge is one of the most iconic landmarks in the Bay Area. You can walk, bike, or drive across it, and it offers stunning views of the bay. Alcatraz Island is another popular attraction. This former federal prison is now a museum and offers tours of the cells and history of the island. Fisherman’s Wharf is a bustling tourist destination with plenty of shops, restaurants, and attractions. It’s also home to the famous Pier 39, a popular spot for taking pictures and watching the sea lions. The Palace of Fine Arts is a beautiful building with a history of its own. It was originally built for the 1915 Panama-Pacific Exposition and now houses an art museum.\n",
        "The Bay Area is also full of historical sites and attractions. The Presidio of San Francisco is a former military post that has since been converted into a park. The Presidio offers beautiful views of the Golden Gate Bridge and the Pacific Ocean, as well as its\n",
        "\n",
        "2024-04-29:04:52:15,982 INFO     [generate.py:136] Time for inference: 20.16 sec total, 14.88 tokens/sec\n",
        "\n",
        "2024-04-29:04:52:15,982 INFO     [generate.py:139] Bandwidth achieved: 233.52 GB/s\n",
        "\n",
        "2024-04-29:04:52:15,982 INFO     [generate.py:140] Memory used: 15.72 GB"
      ],
      "metadata": {
        "id": "Uas51EXRI6x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/frankmorales2020\n",
        "!huggingface-cli upload torchtune-Llama-2-7b /tmp/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "45EIH_SWDK_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/frankmorales2020/torchtune-Llama-2-7b"
      ],
      "metadata": {
        "id": "AF2Cp26mJWD2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOKgXmKe916thQc8UE7tp2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
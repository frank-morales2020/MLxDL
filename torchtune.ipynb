{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOh9SL4FB3MK76CEBErxiYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pytorch/torchtune"
      ],
      "metadata": {
        "id": "4EMN7yyfFm7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-gci3P4FGSO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pytorch/torchtune.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtune -q"
      ],
      "metadata": {
        "id": "qPrRqDF1IdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune -h"
      ],
      "metadata": {
        "id": "Q1D0Y_FwSJVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/torchtune/torchtune"
      ],
      "metadata": {
        "id": "LXavtnr1JkgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/torchtune/torchtune/_cli/tune.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR8FkQGPMAFX",
        "outputId": "b41583d7-43eb-42ad-dbe6-7d6364d99e66"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#meta-llama/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "6X3PUlxmOSGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "vbsklIk4PHMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZYsw91hO_us",
        "outputId": "03bfdd73-ce0b-49b5-8f87-1bd3de4f1e7d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!tune download meta-llama/Llama-2-7b-hf \\\n",
        "#--output-dir /tmp/Llama-2-7b-hf \\\n",
        "#--hf-token <HF_TOKEN> \\"
      ],
      "metadata": {
        "id": "kwtSyElRWqGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p /tmp/Llama-2-7b-hf\n",
        "!tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "kUl8r3cbLwQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run lora_finetune_single_device --config llama2/7B_lora_single_device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye85PGqPF6ah",
        "outputId": "cdae02e4-3768-4bd4-b028-b4492aafef88"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 2\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  adapter_checkpoint: null\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf\n",
            "  recipe_checkpoint: null\n",
            "compile: false\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
            "  train_on_input: true\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "epochs: 1\n",
            "gradient_accumulation_steps: 64\n",
            "log_every_n_steps: null\n",
            "loss:\n",
            "  _component_: torch.nn.CrossEntropyLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.utils.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/lora_finetune_output\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.lora_llama2_7b\n",
            "  apply_lora_to_mlp: false\n",
            "  apply_lora_to_output: false\n",
            "  lora_alpha: 16\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - v_proj\n",
            "  lora_rank: 8\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  lr: 0.0003\n",
            "  weight_decay: 0.01\n",
            "output_dir: /tmp/lora_finetune_output\n",
            "profiler:\n",
            "  _component_: torchtune.utils.profiler\n",
            "  enabled: false\n",
            "  output_dir: /tmp/lora_finetune_output/torchtune_perf_tracing.json\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1366068135. Local seed is seed + rank = 1366068135 + 0\n",
            "Writing logs to /tmp/lora_finetune_output/log_1714250170.txt\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:Memory Stats after model init:\n",
            "{'peak_memory_active': 13.959471616, 'peak_memory_alloc': 13.959471616, 'peak_memory_reserved': 13.97751808}\n",
            "INFO:torchtune.utils.logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils.logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils.logging:Loss is initialized.\n",
            "Downloading readme: 100% 11.6k/11.6k [00:00<00:00, 36.8MB/s]\n",
            "Downloading data: 100% 44.3M/44.3M [00:00<00:00, 87.4MB/s]\n",
            "Generating train split: 100% 51760/51760 [00:00<00:00, 129798.56 examples/s]\n",
            "INFO:torchtune.utils.logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n",
            "1|25880|Loss: 0.6916711330413818: 100% 25880/25880 [4:57:55<00:00,  1.45it/s]\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 9.98 GB saved to /tmp/Llama-2-7b-hf/hf_model_0001_0.pt\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 3.50 GB saved to /tmp/Llama-2-7b-hf/hf_model_0002_0.pt\n",
            "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.01 GB saved to /tmp/Llama-2-7b-hf/adapter_0.pt\n"
          ]
        }
      ]
    }
  ]
}
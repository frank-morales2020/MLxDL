{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EMN7yyfFm7H"
      },
      "source": [
        "https://github.com/pytorch/torchtune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUU_iMNH6XCT",
        "outputId": "a5094793-a3b7-48bb-c620-ef8314c1f3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Apr 28 04:14:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   44C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-gci3P4FGSO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pytorch/torchtune.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPrRqDF1IdfP"
      },
      "outputs": [],
      "source": [
        "!pip install torchtune -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1D0Y_FwSJVj",
        "outputId": "6badfb75-77a8-4f15-ec69-81f2f9176b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ],
      "source": [
        "!tune -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXavtnr1JkgM"
      },
      "outputs": [],
      "source": [
        "%cd /content/torchtune/torchtune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR8FkQGPMAFX",
        "outputId": "b41583d7-43eb-42ad-dbe6-7d6364d99e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ],
      "source": [
        "!python /content/torchtune/torchtune/_cli/tune.py -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X3PUlxmOSGJ"
      },
      "outputs": [],
      "source": [
        "#meta-llama/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbsklIk4PHMz"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZYsw91hO_us",
        "outputId": "ecbcbe3a-26dd-42d5-b4fc-cf40ad7f58e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwtSyElRWqGW"
      },
      "outputs": [],
      "source": [
        "#!tune download meta-llama/Llama-2-7b-hf \\\n",
        "#--output-dir /tmp/Llama-2-7b-hf \\\n",
        "#--hf-token <HF_TOKEN> \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6oWyZF-EVN"
      },
      "source": [
        "'HF_TOKEN' is in my Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f0zvfOjDqEy",
        "outputId": "d9c99450-c8f8-4e0a-b32b-e6cc8271c37c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/bin/tune\n"
          ]
        }
      ],
      "source": [
        "!which tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUl8r3cbLwQF"
      },
      "outputs": [],
      "source": [
        "#%mkdir -p /tmp/Llama-2-7b-hf\n",
        "#!tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf\n",
        "\n",
        "!tune download mistralai/Mistral-7B-v0.1 --output-dir /tmp/Mistral-7B-v0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7yxsyvRUXL1",
        "outputId": "f2a10279-128b-44c1-806c-5b2f0b508d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lrwxrwxrwx 1 root root  139 Apr 28 08:15 /tmp/Mistral-7B-v0.1/pytorch_model-00001-of-00002.bin -> ../../root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/blobs/67b1ea77d83cf017d6aa2fd9aadc6ad043a0cb3233a1cf9c422916b88350991f\n",
            "lrwxrwxrwx 1 root root  139 Apr 28 08:15 /tmp/Mistral-7B-v0.1/pytorch_model-00002-of-00002.bin -> ../../root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/blobs/1feecce04754087e8e9a320847916ed57c6539ed0e5e2cd0ebdc7a816cc3773e\n",
            "-rw-r--r-- 1 root root 1.8M Apr 28 08:15 /tmp/Mistral-7B-v0.1/tokenizer.json\n",
            "-rw-r--r-- 1 root root 482K Apr 28 08:15 /tmp/Mistral-7B-v0.1/tokenizer.model\n",
            "-rw-r--r-- 1 root root  967 Apr 28 08:15 /tmp/Mistral-7B-v0.1/tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   72 Apr 28 08:15 /tmp/Mistral-7B-v0.1/special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 1.4K Apr 28 08:15 /tmp/Mistral-7B-v0.1/README.md\n",
            "-rw-r--r-- 1 root root  116 Apr 28 08:15 /tmp/Mistral-7B-v0.1/generation_config.json\n",
            "-rw-r--r-- 1 root root  25K Apr 28 08:15 /tmp/Mistral-7B-v0.1/model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root  24K Apr 28 08:15 /tmp/Mistral-7B-v0.1/pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root  571 Apr 28 08:15 /tmp/Mistral-7B-v0.1/config.json\n",
            "-rw-r--r-- 1 root root    0 Apr 28 08:06 /tmp/Mistral-7B-v0.1/log_1714291582.txt\n"
          ]
        }
      ],
      "source": [
        "#!ls -ltha /tmp/Llama-2-7b-hf/*\n",
        "\n",
        "!ls -ltha /tmp/Mistral-7B-v0.1/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHOPm_Wqu_vJ"
      },
      "source": [
        "QLoRA has about 75% smaller peak GPU memory usage compared to LoRA. LoRA is about 66% faster than QLoRA in terms of tuning speed. While both methods are relatively inexpensive, LoRA is up to 40% less expensive than QLoRA. Higher max sequence length increases GPU memory consumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exzSiuX2t5QX",
        "outputId": "05bd7953-39d8-4ee8-b536-4594b89ef3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RECIPE                                   CONFIG                                  \n",
            "full_finetune_single_device              llama2/7B_full_low_memory               \n",
            "                                         llama3/8B_full_single_device            \n",
            "                                         mistral/7B_full_low_memory              \n",
            "full_finetune_distributed                llama2/7B_full                          \n",
            "                                         llama2/13B_full                         \n",
            "                                         llama3/8B_full                          \n",
            "                                         mistral/7B_full                         \n",
            "                                         gemma/2B_full                           \n",
            "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
            "                                         llama2/7B_qlora_single_device           \n",
            "                                         llama3/8B_lora_single_device            \n",
            "                                         llama3/8B_qlora_single_device           \n",
            "                                         llama2/13B_qlora_single_device          \n",
            "                                         mistral/7B_lora_single_device           \n",
            "                                         mistral/7B_qlora_single_device          \n",
            "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
            "lora_finetune_distributed                llama2/7B_lora                          \n",
            "                                         llama2/13B_lora                         \n",
            "                                         llama2/70B_lora                         \n",
            "                                         llama3/8B_lora                          \n",
            "                                         mistral/7B_lora                         \n",
            "generate                                 generation                              \n",
            "eleuther_eval                            eleuther_evaluation                     \n",
            "quantize                                 quantization                            \n"
          ]
        }
      ],
      "source": [
        "!tune ls\n",
        "\n",
        "#RECIPE                                   CONFIG\n",
        "#full_finetune_single_device              llama2/7B_full_low_memory\n",
        "#                                         mistral/7B_full_low_memory\n",
        "#full_finetune_distributed                llama2/7B_full\n",
        "#                                         llama2/13B_full\n",
        "#                                         mistral/7B_full\n",
        "#lora_finetune_single_device              llama2/7B_lora_single_device\n",
        "#                                         llama2/7B_qlora_single_device\n",
        "#                                         mistral/7B_lora_single_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XywjcsOj__LT"
      },
      "outputs": [],
      "source": [
        "#ERROR WITH T4 GPU:  RuntimeError: bf16 precision was requested but not available on this hardware. Please use fp32 precision instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJYIEyyoVifn"
      },
      "source": [
        "https://pytorch.org/torchtune/stable/_modules/torchtune/utils/precision.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu3fJH7JZsQ2"
      },
      "source": [
        "https://pytorch.org/torchtune/stable/tutorials/first_finetune_tutorial.html\n",
        "\n",
        "\n",
        "https://pytorch.org/torchtune/stable/tutorials/e2e_flow.html#e2e-flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye85PGqPF6ah",
        "outputId": "37d61d55-d52f-47e8-f833-0528a43901f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 4\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Mistral-7B-v0.1\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: MISTRAL\n",
            "  output_dir: /tmp/Mistral-7B-v0.1\n",
            "  recipe_checkpoint: null\n",
            "compile: false\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_dataset\n",
            "  train_on_input: true\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "epochs: 3\n",
            "gradient_accumulation_steps: 4\n",
            "log_every_n_steps: null\n",
            "loss:\n",
            "  _component_: torch.nn.CrossEntropyLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.utils.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/Mistral-7B-v0.1\n",
            "model:\n",
            "  _component_: torchtune.models.mistral.lora_mistral_7b\n",
            "  apply_lora_to_mlp: true\n",
            "  apply_lora_to_output: true\n",
            "  lora_alpha: 16\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - k_proj\n",
            "  - v_proj\n",
            "  lora_rank: 64\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  lr: 2.0e-05\n",
            "output_dir: /tmp/Mistral-7B-v0.1\n",
            "profiler:\n",
            "  _component_: torchtune.utils.profiler\n",
            "  enabled: false\n",
            "  output_dir: /tmp/alpaca-llama2-finetune/torchtune_perf_tracing.json\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
            "  path: /tmp/Mistral-7B-v0.1/tokenizer.model\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 2051386828. Local seed is seed + rank = 2051386828 + 0\n",
            "Writing logs to /tmp/Mistral-7B-v0.1/log_1714292225.txt\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:Memory Stats after model init:\n",
            "{'peak_memory_active': 15.789400576, 'peak_memory_alloc': 15.789400576, 'peak_memory_reserved': 15.797846016}\n",
            "INFO:torchtune.utils.logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils.logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils.logging:Loss is initialized.\n",
            "Downloading readme: 100% 7.47k/7.47k [00:00<00:00, 21.7MB/s]\n",
            "Downloading data: 100% 24.2M/24.2M [00:00<00:00, 35.6MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 345506.76 examples/s]\n",
            "INFO:torchtune.utils.logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n",
            "1|13001|Loss: 1.1534013748168945: 100% 13001/13001 [3:33:45<00:00,  1.01it/s]\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 9.94 GB saved to /tmp/Mistral-7B-v0.1/hf_model_0001_0.pt\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 4.54 GB saved to /tmp/Mistral-7B-v0.1/hf_model_0002_0.pt\n",
            "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.31 GB saved to /tmp/Mistral-7B-v0.1/adapter_0.pt\n",
            "INFO:torchtune.utils.logging:Recipe checkpoint of size 0.61 GB saved to /tmp/Mistral-7B-v0.1/recipe_state.pt\n",
            "2|13001|Loss: 0.5873649716377258: 100% 13001/13001 [3:32:57<00:00,  1.02it/s]\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 9.94 GB saved to /tmp/Mistral-7B-v0.1/hf_model_0001_1.pt\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 4.54 GB saved to /tmp/Mistral-7B-v0.1/hf_model_0002_1.pt\n",
            "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.31 GB saved to /tmp/Mistral-7B-v0.1/adapter_1.pt\n",
            "INFO:torchtune.utils.logging:Recipe checkpoint of size 0.61 GB saved to /tmp/Mistral-7B-v0.1/recipe_state.pt\n",
            "3|13001|Loss: 0.668715238571167: 100% 13001/13001 [3:33:25<00:00,  1.02it/s]\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 9.94 GB saved to /tmp/Mistral-7B-v0.1/hf_model_0001_2.pt\n",
            "INFO:torchtune.utils.logging:Model checkpoint of size 4.54 GB saved to /tmp/Mistral-7B-v0.1/hf_model_0002_2.pt\n",
            "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.31 GB saved to /tmp/Mistral-7B-v0.1/adapter_2.pt\n"
          ]
        }
      ],
      "source": [
        "#!tune run lora_finetune_single_device --config llama2/7B_lora_single_device\n",
        "\n",
        "!tune run lora_finetune_single_device --config mistral/7B_lora_single_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWviV1LYjA5S"
      },
      "outputs": [],
      "source": [
        "#[_checkpointer.py:473] Model checkpoint of size 9.98 GB saved to <checkpoint_dir>/hf_model_0001_0.pt\n",
        "\n",
        "#[_checkpointer.py:473] Model checkpoint of size 3.50 GB saved to <checkpoint_dir>/hf_model_0002_0.pt\n",
        "\n",
        "#[_checkpointer.py:484] Adapter checkpoint of size 0.01 GB saved to <checkpoint_dir>/adapter_0.pt\n",
        "\n",
        "#checkpoint_dir='/tmp/Llama-2-7b-hf/'\n",
        "#/tmp/Llama-2-7b-hf/tokenizer.model\n",
        "#INFO:torchtune.utils.logging:Model checkpoint of size 9.98 GB saved to /tmp/Llama-2-7b-hf/hf_model_0001_0.pt\n",
        "#INFO:torchtune.utils.logging:Model checkpoint of size 3.50 GB saved to /tmp/Llama-2-7b-hf/hf_model_0002_0.pt\n",
        "#INFO:torchtune.utils.logging:Adapter checkpoint of size 0.01 GB saved to /tmp/Llama-2-7b-hf/adapter_0.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EXFgoTYGiQWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82dd625-8604-44fd-f43f-a9f87ac3638f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied file to /content/custom_eval_config.yaml\n"
          ]
        }
      ],
      "source": [
        "#!tune cp eleuther_evaluation ./custom_eval_config.yaml \\\n",
        "\n",
        "!tune cp eleuther_evaluation /content/custom_eval_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "https://huggingface.co/frankmorales2020/Llama-2-7b-hf-text-to-sql-flash-attention-2\n",
        "\n",
        "\n",
        "huggingface-cli upload 'hf-repo-id' 'checkpoint-dir'\n",
        "\n",
        "https://huggingface.co/'hf-repo-id'/tree/main/.\n",
        "\n"
      ],
      "metadata": {
        "id": "LB0b7zxs58wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feHuTTazK-9c",
        "outputId": "94816bea-ad2e-41cd-85ef-26045c868bb5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for EleutherEvalRecipe in eleuther_eval.py\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"]\n",
            "\n",
            "# /usr/local/lib/python3.10/dist-packages/torchtune/models/mistral\n",
            "# /usr/local/lib/python3.10/dist-packages/torchtune/models/llama2\n",
            "\n",
            "# /tmp/Mistral-7B-v0.1/pytorch_model-00001-of-00002.bin\n",
            "# /tmp/Mistral-7B-v0.1/tokenizer.model\n",
            "\n",
            "# Model Arguments\n",
            "model:\n",
            "   _component_: torchtune.models.mistral.mistral_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Mistral-7B-v0.1/\n",
            "  checkpoint_files: [\n",
            "    pytorch_model-00001-of-00002.bin,\n",
            "    pytorch_model-00002-of-00002.bin,\n",
            "  ]\n",
            "  recipe_checkpoint: null\n",
            "  output_dir: /tmp/Mistral-7B-v0.1/\n",
            "  model_type: MISTRAL\n",
            "\n",
            "# Tokenizer\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
            "  path: /tmp/Mistral-7B-v0.1/tokenizer.model\n",
            "\n",
            "\n",
            "# Environment\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "seed: 217\n",
            "\n",
            "# EleutherAI specific eval args\n",
            "tasks: [\"truthfulqa_mc2\"]\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "\n",
            "# Quantization specific args\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm_eval==0.4.* -q"
      ],
      "metadata": {
        "id": "RdG5vSLOG71d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFmQRzg2iTUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4069dcb9-5e04-414c-fa85-a28f1886c346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-28 20:01:26.615603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-28 20:01:26.615672: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-28 20:01:26.620506: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-28 20:01:27.796575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-04-28:20:01:30,549 INFO     [_utils.py:34] Running EleutherEvalRecipe with resolved config:\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Mistral-7B-v0.1/\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: MISTRAL\n",
            "  output_dir: /tmp/Mistral-7B-v0.1/\n",
            "  recipe_checkpoint: null\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "model:\n",
            "  _component_: torchtune.models.mistral.mistral_7b\n",
            "quantizer: null\n",
            "seed: 217\n",
            "tasks:\n",
            "- truthfulqa_mc2\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
            "  path: /tmp/Mistral-7B-v0.1/tokenizer.model\n",
            "\n",
            "2024-04-28:20:01:30,677 DEBUG    [seed.py:59] Setting manual seed to local seed 217. Local seed is seed + rank = 217 + 0\n",
            "2024-04-28:20:01:35,232 INFO     [eleuther_eval.py:168] Model is initialized with precision torch.bfloat16.\n",
            "2024-04-28:20:01:35,247 INFO     [eleuther_eval.py:152] Tokenizer is initialized from file.\n",
            "2024-04-28:20:01:35,695 INFO     [huggingface.py:162] Using device 'cuda:0'\n",
            "2024-04-28:20:01:44,689 INFO     [eleuther_eval.py:189] Running evaluation on ['truthfulqa_mc2'] tasks.\n",
            "2024-04-28:20:01:44,690 INFO     [task.py:395] Building contexts for truthfulqa_mc2 on rank 0...\n",
            "100% 817/817 [00:01<00:00, 675.36it/s]\n",
            "2024-04-28:20:01:45,971 INFO     [evaluator.py:362] Running loglikelihood requests\n",
            "Running loglikelihood requests:  47% 2785/5882 [03:53<04:07, 12.49it/s]"
          ]
        }
      ],
      "source": [
        "#RECIPE                                   CONFIG\n",
        "#eleuther_eval                            eleuther_evaluation\n",
        "\n",
        "#/tmp/Mistral-7B-v0.1/tokenizer_config.json\n",
        "#/tmp/Mistral-7B-v0.1/tokenizer.model\n",
        "# /usr/local/lib/python3.10/dist-packages/torchtune/models/mistral\n",
        "# /usr/local/lib/python3.10/dist-packages/torchtune/models/llama2\n",
        "\n",
        "!tune run eleuther_eval --config /content/custom_eval_config.yaml checkpointer.checkpoint_dir='/tmp/Mistral-7B-v0.1/' \\\n",
        "tokenizer.path='/tmp/Mistral-7B-v0.1'/tokenizer.model\n",
        "\n",
        "#[evaluator.py:324] Running loglikelihood requests\n",
        "#[eleuther_eval.py:195] Eval completed in 121.27 seconds.\n",
        "#[eleuther_eval.py:197] truthfulqa_mc2: {'acc,none': 0.388..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tune cp generation /content/custom_generation_config.yaml"
      ],
      "metadata": {
        "id": "-5SuCIzRTV6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOPE4lJHasj2OXHhAa8bHBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
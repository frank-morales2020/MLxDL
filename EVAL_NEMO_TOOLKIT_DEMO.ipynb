{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1So2NgolBn6UsSGglq8bjEO5cdNOTV5Lm",
      "authorship_tag": "ABX9TyP+X3Nza0Bg2KpZN0qcX536",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/EVAL_NEMO_TOOLKIT_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NVIDIA/NeMo-Agent-Toolkit/blob/develop/docs/source/improve-workflows/evaluate.md"
      ],
      "metadata": {
        "id": "gBm1lFxMsxLW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s4yNTaXpsT_p"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/NVIDIA/NeMo-Agent-Toolkit.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "OvNLxwaUuF9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd131d66-f1c6-471c-bbf5-621e68c95c4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/tools/NeMo-Agent-Toolkit/examples/evaluation_and_profiling/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_q7HmOe6DBW",
        "outputId": "73b9e7a8-508c-49c3-dee4-56b6eda210a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "email_phishing_analyzer  simple_web_query_eval\n",
            "simple_calculator_eval\t swe_bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/NeMo-Agent-Toolkit/examples/"
      ],
      "metadata": {
        "id": "JW4NWkOsJtoq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -pr  /content/drive/MyDrive/tools/NeMo-Agent-Toolkit/examples/evaluation_and_profiling   /content/NeMo-Agent-Toolkit/examples/"
      ],
      "metadata": {
        "id": "emuwQuwkJkSb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/NeMo-Agent-Toolkit/examples/evaluation_and_profiling/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAGq1fmY61Pr",
        "outputId": "357b884f-1b1c-4105-ebbd-31113218d3bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "email_phishing_analyzer  simple_web_query_eval\n",
            "simple_calculator_eval\t swe_bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvidia-nat -q"
      ],
      "metadata": {
        "id": "NJohl2l1s8A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install \"nvidia-nat[all]\""
      ],
      "metadata": {
        "id": "o2FNHeR-tFSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nat --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8naRVl-ltWmd",
        "outputId": "1f8892f6-a790-4800-f9ff-9e594dcf6d58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nat, version 1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('NVIDIA_API_KEY')\n",
        "import os\n",
        "os.environ['NVIDIA_API_KEY'] = api_key\n",
        "\n",
        "api_key_TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ['TAVILY_API_KEY'] = api_key_TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "1Kf-BVgIuKt3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/data/workflow.yml"
      ],
      "metadata": {
        "id": "K6mMvrB62PIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd02e5c-2112-4abd-aef4-54b8f1c68321"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "functions:\n",
            "   # Add a tool to search wikipedia\n",
            "   wikipedia_search:\n",
            "      _type: wiki_search\n",
            "      max_results: 2\n",
            "\n",
            "llms:\n",
            "   # Tell NeMo Agent Toolkit which LLM to use for the agent\n",
            "   nim_llm:\n",
            "      _type: nim\n",
            "      model_name: meta/llama-3.1-70b-instruct\n",
            "      temperature: 0.0\n",
            "\n",
            "workflow:\n",
            "   # Use an agent that 'reasons' and 'acts'\n",
            "   _type: react_agent\n",
            "   # Give it access to our wikipedia search tool\n",
            "   tool_names: [wikipedia_search]\n",
            "   # Tell it which LLM to use\n",
            "   llm_name: nim_llm\n",
            "   # Make it verbose\n",
            "   verbose: true\n",
            "   # Retry up to 3 times\n",
            "   parse_agent_response_max_retries: 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/NeMo-Agent-Toolkit/"
      ],
      "metadata": {
        "id": "S4U4zu4L_SE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a0af95-a7eb-4cf8-d696-62c856a0835d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NeMo-Agent-Toolkit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating a Workflow"
      ],
      "metadata": {
        "id": "INFDSLeqtwtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "# Initialize only what you need (e.g., fonts for text processing)\n",
        "pygame.font.init()\n",
        "# Do NOT call pygame.init() or pygame.mixer.init()\n",
        "\n",
        "import os\n",
        "# Force SDL to use a dummy video and audio driver\n",
        "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
        "os.environ['SDL_AUDIODRIVER'] = 'dummy'\n",
        "\n",
        "# Fix the XDG_RUNTIME_DIR warning by creating a temp directory\n",
        "os.environ['XDG_RUNTIME_DIR'] = '/tmp/runtime-root'\n",
        "if not os.path.exists('/tmp/runtime-root'):\n",
        "    os.makedirs('/tmp/runtime-root', mode=0o700)\n",
        "\n",
        "\n",
        "!export XDG_RUNTIME_DIR=/tmp/runtime-root && mkdir -p /tmp/runtime-root && chmod 700 /tmp/runtime-root"
      ],
      "metadata": {
        "id": "GyD6Q-am0Kfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick verification - check if keys exist in the environment\n",
        "nv_key = os.getenv('NVIDIA_API_KEY')\n",
        "tv_key = os.getenv('TAVILY_API_KEY')\n",
        "\n",
        "if nv_key and tv_key:\n",
        "    print(\"✅ Success: Both NVIDIA and Tavily keys are successfully set in the environment.\")\n",
        "else:\n",
        "    if not nv_key: print(\"❌ Error: NVIDIA_API_KEY is missing.\")\n",
        "    if not tv_key: print(\"❌ Error: TAVILY_API_KEY is missing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR2kWQ0X3CiF",
        "outputId": "2c9f981e-9720-47e7-b2ff-35e39794b806"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Success: Both NVIDIA and Tavily keys are successfully set in the environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nat eval --config_file=/content/NeMo-Agent-Toolkit/examples/evaluation_and_profiling/simple_web_query_eval/configs/eval_config.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "612KFBwWtyVE",
        "outputId": "65ae70ce-a382-4bc3-9648-742fa28f39c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "2026-01-20 03:35:02 - WARNING  - py.warnings:112 - /usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "2026-01-20 03:35:02 - WARNING  - py.warnings:112 - /usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "\n",
            "2026-01-20 03:35:06 - INFO     - nat.eval.evaluate:446 - Starting evaluation run with config file: /content/NeMo-Agent-Toolkit/examples/evaluation_and_profiling/simple_web_query_eval/configs/eval_config.yml\n",
            "2026-01-20 03:35:14 - INFO     - datasets:112 - TensorFlow version 2.19.0 available.\n",
            "2026-01-20 03:35:14 - INFO     - datasets:125 - JAX version 0.7.2 available.\n",
            "Running workflow:   0% 0/5 [00:00<?, ?it/s]2026-01-20 03:40:15 - ERROR    - nat.agent.react_agent.agent:241 - [AGENT] Failed to call agent_node: \n",
            "2026-01-20 03:40:15 - ERROR    - nat.agent.react_agent.register:165 - [AGENT] ReAct Agent failed with exception: \n",
            "2026-01-20 03:40:15 - ERROR    - nat.builder.function:162 - Error with ainvoke in function with input: What are the main features of LangSmith?. Error: \n",
            "2026-01-20 03:40:15 - ERROR    - nat.runtime.runner:199 - Error running workflow: \n",
            "2026-01-20 03:40:15 - ERROR    - nat.eval.evaluate:181 - Failed to run the workflow: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aiohttp/streams.py\", line 371, in _wait\n",
            "    await waiter\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 197, in result\n",
            "    raise self._make_cancelled_error()\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/eval/evaluate.py\", line 174, in run_one\n",
            "    base_output = await runner_result\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/runtime/runner.py\", line 175, in result\n",
            "    result = await self._entry_fn.ainvoke(self._input_message, to_type=to_type)  # type: ignore\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/builder/function.py\", line 153, in ainvoke\n",
            "    result = await self._ainvoke(converted_input)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/builder/function.py\", line 329, in _ainvoke\n",
            "    return await self._ainvoke_fn(value)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/agent/react_agent/register.py\", line 145, in _response_fn\n",
            "    state = await graph.ainvoke(state, config={'recursion_limit': (config.max_tool_calls + 1) * 2})\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 3171, in ainvoke\n",
            "    async for chunk in self.astream(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 2993, in astream\n",
            "    async for _ in runner.atick(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\", line 295, in atick\n",
            "    await arun_with_retry(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\", line 137, in arun_with_retry\n",
            "    return await task.proc.ainvoke(task.input, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 706, in ainvoke\n",
            "    input = await asyncio.create_task(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 474, in ainvoke\n",
            "    ret = await self.afunc(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/agent/react_agent/agent.py\", line 160, in agent_node\n",
            "    output_message = await self._stream_llm(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/agent/base.py\", line 106, in _stream_llm\n",
            "    async for event in runnable.astream(inputs, config=config):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3685, in astream\n",
            "    async for chunk in self.atransform(input_aiter(), config, **kwargs):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3667, in atransform\n",
            "    async for chunk in self._atransform_stream_with_config(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 2486, in _atransform_stream_with_config\n",
            "    chunk = await coro_with_context(py_anext(iterator), context)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3634, in _atransform\n",
            "    async for output in final_pipeline:\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 5949, in atransform\n",
            "    async for item in self.bound.atransform(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/utils/exception_handlers/automatic_retries.py\", line 205, in _agen_with_retry\n",
            "    async for item in fn(*call_args, **call_kwargs):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 1634, in atransform\n",
            "    async for output in self.astream(final, config, **kwargs):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nat/utils/exception_handlers/automatic_retries.py\", line 197, in _agen_with_retry\n",
            "    async for item in fn(*args, **kw):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 615, in astream\n",
            "    async for chunk in self._astream(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_nvidia_ai_endpoints/chat_models.py\", line 664, in _astream\n",
            "    async for response in self._client.aget_req_stream(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_nvidia_ai_endpoints/_common.py\", line 798, in aget_req_stream\n",
            "    line = await reader.readline()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aiohttp/streams.py\", line 376, in readline\n",
            "    return await self.readuntil()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aiohttp/streams.py\", line 410, in readuntil\n",
            "    await self._wait(\"readuntil\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aiohttp/streams.py\", line 370, in _wait\n",
            "    with self._timer:\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aiohttp/helpers.py\", line 713, in __exit__\n",
            "    raise asyncio.TimeoutError from exc_val\n",
            "TimeoutError\n",
            "During task with name 'agent' and id 'a664a106-9054-56f9-6ca0-9963b863552d'\n",
            "Running workflow:  20% 1/5 [05:00<20:00, 300.23s/it]2026-01-20 03:40:15 - ERROR    - asyncio:1833 - Task exception was never retrieved\n",
            "future: <Task finished name='Task-28' coro=<Runner.result() done, defined at /usr/local/lib/python3.12/dist-packages/nat/runtime/runner.py:133> exception=RuntimeError('cannot reuse already awaited coroutine')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
            "    result = coro.throw(exc)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "RuntimeError: cannot reuse already awaited coroutine\n",
            "2026-01-20 03:42:22 - INFO     - nat.agent.react_agent.agent:169 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[33mAgent input: How does LangSmith help with LLM application observability?\n",
            "\u001b[36mAgent's thoughts: \n",
            "Thought: I now know the final answer \n",
            "\n",
            "Final Answer: LangSmith helps with LLM application observability by providing tools and features that allow developers to monitor, track, and analyze the performance and behavior of their LLM applications.\u001b[39m\n",
            "------------------------------\n",
            "Running workflow:  40% 2/5 [07:07<09:55, 198.40s/it]2026-01-20 03:42:31 - INFO     - nat.agent.react_agent.agent:169 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[33mAgent input: What is the purpose of LangSmith's evaluation features?\n",
            "\u001b[36mAgent's thoughts: \n",
            "Thought: I do not have information about the previous conversation history. \n",
            "\n",
            "However, I can try to answer the question based on my general knowledge. \n",
            "\n",
            "The purpose of LangSmith's evaluation features is likely to assess the performance of language models, such as myself, in various tasks and provide feedback for improvement. These features may include metrics such as accuracy, fluency, and coherence, as well as tools for testing and evaluating language understanding and generation capabilities.\n",
            "\n",
            "However, I do not have specific information about LangSmith's evaluation features, and my answer is based on general knowledge about language model evaluation.\n",
            "\n",
            "If you need more specific information, please provide more context or details about LangSmith's evaluation features.\n",
            "\n",
            "Final Answer: The purpose of LangSmith's evaluation features is likely to assess the performance of language models and provide feedback for improvement.\u001b[39m\n",
            "------------------------------\n",
            "Running workflow:  60% 3/5 [07:15<03:43, 111.73s/it]2026-01-20 03:42:33 - INFO     - nat.agent.react_agent.agent:169 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[33mAgent input: How does LangSmith support prompt engineering?\n",
            "\u001b[36mAgent's thoughts: \n",
            "Thought: I do not have enough information to answer the question about LangSmith's support for prompt engineering. \n",
            "\n",
            "Action: langsmith_search\n",
            "Action Input: {'question': \"LangSmith prompt engineering support\"}\u001b[39m\n",
            "------------------------------\n",
            "2026-01-20 03:42:34 - INFO     - nat.agent.base:221 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[37mCalling tools: langsmith_search\n",
            "\u001b[33mTool's input: {'question': 'LangSmith prompt engineering support'}\n",
            "\u001b[36mTool's response: \n",
            "<Document href=\"https://mirascope.com/blog/langsmith-prompt-management\"/>\n",
            "LangSmith’s prompt management system helps teams version, test, and evaluate LLM prompts, especially within LangChain-based ecosystems. It versions the entire function, not just the prompt, including logic, models, parameters, and structured return types. You upload these datasets into LangSmith, where they’re used to run repeatable evaluations across different prompt versions. Lilypad is an open source prompt engineering framework that supports LLM integration and treats prompts as code (specifically, as typed, versioned Python functions) rather than as files in a content management system. Lilypad automatically snapshots the exact code: prompt logic, inputs, parameters, and even return types, every time a function runs. This means every prompt you run is captured as a full versioned trace, complete with metadata: the prompt, parameters, model provider, and output. With typed functions, automatic versioning, ric...(rest of response truncated)\u001b[39m\n",
            "------------------------------\n",
            "2026-01-20 03:42:44 - INFO     - nat.agent.react_agent.agent:193 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[33mAgent input: How does LangSmith support prompt engineering?\n",
            "\u001b[36mAgent's thoughts: \n",
            "Thought: I now know the final answer \n",
            "\n",
            "Final Answer: LangSmith supports prompt engineering by providing a prompt management system that allows teams to version, test, and evaluate LLM prompts. It versions the entire function, including logic, models, parameters, and structured return types, and allows for repeatable evaluations across different prompt versions. LangSmith also integrates with Lilypad, an open-source prompt engineering framework that treats prompts as code and provides automatic versioning, rich trace metadata, and human-in-the-loop LLM evaluation. Additionally, LangSmith provides tools and features such as the Prompt Playground, Polly, and the public prompt hub to help users create, manage, and optimize their prompts.\u001b[39m\n",
            "------------------------------\n",
            "Running workflow:  80% 4/5 [07:29<01:12, 72.95s/it] 2026-01-20 03:42:47 - INFO     - nat.agent.react_agent.agent:169 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[33mAgent input: Is LangSmith framework-specific or can it be used with any LLM application?\n",
            "\u001b[36mAgent's thoughts: \n",
            "Thought: I do not know the answer to this question, I need to search the web to find relevant information.\n",
            "\n",
            "Action: langsmith_search\n",
            "Action Input: {'question': 'Is LangSmith framework-specific or can it be used with any LLM application?'}\u001b[39m\n",
            "------------------------------\n",
            "2026-01-20 03:42:48 - INFO     - nat.agent.base:221 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[37mCalling tools: langsmith_search\n",
            "\u001b[33mTool's input: {'question': 'Is LangSmith framework-specific or can it be used with any LLM application?'}\n",
            "\u001b[36mTool's response: \n",
            "<Document href=\"https://www.ibm.com/think/topics/langsmith\"/>\n",
            "While LangChain helps you build workflows, LangSmith helps ensure that they run smoothly by offering tools for debugging, monitoring and managing complex AI systems. LangSmith operates by embedding itself into the LLM application stack, whether you're using LangChain or building custom pipelines to provide visibility, traceability and control at every stage of development and production. LangSmith provides rich visualization tools to display LLM call traces, helping developers understand and debug complex workflows easily. LangSmith works through a simple Python SDK that helps developers build and manage AI applications easily. Factory, a company building AI agents to automate the Software Development Lifecycle (SDLC), uses LangSmith to help ensure secure, reliable LLM operations in enterprise environments. LangChain is well suited for designing and prototyping complex language model workflows, enabling seamless integration ...(rest of response truncated)\u001b[39m\n",
            "------------------------------\n",
            "2026-01-20 03:42:54 - INFO     - nat.agent.react_agent.agent:193 - \n",
            "------------------------------\n",
            "[AGENT]\n",
            "\u001b[33mAgent input: Is LangSmith framework-specific or can it be used with any LLM application?\n",
            "\u001b[36mAgent's thoughts: \n",
            "Thought: I now know the final answer \n",
            "\n",
            "Final Answer: LangSmith is not framework-specific and can be used with any LLM application. It provides tools for debugging, monitoring, and managing complex AI systems, and can be used with or without LangChain. LangSmith operates by embedding itself into the LLM application stack, and provides rich visualization tools to display LLM call traces, helping developers understand and debug complex workflows easily. It also works through a simple Python SDK that helps developers build and manage AI applications easily.\u001b[39m\n",
            "------------------------------\n",
            "Running workflow: 100% 5/5 [07:39<00:00, 91.81s/it]\n",
            "Evaluating Ragas nv_accuracy:   0% 0/5 [00:00<?, ?it/s]\n",
            "Evaluating Ragas nv_response_groundedness:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Evaluating Ragas nv_context_relevance:   0% 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Evaluating Trajectory:   0% 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "Evaluating Ragas nv_response_groundedness:  20% 1/5 [00:00<00:02,  1.39it/s]\u001b[A\n",
            "\n",
            "Evaluating Ragas nv_accuracy:  20% 1/5 [00:02<00:10,  2.54s/it]\n",
            "\n",
            "Evaluating Ragas nv_context_relevance:  40% 2/5 [00:01<00:03,  1.02s/it]\u001b[A\u001b[A2026-01-20 03:42:57 - INFO     - nat.utils.exception_handlers.automatic_retries:103 - Retrying on exception [429] Too Many Requests\n",
            "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
            "{'status': 429, 'title': 'too many requests'}\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Ragas nv_accuracy:  40% 2/5 [00:03<00:04,  1.44s/it]2026-01-20 03:42:57 - INFO     - nat.utils.exception_handlers.automatic_retries:103 - Retrying on exception [429] Too Many Requests\n",
            "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
            "{'status': 429, 'title': 'too many requests'}\n",
            "\n",
            "\n",
            "Evaluating Ragas nv_context_relevance:  60% 3/5 [00:02<00:02,  1.02s/it]\u001b[A\u001b[A2026-01-20 03:42:58 - INFO     - nat.utils.exception_handlers.automatic_retries:103 - Retrying on exception [429] Too Many Requests\n",
            "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
            "{'status': 429, 'title': 'too many requests'}\n",
            "Evaluating Ragas nv_accuracy:  60% 3/5 [00:04<00:02,  1.39s/it]\n",
            "\n",
            "Evaluating Ragas nv_context_relevance:  80% 4/5 [00:04<00:01,  1.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Evaluating Trajectory:  40% 2/5 [00:03<00:05,  1.86s/it]\u001b[A\u001b[A\u001b[A2026-01-20 03:42:59 - INFO     - nat.utils.exception_handlers.automatic_retries:103 - Retrying on exception [429] Too Many Requests\n",
            "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
            "{'status': 429, 'title': 'too many requests'}\n",
            "Evaluating Ragas nv_accuracy:  80% 4/5 [00:05<00:01,  1.10s/it]2026-01-20 03:42:59 - INFO     - nat.utils.exception_handlers.automatic_retries:103 - Retrying on exception [429] Too Many Requests\n",
            "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
            "{'status': 429, 'title': 'too many requests'}\n",
            "2026-01-20 03:42:59 - INFO     - nat.utils.exception_handlers.automatic_retries:103 - Retrying on exception [429] Too Many Requests\n",
            "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
            "{'status': 429, 'title': 'too many requests'}\n",
            "\n",
            "\n",
            "Evaluating Ragas nv_context_relevance: 100% 5/5 [00:05<00:00,  1.19s/it]\n",
            "Evaluating Ragas nv_response_groundedness: 100% 5/5 [00:06<00:00,  1.34s/it]\n",
            "Evaluating Ragas nv_accuracy: 100% 5/5 [00:08<00:00,  1.62s/it]\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Trajectory:  60% 3/5 [00:07<00:05,  2.66s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Evaluating Trajectory:  80% 4/5 [00:09<00:02,  2.29s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Evaluating Trajectory: 100% 5/5 [00:11<00:00,  2.21s/it]\n",
            "2026-01-20 03:43:06 - INFO     - nat.profiler.profile_runner:127 - Wrote combined data to: .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/all_requests_profiler_traces.json\n",
            "2026-01-20 03:43:07 - INFO     - nat.profiler.profile_runner:146 - Wrote merged standardized DataFrame to .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/standardized_data_all.csv\n",
            "2026-01-20 03:43:07 - INFO     - nat.profiler.profile_runner:200 - Wrote inference optimization results to: .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/inference_optimization.json\n",
            "2026-01-20 03:43:08 - INFO     - nat.profiler.profile_runner:224 - Nested stack analysis complete\n",
            "2026-01-20 03:43:08 - INFO     - nat.profiler.profile_runner:235 - Concurrency spike analysis complete\n",
            "2026-01-20 03:43:08 - INFO     - nat.profiler.profile_runner:264 - Wrote workflow profiling report to: .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/workflow_profiling_report.txt\n",
            "2026-01-20 03:43:08 - INFO     - nat.profiler.profile_runner:271 - Wrote workflow profiling metrics to: .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/workflow_profiling_metrics.json\n",
            "2026-01-20 03:43:08 - INFO     - nat.eval.evaluate:335 - Workflow output written to .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/workflow_output.json\n",
            "2026-01-20 03:43:08 - INFO     - nat.eval.evaluate:346 - Evaluation results written to .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/relevance_output.json\n",
            "2026-01-20 03:43:08 - INFO     - nat.eval.evaluate:346 - Evaluation results written to .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/groundedness_output.json\n",
            "2026-01-20 03:43:08 - INFO     - nat.eval.evaluate:346 - Evaluation results written to .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/accuracy_output.json\n",
            "2026-01-20 03:43:08 - INFO     - nat.eval.evaluate:346 - Evaluation results written to .tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/trajectory_accuracy_output.json\n",
            "2026-01-20 03:43:08 - WARNING  - nat.eval.evaluate:358 - Workflow execution was interrupted due to an error. The results may be incomplete. You can re-execute evaluation for incomplete results by running `eval` with the --skip_completed_entries flag.\n",
            "2026-01-20 03:43:08 - INFO     - nat.eval.utils.output_uploader:61 - No S3 config provided; skipping upload.\n",
            "\u001b[0m\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your results\n",
        "results_path = \".tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/eval/accuracy_output.json\"\n",
        "\n",
        "with open(results_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# NAT 1.3 uses 'eval_output_items' for individual entries\n",
        "if 'eval_output_items' in data:\n",
        "    # Use json_normalize to flatten the nested 'reasoning' dictionary automatically\n",
        "    df = pd.json_normalize(data['eval_output_items'])\n",
        "\n",
        "    print(f\"--- Global Average Accuracy: {data.get('average_score')} ---\")\n",
        "\n",
        "    # In this version, 'reasoning' fields are flattened to 'reasoning.user_input', etc.\n",
        "    # We'll select the most relevant columns for display:\n",
        "    cols_to_show = ['reasoning.user_input', 'reasoning.response', 'score']\n",
        "\n",
        "    # Filter only available columns to avoid future KeyErrors\n",
        "    available_cols = [c for c in cols_to_show if c in df.columns]\n",
        "    print(df[available_cols].head())\n",
        "else:\n",
        "    print(f\"❌ Key not found. Available keys: {list(data.keys())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pPkL0zn3RqX",
        "outputId": "a50d7547-5f06-4a06-dc66-8b70780112df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Global Average Accuracy: 0.7 ---\n",
            "                                reasoning.user_input  \\\n",
            "0           What are the main features of LangSmith?   \n",
            "1  How does LangSmith help with LLM application o...   \n",
            "2  What is the purpose of LangSmith's evaluation ...   \n",
            "3     How does LangSmith support prompt engineering?   \n",
            "4  Is LangSmith framework-specific or can it be u...   \n",
            "\n",
            "                                  reasoning.response  score  \n",
            "0                                                      1.00  \n",
            "1  LangSmith helps with LLM application observabi...   0.50  \n",
            "2  The purpose of LangSmith's evaluation features...   0.50  \n",
            "3  LangSmith supports prompt engineering by provi...   0.75  \n",
            "4  LangSmith is not framework-specific and can be...   0.75  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nat eval --config_file examples/evaluation_and_profiling/simple_web_query_eval/src/nat_simple_web_query_eval/configs/eval_config_llama31.yml"
      ],
      "metadata": {
        "id": "_uhHodA04n_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your NAT 1.3 accuracy results\n",
        "results_path = \".tmp/nat/examples/evaluation_and_profiling/simple_web_query_eval/llama-31-8b/accuracy_output.json\"\n",
        "\n",
        "with open(results_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Flattening the NAT 1.3 'eval_output_items' structure\n",
        "if 'eval_output_items' in data:\n",
        "    # Use json_normalize to flatten the nested reasoning fields\n",
        "    df = pd.json_normalize(data['eval_output_items'])\n",
        "\n",
        "    # Selecting the most useful columns for review\n",
        "    # user_input: The question\n",
        "    # reference: The 'Gold Standard' answer from your dataset\n",
        "    # response: What your agent actually replied\n",
        "    display_df = df[[\n",
        "        'reasoning.user_input',\n",
        "        'reasoning.reference',\n",
        "        'reasoning.response',\n",
        "        'score'\n",
        "    ]].copy()\n",
        "\n",
        "    # Set display options for easier reading\n",
        "    pd.set_option('display.max_colwidth', 500)\n",
        "\n",
        "    print(f\"--- Global Average Accuracy: {data.get('average_score')} ---\")\n",
        "    display(display_df.sort_values(by='score')) # Sorting by lowest score to see failures first\n",
        "else:\n",
        "    print(f\"❌ Could not find 'eval_output_items'. Available keys: {list(data.keys())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "hiPn8LMeOtqk",
        "outputId": "516f28a1-cbcb-4653-904d-cd1839dc2e41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Global Average Accuracy: 0.75 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                          reasoning.user_input  \\\n",
              "0                                     What are the main features of LangSmith?   \n",
              "2                      What is the purpose of LangSmith's evaluation features?   \n",
              "1                  How does LangSmith help with LLM application observability?   \n",
              "3                               How does LangSmith support prompt engineering?   \n",
              "4  Is LangSmith framework-specific or can it be used with any LLM application?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                          reasoning.reference  \\\n",
              "0                                                          LangSmith offers three main features: Observability for monitoring and analyzing traces, Evals for evaluating application performance and getting human feedback, and Prompt Engineering for iterating on prompts with version control and collaboration features.   \n",
              "2  LangSmith's evaluation features help in building and running high-quality evaluations for AI applications. It allows you to quickly assess application performance using off-the-shelf evaluators, analyze evaluation results in the UI, compare results over time, and collect human feedback to improve the application.   \n",
              "1                                LangSmith provides LLM-native observability features that help throughout all stages of application development. It allows you to add tracing to your application, create dashboards to view key metrics like RPS, error rates and costs, and get meaningful insights from your application.   \n",
              "3                                                                            LangSmith provides tools for prompt engineering that help you find the perfect prompt for your application. You can create prompts, iterate on models and prompts using the Playground, and manage prompts programmatically in your application.   \n",
              "4                                                           LangSmith is framework-agnostic and can be used with or without LangChain's open source frameworks. It can be integrated with any LLM application, though it offers specific integration options for LangChain and LangGraph users through environment variables.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    reasoning.response  \\\n",
              "0  The main features of LangSmith include:\\n\\n1. Tracing and Debugging: LangSmith traces the flow of data through your application, making it easier.\\n2. LLM-native observability: It provides LLM-native observability that allows you to trace entire application pipelines, monitor key performance metrics in production, and gain the meaningful insights needed to debug, optimize, and improve your products.\\n3. Practical workflow features: LangSmith supports practical workflow features such as the a...   \n",
              "2                                                                                                                                                                                                                                                                                                     The purpose of LangSmith's evaluation features is to provide a robust platform for testing, monitoring, and optimizing LLM applications, ensuring they are predictable, reliable, and meet company expectations.   \n",
              "1  LangSmith Observability provides complete visibility into agent behavior with tracing, real-time monitoring, alerting, and high-level insights into usage. Key features include:\\n\\n1. Tracing setup: Configure tracing with basic options, framework integrations, or advanced settings for full control.\\n2. Viewing & managing traces: Access and manage traces via UI or API with filtering, exporting, sharing, and comparison tools.\\n3. Monitoring performance: Create dashboards and set alerts to track...   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
              "\n",
              "   score  \n",
              "0   0.25  \n",
              "2   0.50  \n",
              "1   1.00  \n",
              "3   1.00  \n",
              "4   1.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a26f4e0-25c6-4406-9c76-f4b5fccfaddc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reasoning.user_input</th>\n",
              "      <th>reasoning.reference</th>\n",
              "      <th>reasoning.response</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the main features of LangSmith?</td>\n",
              "      <td>LangSmith offers three main features: Observability for monitoring and analyzing traces, Evals for evaluating application performance and getting human feedback, and Prompt Engineering for iterating on prompts with version control and collaboration features.</td>\n",
              "      <td>The main features of LangSmith include:\\n\\n1. Tracing and Debugging: LangSmith traces the flow of data through your application, making it easier.\\n2. LLM-native observability: It provides LLM-native observability that allows you to trace entire application pipelines, monitor key performance metrics in production, and gain the meaningful insights needed to debug, optimize, and improve your products.\\n3. Practical workflow features: LangSmith supports practical workflow features such as the a...</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the purpose of LangSmith's evaluation features?</td>\n",
              "      <td>LangSmith's evaluation features help in building and running high-quality evaluations for AI applications. It allows you to quickly assess application performance using off-the-shelf evaluators, analyze evaluation results in the UI, compare results over time, and collect human feedback to improve the application.</td>\n",
              "      <td>The purpose of LangSmith's evaluation features is to provide a robust platform for testing, monitoring, and optimizing LLM applications, ensuring they are predictable, reliable, and meet company expectations.</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does LangSmith help with LLM application observability?</td>\n",
              "      <td>LangSmith provides LLM-native observability features that help throughout all stages of application development. It allows you to add tracing to your application, create dashboards to view key metrics like RPS, error rates and costs, and get meaningful insights from your application.</td>\n",
              "      <td>LangSmith Observability provides complete visibility into agent behavior with tracing, real-time monitoring, alerting, and high-level insights into usage. Key features include:\\n\\n1. Tracing setup: Configure tracing with basic options, framework integrations, or advanced settings for full control.\\n2. Viewing &amp; managing traces: Access and manage traces via UI or API with filtering, exporting, sharing, and comparison tools.\\n3. Monitoring performance: Create dashboards and set alerts to track...</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does LangSmith support prompt engineering?</td>\n",
              "      <td>LangSmith provides tools for prompt engineering that help you find the perfect prompt for your application. You can create prompts, iterate on models and prompts using the Playground, and manage prompts programmatically in your application.</td>\n",
              "      <td></td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Is LangSmith framework-specific or can it be used with any LLM application?</td>\n",
              "      <td>LangSmith is framework-agnostic and can be used with or without LangChain's open source frameworks. It can be integrated with any LLM application, though it offers specific integration options for LangChain and LangGraph users through environment variables.</td>\n",
              "      <td></td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a26f4e0-25c6-4406-9c76-f4b5fccfaddc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a26f4e0-25c6-4406-9c76-f4b5fccfaddc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a26f4e0-25c6-4406-9c76-f4b5fccfaddc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"\\u274c Could not find 'eval_output_items'\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"reasoning.user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What is the purpose of LangSmith's evaluation features?\",\n          \"Is LangSmith framework-specific or can it be used with any LLM application?\",\n          \"How does LangSmith help with LLM application observability?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reasoning.reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"LangSmith's evaluation features help in building and running high-quality evaluations for AI applications. It allows you to quickly assess application performance using off-the-shelf evaluators, analyze evaluation results in the UI, compare results over time, and collect human feedback to improve the application.\",\n          \"LangSmith is framework-agnostic and can be used with or without LangChain's open source frameworks. It can be integrated with any LLM application, though it offers specific integration options for LangChain and LangGraph users through environment variables.\",\n          \"LangSmith provides LLM-native observability features that help throughout all stages of application development. It allows you to add tracing to your application, create dashboards to view key metrics like RPS, error rates and costs, and get meaningful insights from your application.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reasoning.response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"The purpose of LangSmith's evaluation features is to provide a robust platform for testing, monitoring, and optimizing LLM applications, ensuring they are predictable, reliable, and meet company expectations.\",\n          \"\",\n          \"The main features of LangSmith include:\\n\\n1. Tracing and Debugging: LangSmith traces the flow of data through your application, making it easier.\\n2. LLM-native observability: It provides LLM-native observability that allows you to trace entire application pipelines, monitor key performance metrics in production, and gain the meaningful insights needed to debug, optimize, and improve your products.\\n3. Practical workflow features: LangSmith supports practical workflow features such as the ability to send traces to a specific, named project and apply filters to the traces within that project, making it easier to organize and analyze data for different applications or environments (e.g., development, staging, production).\\n4. Comprehensive suite of tools: LangSmith provides a comprehensive suite of tools including monitoring, evaluation, debugging, testing, tracing, and observability features.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3535533905932738,\n        \"min\": 0.25,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.25,\n          0.5,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Openai_pgvector_helloworld_model_gpt_4_0613_JUNE2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/timescale/vector-cookbook.git"
      ],
      "metadata": {
        "id": "UFbxklZ-Llqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fa4e43-c38d-4820-dd07-fa1dfb07954a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vector-cookbook'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 125 (delta 53), reused 71 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (125/125), 8.49 MiB | 37.97 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/vector-cookbook/openai_pgvector_helloworld/requirements.txt -q\n",
        "!pip install colab-env --upgrade -q"
      ],
      "metadata": {
        "id": "JTGvqnDw4RnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87zRHCm_vWK2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import tiktoken\n",
        "import psycopg2\n",
        "import ast\n",
        "import pgvector\n",
        "import math\n",
        "from psycopg2.extras import execute_values\n",
        "from pgvector.psycopg2 import register_vector\n",
        "\n",
        "import openai\n",
        "import colab_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UKYdg-97vWK3"
      },
      "outputs": [],
      "source": [
        "openai.api_key  = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq-ZGGm4vWK4"
      },
      "source": [
        "## Part 1: Create Embeddings\n",
        "First, we'll create embeddings using the OpenAI API on some text we want to augment our LLM with.\n",
        "In this example, we'll use content from the Timescale blog about real world use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WooyY0S1vWK4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "a5b62eb1-822d-4634-9156-234a43bcc9fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  How to Build a Weather Station With Elixir, Ne...   \n",
              "1  CloudQuery on Using PostgreSQL for Cloud Asset...   \n",
              "2  How a Data Scientist Is Building a Time-Series...   \n",
              "3  How Conserv Safeguards History: Building an En...   \n",
              "4  How Messari Uses Data to Open the Cryptoeconom...   \n",
              "\n",
              "                                             content  \\\n",
              "0  This is an installment of our “Community Membe...   \n",
              "1  This is an installment of our “Community Membe...   \n",
              "2  This is an installment of our “Community Membe...   \n",
              "3  This is an installment of our “Community Membe...   \n",
              "4  This is an installment of our “Community Membe...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.timescale.com/blog/how-to-build-a-...  \n",
              "1  https://www.timescale.com/blog/cloudquery-on-u...  \n",
              "2  https://www.timescale.com/blog/how-a-data-scie...  \n",
              "3  https://www.timescale.com/blog/how-conserv-saf...  \n",
              "4  https://www.timescale.com/blog/how-messari-use...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3e01d4e-2d61-4735-a520-b74ddec1e93a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CloudQuery on Using PostgreSQL for Cloud Asset...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/cloudquery-on-u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How a Data Scientist Is Building a Time-Series...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/how-a-data-scie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How Conserv Safeguards History: Building an En...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/how-conserv-saf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How Messari Uses Data to Open the Cryptoeconom...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/how-messari-use...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3e01d4e-2d61-4735-a520-b74ddec1e93a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3e01d4e-2d61-4735-a520-b74ddec1e93a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3e01d4e-2d61-4735-a520-b74ddec1e93a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a89e2d4c-3a05-48a8-a18b-b3b65caf3e53\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a89e2d4c-3a05-48a8-a18b-b3b65caf3e53')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a89e2d4c-3a05-48a8-a18b-b3b65caf3e53 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Using IoT Sensors, TimescaleDB, and Grafana to Control the Temperature of the Nuclear Fusion Experiment at the Max Planck Institute\",\n          \"How Flowkey Scaled Its AWS Redshift Data Warehouse With Timescale\",\n          \"How to Build a Weather Station With Elixir, Nerves, and TimescaleDB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"This is an installment of our \\u201cCommunity Member Spotlight\\u201d series, where we invite our customers or users to share their work, shining a light on their success and inspiring others with new ways to use technology to solve problems.In this edition, David Bailey, an intern at the Wendelstein 7-X fusion reactor experiment at theMax Planck Institute for Plasma Physics, explains how he\\u2019s using TimescaleDB and Grafana to monitor the reliability of the reactor\\u2019s heating system. Currently working on the designs of the sensor boards that measure the microwaves feeding the reactor, David needs a tight grasp on his systems and data to prevent the experiment from being aborted due to excessive heating\\u2014and is successfully achieving it with a little help from TimescaleDB.About the UserI am David Bailey, and I am currently completing an internship at the Wendelstein 7-X fusion reactor experiment, which is operated by the Max Planck Institute for Plasma Physics in Greifswald, Germany. The experiment aims to help us understand, and someday utilize, the power of fusion for further scientific endeavors and power generation. It does this by using a novel way of containing the hot gasses needed for the experiments and tweaking its design to reach longer and longer experiment runs.David BaileyThe end goal is to reach 30 minutes of uninterrupted containment of plasma heated to millions of degrees. To be able to do this, there is a lot of data to collect and process and a number of problems left to solve\\u2014one of which I can help with!About the ProjectMy specific task at the internship is to help with the heating system. Because the experiment is not set up for self-sustaining fusion, where the gasses would heat themselves, we constantly have to feed in energy to keep it hot: 10 megawatts, to be precise!We do this with various microwave sources\\u2014similar to scaled-up versions of household microwaves\\u2014and a big array of mirrors to guide the energy to the reactor vessel.Schematic of the W7-X microwave heating system and mirror assembly. Credit: Torsten Stange, David Bailey; Max Planck Institute GreifswaldThere are dangers in using these highly powerful microwaves: if dust or a fine droplet of water gets in the way of the energy, an arc can form\\u2014a lightning bolt of sorts. If the energy is not turned off fast enough, this arc can do a lot of damage, and it also means that we need to abort the experiment to wait for things to cool off again\\u2014not good if you want to run everything reliably for long periods of time!Image of a forming microwave arc, taken at the Max Planck IPP Greifswald experiment siteMy task is to design sensor boards to precisely track the amount of energy coming out of the microwave array. The ultimate goal is to detect changes in the power output on the microsecond scale, so the power can be turned off before the arc fully forms. If done right, we can turn the heating back on without a pause and continue the experiment!One of the key aspects of what I am designing is that it needs to be reliable. If it's not sensitive enough, or too sensitive, or if there are issues with the communication with the rest of the system, it can severely impact the rest of the experiment in a negative way.One of the sensors that David is working onThe only way to ensure something is reliable is through data\\u2014a lot of it. A problem might present itself only after hundreds of hours or in subtle ways that are only apparent across days of data, but it can still be relevant and important to know about.\\u200bWriting a program to handle this amount of data myself would've been an unnecessary effort. It needs a tool that has the necessary functionality in it already, such as statistical operators, compression, etc., and you can get all of this in time-series databases, such as TimescaleDB!To track that the sensors and system are working as expected, I collect and handle several types of data using TimescaleDB:I am recording the general health metrics of the board in question: temperature, voltage levels, etc. These shouldn't change, but the harsh environment close to the reactor might cause issues, which would be very important to know about!Log messages of the boards themselves to understand what the software was doing.And finally, detection events: every time the board sees something suspicious, it sends a measurement series of this event, about 1,000 samples taken over a millisecond. We can use this during the initial development phase to ensure everything is working correctly. I can use a function generator to send a predetermined reference signal and compare that to what the board tells me it saw. If there are discrepancies, those might point to errors in the soft- or hardware and help me fix them.When the system is deployed, we can use these measurements to refine the detection system, to be able to set it as fast as possible without too many false positives, and gather data about how the actual physical events look to the board.Choosing (and Using!) TimescaleDB\\u2728Editor's Note:See how you can get started with Grafana and TimescaleDB with ourdocsorvideos.I started using Timescale to track data formy self-built smart homeas a small playground for me\\u2014that's still running! Aside from that, I only use it for the monitoring system mentioned above, though I hope to motivate others at my research institute to use it! It is also my go-to for other projects, should I need a way to store measurements efficiently.The top factors in my decision to use TimescaleDB were, first, the open-source nature of it. Open-source software can be much more beneficial to a wider range of people, mature faster, generally has more flexible features, and isn't so locked into a specific environment. It's also much more approachable to individuals because of no fussy licensing/digital rights management.Then, the documentation. You can read up on a lot of things that TimescaleDB can do, complete with great examples! It doesn't feel like you're alone and have to figure it out all by yourself\\u2014there's rich, easy-to-understand documentation available, and if that doesn't have what you need, the community has been very helpful, too.\\u2728Editor's Note:Need help with TimescaleDB? Join ourSlack CommunityorForumand ask away!And lastly,it's still all just SQL! You don't need to learn another database-specific format, but you can start using it immediately if you've worked with any standard database before! That was helpful because I already knew the basics of SQL, giving me a great starting point. Plus, you can use PostgreSQL's rich relational database system to easily store non-time-series data alongside your measurements.Because the Wendelstein is fairly old, database tools didn\\u2019t exist when it started. As such, they decided to write their own tool,called ArchiveDB, and never quite brought it to the modern age. It can only store time-series data in a particular format and has no relational or statistical tools aside from minimum/maximum aggregates, no continuous aggregates, and probably no compression.\\u201cIn the future, I want to motivate my research institute to install a proper multi-node TimescaleDB cluster. It could bring many much-needed modern features to the table, and distributed hypertables and the matching distributed computing of statistics could be incredibly useful\\u201dStoring all my measurement data in it would have been possible, but I would have had to write all of the statistical processing myself. Using Timescale was a major time-saver! I tried other tools, but none of them quite fit what I needed. I didn't want to spend too much time on an unfamiliar tool with sparse documentation.First, I tried the company\\u2019s standard database, ArchiveDB, but it didn\\u2019t offer much more functionality than simply saving a CSV file. I also tried another PostgreSQL extension,Citus. It didn\\u2019t lend itself nicely to my use case because the documentation was not as clear and easy, and it seemed much more tailored to a distributed setup from the start, making trying it out on my local machine a bit tricky. I believe I gaveInfluxDBa try, but writing in PostgreSQL felt more natural. Additionally, it was helpful to have the regular relational database from PostgreSQL and the rich set of aggregate functions.I might have used PostgreSQL without TimescaleDB,or perhaps InfluxDB had it not been for Timescale.The Grafana panels observing the sample measurements. A single event is plotted above for inspection using a scatter plot, while the accuracy of the individual measurements is plotted in a time-series belowOne of the things I am verifying right now is the measurements\\u2019 consistency. This means running thousands of known reference signals into the boards I am testing to see if they behave as expected. I could automate this using a query that JOINs a regular table containing the reference data with aTimescaleDB hypertablecontaining the measurements.With a bit of clever indexing, I was able to use the corr(x, y) function to check if things lined up. Thanks to the rich data types of PostgreSQL, I can cache metadata such as this correlation in a JSONB field. This speeds up later analysis of the data and allows me to store all sorts of extra values for individual events.The resulting data I then downsampled usingminto find the worst offenders.WITH missing_bursts AS ( \\n  SELECT burst_id FROM adc_burst_meta WHERE NOT metadata ? 'correlation' AND $__timeFilter(time)),\\nraw_correlations AS (\\n    \\tSELECT\\n    \\tburst_id,\\n    \\tcorr(value, reference_value) AS \\\"correlation\\\"\\n    \\tFROM adc_burst_data\\n    \\tINNER JOIN missing_bursts USING(burst_id)\\n    \\tINNER JOIN adc_burst_reference USING(burst_offset)\\n    \\tGROUP BY burst_id\\n)    \\nupdate_statement AS (\\n     UPDATE adc_burst_meta \\n    SET metadata = jsonb_set(metadata, '{correlation}', correlation::text::jsonb) FROM raw_correlations WHERE adc_burst_meta.burst_id = raw_correlations.burst_id\\n)\\n\\nSELECT $__timeGroup(time, $__interval) AS \\u201ctime\\u201d, \\n   min((metadata->>\\u2019correlation\\u2019)::numeric)\\nFROM adc_burst_meta \\nWHERE $__timeFilter(time) \\nGROUP BY \\u201ctime\\u201dThat made even small outliers very easy to spot across hundreds of thousands of samples and made analyzing and tagging data a breeze! This insight is incredibly useful when you want to ensure your system works as intended. I could easily find the worst measurements in my sample set, which allowed me to quickly see if, how, and in what way my system was having issues.Current Deployment and Future Plans\\u200bRight now, I deploy TimescaleDB on a small, local computer.Running the Docker image has been very useful in getting a fast and easy setup, too!In the future, I want to motivate my research institute to install a proper multi-node TimescaleDB cluster. It could bring many much-needed modern features to the table, and distributed hypertables and the matching distributed computing of statistics could be incredibly useful! My queries could work orders of magnitude faster than my local machine with little extra effort.For the most part, I interact with TimescaleDB through a simple Ruby script, which acts as an adapter between the hardware itself and the database. I also used Jupyter notebooks and theJupyter IRuby Kernelto do more in-depth data analysis.\\u2728Editor's Note:Check out this videoto learn how to wrap TimescaleDB functions for the Ruby ecosystem.\\u201cGrafana and Timescale really go hand in hand, and both of them are excellent open-source tools with rich documentation\\u201dI do use Grafana extensively! Grafana and Timescale really go hand in hand, and both of them are excellent open-source tools with rich documentation. Both have Docker images and can be set up quickly and easily. It is great to plot the measurements with just a few SQL queries.\\u200bWithout Grafana, I would have had to write a lot of the plots myself, and correlating different time-series events with each other would have been much harder. Either I would have had to spend more time implementing that myself, or I wouldn't have gotten this level of information from my measurements.\\u200bThe main benefit of using TimescaleDB is that you get a well-balanced mixture. Being built on top of PostgreSQL and still giving you access to all regular relational database features, you can use TimescaleDB for a much larger variety of tasks.\\u200bYou can also dive right in, even with minimal SQL knowledge\\u2014and if you ever do get stuck, theTimescaleandPostgreSQL documentationare well written and extensive, so you can almost always find a solution!\\u200bLastly, there is no \\\"too small\\\" for TimescaleDB either. Being open source, quick to set up, and easy to use while performing well even on a spare laptop or PC, it can be a valuable tool for any sort of data acquisition\\u2014even if it feels like a \\\"small\\\" task!\\u200bI learned a lot about my system already by using TimescaleDB and can be much more confident in how to proceed thanks to it. In a way, it has even changed my developer experience. I have the data acquisition and TimescaleDB running continuously while working on new software features or modifying the hardware. If I were to introduce a bug that could mess up the measurements, I might see those much sooner in the data. I can then react appropriately and quickly while still developing.\\u200bI can worry less about ensuring everything works and focus more on adding new features!Advice and ResourcesGive TimescaleDB a try whenever you need to record any kind of time-series data. It's much easier to use than writing your own script or dumping it into a CSV file. The built-in functionality can take care of a lot of number crunching very efficiently, allowing you to benefit from years of finely developed functions. It's well worth the trouble, and there's no such thing as \\\"too small\\\" a use case for it!\\u200bTimescaleDB has helped me be more confident in developing my hardware, and I want to motivate others in my field to try it out themselves. A lot can be gained from a few days or months of data, even if at first you don't think so\\u2014some insights only pop out at larger scales, no matter what you're working on.I am very excited to share my use case of TimescaleDB. Whenever I think of databases, I am drawn to these larger web services and big tech companies\\u2014but that doesn't have to be the case! That can also be helpful advice for others looking for the right database for their use case.We\\u2019d like to thank David and all of the folks at the Max Planck Institute for Plasma Physics for sharing their story on how they\\u2019re monitoring the Wendelstein 7-X fusion reactor\\u2019s heating system using TimescaleDB.We\\u2019re always keen to feature new community projects and stories on our blog. If you have a story or project you\\u2019d like to share, reach out on Slack (@Ana Tavares), and we\\u2019ll go from there.The open-source relational database for time-series and analytics.Try Timescale for free\",\n          \"This is an installment of our \\u201cCommunity Member Spotlight\\u201d series, where we invite our customers to share their work, shining a light on their success and inspiring others with new ways to use technology to solve problems.In this edition, Nikola Chochkov, lead data scientist atflowkey, shares how his team migrated from Amazon Redshift to TimescaleDB and is driving rapid growth and experimentation by analyzing the users\\u2019 behavioral data using TimescaleDB along with Metabase or Jupyter/Rmarkedown Notebooks.About the CompanyFlowkeyis a leading app for learning to play the piano, with over 10 million registered users in more than 100 countries. The company was launched in 2015 and quickly became one of the global leaders in its category.Here is a video from our founder, Jonas G\\u00f6\\u00dfling, explaining how it all started.About the TeamWe are a team of around 40 people, with more than 10 of us working in Data and Engineering.We have a Marketing team (responsible for user acquisition, customer relationship management, collaborations, etc.), a Creative team (building all of our visual content, our design, and advertising), Course and Song teams (creating our in-app learning content\\u2014e.g., the courses series and the piano renditions of the songs in our library). We also have Customer Support, Product, Engineering, Data, and Operations teams.Many of us take on more than one role, and for many of us, flowkey has been the first significant career step. Not for me personally, though, but still.\\ud83d\\ude42About the ProjectThe flowkey appWe are a business that depends heavily on analytics for business decision-making. Our experimentation\\u2014a major driver of our growth\\u2014is powered by the data analysis of user behavioral data (app usage). This data consists of user events, which we track from our product.\\u201cWe realized we needed to scale our previous Amazon Redshift data warehouse model into a more suitable solution for categorical, time-series data analysis. We found out about TimescaleDB from being part of the PostgreSQL community\\u201dFor example, a Learn Session starts at a given timestamp for a user, and we record this event.\\u2728Editor\\u2019s Note:Time-series data is a sequence of data points collected over time intervals, allowing us to track changes over time. To learn more, readWhat Is Time-Series Data (With Examples).When we launch a new feature, we typically A/B test it and evaluate its impact based on measuring key performance indicators (KPIs), which are predefined for the test. Every day, we receive millions of incoming events, tracked around our product, in the format:(user_id, event, timestamp, properties)Thepropertiesfield is a schemaless object that depends on the particular event type that we track from the app. For example, a Learn Session event would have asongIdand alearnMode, while a Subscription Offer interaction event would have aproductId, etc.Choosing (and Using!) TimescaleDBWith time, we realized we needed to scale our previous Amazon Redshift data warehouse model into a more suitable solution for categorical, time-series data analysis.We found out about TimescaleDB from being part of the PostgreSQL community, and when we were faced with the problem at hand, it was a natural way forward for us.After doing our research, we realized that TimescaleDB suited our needs perfectly. Here's a list of our arguments:Our data analysts are well-versed in SQL and PostgreSQL.The events\\u2019 raw data is available in a PostgreSQL schema alongside all our other business intelligence data.TimescaleDB is an actively developed, open-source solution. It allowed us to deploy it on our self-hosted PostgreSQL data warehouse.A TimescaleDB hypertable model would allow us to accommodate the schemaless JSON structure of our events.select event, platform, time, jsonb_pretty(data) from events_today limit 5;\\n            event             | platform |          time           |                      jsonb_pretty\\n------------------------------+----------+-------------------------+---------------------------------------------------------\\n SONG_OPEN_UI_ELEMENT_CLICKED | ios      | 2022-11-03 00:00:00.034 | {                                                      +\\n                              |          |                         |     \\\"songId\\\": \\\"E2kCpqHCwB2xYf7LL\\\",                     +\\n                              |          |                         |     \\\"context\\\": \\\"song-cover\\\",                           +\\n                              |          |                         |     \\\"listIndex\\\": 6,                                    +\\n                              |          |                         |     \\\"routeName\\\": \\\"Songs\\\",                              +\\n                              |          |                         |     \\\"currentTab\\\": \\\"SongsTab\\\"                          +\\n                              |          |                         | }\\n ONBOARDING_QUESTION_VIEWED   | web      | 2022-11-03 00:00:00.145 | {                                                      +\\n                              |          |                         |     \\\"context\\\": \\\"preferredCategories\\\"                   +\\n                              |          |                         | }\\n SONG_PLAYER_SCROLLED         | ios      | 2022-11-03 00:00:00.157 | {                                                      +\\n                              |          |                         |     \\\"level\\\": 1,                                        +\\n                              |          |                         |     \\\"songId\\\": \\\"Lui27TDJ4vZBevxzc\\\",                     +\\n                              |          |                         |     \\\"direction\\\": \\\"backwards\\\",                          +\\n                              |          |                         |     \\\"songGroupId\\\": \\\"vhisAJBkPwvn6tdoq\\\",                +\\n                              |          |                         |     \\\"loopBoundsMs\\\": null,                              +\\n                              |          |                         |     \\\"finalPositionMs\\\": 0,                              +\\n                              |          |                         |     \\\"initialPositionMs\\\": 24751                         +\\n                              |          |                         | }\\n AB_TEST_VARIANT_ASSIGNED     | ios      | 2022-11-03 00:00:00.249 | {                                                      +\\n                              |          |                         |     \\\"variant\\\": \\\"CONTROL\\\",                              +\\n                              |          |                         |     \\\"experimentName\\\": \\\"player_onboarding_video_09_2022\\\"+\\n                              |          |                         | }\\n ONBOARDING_ANSWER_SUBMITTED  | web      | 2022-11-03 00:00:00.314 | {                                                      +\\n                              |          |                         |     \\\"answers\\\": [                                       +\\n                              |          |                         |         \\\"dont-know\\\"                                    +\\n                              |          |                         |     ],                                                 +\\n                              |          |                         |     \\\"context\\\": \\\"learningGoals\\\"                         +\\n                              |          |                         | }TimescaleDB offers a great set of SQL analytical functions.TimescaleDB offers continuous aggregates, which integrate very well with how we do analytics and real-time data monitoring.Data migration and update (e.g., renaming of events or JSON properties) are available.\\u2728Editor\\u2019s Note:Faster queries, reduced storage costs, and greater flexibility. Learn more abouthierarchical continuous aggregates.\\u201cWe use compression, which has cut our disk space usage by 28 percent\\u201dCurrent Deployment & Future PlansOur data warehouse is deployed on self-hosted machines and works well for us. We employ other PostgreSQL extensions that aren't currently supported by the Timescale cloud offering, which was important to us when we launched. These includeMongo FDWandAdjust\\u2019siStore extensionfor cohort analysis data storage.\\u2728Editor's Note:We're working on expanding the catalog of PostgreSQL extensions offered in Timescale's cloud offering. Stay tuned!We employ TimescaleDB's awesome data retention (automated through a user action), and thanks to that, our most recent (and more relevant to our analytics) data is available to us on SSD chunks, while historical data is kept on HDDs.Furthermore, we use compression, which has cut our disk space usage by 28 percent. Our data contains JSONB fields, which are difficult to be compressed. We are pretty happy with it, though, so it's a win. \\ud83d\\ude42When we do business analytics, we employMetabaseorJupyter/Rmarkdown Notebooksto derive insights. We established a workflow of writing custom continuous aggregates for the duration of experiments, which are then easy to keep and fully deploy or discard, depending on the decision made for the experiment.\\u2728Editor's Note:Learn how toconnect to Timescale from a Jupyter notebookfor better data querying, cleaning, and analysis.This allows us to iterate our experiments quickly and increase the bandwidth of change, which we can successfully bring to the product.RoadmapWe just finished migrating our setup to a more powerful cluster of machines, which allowed us to benefit from the data tiering options mentioned above. Right now, our system is scalable, and we don't expect any major upgrades to this system to come up soon.Advice & ResourcesWe recommend theTimescale documentationas well as theSlack Community.Want more insider tips?Sign up for our newsletterfor more Developer Q&As, technical articles, and tutorials to help you do more with your data. We deliver it straight to your inbox twice a month.We\\u2019d like to thank Nikola and all of the folks at flowkey for sharing their story on how they\\u2019re improving their online piano lessons by analyzing millions of user events daily using TimescaleDB.We\\u2019re always keen to feature new community projects and stories on our blog. If you have a story or project you\\u2019d like to share, reach out on Slack (@Ana Tavares), and we\\u2019ll go from there.The open-source relational database for time-series and analytics.Try Timescale for free\",\n          \"This is an installment of our \\u201cCommunity Member Spotlight\\u201d series, where we invite our customers to share their work, shining a light on their success and inspiring others with new ways to use technology to solve problems.In this edition,Alexander Koutmos, author of the Build a Weather Station with Elixir and Nerves book, joins us to share how he uses Grafana and TimescaleDB to store and visualize weather data collected from IoT sensors.About the teamThe bookBuild a Weather Station with Elixir and Nerveswas a joint effort between Bruce Tate, Frank Hunleth, and me.I have been writing software professionally for almost a decade and have been working primarily with Elixir since 2016. I currently maintain a few Elixir libraries onHexand also runStagira, a software consultancy company.Bruce Tateis a kayaker, programmer, and father of two from Chattanooga, Tennessee. He is the author of more than ten books and has been around Elixir from the beginning. He is the founder ofGroxio, a company that trains Elixir developers.Frank Hunlethis an embedded systems programmer, OSS maintainer, and Nerves core team member. When not in front of a computer, he loves running and spending time with his family.About the projectIn the Pragmatic Bookshelf book,Build a Weather Station with Elixir and Nerves, we take a project-based approach and guide the reader to create a Nerves-powered IoT weather station.For those unfamiliar with the Elixir ecosystem,Nervesis an IoT framework that allows you to build and deploy IoT applications on a wide array of embedded devices. At a high level, Nerves allows you to focus on building your project and takes care of a lot of the boilerplate associated with running Elixir on embedded devices.The goal of the book is to guide the reader through the process of building an end-to-end IoT solution for capturing, persisting, and visualizing weather data.Assembled weather station hooked up to development machine.One of the motivating factors for this book was to create a real-world project where readers could get hands-on experience with hardware without worrying too much about the nitty-gritty of soldering components together. Experimenting with hardware can often feel intimidating and confusing, but with Elixir and Nerves, we feel confident that even beginners get comfortable and productive quickly. As a result, in the book, we leverage a Raspberry Pi Zero W along with a few I2C enabled sensors to capture weather and environmental data. In all, the reader will capture and persist into TimescaleDB the current: altitude, atmospheric pressure, temperature, CO2 levels,TVOClevels, and the ambient light.Once the environmental data is captured on the Nerves device, it is published to a backend REST API and stored in TimescaleDB for later analytics/visualization. Luckily, TimescaleDB is an extension on top of PostgreSQL, allowing Elixir developers to use existing database tooling likeEctoto interface with time-series enabled tables.After the time-series weather data is stored in TimescaleDB, we walk the reader through how to visualize this data using the popular open-source visualization toolGrafana.Using Grafana for visualizing the weather was an easy choice given that Grafana natively supports TimescaleDB and is able to easily plot time-series data stored in TimescaleDB hypertables.\\u2728Editor\\u2019s Note:Check outGrafana 101 video seriesandGrafana tutorialsto learn everything from building awesome, interactive visualizations to setting up custom alerts, sharing dashboards with teammates, and solving common issues.The diagram shows all of the various components of the weather station system and how they interact with one another.By the end of the book, readers have a fully-featured IoT application and API backend that can power a live Grafana dashboard in order to plot their TimescaleDB data published from their Nerves weather station.Screenshot of Grafana dashboard, showing various graphs for various weather data\\ud83d\\udcd6If you are interested in learning about how to build an end-to-end IoT weather monitoring solution, be sure tocheck out the book and the accompanying code. If you are interested in learning more about Nerves and Elixir,check out the Nerves documentation.Choosing (and using!) TimescaleDBFrom the onset of the book, we knew that we wanted to use a purpose-built time-series database to persist the weather station data. We wanted the project to be as realistic as possible and something that could possibly be expanded for use in the real world.With that goal in mind, TimescaleDB was an obvious choice given that PostgreSQL has become a ubiquitous database and it has great support in the Elixir community. In addition,leveraging TimescaleDB on top of PostgreSQL does not add a tremendous amount of overhead or complexity and allows new users to easily leverage the benefits of a time-series database without having to learn any new query languages or databases. Specifically, all it took for readers to start leveraging TimescaleDB was to run a single SQL command in their database migration:SELECT create_hypertable('weather_conditions', 'timestamp').\\\"It\\u2019s this kind of pragmatism and ease of use that makes TimescaleDB a great time-series database for projects both small and large. \\\"-Alexander KoutmousAll in all, leveraging TimescaleDB as the time-series database for the project worked out great and allowed us to show readers how they can set up a production-ready IoT project in a relatively short amount of time.\\u2728Editor\\u2019s Note:To start with TimescaleDB today,sign up for a free 30-day trialorinstall TimescaleDB on your own server.Getting started advice & resourcesAny time we had questions about the inner workings of TimescaleDB, how to set it up, or what the various configuration options are, we turned to the official TimescaleDB docs. Some of the articles that helped us get started included:\\u2022Using TimescaleDB via Docker\\u2022 Understanding some ofthe fundamental TimescaleDB concepts\\u2022 Getting an overview of some of theTimescaleDB best practicesWe\\u2019d like to thank Alex, Bruce, and Frank for sharing their story, as well as for writing a book that makes building full-stack IoT solutions accessible for complete beginners. We congratulate them and the entire Nerves community on their success, and we cannot wait to read the final version of their book that will be released in January 2022 \\ud83c\\udf8aWe\\u2019re always keen to feature new community projects and stories on our blog. If you have a story or project you\\u2019d like to share, reach out on Slack (@Lucie \\u0160ime\\u010dkov\\u00e1), and we\\u2019ll go from there.Additionally, if you\\u2019re looking for more ways to get involved and show your expertise, check out theTimescaleHeroes program.The open-source relational database for time-series and analytics.Try Timescale for free\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"https://www.timescale.com/blog/using-iot-sensors-timescaledb-and-grafana-to-control-the-temperature-of-the-nuclear-fusion-experiment-in-the-max-planck-institute/\",\n          \"https://www.timescale.com/blog/how-flowkey-solved-its-time-series-data-problem-by-migrating-from-aws-redshift-to-timescale/\",\n          \"https://www.timescale.com/blog/how-to-build-a-weather-station-with-elixir-nerves-and-timescaledb/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Load your CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/vector-cookbook/openai_pgvector_helloworld/blog_posts_data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rprQRjK_vWK4"
      },
      "source": [
        "### 1.1 Calculate cost of embedding data\n",
        "It's usually a good idea to calculate how much creating embeddings for your selected content will cost.\n",
        "We use a number of helper functions to calculate a cost estimate before creating the embeddings to help us avoid surprises.\n",
        "\n",
        "For this toy example, since we're using a small dataset, the total cost will be less than $0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wKDvEXALvWK4"
      },
      "outputs": [],
      "source": [
        "# Helper functions to help us create the embeddings\n",
        "\n",
        "# Helper func: calculate number of tokens\n",
        "def num_tokens_from_string(string: str, encoding_name = \"cl100k_base\") -> int:\n",
        "    if not string:\n",
        "        return 0\n",
        "    # Returns the number of tokens in a text string\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Helper function: calculate length of essay\n",
        "def get_essay_length(essay):\n",
        "    word_list = essay.split()\n",
        "    num_words = len(word_list)\n",
        "    return num_words\n",
        "\n",
        "# Helper function: calculate cost of embedding num_tokens\n",
        "# Assumes we're using the text-embedding-ada-002 model\n",
        "# See https://openai.com/pricing\n",
        "def get_embedding_cost(num_tokens):\n",
        "    return num_tokens/1000*0.0001\n",
        "\n",
        "# Helper function: calculate total cost of embedding all content in the dataframe\n",
        "def get_total_embeddings_cost():\n",
        "    total_tokens = 0\n",
        "    for i in range(len(df.index)):\n",
        "        text = df['content'][i]\n",
        "        token_len = num_tokens_from_string(text)\n",
        "        total_tokens = total_tokens + token_len\n",
        "    total_cost = get_embedding_cost(total_tokens)\n",
        "    return total_cost\n",
        "\n",
        "\n",
        "def get_embeddings(text, model=\"text-embedding-ada-002\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return openai.embeddings.create(input = [text], model=model).data[0].embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cHfRHG6pvWK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cc6e4e-6758-4598-eeca-17bd88747849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estimated price to embed this content = $0.0060178\n"
          ]
        }
      ],
      "source": [
        "# quick check on total token amount for price estimation\n",
        "total_cost = get_total_embeddings_cost()\n",
        "print(\"estimated price to embed this content = $\" + str(total_cost))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAsZjR8hvWK5"
      },
      "source": [
        "### 1.2 Create smaller chunks of content\n",
        "The OpenAI API has a limit to the maximum amount of tokens it create create an embedding for in a single request. To get around this limit we'll break up our text into smaller chunks. In general its a best practice to create embeddings of a certain size in order to get better retrieval. For our purposes, we'll aim for chunks of around 512 tokens each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkCV1FpnvWK6"
      },
      "source": [
        "Note: If you prefer to skip this step, you can use use the provided file: blog_data_and_embeddings.csv which contains the data and embeddings that you'll generate in this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dg2JxufdvWK6"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# Create new list with small content chunks to not hit max token limits\n",
        "# Note: the maximum number of tokens for a single request is 8191\n",
        "# https://openai.com/docs/api-reference/requests\n",
        "###############################################################################\n",
        "# list for chunked content and embeddings\n",
        "new_list = []\n",
        "# Split up the text into token sizes of around 512 tokens\n",
        "for i in range(len(df.index)):\n",
        "    text = df['content'][i]\n",
        "    token_len = num_tokens_from_string(text)\n",
        "    if token_len <= 512:\n",
        "        new_list.append([df['title'][i], df['content'][i], df['url'][i], token_len])\n",
        "    else:\n",
        "        # add content to the new list in chunks\n",
        "        start = 0\n",
        "        ideal_token_size = 512\n",
        "        # 1 token ~ 3/4 of a word\n",
        "        ideal_size = int(ideal_token_size // (4/3))\n",
        "        end = ideal_size\n",
        "        #split text by spaces into words\n",
        "        words = text.split()\n",
        "\n",
        "        #remove empty spaces\n",
        "        words = [x for x in words if x != ' ']\n",
        "\n",
        "        total_words = len(words)\n",
        "\n",
        "        #calculate iterations\n",
        "        chunks = total_words // ideal_size\n",
        "        if total_words % ideal_size != 0:\n",
        "            chunks += 1\n",
        "\n",
        "        new_content = []\n",
        "        for j in range(chunks):\n",
        "            if end > total_words:\n",
        "                end = total_words\n",
        "            new_content = words[start:end]\n",
        "            new_content_string = ' '.join(new_content)\n",
        "            new_content_token_len = num_tokens_from_string(new_content_string)\n",
        "            if new_content_token_len > 0:\n",
        "                new_list.append([df['title'][i], new_content_string, df['url'][i], new_content_token_len])\n",
        "            start += ideal_size\n",
        "            end += ideal_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qkl-A4hgvWK7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "99602c24-7bbd-42ac-a7fb-077543d53f29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  How to Build a Weather Station With Elixir, Ne...   \n",
              "1  How to Build a Weather Station With Elixir, Ne...   \n",
              "2  How to Build a Weather Station With Elixir, Ne...   \n",
              "3  CloudQuery on Using PostgreSQL for Cloud Asset...   \n",
              "4  CloudQuery on Using PostgreSQL for Cloud Asset...   \n",
              "\n",
              "                                             content  \\\n",
              "0  This is an installment of our “Community Membe...   \n",
              "1  capture weather and environmental data. In all...   \n",
              "2  command in their database migration:SELECT cre...   \n",
              "3  This is an installment of our “Community Membe...   \n",
              "4  Architecture with CloudQuery SDK- Writing plug...   \n",
              "\n",
              "                                                 url  tokens  \\\n",
              "0  https://www.timescale.com/blog/how-to-build-a-...     501   \n",
              "1  https://www.timescale.com/blog/how-to-build-a-...     512   \n",
              "2  https://www.timescale.com/blog/how-to-build-a-...     374   \n",
              "3  https://www.timescale.com/blog/cloudquery-on-u...     519   \n",
              "4  https://www.timescale.com/blog/cloudquery-on-u...     511   \n",
              "\n",
              "                                          embeddings  \n",
              "0  [0.021600862964987755, 0.021938595920801163, -...  \n",
              "1  [0.016168784350156784, 0.01135314628481865, 0....  \n",
              "2  [0.02252391166985035, -0.0019157830392941833, ...  \n",
              "3  [0.009631811641156673, -0.0043013859540224075,...  \n",
              "4  [0.020583339035511017, 0.01015426404774189, 0....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eabd5dd6-32a0-4672-b0b2-2251b3c6412e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>url</th>\n",
              "      <th>tokens</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
              "      <td>501</td>\n",
              "      <td>[0.021600862964987755, 0.021938595920801163, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
              "      <td>capture weather and environmental data. In all...</td>\n",
              "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
              "      <td>512</td>\n",
              "      <td>[0.016168784350156784, 0.01135314628481865, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
              "      <td>command in their database migration:SELECT cre...</td>\n",
              "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
              "      <td>374</td>\n",
              "      <td>[0.02252391166985035, -0.0019157830392941833, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CloudQuery on Using PostgreSQL for Cloud Asset...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/cloudquery-on-u...</td>\n",
              "      <td>519</td>\n",
              "      <td>[0.009631811641156673, -0.0043013859540224075,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CloudQuery on Using PostgreSQL for Cloud Asset...</td>\n",
              "      <td>Architecture with CloudQuery SDK- Writing plug...</td>\n",
              "      <td>https://www.timescale.com/blog/cloudquery-on-u...</td>\n",
              "      <td>511</td>\n",
              "      <td>[0.020583339035511017, 0.01015426404774189, 0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eabd5dd6-32a0-4672-b0b2-2251b3c6412e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eabd5dd6-32a0-4672-b0b2-2251b3c6412e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eabd5dd6-32a0-4672-b0b2-2251b3c6412e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bec323b1-a652-4f25-bac2-c9e05a199269\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bec323b1-a652-4f25-bac2-c9e05a199269')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bec323b1-a652-4f25-bac2-c9e05a199269 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_new",
              "summary": "{\n  \"name\": \"df_new\",\n  \"rows\": 129,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Using IoT Sensors, TimescaleDB, and Grafana to Control the Temperature of the Nuclear Fusion Experiment at the Max Planck Institute\",\n          \"How Flowkey Scaled Its AWS Redshift Data Warehouse With Timescale\",\n          \"How to Build a Weather Station With Elixir, Nerves, and TimescaleDB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 129,\n        \"samples\": [\n          \"Technologies), and iTKO (bought by CA Technologies).My name isKen Ahrens. I am co-founder and CEO of Speedcale. Much of my career has been focused on helping companies develop and manage complex web applications. I previously ran North America teams for New Relic and CA/Broadcom. Previous startups included Pentaho (acquired by Hitachi), ITKO (acquired by CA/Broadcom), and ILC (acquired by General Dynamics). My first foray into programming started with a brand new language called Java at Georgia Tech and has grown into a lifetime interest.Matthew LeRay, co-founder and CTO at Speedscale, has invested the past 20 years in improving the performance of applications across multiple generations of technology. Previously, he was head of product at Observe, SVP at CA Technologies (acquired by Broadcom), and engineering leader at ILC (acquired by General Dynamics). He is an alumnus of Georgia Tech in both Computer Engineering and Business. His first love is debugging Golang code but he occasionally takes a break to craft hand-carved guitars.Nate Lee, co-founder and VP of Sales & Marketing, has served a variety of roles within the software industry. Most recently, he was in enterprise sales for the digital transformation consultancy Contino (acquired by Cognizant). Prior to Contino, he served as Product Manager at CA Technologies, by way of iTKO where he was a Presales Engineer for 6 years. Before iTKO, he spent time as a support leader at IBM Internet Security Systems, and engineer at ILC (acquired by General Dynamics). He graduated from Georgia Tech with an MBA in Technology, and a BS in Computer Science. You\\u2019ll most likely find him outdoors on 2 wheels when he\\u2019s not innovating with his Speedscale buddies.As a small, nimble startup, our normal workweek is comprised mostly of helping customers scope their use cases and deciding where to start with our testing framework (recording traffic, generating traffic replay \\u201cscenarios\\u201d comprised of tests and mocks of auto-identified dependencies). We are an engineering startup with a heavy emphasis on Kubernetes and Golang. We also are improving the protocol and version support of both our Enterprise Kubernetes version and Desktop version called theSpeedscale CLI.About the projectWe noticed the application of outdated testing practices to modern cloud infrastructure (eg. UI testing, manual testing, API test tools with no mocking). As the number of connections in distributed, containerized applications grew, the need\",\n          \"a SQL databaseandopen source).I started to develop my crypto trading bot in mid- 2017, about six months after my first encounter with the crypto ecosystem \\u2013 and I\\u2019ve continued working on it in my spare time for the last two and a half years.Editor\\u2019s Note: Felipe recently hosted aReddit AMA (Ask Me Anything)to share how he\\u2019s finally \\u201cperfected\\u201d his model, plus his experiences and advice for aspiring crypto developers and traders.About the projectI needed a bot that gave me a high-performance, scalable way to calculate technical indicators and process sentiment data in real-time.To do everything I need in terms of my technical indicators calculation, I collectcandlestick chartdata and market depth via an always-up websocket connection that tracks every Bitcoin market on theBinance exchange(~215 in total, 182 being tradeable, at this moment).The machine learning sentiment analysis started as a simple experiment to see if external news affected the market. For example: if a famous person in the crypto ecosystem tweeted that a big exchange was hacked, the price will probably fall and affect the whole market. Likewise, very good news should impact the price in a positive way. I calculated sentiment analysis scores in real-time, as soon as new data was ingested from sources like Twitter, Reddit, RSS feeds, and etc. Then, using these scores, I could determine market conditions at the moment.Now, I combine these two components with a weighted average, 60% technical indicators and 40% sentiment analysis.Felipe's TradingBot dashboard, where he tracks all ongoing trades and resultsQuick breakdown of Felipe\\u2019s results and success rates week-over-week (for the period of 10 March 2020 - 10 July 2020)Using TimescaleDBAt the beginning, I tried to save the collected data in simple files, but quickly realized that wasn\\u2019t a good way to store and process this data. I started looking for an alternative: a performant database.I went through several databases, and all of them always lacked something I wound up needing to continue my project. I tried MongoDB, InfluxDB, and Druid, but none of them 100% met my needs.Of the databases I tried,InfluxDB was a good option; however, every query that I tried to run was painful, due to their own query language (InfluxQL).As soon as my series started to grow exponentially to higher levels, the server didn't have enough memory to handle them all in real-time.\",\n          \"used to help our users make decisions. And, just like the rest of the crypto space, we are also scaling quickly, both in terms of our business and the amount of data we ingest.Choosing (and using!) TimescaleDBWe\\u2019re wrapping up a complete transition to TimescaleDB fromInfluxDB. It would be reasonable to say that we used InfluxDB until it fell over; we asked it to do a huge amount of ingestion and continuous aggregation, not to mention queries around the clock, to support the myriad requests our users can make.Over time, we pushed it enough that it became less stable, so eventually, it became clear that InfluxDB wasn\\u2019t going to scale with us. Thus,Kevin Pyc(who served as the entire back-end \\u201cteam\\u201d until earlier this year) became interested in TimescaleDB as a possible alternative.The pure PostgreSQL interface and impressive performance characteristics sold him on TimescaleDB as a good option for us.From there, the entire tech group convened and agreed to try TimescaleDB. We were aware of its performance claims but needed to test it out for ourselves, for our exact use case. I began by reimplementing our real-time trade ingestion database adapter on TimescaleDB \\u2013 and on every test, TimescaleDB blew my expectations out of the water.The most significant aspects of our system are INSERT and SELECT performance.INSERTs of real-time trade data are constant, 24/7, and rarely dip below 2,000 rows per second. At peak times, they can exceed 4,500\\u2014and, of course, we expect this number to continually increase as the industry continues to grow and we see more and more trades.SELECT performance impacts our APIs\\u2019 response time for anything we haven\\u2019t cached; we briefly cache many of the queries needed for the live site, but less common queries end up hitting the database.When we tested these with TimescaleDB, both of our SELECT and INSERT performance results flatly outperformed InfluxDB. In testing, even thoughTimescale Cloudis currently only located in us-east-1 and most of our infrastructure is in an us-west region, we saw an average of ~40ms improvement in both types of queries. Plus, we could batch-insert 500 rows of data, instead of 100, with no discernible drop in execution time relative to InfluxDB.These impressive performance benchmarks, combined with the fact that we can use Postgres with foreign key relationships to derive new datasets from our existing ones\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"https://www.timescale.com/blog/using-iot-sensors-timescaledb-and-grafana-to-control-the-temperature-of-the-nuclear-fusion-experiment-in-the-max-planck-institute/\",\n          \"https://www.timescale.com/blog/how-flowkey-solved-its-time-series-data-problem-by-migrating-from-aws-redshift-to-timescale/\",\n          \"https://www.timescale.com/blog/how-to-build-a-weather-station-with-elixir-nerves-and-timescaledb/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 135,\n        \"min\": 14,\n        \"max\": 738,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          556,\n          526,\n          472\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Create embeddings for each piece of content\n",
        "for i in range(len(new_list)):\n",
        "    text = new_list[i][1]\n",
        "    embedding = get_embeddings(text)\n",
        "    new_list[i].append(embedding)\n",
        "\n",
        "# Create a new dataframe from the list\n",
        "df_new = pd.DataFrame(new_list, columns=['title', 'content', 'url', 'tokens', 'embeddings'])\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XsmtzoVtvWK7"
      },
      "outputs": [],
      "source": [
        "df_new.to_csv('blog_data_and_embeddings.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcqlMzQMvWK7"
      },
      "source": [
        "## Part 2: Store embeddings with pgvector\n",
        "In this section, we'll store our embeddings and associated metadata.\n",
        "\n",
        "We'll use PostgreSQL as a vector database, with the pgvector extension.\n",
        "\n",
        "You can create a cloud PostgreSQL database for free on [Timescale](https://console.cloud.timescale.com/signup) or use a local PostgreSQL database for this step.\n",
        "\n",
        "In this Notebook version, we use PostgreSQL with pgvector extension locally in Google Cloud, developed by Frank Morales on 04/12/2023. Initially, this Notebook was using PostgreSQL with Timescale's configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISpgdQOuvWK7"
      },
      "source": [
        "### 2.2 Connect to and configure your vector database\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install PSQL and DEV Libraries locally added by FRANK MORALES December 4th, 2023.\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My6JR0005VTJ",
        "outputId": "c6b6cf16-2d18-4624-e680-67e491980cde"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Restarting PostgreSQL 14 database server\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install postgresql-server-dev-14 -q"
      ],
      "metadata": {
        "id": "IO4nuyYR-hJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pgvector/pgvector.git\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install\n",
        "#print('END: PG VECTOR COMPILATION')"
      ],
      "metadata": {
        "id": "CgURlYw75dF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RXjmCF2ZvWK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310ece1a-f449-471f-ff1b-afb34b54521f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALTER ROLE\n",
            "CREATE EXTENSION\n"
          ]
        }
      ],
      "source": [
        "# PostGRES SQL Settings\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "connection_string = 'postgresl://postgres:postgres@localhost:5432/postgres'\n",
        "\n",
        "#CREATE EXTENSION IF NOT EXISTS btree_gist\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION IF NOT EXISTS vector\"\n",
        "\n",
        "import psycopg2 as ps\n",
        "\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "\n",
        "conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "cur = conn.cursor() # creating a cursor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xNtWWY4ZvWK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc354545-a8bd-4238-8a4d-b22f7b2e8968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  table \"embeddings\" does not exist\n"
          ]
        }
      ],
      "source": [
        "# Connect to PostgreSQL database in Timescale using connection string\n",
        "#conn = psycopg2.connect(connection_string)\n",
        "\n",
        "cur = conn.cursor()\n",
        "\n",
        "#install pgvector\n",
        "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\");\n",
        "conn.commit()\n",
        "\n",
        "# Register the vector type with psycopg2\n",
        "register_vector(conn)\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE embeddings\"\n",
        "\n",
        "# Create table to store embeddings and metadata\n",
        "table_create_command = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS embeddings (\n",
        "            id bigserial primary key,\n",
        "            title text,\n",
        "            url text,\n",
        "            content text,\n",
        "            tokens integer,\n",
        "            embedding vector(1536)\n",
        "            );\n",
        "            \"\"\"\n",
        "\n",
        "cur.execute(table_create_command)\n",
        "cur.close()\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_G2HdHlvWK8"
      },
      "source": [
        "Optional: Uncomment and execute the following code only if you need to read the embeddings and metadata from the provided CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hltNpPkFvWK8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "90a8500e-cd08-4345-bb2a-cd19bd5361f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndf = pd.read_csv('blog_data_and_embeddings.csv')\\ntitles = df['title']\\nurls = df['url']\\ncontents = df['content']\\ntokens = df['tokens']\\nembeds = [list(map(float, ast.literal_eval(embed_str))) for embed_str in df['embeddings']]\\n\\ndf_new = pd.DataFrame({\\n    'title': titles,\\n    'url': urls,\\n    'content': contents,\\n    'tokens': tokens,\\n    'embeddings': embeds\\n})\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Uncomment and execute this cell only if you need to read the blog data and embeddings from the provided CSV file\n",
        "# Otherwise, skip to next cell\n",
        "'''\n",
        "df = pd.read_csv('blog_data_and_embeddings.csv')\n",
        "titles = df['title']\n",
        "urls = df['url']\n",
        "contents = df['content']\n",
        "tokens = df['tokens']\n",
        "embeds = [list(map(float, ast.literal_eval(embed_str))) for embed_str in df['embeddings']]\n",
        "\n",
        "df_new = pd.DataFrame({\n",
        "    'title': titles,\n",
        "    'url': urls,\n",
        "    'content': contents,\n",
        "    'tokens': tokens,\n",
        "    'embeddings': embeds\n",
        "})\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddliDgoPvWK8"
      },
      "source": [
        "### 2.3 Ingest and store vector data into PostgreSQL using pgvector\n",
        "In this section, we'll batch insert our embeddings and metadata into PostgreSQL and also create an index to help speed up search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Cd01ukY_vWK8"
      },
      "outputs": [],
      "source": [
        "register_vector(conn)\n",
        "cur = conn.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KxHkz-NDvWK8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fcf6a05e-ad88-45b9-fd82-b2a5bb2ec576"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  How to Build a Weather Station With Elixir, Ne...   \n",
              "1  How to Build a Weather Station With Elixir, Ne...   \n",
              "2  How to Build a Weather Station With Elixir, Ne...   \n",
              "3  CloudQuery on Using PostgreSQL for Cloud Asset...   \n",
              "4  CloudQuery on Using PostgreSQL for Cloud Asset...   \n",
              "\n",
              "                                             content  \\\n",
              "0  This is an installment of our “Community Membe...   \n",
              "1  capture weather and environmental data. In all...   \n",
              "2  command in their database migration:SELECT cre...   \n",
              "3  This is an installment of our “Community Membe...   \n",
              "4  Architecture with CloudQuery SDK- Writing plug...   \n",
              "\n",
              "                                                 url  tokens  \\\n",
              "0  https://www.timescale.com/blog/how-to-build-a-...     501   \n",
              "1  https://www.timescale.com/blog/how-to-build-a-...     512   \n",
              "2  https://www.timescale.com/blog/how-to-build-a-...     374   \n",
              "3  https://www.timescale.com/blog/cloudquery-on-u...     519   \n",
              "4  https://www.timescale.com/blog/cloudquery-on-u...     511   \n",
              "\n",
              "                                          embeddings  \n",
              "0  [0.021600862964987755, 0.021938595920801163, -...  \n",
              "1  [0.016168784350156784, 0.01135314628481865, 0....  \n",
              "2  [0.02252391166985035, -0.0019157830392941833, ...  \n",
              "3  [0.009631811641156673, -0.0043013859540224075,...  \n",
              "4  [0.020583339035511017, 0.01015426404774189, 0....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b65aa841-48d5-4577-bd84-5f130e338529\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>url</th>\n",
              "      <th>tokens</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
              "      <td>501</td>\n",
              "      <td>[0.021600862964987755, 0.021938595920801163, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
              "      <td>capture weather and environmental data. In all...</td>\n",
              "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
              "      <td>512</td>\n",
              "      <td>[0.016168784350156784, 0.01135314628481865, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How to Build a Weather Station With Elixir, Ne...</td>\n",
              "      <td>command in their database migration:SELECT cre...</td>\n",
              "      <td>https://www.timescale.com/blog/how-to-build-a-...</td>\n",
              "      <td>374</td>\n",
              "      <td>[0.02252391166985035, -0.0019157830392941833, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CloudQuery on Using PostgreSQL for Cloud Asset...</td>\n",
              "      <td>This is an installment of our “Community Membe...</td>\n",
              "      <td>https://www.timescale.com/blog/cloudquery-on-u...</td>\n",
              "      <td>519</td>\n",
              "      <td>[0.009631811641156673, -0.0043013859540224075,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CloudQuery on Using PostgreSQL for Cloud Asset...</td>\n",
              "      <td>Architecture with CloudQuery SDK- Writing plug...</td>\n",
              "      <td>https://www.timescale.com/blog/cloudquery-on-u...</td>\n",
              "      <td>511</td>\n",
              "      <td>[0.020583339035511017, 0.01015426404774189, 0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b65aa841-48d5-4577-bd84-5f130e338529')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b65aa841-48d5-4577-bd84-5f130e338529 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b65aa841-48d5-4577-bd84-5f130e338529');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d3243896-d6a1-442a-9dd7-8bb0ff55d44f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d3243896-d6a1-442a-9dd7-8bb0ff55d44f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d3243896-d6a1-442a-9dd7-8bb0ff55d44f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_new",
              "summary": "{\n  \"name\": \"df_new\",\n  \"rows\": 129,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Using IoT Sensors, TimescaleDB, and Grafana to Control the Temperature of the Nuclear Fusion Experiment at the Max Planck Institute\",\n          \"How Flowkey Scaled Its AWS Redshift Data Warehouse With Timescale\",\n          \"How to Build a Weather Station With Elixir, Nerves, and TimescaleDB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 129,\n        \"samples\": [\n          \"Technologies), and iTKO (bought by CA Technologies).My name isKen Ahrens. I am co-founder and CEO of Speedcale. Much of my career has been focused on helping companies develop and manage complex web applications. I previously ran North America teams for New Relic and CA/Broadcom. Previous startups included Pentaho (acquired by Hitachi), ITKO (acquired by CA/Broadcom), and ILC (acquired by General Dynamics). My first foray into programming started with a brand new language called Java at Georgia Tech and has grown into a lifetime interest.Matthew LeRay, co-founder and CTO at Speedscale, has invested the past 20 years in improving the performance of applications across multiple generations of technology. Previously, he was head of product at Observe, SVP at CA Technologies (acquired by Broadcom), and engineering leader at ILC (acquired by General Dynamics). He is an alumnus of Georgia Tech in both Computer Engineering and Business. His first love is debugging Golang code but he occasionally takes a break to craft hand-carved guitars.Nate Lee, co-founder and VP of Sales & Marketing, has served a variety of roles within the software industry. Most recently, he was in enterprise sales for the digital transformation consultancy Contino (acquired by Cognizant). Prior to Contino, he served as Product Manager at CA Technologies, by way of iTKO where he was a Presales Engineer for 6 years. Before iTKO, he spent time as a support leader at IBM Internet Security Systems, and engineer at ILC (acquired by General Dynamics). He graduated from Georgia Tech with an MBA in Technology, and a BS in Computer Science. You\\u2019ll most likely find him outdoors on 2 wheels when he\\u2019s not innovating with his Speedscale buddies.As a small, nimble startup, our normal workweek is comprised mostly of helping customers scope their use cases and deciding where to start with our testing framework (recording traffic, generating traffic replay \\u201cscenarios\\u201d comprised of tests and mocks of auto-identified dependencies). We are an engineering startup with a heavy emphasis on Kubernetes and Golang. We also are improving the protocol and version support of both our Enterprise Kubernetes version and Desktop version called theSpeedscale CLI.About the projectWe noticed the application of outdated testing practices to modern cloud infrastructure (eg. UI testing, manual testing, API test tools with no mocking). As the number of connections in distributed, containerized applications grew, the need\",\n          \"a SQL databaseandopen source).I started to develop my crypto trading bot in mid- 2017, about six months after my first encounter with the crypto ecosystem \\u2013 and I\\u2019ve continued working on it in my spare time for the last two and a half years.Editor\\u2019s Note: Felipe recently hosted aReddit AMA (Ask Me Anything)to share how he\\u2019s finally \\u201cperfected\\u201d his model, plus his experiences and advice for aspiring crypto developers and traders.About the projectI needed a bot that gave me a high-performance, scalable way to calculate technical indicators and process sentiment data in real-time.To do everything I need in terms of my technical indicators calculation, I collectcandlestick chartdata and market depth via an always-up websocket connection that tracks every Bitcoin market on theBinance exchange(~215 in total, 182 being tradeable, at this moment).The machine learning sentiment analysis started as a simple experiment to see if external news affected the market. For example: if a famous person in the crypto ecosystem tweeted that a big exchange was hacked, the price will probably fall and affect the whole market. Likewise, very good news should impact the price in a positive way. I calculated sentiment analysis scores in real-time, as soon as new data was ingested from sources like Twitter, Reddit, RSS feeds, and etc. Then, using these scores, I could determine market conditions at the moment.Now, I combine these two components with a weighted average, 60% technical indicators and 40% sentiment analysis.Felipe's TradingBot dashboard, where he tracks all ongoing trades and resultsQuick breakdown of Felipe\\u2019s results and success rates week-over-week (for the period of 10 March 2020 - 10 July 2020)Using TimescaleDBAt the beginning, I tried to save the collected data in simple files, but quickly realized that wasn\\u2019t a good way to store and process this data. I started looking for an alternative: a performant database.I went through several databases, and all of them always lacked something I wound up needing to continue my project. I tried MongoDB, InfluxDB, and Druid, but none of them 100% met my needs.Of the databases I tried,InfluxDB was a good option; however, every query that I tried to run was painful, due to their own query language (InfluxQL).As soon as my series started to grow exponentially to higher levels, the server didn't have enough memory to handle them all in real-time.\",\n          \"used to help our users make decisions. And, just like the rest of the crypto space, we are also scaling quickly, both in terms of our business and the amount of data we ingest.Choosing (and using!) TimescaleDBWe\\u2019re wrapping up a complete transition to TimescaleDB fromInfluxDB. It would be reasonable to say that we used InfluxDB until it fell over; we asked it to do a huge amount of ingestion and continuous aggregation, not to mention queries around the clock, to support the myriad requests our users can make.Over time, we pushed it enough that it became less stable, so eventually, it became clear that InfluxDB wasn\\u2019t going to scale with us. Thus,Kevin Pyc(who served as the entire back-end \\u201cteam\\u201d until earlier this year) became interested in TimescaleDB as a possible alternative.The pure PostgreSQL interface and impressive performance characteristics sold him on TimescaleDB as a good option for us.From there, the entire tech group convened and agreed to try TimescaleDB. We were aware of its performance claims but needed to test it out for ourselves, for our exact use case. I began by reimplementing our real-time trade ingestion database adapter on TimescaleDB \\u2013 and on every test, TimescaleDB blew my expectations out of the water.The most significant aspects of our system are INSERT and SELECT performance.INSERTs of real-time trade data are constant, 24/7, and rarely dip below 2,000 rows per second. At peak times, they can exceed 4,500\\u2014and, of course, we expect this number to continually increase as the industry continues to grow and we see more and more trades.SELECT performance impacts our APIs\\u2019 response time for anything we haven\\u2019t cached; we briefly cache many of the queries needed for the live site, but less common queries end up hitting the database.When we tested these with TimescaleDB, both of our SELECT and INSERT performance results flatly outperformed InfluxDB. In testing, even thoughTimescale Cloudis currently only located in us-east-1 and most of our infrastructure is in an us-west region, we saw an average of ~40ms improvement in both types of queries. Plus, we could batch-insert 500 rows of data, instead of 100, with no discernible drop in execution time relative to InfluxDB.These impressive performance benchmarks, combined with the fact that we can use Postgres with foreign key relationships to derive new datasets from our existing ones\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"https://www.timescale.com/blog/using-iot-sensors-timescaledb-and-grafana-to-control-the-temperature-of-the-nuclear-fusion-experiment-in-the-max-planck-institute/\",\n          \"https://www.timescale.com/blog/how-flowkey-solved-its-time-series-data-problem-by-migrating-from-aws-redshift-to-timescale/\",\n          \"https://www.timescale.com/blog/how-to-build-a-weather-station-with-elixir-nerves-and-timescaledb/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 135,\n        \"min\": 14,\n        \"max\": 738,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          556,\n          526,\n          472\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Remind ourselves of the dataframe structure\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7x0I-CuvWK8"
      },
      "source": [
        "Batch insert embeddings using psycopg2's ```execute_values()```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wvGaDQ7BvWK8"
      },
      "outputs": [],
      "source": [
        "#Batch insert embeddings and metadata from dataframe into PostgreSQL database\n",
        "\n",
        "# Prepare the list of tuples to insert\n",
        "data_list = [(row['title'], row['url'], row['content'], int(row['tokens']), np.array(row['embeddings'])) for index, row in df_new.iterrows()]\n",
        "# Use execute_values to perform batch insertion\n",
        "execute_values(cur, \"INSERT INTO embeddings (title, url, content, tokens, embedding) VALUES %s\", data_list)\n",
        "# Commit after we insert all embeddings\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvIqbP0MvWK8"
      },
      "source": [
        "Sanity check by running some simple queries against the embeddings table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aBd8ptk9vWK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0f242c-474c-4006-8b89-a356b3d6cd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vector records in table:  258 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "cur.execute(\"SELECT COUNT(*) as cnt FROM embeddings;\")\n",
        "num_records = cur.fetchone()[0]\n",
        "print(\"Number of vector records in table: \", num_records,\"\\n\")\n",
        "# Correct output should be 129"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0yiaK8tGvWK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969cdc7c-6088-416f-fd70-f0ae3f023a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First record in table:  [(1, 'How to Build a Weather Station With Elixir, Nerves, and TimescaleDB', 'https://www.timescale.com/blog/how-to-build-a-weather-station-with-elixir-nerves-and-timescaledb/', 'This is an installment of our “Community Member Spotlight” series, where we invite our customers to share their work, shining a light on their success and inspiring others with new ways to use technology to solve problems.In this edition,Alexander Koutmos, author of the Build a Weather Station with Elixir and Nerves book, joins us to share how he uses Grafana and TimescaleDB to store and visualize weather data collected from IoT sensors.About the teamThe bookBuild a Weather Station with Elixir and Nerveswas a joint effort between Bruce Tate, Frank Hunleth, and me.I have been writing software professionally for almost a decade and have been working primarily with Elixir since 2016. I currently maintain a few Elixir libraries onHexand also runStagira, a software consultancy company.Bruce Tateis a kayaker, programmer, and father of two from Chattanooga, Tennessee. He is the author of more than ten books and has been around Elixir from the beginning. He is the founder ofGroxio, a company that trains Elixir developers.Frank Hunlethis an embedded systems programmer, OSS maintainer, and Nerves core team member. When not in front of a computer, he loves running and spending time with his family.About the projectIn the Pragmatic Bookshelf book,Build a Weather Station with Elixir and Nerves, we take a project-based approach and guide the reader to create a Nerves-powered IoT weather station.For those unfamiliar with the Elixir ecosystem,Nervesis an IoT framework that allows you to build and deploy IoT applications on a wide array of embedded devices. At a high level, Nerves allows you to focus on building your project and takes care of a lot of the boilerplate associated with running Elixir on embedded devices.The goal of the book is to guide the reader through the process of building an end-to-end IoT solution for capturing, persisting, and visualizing weather data.Assembled weather station hooked up to development machine.One of the motivating factors for this book was to create a real-world project where readers could get hands-on experience with hardware without worrying too much about the nitty-gritty of soldering components together. Experimenting with hardware can often feel intimidating and confusing, but with Elixir and Nerves, we feel confident that even beginners get comfortable and productive quickly. As a result, in the book, we leverage a Raspberry Pi Zero W along with a few I2C enabled sensors to', 501, array([ 0.02160086,  0.0219386 , -0.00562185, ..., -0.01260871,\n",
            "       -0.02191045, -0.03689737], dtype=float32))]\n"
          ]
        }
      ],
      "source": [
        "# print the first record in the table, for sanity-checking\n",
        "cur.execute(\"SELECT * FROM embeddings LIMIT 1;\")\n",
        "records = cur.fetchall()\n",
        "print(\"First record in table: \", records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txnv9g5bvWK8"
      },
      "source": [
        "Create index on embedding column for faster cosine similarity comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-OkO6CCBvWK9"
      },
      "outputs": [],
      "source": [
        "# Create an index on the data for faster retrieval\n",
        "# this isn't really needed for 129 vectors, but it shows the usage for larger datasets\n",
        "# Note: always create this type of index after you have data already inserted into the DB\n",
        "\n",
        "#calculate the index parameters according to best practices\n",
        "num_lists = num_records / 1000\n",
        "if num_lists < 10:\n",
        "    num_lists = 10\n",
        "if num_records > 1000000:\n",
        "    num_lists = math.sqrt(num_records)\n",
        "\n",
        "#use the cosine distance measure, which is what we'll later use for querying\n",
        "cur.execute(f'CREATE INDEX ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {num_lists});')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0_iq9ZfvWK9"
      },
      "source": [
        "## Part 3: Nearest Neighbor Search using pgvector\n",
        "\n",
        "In this final part of the tutorial, we will query our embeddings table.\n",
        "\n",
        "We'll showcase an example of RAG: Retrieval Augmented Generation, where we'll retrieve relevant data from our vector database and give it to the LLM as context to use when it generates a response to a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-UaixlGevWK9"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages, model=\"gpt-4-0613\", temperature=0, max_tokens=1000):\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-Khyf7BIvWK-"
      },
      "outputs": [],
      "source": [
        "# Helper function: Get top 3 most similar documents from the database\n",
        "def get_top3_similar_docs(query_embedding, conn):\n",
        "    embedding_array = np.array(query_embedding)\n",
        "    # Register pgvector extension\n",
        "    register_vector(conn)\n",
        "    cur = conn.cursor()\n",
        "    # Get the top 3 most similar documents using the KNN <=> operator\n",
        "    cur.execute(\"SELECT content FROM embeddings ORDER BY embedding <=> %s LIMIT 3\", (embedding_array,))\n",
        "    top3_docs = cur.fetchall()\n",
        "    return top3_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmoTA6m2vWK-"
      },
      "source": [
        "### 3.1 Define a prompt for the LLM\n",
        "Here we'll define the prompt we want the LLM to provide a reponse to.\n",
        "\n",
        "We've picked an example relevant to the blog post data stored in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4uKnSdqgvWK-"
      },
      "outputs": [],
      "source": [
        "# Question about Timescale we want the model to answer\n",
        "input = \"How is Timescale used in IoT?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vyuYlDb5vWK_"
      },
      "outputs": [],
      "source": [
        "# Function to process input with retrieval of most similar documents from the database\n",
        "def process_input_with_retrieval(user_input):\n",
        "    delimiter = \"```\"\n",
        "\n",
        "    #Step 1: Get documents related to the user input from database\n",
        "    related_docs = get_top3_similar_docs(get_embeddings(user_input), conn)\n",
        "\n",
        "    # Step 2: Get completion from OpenAI API\n",
        "    # Set system message to help set appropriate tone and context for model\n",
        "    system_message = f\"\"\"\n",
        "    You are a friendly chatbot. \\\n",
        "    You can answer questions about timescaledb, its features and its use cases. \\\n",
        "    You respond in a concise, technically credible tone. \\\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare messages to pass to model\n",
        "    # We use a delimiter to help the model understand the where the user_input starts and ends\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": f\"{delimiter}{user_input}{delimiter}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Relevant Timescale case studies information: \\n {related_docs[0][0]} \\n {related_docs[1][0]} {related_docs[2][0]}\"}\n",
        "    ]\n",
        "\n",
        "    final_response = get_completion_from_messages(messages)\n",
        "    return final_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "smLw3xx-vWK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc1dc44-8d30-465b-b48b-a0ca831f9eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How is Timescale used in IoT?\n",
            "\n",
            "TimescaleDB is used in IoT for various purposes:\n",
            "\n",
            "1. **Data Collection**: IoT devices generate a lot of time-series data, such as temperature, wind speed, etc. TimescaleDB is used to store and manage this data efficiently.\n",
            "\n",
            "2. **Alarm Data**: IoT devices can send alarm information if a sensor or other parts malfunction. TimescaleDB can store these events for quick troubleshooting.\n",
            "\n",
            "3. **Status Data**: IoT devices report statistical data like disk utilization, temperature, and load. TimescaleDB can store this data for trend spotting and troubleshooting.\n",
            "\n",
            "4. **Administrative Data**: TimescaleDB can store metadata about the device, such as the date it was added to the system or the device's display name. This allows for easy correlation of metadata with time-series data for the events.\n",
            "\n",
            "TimescaleDB is chosen for IoT applications due to its ease of use, efficient handling of time-series data, and the ability to use SQL for querying data. It requires less administration compared to other databases and is easier for developers to onboard.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = process_input_with_retrieval(input)\n",
        "print(input)\n",
        "print()\n",
        "print(response)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "208tdm80vWK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f1189a-7fa8-463c-f646-88baae3fc511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me about Edeva and Hopara. How do they use Timescale?\n",
            "\n",
            "Edeva and Hopara are two companies that use TimescaleDB for different purposes.\n",
            "\n",
            "Edeva is a Swedish company that uses TimescaleDB to power their smart city infrastructure. They have developed an intelligent traffic system called Actibump, which uses radar to detect oncoming vehicles and adjusts the road surface to control speed. The system generates a large amount of time-series data, which is stored and analyzed using TimescaleDB. This allows Edeva to monitor the performance of their systems in real-time, and also to analyze historical data to identify trends and make improvements.\n",
            "\n",
            "Hopara, on the other hand, is a company that provides a visualization platform for any kind of data. They use TimescaleDB to power their real-time monitoring applications, often from IoT data collection or asset tracking of sensor-tagged devices. The real-time data is stored in a Timescale database for effective real-time querying. This allows Hopara to provide their users with up-to-date, actionable insights.\n"
          ]
        }
      ],
      "source": [
        "# We can also ask the model questions about specific documents in the database\n",
        "input_2 = \"Tell me about Edeva and Hopara. How do they use Timescale?\"\n",
        "response_2 = process_input_with_retrieval(input_2)\n",
        "print(input_2)\n",
        "print()\n",
        "print(response_2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GcqlMzQMvWK7",
        "ddliDgoPvWK8"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
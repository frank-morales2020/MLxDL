{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "5-6vhZfgBWWn"
      ],
      "authorship_tag": "ABX9TyN/D7YVQXuT1UPAM9Hv7F0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Mistral_Integration_with_Langchain_PostgreSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "QR-Ls8u1DWR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "5-6vhZfgBWWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Libraries to access Google Drive and OpenAI resources.\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "II600ZhIB3jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f73f58-5762-468c-b9de-c34f7c63e6f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definitions"
      ],
      "metadata": {
        "id": "TTWeLDwRBkqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_document(id,embedding):\n",
        "    #review_embedding=get_embedding(text)\n",
        "    ### INSERT INTO DB\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO documents\n",
        "        (id, embedding)\n",
        "        VALUES ('%s',\n",
        "                '%s')\"\"\" % (id,embedding))\n",
        "\n",
        "    conn.commit()\n",
        "    print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()"
      ],
      "metadata": {
        "id": "p6Qe93gMlSIB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_selection(query):\n",
        "      # PostGRES SQL Settings\n",
        "      import psycopg2 as ps\n",
        "      DB_NAME = \"postgres\"\n",
        "      DB_USER = \"postgres\"\n",
        "      DB_PASS = \"postgres\"\n",
        "      DB_HOST = \"localhost\"\n",
        "      DB_PORT = \"5432\"\n",
        "      conn = ps.connect(database=DB_NAME,\n",
        "                user=DB_USER,\n",
        "                password=DB_PASS,\n",
        "                host=DB_HOST,\n",
        "                port=DB_PORT)\n",
        "      cur = conn.cursor() # creating a cursor\n",
        "      cur.execute(\"\"\"\n",
        "          %s \"\"\"%query)\n",
        "      records = cur.fetchall()\n",
        "      print(\"Total rows are:  \", len(records))\n",
        "      print(\"Printing each row\")\n",
        "      print()\n",
        "      n=0\n",
        "      for row in records:\n",
        "          n=n+1\n",
        "          print(\"ROW %s: \"%n, row)\n",
        "      conn.close()\n",
        "      cur.close()\n",
        "      print()\n",
        "      print(\"QUERY SELECTION successfully\")\n",
        "      print()"
      ],
      "metadata": {
        "id": "2xUxFbS934W5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_extension(extension):\n",
        "      # PostGRES SQL Settings\n",
        "      import psycopg2 as ps\n",
        "      DB_NAME = \"postgres\"\n",
        "      DB_USER = \"postgres\"\n",
        "      DB_PASS = \"postgres\"\n",
        "      DB_HOST = \"localhost\"\n",
        "      DB_PORT = \"5432\"\n",
        "      conn = ps.connect(database=DB_NAME,\n",
        "                user=DB_USER,\n",
        "                password=DB_PASS,\n",
        "                host=DB_HOST,\n",
        "                port=DB_PORT)\n",
        "      cur = conn.cursor() # creating a cursor\n",
        "      cur.execute(\"\"\"DROP EXTENSION IF EXISTS %s CASCADE\"\"\"%extension)\n",
        "      cur.query\n",
        "      conn.commit()\n",
        "      cur.close()\n",
        "      conn.close()\n",
        "\n",
        "      print()\n",
        "      print(\"DROP EXTENSION %s successfully\"%extension)\n",
        "      print()"
      ],
      "metadata": {
        "id": "QWP0GA9Z4X4L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PostgreSQL: Definition and Configuration"
      ],
      "metadata": {
        "id": "3V4bZItQByQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install PSQL WITH DEV Libraries AND PG embedding\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "5bHMdQLvCGRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -pr /content/gdrive/MyDrive/tools/pg_embedding /content/\n",
        "%cd /content/pg_embedding/\n",
        "print()\n",
        "print('START: PG embedding COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "#!make uninstall\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()"
      ],
      "metadata": {
        "id": "CHZtNbRxBjX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67650338-2743-40e1-d219-d6639a616564"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pg_embedding\n",
            "\n",
            "START: PG embedding COMPILATION\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o embedding.o embedding.c\n",
            "g++ -Wall -Wpointer-arith -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -std=c++11 -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o hnswalg.o hnswalg.cpp\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o distfunc.o distfunc.c\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -shared -o embedding.so embedding.o hnswalg.o distfunc.o -lstdc++ -L/usr/lib/x86_64-linux-gnu -Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro -Wl,-z,now -L/usr/lib/llvm-14/lib  -Wl,--as-needed  \n",
            "/usr/bin/clang-14 -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -Wno-unused-command-line-argument -Wno-compound-token-split-by-macro -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o embedding.bc embedding.c\n",
            "/usr/bin/clang-14 -xc++ -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o hnswalg.bc hnswalg.cpp\n",
            "/usr/bin/clang-14 -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -Wno-unused-command-line-argument -Wno-compound-token-split-by-macro -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o distfunc.bc distfunc.c\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/usr/bin/install -c -m 755  embedding.so '/usr/lib/postgresql/14/lib/embedding.so'\n",
            "/usr/bin/install -c -m 644 .//embedding.control '/usr/share/postgresql/14/extension/'\n",
            "/usr/bin/install -c -m 644 .//embedding--0.3.5--0.3.6.sql .//embedding--0.3.5.sql .//embedding--0.3.6.sql  '/usr/share/postgresql/14/extension/'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode/embedding'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode'/embedding/\n",
            "/usr/bin/install -c -m 644 embedding.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "/usr/bin/install -c -m 644 hnswalg.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "/usr/bin/install -c -m 644 distfunc.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "cd '/usr/lib/postgresql/14/lib/bitcode' && /usr/lib/llvm-14/bin/llvm-lto -thinlto -thinlto-action=thinlink -o embedding.index.bc embedding/embedding.bc embedding/hnswalg.bc embedding/distfunc.bc\n",
            "END: PG embedding COMPILATION\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo -u postgres psql -c \"DROP EXTENSION IF EXISTS embedding CASCADE\"\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "print('')\n",
        "print(\"Extensions Available:\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print('')\n",
        "print(\"Extensions Used:\")\n",
        "query_selection(\"SELECT * FROM pg_extension\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3HFj_ZuNAKB",
        "outputId": "58f74164-2ee8-4612-8204-0238dab7c37f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "\n",
            "Extensions Available:\n",
            "Total rows are:   47\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extensions Used:\n",
            "Total rows are:   1\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PostGRES SQL Settings\n",
        "import psycopg2 as ps\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION vector CASCADE\"\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding CASCADE\"\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id BIGSERIAL PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{1,2,3}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{4,5,6}')\"%h\n",
        "print(hh)\n",
        "\n",
        "#del insert_document\n",
        "insert_document(1,'{1,2,3}')\n",
        "insert_document(2,'{4,5,6}')\n",
        "\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuZRG19-INLa",
        "outputId": "9fac9869-514f-4062-cab5-e53848444cba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "CREATE EXTENSION\n",
            "ERROR:  table \"documents\" does not exist\n",
            "CREATE TABLE\n",
            "INSERT INTO documents(id, embedding) VALUES (1,'{1,2,3}'), (2,'{4,5,6}')\n",
            "INSERT EMBEDDING {1,2,3} successfully\n",
            "INSERT EMBEDDING {4,5,6} successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -pr /content/gdrive/MyDrive/tools/pgvector /content/\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install\n",
        "#!make uninstall\n",
        "print('END: PG VECTOR COMPILATION')\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vsRfc01GoSN",
        "outputId": "07bdfe28-91b4-4ba6-a70f-d4e35527973f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pgvector\n",
            "\n",
            "START: PG VECTOR COMPILATION\n",
            "make: Nothing to be done for 'all'.\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/usr/bin/install -c -m 755  vector.so '/usr/lib/postgresql/14/lib/vector.so'\n",
            "/usr/bin/install -c -m 644 .//vector.control '/usr/share/postgresql/14/extension/'\n",
            "/usr/bin/install -c -m 644 .//sql/vector--0.1.0--0.1.1.sql .//sql/vector--0.1.1--0.1.3.sql .//sql/vector--0.1.3--0.1.4.sql .//sql/vector--0.1.4--0.1.5.sql .//sql/vector--0.1.5--0.1.6.sql .//sql/vector--0.1.6--0.1.7.sql .//sql/vector--0.1.7--0.1.8.sql .//sql/vector--0.1.8--0.2.0.sql .//sql/vector--0.2.0--0.2.1.sql .//sql/vector--0.2.1--0.2.2.sql .//sql/vector--0.2.2--0.2.3.sql .//sql/vector--0.2.3--0.2.4.sql .//sql/vector--0.2.4--0.2.5.sql .//sql/vector--0.2.5--0.2.6.sql .//sql/vector--0.2.6--0.2.7.sql .//sql/vector--0.2.7--0.3.0.sql .//sql/vector--0.3.0--0.3.1.sql .//sql/vector--0.3.1--0.3.2.sql .//sql/vector--0.3.2--0.4.0.sql .//sql/vector--0.4.0--0.4.1.sql .//sql/vector--0.4.1--0.4.2.sql .//sql/vector--0.4.2--0.4.3.sql .//sql/vector--0.4.3--0.4.4.sql .//sql/vector--0.4.4--0.5.0.sql .//sql/vector--0.5.0--0.5.1.sql .//sql/vector--0.5.1.sql  '/usr/share/postgresql/14/extension/'\n",
            "/bin/mkdir -p '/usr/include/postgresql/14/server/extension/vector/'\n",
            "/usr/bin/install -c -m 644   .//src/vector.h '/usr/include/postgresql/14/server/extension/vector/'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode/vector'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnsw.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswbuild.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswinsert.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswscan.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswvacuum.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfbuild.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfflat.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfinsert.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfkmeans.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfscan.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfvacuum.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/vector.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "cd '/usr/lib/postgresql/14/lib/bitcode' && /usr/lib/llvm-14/bin/llvm-lto -thinlto -thinlto-action=thinlink -o vector.index.bc vector/src/hnsw.bc vector/src/hnswbuild.bc vector/src/hnswinsert.bc vector/src/hnswscan.bc vector/src/hnswutils.bc vector/src/hnswvacuum.bc vector/src/ivfbuild.bc vector/src/ivfflat.bc vector/src/ivfinsert.bc vector/src/ivfkmeans.bc vector/src/ivfscan.bc vector/src/ivfutils.bc vector/src/ivfvacuum.bc vector/src/vector.bc\n",
            "END: PG VECTOR COMPILATION\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo -u postgres psql -c \"DROP EXTENSION IF EXISTS embedding CASCADE\"\n",
        "print('')\n",
        "print(\"Extensions Available:\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print('')\n",
        "print(\"Extensions Used:\")\n",
        "query_selection(\"SELECT * FROM pg_extension\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_gzw1GqCbve",
        "outputId": "2fe8f579-7a77-4578-ee07-ae9b49100432"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extensions Available:\n",
            "Total rows are:   48\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('vector',)\n",
            "ROW 48:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extensions Used:\n",
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "ROW 2:  (16384, 'embedding', 10, 2200, True, '0.3.6', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PostGRES SQL Settings\n",
        "import psycopg2 as ps\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP EXTENSION embedding CASCADE\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION vector\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id BIGSERIAL PRIMARY KEY, embedding vector(3))\"\n",
        "\n",
        "insert_document(1,'[1,2,3]')\n",
        "insert_document(2,'[4,5,6]')\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw (embedding vector_l2_ops)\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding ann_cos_ops) WITH (m=3)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXP1e65MejS3",
        "outputId": "178e7a53-f6f2-4434-c886-7d5462982797"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "DROP EXTENSION\n",
            "CREATE EXTENSION\n",
            "DROP TABLE\n",
            "CREATE TABLE\n",
            "INSERT EMBEDDING [1,2,3] successfully\n",
            "INSERT EMBEDDING [4,5,6] successfully\n",
            "CREATE INDEX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## <-> and <=> Euclidean (L2) and Cosine distances NO Manhattan distance: <~>\n",
        "query_selection(\"SELECT * FROM documents ORDER BY embedding::vector <-> '[3,1,2]' LIMIT 5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k04syAHjJnWZ",
        "outputId": "9e79282f-1069-4ac6-d771-40e6950ce476"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (1, '[1,2,3]')\n",
            "ROW 2:  (2, '[4,5,6]')\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('')\n",
        "print(\"Extensions Available:\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print('')\n",
        "print(\"Extensions Used:\")\n",
        "query_selection(\"SELECT * FROM pg_extension\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cYNpki6VpeX",
        "outputId": "7ece6070-1922-43e4-d832-6e9b29c9d95e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extensions Available:\n",
            "Total rows are:   48\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('vector',)\n",
            "ROW 48:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extensions Used:\n",
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "ROW 2:  (16414, 'vector', 10, 2200, True, '0.5.1', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Euclidean (L2) distance index:\n",
        "#CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5);\n",
        "#SET enable_seqscan = off;\n",
        "#SELECT id FROM documents ORDER BY embedding <-> array[3,3,3] LIMIT 1;\n",
        "\n",
        "# Cosine distance index:\n",
        "#CREATE INDEX ON documents USING hnsw(embedding ann_cos_ops) WITH (dims=3, m=3, efconstruction=5, efsearch=5);\n",
        "#SET enable_seqscan = off;\n",
        "#SELECT id FROM documents ORDER BY embedding <=> array[3,3,3] LIMIT 1;\n",
        "\n",
        "# Manhattan distance index:\n",
        "#CREATE INDEX ON documents USING hnsw(embedding ann_manhattan_ops) WITH (dims=3, m=3, efconstruction=5, efsearch=5);\n",
        "#SET enable_seqscan = off;\n",
        "#SELECT id FROM documents ORDER BY embedding <~> array[3,3,3] LIMIT 1;"
      ],
      "metadata": {
        "id": "DOe_mjxVmeNN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS - Document Upload"
      ],
      "metadata": {
        "id": "0vk3VbT-q4X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://platform.openai.com/docs/guides/text-generation\n",
        "%pip install openai==0.28  --root-user-action=ignore\n",
        "%pip install \"unstructured[all-docs]\"\n",
        "!pip install gradio --quiet\n",
        "!pip install xformer --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install unstructured --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "!pip install pypdf\n",
        "%pip install tiktoken"
      ],
      "metadata": {
        "id": "2_UzTRgRsA7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf /content/*.pdf\n",
        "!mkdir -p /content/data/\n",
        "%cd /content/data/\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "urls = [\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "filenames = [\n",
        "    'AMZN-2022-Shareholder-Letter.pdf',\n",
        "    'AMZN-2021-Shareholder-Letter.pdf',\n",
        "    'AMZN-2020-Shareholder-Letter.pdf',\n",
        "    'AMZN-2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "metadata = [\n",
        "    dict(year=2022, source=filenames[0]),\n",
        "    dict(year=2021, source=filenames[1]),\n",
        "    dict(year=2020, source=filenames[2]),\n",
        "    dict(year=2019, source=filenames[3])]\n",
        "\n",
        "data_root = \"/content/data/\"\n",
        "\n",
        "for idx, url in enumerate(urls):\n",
        "    file_path = data_root + filenames[idx]\n",
        "    #print(file_path)\n",
        "    urlretrieve(url, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJGGZWI_q_Cb",
        "outputId": "eefc5ddc-4b03-47f4-f572-3e54235e8d19"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader, PdfWriter\n",
        "import glob\n",
        "\n",
        "local_pdfs = glob.glob(data_root + '*.pdf')\n",
        "\n",
        "for local_pdf in local_pdfs:\n",
        "    pdf_reader = PdfReader(local_pdf)\n",
        "    pdf_writer = PdfWriter()\n",
        "    for pagenum in range(len(pdf_reader.pages)-3):\n",
        "        page = pdf_reader.pages[pagenum]\n",
        "        pdf_writer.add_page(page)\n",
        "\n",
        "    with open(local_pdf, 'wb') as new_file:\n",
        "        new_file.seek(0)\n",
        "        pdf_writer.write(new_file)\n",
        "        new_file.truncate()"
      ],
      "metadata": {
        "id": "FwBAG6UprEbM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "\n",
        "#%cd /content/data\n",
        "\n",
        "documents = []\n",
        "\n",
        "for idx, file in enumerate(filenames):\n",
        "    loader = PyPDFLoader(data_root + file)\n",
        "    document = loader.load()\n",
        "    for document_fragment in document:\n",
        "        document_fragment.metadata = metadata[idx]\n",
        "\n",
        "    documents += document\n",
        "\n",
        "# - in our testing Character split works better with this PDF data set\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 100,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVN8KXBUrI7v",
        "outputId": "adc9ed0e-7b39-4199-8e83-def543e81220"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 25\n",
            "# of Document Chunks: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain and LLM Configurations"
      ],
      "metadata": {
        "id": "Yo-ZsxiyCl3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "#!pip install accelerate\n",
        "\n",
        "import torch\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#from transformers import AutoTokenizer, MistralForCausalLM"
      ],
      "metadata": {
        "id": "1ygWwhAGQkpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "MODEL_NAME='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 512\n",
        "generation_config.temperature = 0.9\n",
        "generation_config.top_p = 0.9\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "#model.to(device)\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "Ni9Z3osgRDtq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "OBoSd2ZSQmOK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Vector Queries"
      ],
      "metadata": {
        "id": "iqneJqxTciYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "import os\n",
        "collection_name='AWS'\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "%pip install colab-env\n",
        "import colab_env\n",
        "\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "# https://supabase.com/blog/fewer-dimensions-are-better-pgvector\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "# PostGRES SQL Settings\n",
        "import psycopg2 as ps\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"DROP EXTENSION IF EXISTS vector CASCADE\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id BIGSERIAL PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{1,2,3}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{4,5,6}')\"%h\n",
        "print(hh)\n",
        "\n",
        "#del insert_document\n",
        "insert_document(1,'{1,2,3}')\n",
        "insert_document(2,'{4,5,6}')\n",
        "\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX9u6W4FqLgs",
        "outputId": "7eba392f-80d9-4704-f83a-7ad90a05fa90"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab-env in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: python-dotenv<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from colab-env) (0.21.1)\n",
            "/content\n",
            "/content\n",
            "NOTICE:  drop cascades to column embedding of table documents\n",
            "DROP EXTENSION\n",
            "CREATE EXTENSION\n",
            "DROP TABLE\n",
            "CREATE TABLE\n",
            "INSERT INTO documents(id, embedding) VALUES (1,'{1,2,3}'), (2,'{4,5,6}')\n",
            "INSERT EMBEDDING {1,2,3} successfully\n",
            "INSERT EMBEDDING {4,5,6} successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "#del embeddings\n",
        "print(\"Extension available\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print()\n",
        "print(\"Extension used\")\n",
        "query_selection(\"SELECT * FROM pg_extension order by 1\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WAxu1Z2AX7R",
        "outputId": "a343f929-fc56-4d19-bf42-086cc5bca0cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extension available\n",
            "Total rows are:   48\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('vector',)\n",
            "ROW 48:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extension used\n",
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "ROW 2:  (16523, 'embedding', 10, 2200, True, '0.3.6', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")\n",
        "\n",
        "query = \"How has AWS evolved?\"\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print(query)\n",
        "\n",
        "#query = db.embedding_function.embed_query(query)\n",
        "results_with_scores = db.similarity_search_with_score(query)\n",
        "\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"Content: {doc.page_content}\\nMetadata: {doc.metadata}\\nScore: {score}\\n\\n\")\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "W8OUhRQ3sYaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ce2cc6-c4e9-4898-b079-0a3fd34457e0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "How has AWS evolved?\n",
            "Content: customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.5203086\n",
            "\n",
            "\n",
            "Content: in AWS. Our new customer pipeline is robust, as are our active migrations. Many companies usediscontinuous periods like this to step back and determine what they strategically want to change, and wefind an increasing number of enterprises opting out of managing their own infrastructure, and preferring tomove to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantlyfor customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.52052546\n",
            "\n",
            "\n",
            "Content: done innovating here,and this long-term investment should prove fruitful for both customers and AWS. AWS is still in the earlystages of its evolution, and has a chance for unusual growth in the next decade.\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.52200025\n",
            "\n",
            "\n",
            "Content: We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.52281857\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter={\"year\": 2021}\n",
        "\n",
        "results_with_scores = db.similarity_search_with_score(query,\n",
        "  filter=filter)\n",
        "\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"Content: {doc.page_content}\\nMetadata: {doc.metadata}\\nScore: {score}\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLTZR4MwkI6-",
        "outputId": "b9b8f261-daae-40cd-99b4-c808c659b9f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content: We also are trying to increase the amount of affordable housing in the communities in which we have a\n",
            "large presence. Our more than $2 billion Housing Equity Fund that we started a year ago has already allocated$1.2 billion toward affordable housing initiatives in the areas around Washington state’s Puget Soundregion, Arlington (Virginia), and Nashville (Tennessee).\n",
            "A final quick example is Kuiper, our low Earth orbit satellite network that we’re spending over $10 billion\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.7214524\n",
            "\n",
            "\n",
            "Content: 7/Brace Yourself for Failure : If you invent a lot, you will fail more often than you wish. Nobody likes this\n",
            "part, but it comes with the territory. When it’s clear that we’ve launched something that won’t work, we makesure we’ve learned from what didn’t go well, and secure great landing places for team members whodelivered well—or your best people will hesitate to work on new initiatives.\n",
            "Albert Einstein is sometimes credited with describing compound interest as the eighth wonder of the world\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.7280068\n",
            "\n",
            "\n",
            "Content: the $15 minimum wage (which is more than double the federal minimum wage), but haven’t stopped there. Wecontinued to increase compensation such that our average starting hourly salary is currently over $18.Along with this compensation, we offer very robust benefits, including full health insurance, a 401K plan,up to 20 weeks of parental leave, and full tuition coverage for associates who want to get a college education(whether they remain with us or not). We’re not close to being done in how we improve the\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.73252106\n",
            "\n",
            "\n",
            "Content: Conversely, our Consumer revenue grew dramatically in 2020. In 2020, Amazon’s North America and\n",
            "International Consumer revenue grew 39% Y oY on the very large 2019 revenue base of $245 billion; and,this extraordinary growth extended into 2021 with revenue increasing 43% Y oY in Q1 2021. These areastounding numbers. We realized the equivalent of three years’ forecasted growth in about 15 months.\n",
            "As the world opened up again starting in late Q2 2021, and more people ventured out to eat, shop, and travel,\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.7342812\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter={\"year\": 2022}\n",
        "\n",
        "results_with_scores = db.similarity_search_with_score(query,\n",
        "  filter=filter)\n",
        "\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"Content: {doc.page_content}\\nMetadata: {doc.metadata}\\nScore: {score}\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGWYwUD0kZ0A",
        "outputId": "87a96f32-a0fe-48cb-915c-cd483977a9e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content: with new challenges. For instance, in the 2001 dot-com crash, we had to secure letters of credit to buyinventory for the holidays, streamline costs to deliver better profitability for the business, yet still prioritizedthe long-term customer experience and business we were trying to build (if you remember, we actuallylowered prices in most of our categories during that tenuous 2001 period). Y ou saw this sort of balancingagain in 2008-2009 as we endured the recession provoked by the mortgage-backed\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.72542846\n",
            "\n",
            "\n",
            "Content: position than when we entered it. There are several reasons for it and I’ve mentioned many of them above.But, there are two relatively simple statistics that underline our immense future opportunity. While we have aconsumer business that’s $434B in 2022, the vast majority of total market segment share in global retailstill resides in physical stores (roughly 80%). And, it’s a similar story for Global IT spending, where we haveAWS revenue of $80B in 2022, with about 90% of Global IT spending still\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.72808945\n",
            "\n",
            "\n",
            "Content: When I joined Amazon in 1997, we had booked $15M in revenue in 1996, were a books-only retailer, didnot have a third-party marketplace, and only shipped to addresses in the US. Today, Amazon sells nearly everyphysical and digital retail item you can imagine, with a vibrant third-party seller ecosystem that accountsfor 60% of our unit sales, and reaches customers in virtually every country around the world. Similarly,building a business around a set of technology infrastructure services in the cloud was not\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.7299149\n",
            "\n",
            "\n",
            "Content: invention, and asked ourselves whether we had conviction about each initiative’s long-term potential to driveenough revenue, operating income, free cash flow, and return on invested capital. In some cases, it led to usshuttering certain businesses. For instance, we stopped pursuing physical store concepts like our Bookstoresand 4 Star stores, closed our Amazon Fabric and Amazon Care efforts, and moved on from some newerdevices where we didn’t see a path to meaningful returns. In other cases, we looked at\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.7301933\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt and Completions Examples"
      ],
      "metadata": {
        "id": "yM9zL38DQuKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query1 = \"who is the President of the USA?\"\n",
        "query2 = \"Who won the baseball World Series in 2020? and Who Lost\"\n",
        "\n",
        "device=\"cuda\"\n",
        "def prompt_completion(query):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"%s\"%query}\n",
        "    ]\n",
        "\n",
        "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = encodeds.to(device)\n",
        "\n",
        "    #https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id\n",
        "\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=512, do_sample=True, negative_prompt_attention_mask='attention_mask',\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(generated_ids)\n",
        "    print()\n",
        "    print()\n",
        "    result=decoded[0].replace('<s> [INST] %s [/INST]'%query,\"\")\n",
        "    result=result.replace('</s>',\"\")\n",
        "    print('Prompt: %s'%query)\n",
        "    print('-'*80)\n",
        "    print('Answer: %s'%result)\n",
        "\n",
        "prompt_completion(query)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query1)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query2)\n",
        "\n",
        "query3 = \"what is the 20.5% of 40?\"\n",
        "query4 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "query5 ='Which country has the most natural lakes? Answer with only the country name.'\n",
        "\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query3)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query4)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query5)\n",
        "\n",
        "\n",
        "query6 = \"How AWS has evolved?\"\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query6)\n",
        "\n",
        "query7 = \"Summarize this: Mistral LLM and Langchain intgegration. Overview and Tutorial with practical examples. Mistral LLM is a large language model developed by Mistral AI, a French startup making waves in\\\n",
        "the tech community. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. \\\n",
        "It uses sliding window attention, grouped query attention, and byte-fallback BPE tokenizer to achieve its impressive performance. \\\n",
        "Mistral LLM and Mistral LLM MoE 8x7B are language models developed by Mistral AI, a French startup. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. \\\n",
        "On the other hand, Mistral LLM MoE 8x7B[1c] is an open-weight model that employs a Mixture of Expert (MoE) architecture to generate human-like responses. While there is no direct information about the relationship between Mistral LLM and Mistral LLM MoE 8x7B, it is safe to assume that Mistral LLM MoE 8x7B is an extension of Mistral LLM. \\\n",
        "Mistral LLM MoE 8x7B is a larger model than Mistral LLM, with eight experts, each with seven billion parameters.\\\n",
        "Mistral LLM is designed for various natural language processing tasks, including text generation, summarization, and question-answering.\\\n",
        "It has outperformed other large language models, such as Llama 2 13B, on all benchmarks tested.\\\n",
        "Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs. \\\n",
        "The model can be used to understand images and answer questions about them. It is best at answering general questions about what is present in the pictures, but it is not yet optimized to answer detailed questions about the location of specific objects in an image.\\\n",
        "Mistral LLM is a significant step forward in natural language processing. Its impressive performance and large parameter count make it a powerful [3] tool for developers working on a wide range of NLP tasks.\\\n",
        "In summary, Mistral LLM is a powerful language model that has been shown to outperform other large language models on all benchmarks tested.\\\n",
        "It is available to developers via the gpt-4-vision-preview model and the Chat Completions API and can be used for a wide range of natural language processing tasks.\\\n",
        "Mistral LLM has been benchmarked against other large language models, such as Llama 2 13B, and it has been shown to outperform all benchmarks tested.\\In particular, Mistral 7B outperforms Llama 2 13B on all metrics measured and is on par with Llama 34B.\\\n",
        "It is important to note that the benchmarks used to compare these models can vary widely, and the results may not be directly comparable.\\\n",
        "However, the fact that Mistral LLM has been shown to outperform other large language models on multiple benchmarks is a testament to its impressive performance.\\\n",
        "LangChain is an open-source AI abstraction library that easily integrates large language models (LLMs) like GPT-4/LLaMa 2 into applications.\\\n",
        "LangChain provides a simplified framework for querying LLMs to generate text, code, translations, and more using Python [9]. LangChain is a generic interface for nearly any LLM, providing a centralized development environment to build and integrate LLM applications with external data sources and software workflows.\\LangChain's integration with LLMs like OpenAI, Cohere, and Hugging Face is fundamental to its functionality [11].\\\n",
        "Mistral LLM is a large language model developed by Mistral AI that has been shown to outperform other large language models, such as Llama 2 13B, on all benchmarks tested.\\\n",
        "Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs [12].\\\n",
        "The model can be used to understand images and answer questions about them.\\\n",
        "While there is no direct information about the integration of LangChain and Mistral LLM, LangChain's ability to integrate with nearly any LLM suggests that it can be used with Mistral LLM.\\\n",
        "The combination of LangChain and Mistral LLM could provide a powerful tool for developers working on natural language processing tasks [9-14]. However, it is essential to consider the model's limitations as you explore what use cases it can apply to.\\\n",
        "As an illustration of how the concepts and terms mentioned above work together, we implemented a jyputer notebook thoroughly tested in Google Colab based on GPU T4 devices.\\\n",
        "The notebook covers how we can use Langchain using OpenAI embedding and a PostgreSQL extension called pg_embedding to upload actual documents from AWS S3.\\\n",
        "Also, as a model for LLM, we used mistral/Mistral-7B-v0.1 from the hugging face repository, which created an entire open-source integrated ecosystem to support generative AI solutions.\\\n",
        "I also share a notebook of using MLxDL/Mistral_IN_AWS.ipynb at main · frank-morales2020/MLxDL (github.com)\\\n",
        "You can access this case in my GitHub ML/DL development repository shown below: MLxDL/Mistral_Integration_with_Langchain_PostgreSQL.ipynb at main · frank-morales2020/MLxDL (github.com)\"\n",
        "\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query7)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owzp3YYXo-dp",
        "outputId": "37444542-0e60-4b95-9265-85589eeb6fe7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  First, let's figure out how many cones the $10 bill can buy.\n",
            "\n",
            "We know that:\n",
            "1 cone costs $1.25\n",
            "\n",
            "So,\n",
            "$10 (10 dollar bills) = 10 x $1.25 = $12.50\n",
            "\n",
            "The $10 bill can buy 12.50 / $1.25 = 10 cones.\n",
            "\n",
            "Now, since you only bought 6 cones with the $10 bill, you didn't get any change back. You spent exactly what you paid.\n",
            "\n",
            "So, you got no dollars back.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: who is the President of the USA?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  As of August 2021, the President of the United States of America is Joseph R. Biden Jr., commonly known as Joe Biden. Biden was inaugurated on January 20, 2021, after being elected in the 2020 presidential election.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Who won the baseball World Series in 2020? and Who Lost\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  The Los Angeles Dodgers won the 2020 baseball World Series. They defeated the Tampa Bay Rays in four games (3-1) to win their first World Series title since 1988.\n",
            "\n",
            "The Tampa Bay Rays lost the World Series in 2020. Despite making their fourth World Series appearance in a row and their first since moving to Tampa Bay from St. Petersburg in 2008, the Rays were unable to keep up with the Dodgers and lost the championship.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: what is the 20.5% of 40?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  20.5 is equal to 0.205 or 20.5 cents. But to calculate the equivalent percentage of 40, we can use the following calculation:\n",
            "\n",
            "0.205 x 100 / 1 = 20.5%\n",
            "\n",
            "Therefore, 20.5% of 40 is 8.20 or 8.20 cents.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. Overfitting occurs when a model is too complex and highly sensitive to the training data, leading to poor performance on unseen data. Regularization helps to reduce the complexity of the model by adding a constraint on the model parameters, such as L1 or L2 regularization. The penalty term encourages the model to use smaller weights or coefficients, resulting in a more generalized model that performs better on new data. The trade-off is that regularization may lead to a less accurate model on the training data, but a more accurate model on new data.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Which country has the most natural lakes? Answer with only the country name.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Canada\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: How AWS has evolved?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  AWS, or Amazon Web Services, has been providing cloud computing services to individuals and organizations for over 15 years. With its vast experience in the cloud computing market, AWS has evolved significantly over the years, making it easier for companies to leverage the power of the cloud for their needs.\n",
            "\n",
            "Here are some of the key ways in which AWS has evolved:\n",
            "\n",
            "1. Adding More Services: AWS has continuously expanded its list of services, giving its users access to a wide range of tools, platforms, and applications to help them build and deploy their applications quickly and easily.\n",
            "\n",
            "2. Enhancing Security: AWS places a strong emphasis on security, and it has introduced several features over the years to help protect its customers' data, such as multi-factor authentication, data encryption, and network security.\n",
            "\n",
            "3. Incorporating Big Data: AWS has made big data more accessible to its users, introducing services such as Amazon Redshift, Amazon Kinesis, and Amazon Athena, that allow users to store, process, and analyze large amounts of data quickly and efficiently.\n",
            "\n",
            "4. Scalability: AWS has made it easy for companies to scale their applications up or down depending on their needs, ensuring that they always have the resources they need to support their growth.\n",
            "\n",
            "5. Pricing Model: AWS has evolved its pricing model to be more flexible and cost-effective, allowing users to pay for the resources they use and avoid over-provisioning, which can save them money in the long run.\n",
            "\n",
            "Overall, AWS has evolved significantly to meet the changing needs of its customers, providing a wide range of tools and services to help businesses of all sizes take advantage of cloud computing.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Summarize this: Mistral LLM and Langchain intgegration. Overview and Tutorial with practical examples. Mistral LLM is a large language model developed by Mistral AI, a French startup making waves inthe tech community. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. It uses sliding window attention, grouped query attention, and byte-fallback BPE tokenizer to achieve its impressive performance. Mistral LLM and Mistral LLM MoE 8x7B are language models developed by Mistral AI, a French startup. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. On the other hand, Mistral LLM MoE 8x7B[1c] is an open-weight model that employs a Mixture of Expert (MoE) architecture to generate human-like responses. While there is no direct information about the relationship between Mistral LLM and Mistral LLM MoE 8x7B, it is safe to assume that Mistral LLM MoE 8x7B is an extension of Mistral LLM. Mistral LLM MoE 8x7B is a larger model than Mistral LLM, with eight experts, each with seven billion parameters.Mistral LLM is designed for various natural language processing tasks, including text generation, summarization, and question-answering.It has outperformed other large language models, such as Llama 2 13B, on all benchmarks tested.Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs. The model can be used to understand images and answer questions about them. It is best at answering general questions about what is present in the pictures, but it is not yet optimized to answer detailed questions about the location of specific objects in an image.Mistral LLM is a significant step forward in natural language processing. Its impressive performance and large parameter count make it a powerful [3] tool for developers working on a wide range of NLP tasks.In summary, Mistral LLM is a powerful language model that has been shown to outperform other large language models on all benchmarks tested.It is available to developers via the gpt-4-vision-preview model and the Chat Completions API and can be used for a wide range of natural language processing tasks.Mistral LLM has been benchmarked against other large language models, such as Llama 2 13B, and it has been shown to outperform all benchmarks tested.\\In particular, Mistral 7B outperforms Llama 2 13B on all metrics measured and is on par with Llama 34B.It is important to note that the benchmarks used to compare these models can vary widely, and the results may not be directly comparable.However, the fact that Mistral LLM has been shown to outperform other large language models on multiple benchmarks is a testament to its impressive performance.LangChain is an open-source AI abstraction library that easily integrates large language models (LLMs) like GPT-4/LLaMa 2 into applications.LangChain provides a simplified framework for querying LLMs to generate text, code, translations, and more using Python [9]. LangChain is a generic interface for nearly any LLM, providing a centralized development environment to build and integrate LLM applications with external data sources and software workflows.\\LangChain's integration with LLMs like OpenAI, Cohere, and Hugging Face is fundamental to its functionality [11].Mistral LLM is a large language model developed by Mistral AI that has been shown to outperform other large language models, such as Llama 2 13B, on all benchmarks tested.Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs [12].The model can be used to understand images and answer questions about them.While there is no direct information about the integration of LangChain and Mistral LLM, LangChain's ability to integrate with nearly any LLM suggests that it can be used with Mistral LLM.The combination of LangChain and Mistral LLM could provide a powerful tool for developers working on natural language processing tasks [9-14]. However, it is essential to consider the model's limitations as you explore what use cases it can apply to.As an illustration of how the concepts and terms mentioned above work together, we implemented a jyputer notebook thoroughly tested in Google Colab based on GPU T4 devices.The notebook covers how we can use Langchain using OpenAI embedding and a PostgreSQL extension called pg_embedding to upload actual documents from AWS S3.Also, as a model for LLM, we used mistral/Mistral-7B-v0.1 from the hugging face repository, which created an entire open-source integrated ecosystem to support generative AI solutions.I also share a notebook of using MLxDL/Mistral_IN_AWS.ipynb at main · frank-morales2020/MLxDL (github.com)You can access this case in my GitHub ML/DL development repository shown below: MLxDL/Mistral_Integration_with_Langchain_PostgreSQL.ipynb at main · frank-morales2020/MLxDL (github.com)\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Mistral LLM is a large language model developed by Mistral AI with 7 billion parameters, which makes it one of the most significant language models available. It has outperformed other large language models on all benchmarks tested and is available to developers via the gpt-4-vision-preview model and the Chat Completions API. Mistral LLM MoE 8x7B is an open-weight model that employs a Mixture of Experts (MoE) architecture to generate human-like responses. LangChain is an open-source AI abstraction library that integrates large language models like GPT-4/LLaMa 2 into applications and provides a simplified framework for querying LLMs to generate text, code, translations, and more using Python. The combination of LangChain and Mistral LLM could provide a powerful tool for developers working on natural language processing tasks. However, it is important to consider the model's limitations as you explore what use cases it can apply to. Also, we have implemented a jyputer notebook that demonstrates how to use Langchain using OpenAI embedding and a PostgreSQL extension called pg_embedding to upload actual documents from AWS S3 and another notebook that uses MLxDL/Mistral_IN_AWS for Mistral integration with Langchain and PostgreSQL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token = os.getenv(\"HF_TOKEN\")\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig, GenerationConfig, pipeline\n",
        "from transformers import AutoTokenizer, MistralForCausalLM\n",
        "\n",
        "#model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",padding_side=\"left\")\n",
        "#tokenizer.pad_token = tokenizer.eos_token # Most LLMs don't have a pad token by default\n",
        "\n",
        "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py\n",
        "\n",
        "# Generate\n",
        "#print()\n",
        "#generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
        "\n",
        "#generate_ids = model.generate(model_inputs, max_new_tokens=512, do_sample=True, negative_prompt_attention_mask='attention_mask',\n",
        "#                pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#response=tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "#print(response)\n",
        "print()\n",
        "\n",
        "#query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query=\"What is your favourite condiment?\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"%s\"%query},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "model_inputs = encodeds.to(device)\n",
        "#model.to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=512, do_sample=True, negative_prompt_attention_mask='attention_mask',\n",
        "                pad_token_id=tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "AY3FuMUBU-9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8fdabf4-0dc6-495b-b49b-bb0d390edbe6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!  [INST] Do you have mayonnaise recipes? [/INST]Of course! Here's a simple recipe for classic mayonnaise that you can easily whip up at home:\n",
            "\n",
            "Ingredients:\n",
            "\n",
            "* 1 cup whole milk\n",
            "* 1 tablespoon white vinegar\n",
            "* 1/2 teaspoon salt\n",
            "* 2 large egg yolks\n",
            "* 1/2 cup vegetable oil\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. In a medium bowl, whisk together the milk, vinegar, and salt until well combined.\n",
            "2. Crack in the egg yolks, one at a time, whisking each one before continuing to the next.\n",
            "3. Slowly drizzle the oil into the mixture, whisking continuously as you go.\n",
            "4. Once all the oil has been added, whisk for a couple more minutes to thicken the sauce and remove any lumps.\n",
            "5. Taste and adjust seasoning as needed.\n",
            "\n",
            "That's it! This classic mayonnaise recipe is perfect for a variety of dishes, from burgers to sandwiches and salads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers --upgrade\n",
        "#device = \"cuda\"\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "query = \"who is Barack Obama?\"\n",
        "result = llm(query)\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print()"
      ],
      "metadata": {
        "id": "jREtnaDouFMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "dd0efdc8-135b-4b70-91d2-43d28fd80a7e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>who is Barack Obama?</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>\nA: Q: Barack Obama was the 44th President of the United States. He served from 2009 to 2017 and was the first African American to hold the office. He is known for his policies related to healthcare, same-sex marriage, and foreign diplomacy.</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who is Steve Jobs?\"\n",
        "result = llm(query)\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "vhbJ_lNEs7IJ",
        "outputId": "202495e7-714f-4e90-f2f1-ec42fda60037"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>who is Steve Jobs?</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>\nA: Steve Jobs was the co-founder and CEO of Apple Inc. He played a key role in popularizing personal computing, launching products like the iPhone and iPad, and transforming Apple into one of the most valuable companies in the world. Jobs passed away in 2011 at the age of 56.</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IN"
      ],
      "metadata": {
        "id": "-I-iJ4fhmN-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "#import colab_env\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "#retriever = db.as_retriever(search_type=\"similarity_score_threshold\",search_kwargs='1')\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "# create a chain to answer questions\n",
        "#qa = RetrievalQA.from_chain_type(\n",
        "#     llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "     llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "query = \"How AWS has evolved?\"\n",
        "\n",
        "result = qa(query)\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print()\n",
        "\n",
        "print()\n",
        "print('CHAIN to answer questions')\n",
        "print(\"-\" * 80)\n",
        "result = qa({\"query\": query})\n",
        "print(f'Query: {result[\"query\"]}\\n')\n",
        "print(f'Result: {result[\"result\"]}\\n')\n",
        "print(f'Context Documents: ')\n",
        "for srcdoc in result[\"source_documents\"]:\n",
        "      print(f'{srcdoc}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "8m-J59SDN9yO",
        "outputId": "fe53d7c0-490c-479e-9dc6-9c3d569e8806"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>How AWS has evolved?</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>{'query': 'How AWS has evolved?', 'result': '\\n\\nAWS started out as an online retail platform for books and then expanded into cloud computing services. Since its launch, it has continued to innovate and offer new features and capabilities to its users. Today, AWS is one of the largest and most successful cloud computing platforms in the world, with a wide range of services and tools that help businesses of all sizes manage their technology infrastructure. The evolution of AWS has been driven by its focus on customer needs and its ability to adapt quickly to changing market conditions.', 'source_documents': [Document(page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}), Document(page_content='We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]}</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "CHAIN to answer questions\n",
            "--------------------------------------------------------------------------------\n",
            "Query: How AWS has evolved?\n",
            "\n",
            "Result:  AWS has evolved significantly since its launch in 2006. Initially, it offered basic cloud computing services such as storage and computing resources. Over time, it has added much more functionality, including machine learning, data analytics, database management, serverless computing, Internet of Things (IoT) connectivity, and more. This has made AWS a game-changer for many industries, enabling them to innovate faster and reduce costs. Today, AWS is one of the largest and most profitable companies in the world, with an $85 billion annual revenue run rate.\n",
            "\n",
            "Context Documents: \n",
            "page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.' metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "\n",
            "page_content='We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009' metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
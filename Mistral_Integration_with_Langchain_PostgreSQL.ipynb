{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "5-6vhZfgBWWn",
        "TTWeLDwRBkqx",
        "yM9zL38DQuKr"
      ],
      "authorship_tag": "ABX9TyNscWglodoSS5uyub5OrrbT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Mistral_Integration_with_Langchain_PostgreSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "QR-Ls8u1DWR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "5-6vhZfgBWWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Libraries to access Google Drive and OpenAI resources.\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "II600ZhIB3jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b596fc-eab4-435d-f176-ae69e3002a8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definitions"
      ],
      "metadata": {
        "id": "TTWeLDwRBkqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_document(id,embedding):\n",
        "    #review_embedding=get_embedding(text)\n",
        "    ### INSERT INTO DB\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO documents\n",
        "        (id, embedding)\n",
        "        VALUES ('%s',\n",
        "                '%s')\"\"\" % (id,embedding))\n",
        "\n",
        "    conn.commit()\n",
        "    print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()"
      ],
      "metadata": {
        "id": "p6Qe93gMlSIB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_selection(query):\n",
        "      # PostGRES SQL Settings\n",
        "      import psycopg2 as ps\n",
        "      DB_NAME = \"postgres\"\n",
        "      DB_USER = \"postgres\"\n",
        "      DB_PASS = \"postgres\"\n",
        "      DB_HOST = \"localhost\"\n",
        "      DB_PORT = \"5432\"\n",
        "      conn = ps.connect(database=DB_NAME,\n",
        "                user=DB_USER,\n",
        "                password=DB_PASS,\n",
        "                host=DB_HOST,\n",
        "                port=DB_PORT)\n",
        "      cur = conn.cursor() # creating a cursor\n",
        "      cur.execute(\"\"\"\n",
        "          %s \"\"\"%query)\n",
        "      records = cur.fetchall()\n",
        "      print(\"Total rows are:  \", len(records))\n",
        "      print(\"Printing each row\")\n",
        "      print()\n",
        "      n=0\n",
        "      for row in records:\n",
        "          n=n+1\n",
        "          print(\"ROW %s: \"%n, row)\n",
        "      conn.close()\n",
        "      cur.close()\n",
        "      print()\n",
        "      print(\"QUERY SELECTION successfully\")\n",
        "      print()"
      ],
      "metadata": {
        "id": "2xUxFbS934W5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_extension(extension):\n",
        "      # PostGRES SQL Settings\n",
        "      import psycopg2 as ps\n",
        "      DB_NAME = \"postgres\"\n",
        "      DB_USER = \"postgres\"\n",
        "      DB_PASS = \"postgres\"\n",
        "      DB_HOST = \"localhost\"\n",
        "      DB_PORT = \"5432\"\n",
        "      conn = ps.connect(database=DB_NAME,\n",
        "                user=DB_USER,\n",
        "                password=DB_PASS,\n",
        "                host=DB_HOST,\n",
        "                port=DB_PORT)\n",
        "      cur = conn.cursor() # creating a cursor\n",
        "      cur.execute(\"\"\"DROP EXTENSION IF EXISTS %s CASCADE\"\"\"%extension)\n",
        "      cur.query\n",
        "      conn.commit()\n",
        "      cur.close()\n",
        "      conn.close()\n",
        "\n",
        "      print()\n",
        "      print(\"DROP EXTENSION %s successfully\"%extension)\n",
        "      print()"
      ],
      "metadata": {
        "id": "QWP0GA9Z4X4L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PostgreSQL: Definition and Configuration"
      ],
      "metadata": {
        "id": "3V4bZItQByQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install PSQL WITH DEV Libraries AND PG embedding\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "5bHMdQLvCGRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -pr /content/gdrive/MyDrive/tools/pg_embedding /content/\n",
        "%cd /content/pg_embedding/\n",
        "print()\n",
        "print('START: PG embedding COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "#!make uninstall\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()"
      ],
      "metadata": {
        "id": "CHZtNbRxBjX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9db951-103b-4722-cf10-1a94e21ed26b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pg_embedding\n",
            "\n",
            "START: PG embedding COMPILATION\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o embedding.o embedding.c\n",
            "g++ -Wall -Wpointer-arith -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -std=c++11 -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o hnswalg.o hnswalg.cpp\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2   -c -o distfunc.o distfunc.c\n",
            "gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wcast-function-type -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -Wno-stringop-truncation -g -g -O2 -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fno-omit-frame-pointer -Ofast -fPIC -shared -o embedding.so embedding.o hnswalg.o distfunc.o -lstdc++ -L/usr/lib/x86_64-linux-gnu -Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro -Wl,-z,now -L/usr/lib/llvm-14/lib  -Wl,--as-needed  \n",
            "/usr/bin/clang-14 -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -Wno-unused-command-line-argument -Wno-compound-token-split-by-macro -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o embedding.bc embedding.c\n",
            "/usr/bin/clang-14 -xc++ -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o hnswalg.bc hnswalg.cpp\n",
            "/usr/bin/clang-14 -Wno-ignored-attributes -fno-strict-aliasing -fwrapv -Wno-unused-command-line-argument -Wno-compound-token-split-by-macro -O2  -I. -I./ -I/usr/include/postgresql/14/server -I/usr/include/postgresql/internal  -Wdate-time -D_FORTIFY_SOURCE=2 -D_GNU_SOURCE -I/usr/include/libxml2  -flto=thin -emit-llvm -c -o distfunc.bc distfunc.c\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/usr/bin/install -c -m 755  embedding.so '/usr/lib/postgresql/14/lib/embedding.so'\n",
            "/usr/bin/install -c -m 644 .//embedding.control '/usr/share/postgresql/14/extension/'\n",
            "/usr/bin/install -c -m 644 .//embedding--0.3.5--0.3.6.sql .//embedding--0.3.5.sql .//embedding--0.3.6.sql  '/usr/share/postgresql/14/extension/'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode/embedding'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode'/embedding/\n",
            "/usr/bin/install -c -m 644 embedding.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "/usr/bin/install -c -m 644 hnswalg.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "/usr/bin/install -c -m 644 distfunc.bc '/usr/lib/postgresql/14/lib/bitcode'/embedding/./\n",
            "cd '/usr/lib/postgresql/14/lib/bitcode' && /usr/lib/llvm-14/bin/llvm-lto -thinlto -thinlto-action=thinlink -o embedding.index.bc embedding/embedding.bc embedding/hnswalg.bc embedding/distfunc.bc\n",
            "END: PG embedding COMPILATION\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo -u postgres psql -c \"DROP EXTENSION IF EXISTS embedding CASCADE\"\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "print('')\n",
        "print(\"Extensions Available:\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print('')\n",
        "print(\"Extensions Used:\")\n",
        "query_selection(\"SELECT * FROM pg_extension\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3HFj_ZuNAKB",
        "outputId": "b4040ea1-3aa3-470a-d6a3-bf8f9a03030b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "\n",
            "Extensions Available:\n",
            "Total rows are:   47\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extensions Used:\n",
            "Total rows are:   1\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PostGRES SQL Settings\n",
        "import psycopg2 as ps\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION vector CASCADE\"\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding CASCADE\"\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id BIGSERIAL PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{1,2,3}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{4,5,6}')\"%h\n",
        "print(hh)\n",
        "\n",
        "#del insert_document\n",
        "insert_document(1,'{1,2,3}')\n",
        "insert_document(2,'{4,5,6}')\n",
        "\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuZRG19-INLa",
        "outputId": "2d38a58e-7a57-4f24-aca6-abcc698468d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "CREATE EXTENSION\n",
            "ERROR:  table \"documents\" does not exist\n",
            "CREATE TABLE\n",
            "INSERT INTO documents(id, embedding) VALUES (1,'{1,2,3}'), (2,'{4,5,6}')\n",
            "INSERT EMBEDDING {1,2,3} successfully\n",
            "INSERT EMBEDDING {4,5,6} successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -pr /content/gdrive/MyDrive/tools/pgvector /content/\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install\n",
        "#!make uninstall\n",
        "print('END: PG VECTOR COMPILATION')\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vsRfc01GoSN",
        "outputId": "84a736d8-95eb-43f9-ba0a-d64593ef3994"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pgvector\n",
            "\n",
            "START: PG VECTOR COMPILATION\n",
            "make: Nothing to be done for 'all'.\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
            "/usr/bin/install -c -m 755  vector.so '/usr/lib/postgresql/14/lib/vector.so'\n",
            "/usr/bin/install -c -m 644 .//vector.control '/usr/share/postgresql/14/extension/'\n",
            "/usr/bin/install -c -m 644 .//sql/vector--0.1.0--0.1.1.sql .//sql/vector--0.1.1--0.1.3.sql .//sql/vector--0.1.3--0.1.4.sql .//sql/vector--0.1.4--0.1.5.sql .//sql/vector--0.1.5--0.1.6.sql .//sql/vector--0.1.6--0.1.7.sql .//sql/vector--0.1.7--0.1.8.sql .//sql/vector--0.1.8--0.2.0.sql .//sql/vector--0.2.0--0.2.1.sql .//sql/vector--0.2.1--0.2.2.sql .//sql/vector--0.2.2--0.2.3.sql .//sql/vector--0.2.3--0.2.4.sql .//sql/vector--0.2.4--0.2.5.sql .//sql/vector--0.2.5--0.2.6.sql .//sql/vector--0.2.6--0.2.7.sql .//sql/vector--0.2.7--0.3.0.sql .//sql/vector--0.3.0--0.3.1.sql .//sql/vector--0.3.1--0.3.2.sql .//sql/vector--0.3.2--0.4.0.sql .//sql/vector--0.4.0--0.4.1.sql .//sql/vector--0.4.1--0.4.2.sql .//sql/vector--0.4.2--0.4.3.sql .//sql/vector--0.4.3--0.4.4.sql .//sql/vector--0.4.4--0.5.0.sql .//sql/vector--0.5.0--0.5.1.sql .//sql/vector--0.5.1.sql  '/usr/share/postgresql/14/extension/'\n",
            "/bin/mkdir -p '/usr/include/postgresql/14/server/extension/vector/'\n",
            "/usr/bin/install -c -m 644   .//src/vector.h '/usr/include/postgresql/14/server/extension/vector/'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode/vector'\n",
            "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnsw.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswbuild.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswinsert.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswscan.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/hnswvacuum.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfbuild.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfflat.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfinsert.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfkmeans.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfscan.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/ivfvacuum.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "/usr/bin/install -c -m 644 src/vector.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
            "cd '/usr/lib/postgresql/14/lib/bitcode' && /usr/lib/llvm-14/bin/llvm-lto -thinlto -thinlto-action=thinlink -o vector.index.bc vector/src/hnsw.bc vector/src/hnswbuild.bc vector/src/hnswinsert.bc vector/src/hnswscan.bc vector/src/hnswutils.bc vector/src/hnswvacuum.bc vector/src/ivfbuild.bc vector/src/ivfflat.bc vector/src/ivfinsert.bc vector/src/ivfkmeans.bc vector/src/ivfscan.bc vector/src/ivfutils.bc vector/src/ivfvacuum.bc vector/src/vector.bc\n",
            "END: PG VECTOR COMPILATION\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo -u postgres psql -c \"DROP EXTENSION IF EXISTS embedding CASCADE\"\n",
        "print('')\n",
        "print(\"Extensions Available:\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print('')\n",
        "print(\"Extensions Used:\")\n",
        "query_selection(\"SELECT * FROM pg_extension\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_gzw1GqCbve",
        "outputId": "021ca072-3ddd-4011-ec8e-aff05fdbd8f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extensions Available:\n",
            "Total rows are:   48\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('vector',)\n",
            "ROW 48:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extensions Used:\n",
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "ROW 2:  (16384, 'embedding', 10, 2200, True, '0.3.6', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PostGRES SQL Settings\n",
        "import psycopg2 as ps\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP EXTENSION embedding CASCADE\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION vector\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id BIGSERIAL PRIMARY KEY, embedding vector(3))\"\n",
        "\n",
        "insert_document(1,'[1,2,3]')\n",
        "insert_document(2,'[4,5,6]')\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw (embedding vector_l2_ops)\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding ann_cos_ops) WITH (m=3)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXP1e65MejS3",
        "outputId": "a6e451be-084d-4c71-a03c-b76430ba9a86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ALTER ROLE\n",
            "DROP EXTENSION\n",
            "CREATE EXTENSION\n",
            "DROP TABLE\n",
            "CREATE TABLE\n",
            "INSERT EMBEDDING [1,2,3] successfully\n",
            "INSERT EMBEDDING [4,5,6] successfully\n",
            "CREATE INDEX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## <-> and <=> Euclidean (L2) and Cosine distances NO Manhattan distance: <~>\n",
        "query_selection(\"SELECT * FROM documents ORDER BY embedding::vector <-> '[3,1,2]' LIMIT 5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k04syAHjJnWZ",
        "outputId": "86f167c7-085b-4690-d847-f026fe02dc42"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (1, '[1,2,3]')\n",
            "ROW 2:  (2, '[4,5,6]')\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('')\n",
        "print(\"Extensions Available:\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print('')\n",
        "print(\"Extensions Used:\")\n",
        "query_selection(\"SELECT * FROM pg_extension\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cYNpki6VpeX",
        "outputId": "7637269b-b590-47ef-b11b-2d03ef3683dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extensions Available:\n",
            "Total rows are:   48\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('vector',)\n",
            "ROW 48:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extensions Used:\n",
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "ROW 2:  (16414, 'vector', 10, 2200, True, '0.5.1', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Euclidean (L2) distance index:\n",
        "#CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5);\n",
        "#SET enable_seqscan = off;\n",
        "#SELECT id FROM documents ORDER BY embedding <-> array[3,3,3] LIMIT 1;\n",
        "\n",
        "# Cosine distance index:\n",
        "#CREATE INDEX ON documents USING hnsw(embedding ann_cos_ops) WITH (dims=3, m=3, efconstruction=5, efsearch=5);\n",
        "#SET enable_seqscan = off;\n",
        "#SELECT id FROM documents ORDER BY embedding <=> array[3,3,3] LIMIT 1;\n",
        "\n",
        "# Manhattan distance index:\n",
        "#CREATE INDEX ON documents USING hnsw(embedding ann_manhattan_ops) WITH (dims=3, m=3, efconstruction=5, efsearch=5);\n",
        "#SET enable_seqscan = off;\n",
        "#SELECT id FROM documents ORDER BY embedding <~> array[3,3,3] LIMIT 1;"
      ],
      "metadata": {
        "id": "DOe_mjxVmeNN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS - Document Upload"
      ],
      "metadata": {
        "id": "0vk3VbT-q4X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://platform.openai.com/docs/guides/text-generation\n",
        "%pip install openai==0.28  --root-user-action=ignore\n",
        "%pip install \"unstructured[all-docs]\"\n",
        "!pip install gradio --quiet\n",
        "!pip install xformer --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install unstructured --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "!pip install pypdf\n",
        "%pip install tiktoken"
      ],
      "metadata": {
        "id": "2_UzTRgRsA7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf /content/*.pdf\n",
        "!mkdir -p /content/data/\n",
        "%cd /content/data/\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "urls = [\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "filenames = [\n",
        "    'AMZN-2022-Shareholder-Letter.pdf',\n",
        "    'AMZN-2021-Shareholder-Letter.pdf',\n",
        "    'AMZN-2020-Shareholder-Letter.pdf',\n",
        "    'AMZN-2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "metadata = [\n",
        "    dict(year=2022, source=filenames[0]),\n",
        "    dict(year=2021, source=filenames[1]),\n",
        "    dict(year=2020, source=filenames[2]),\n",
        "    dict(year=2019, source=filenames[3])]\n",
        "\n",
        "data_root = \"/content/data/\"\n",
        "\n",
        "for idx, url in enumerate(urls):\n",
        "    file_path = data_root + filenames[idx]\n",
        "    #print(file_path)\n",
        "    urlretrieve(url, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJGGZWI_q_Cb",
        "outputId": "a1b8681a-e7c3-49af-8274-94e5c7ec1665"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader, PdfWriter\n",
        "import glob\n",
        "\n",
        "local_pdfs = glob.glob(data_root + '*.pdf')\n",
        "\n",
        "for local_pdf in local_pdfs:\n",
        "    pdf_reader = PdfReader(local_pdf)\n",
        "    pdf_writer = PdfWriter()\n",
        "    for pagenum in range(len(pdf_reader.pages)-3):\n",
        "        page = pdf_reader.pages[pagenum]\n",
        "        pdf_writer.add_page(page)\n",
        "\n",
        "    with open(local_pdf, 'wb') as new_file:\n",
        "        new_file.seek(0)\n",
        "        pdf_writer.write(new_file)\n",
        "        new_file.truncate()"
      ],
      "metadata": {
        "id": "FwBAG6UprEbM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "\n",
        "#%cd /content/data\n",
        "\n",
        "documents = []\n",
        "\n",
        "for idx, file in enumerate(filenames):\n",
        "    loader = PyPDFLoader(data_root + file)\n",
        "    document = loader.load()\n",
        "    for document_fragment in document:\n",
        "        document_fragment.metadata = metadata[idx]\n",
        "\n",
        "    documents += document\n",
        "\n",
        "# - in our testing Character split works better with this PDF data set\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 100,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVN8KXBUrI7v",
        "outputId": "f0ecb1d1-25ae-48fd-e580-345a82f6fd10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 25\n",
            "# of Document Chunks: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain and LLM Configurations"
      ],
      "metadata": {
        "id": "Yo-ZsxiyCl3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "#!pip install accelerate\n",
        "\n",
        "import torch\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#from transformers import AutoTokenizer, MistralForCausalLM"
      ],
      "metadata": {
        "id": "1ygWwhAGQkpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "MODEL_NAME='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 512\n",
        "generation_config.temperature = 0.9\n",
        "generation_config.top_p = 0.9\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "#model.to(device)\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "Ni9Z3osgRDtq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "OBoSd2ZSQmOK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Vector Queries"
      ],
      "metadata": {
        "id": "iqneJqxTciYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "import os\n",
        "collection_name='AWS'\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "%pip install colab-env\n",
        "import colab_env\n",
        "\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "# https://supabase.com/blog/fewer-dimensions-are-better-pgvector\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "# PostGRES SQL Settings\n",
        "import psycopg2 as ps\n",
        "\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"DROP EXTENSION IF EXISTS vector CASCADE\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id BIGSERIAL PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{1,2,3}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{4,5,6}')\"%h\n",
        "print(hh)\n",
        "\n",
        "#del insert_document\n",
        "insert_document(1,'{1,2,3}')\n",
        "insert_document(2,'{4,5,6}')\n",
        "\n",
        "#!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX9u6W4FqLgs",
        "outputId": "577e917b-f0f8-4b27-bc4a-57df7fd49201"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab-env in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: python-dotenv<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from colab-env) (0.21.1)\n",
            "/content\n",
            "/content\n",
            "NOTICE:  drop cascades to column embedding of table documents\n",
            "DROP EXTENSION\n",
            "CREATE EXTENSION\n",
            "DROP TABLE\n",
            "CREATE TABLE\n",
            "INSERT INTO documents(id, embedding) VALUES (1,'{1,2,3}'), (2,'{4,5,6}')\n",
            "INSERT EMBEDDING {1,2,3} successfully\n",
            "INSERT EMBEDDING {4,5,6} successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "#del embeddings\n",
        "print(\"Extension available\")\n",
        "query_selection(\"SELECT name FROM pg_available_extensions order by 1\")\n",
        "print()\n",
        "print(\"Extension used\")\n",
        "query_selection(\"SELECT * FROM pg_extension order by 1\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WAxu1Z2AX7R",
        "outputId": "56350020-def2-4a30-88af-43796b1b1a34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extension available\n",
            "Total rows are:   48\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  ('adminpack',)\n",
            "ROW 2:  ('amcheck',)\n",
            "ROW 3:  ('autoinc',)\n",
            "ROW 4:  ('bloom',)\n",
            "ROW 5:  ('btree_gin',)\n",
            "ROW 6:  ('btree_gist',)\n",
            "ROW 7:  ('citext',)\n",
            "ROW 8:  ('cube',)\n",
            "ROW 9:  ('dblink',)\n",
            "ROW 10:  ('dict_int',)\n",
            "ROW 11:  ('dict_xsyn',)\n",
            "ROW 12:  ('earthdistance',)\n",
            "ROW 13:  ('embedding',)\n",
            "ROW 14:  ('file_fdw',)\n",
            "ROW 15:  ('fuzzystrmatch',)\n",
            "ROW 16:  ('hstore',)\n",
            "ROW 17:  ('insert_username',)\n",
            "ROW 18:  ('intagg',)\n",
            "ROW 19:  ('intarray',)\n",
            "ROW 20:  ('isn',)\n",
            "ROW 21:  ('lo',)\n",
            "ROW 22:  ('ltree',)\n",
            "ROW 23:  ('moddatetime',)\n",
            "ROW 24:  ('old_snapshot',)\n",
            "ROW 25:  ('pageinspect',)\n",
            "ROW 26:  ('pg_buffercache',)\n",
            "ROW 27:  ('pg_freespacemap',)\n",
            "ROW 28:  ('pg_prewarm',)\n",
            "ROW 29:  ('pg_stat_statements',)\n",
            "ROW 30:  ('pg_surgery',)\n",
            "ROW 31:  ('pg_trgm',)\n",
            "ROW 32:  ('pg_visibility',)\n",
            "ROW 33:  ('pgcrypto',)\n",
            "ROW 34:  ('pgrowlocks',)\n",
            "ROW 35:  ('pgstattuple',)\n",
            "ROW 36:  ('plpgsql',)\n",
            "ROW 37:  ('postgres_fdw',)\n",
            "ROW 38:  ('refint',)\n",
            "ROW 39:  ('seg',)\n",
            "ROW 40:  ('sslinfo',)\n",
            "ROW 41:  ('tablefunc',)\n",
            "ROW 42:  ('tcn',)\n",
            "ROW 43:  ('tsm_system_rows',)\n",
            "ROW 44:  ('tsm_system_time',)\n",
            "ROW 45:  ('unaccent',)\n",
            "ROW 46:  ('uuid-ossp',)\n",
            "ROW 47:  ('vector',)\n",
            "ROW 48:  ('xml2',)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n",
            "Extension used\n",
            "Total rows are:   2\n",
            "Printing each row\n",
            "\n",
            "ROW 1:  (13747, 'plpgsql', 10, 11, False, '1.0', None, None)\n",
            "ROW 2:  (16523, 'embedding', 10, 2200, True, '0.3.6', None, None)\n",
            "\n",
            "QUERY SELECTION successfully\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")\n",
        "\n",
        "query = \"How has AWS evolved?\"\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print(query)\n",
        "\n",
        "#query = db.embedding_function.embed_query(query)\n",
        "results_with_scores = db.similarity_search_with_score(query)\n",
        "\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"Content: {doc.page_content}\\nMetadata: {doc.metadata}\\nScore: {score}\\n\\n\")\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "W8OUhRQ3sYaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3518868f-61d7-4f20-eeea-34cabf86ebff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "How has AWS evolved?\n",
            "Content: customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.520295\n",
            "\n",
            "\n",
            "Content: in AWS. Our new customer pipeline is robust, as are our active migrations. Many companies usediscontinuous periods like this to step back and determine what they strategically want to change, and wefind an increasing number of enterprises opting out of managing their own infrastructure, and preferring tomove to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantlyfor customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.5205847\n",
            "\n",
            "\n",
            "Content: done innovating here,and this long-term investment should prove fruitful for both customers and AWS. AWS is still in the earlystages of its evolution, and has a chance for unusual growth in the next decade.\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.52208495\n",
            "\n",
            "\n",
            "Content: We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.5228049\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_query_byyear(query, year):\n",
        "\n",
        "    filter={\"year\": year}\n",
        "\n",
        "    results_with_scores = db.similarity_search_with_score(query,\n",
        "      filter=filter)\n",
        "\n",
        "    print()\n",
        "    print('Number of Results for the year %s: %s'%(year,len(results_with_scores)))\n",
        "    print()\n",
        "\n",
        "    if len(results_with_scores) == 0:\n",
        "        print('No results')\n",
        "        exit()\n",
        "        #print(\"-\" * 80)\n",
        "\n",
        "    for doc, score in results_with_scores:\n",
        "        print(f\"Content: {doc.page_content}\\nMetadata: {doc.metadata}\\nScore: {score}\\n\\n\")\n",
        "\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "G61u7U8XvpnY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year=2018\n",
        "query = \"How has AWS evolved?\"\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print(query)\n",
        "similarity_query_byyear(query, year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3RHkYzZPiJ0",
        "outputId": "e242715b-fa53-4f9c-e2b7-33ff0a7260e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "How has AWS evolved?\n",
            "\n",
            "Number of Results for the year 2018: 0\n",
            "\n",
            "No results\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year=2019\n",
        "query = \"How has AWS evolved?\"\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print(query)\n",
        "similarity_query_byyear(query, year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8nRQonbxVNO",
        "outputId": "c2448b9f-1d30-4a92-d9f9-d47e15952f9d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "How has AWS evolved?\n",
            "\n",
            "Number of Results for the year 2019: 4\n",
            "\n",
            "Content: AWS has been successful inincreasing the energy efficiency of its facilities and equipment, for instance by using more efficient evaporativecooling in certain data centers instead of traditional air conditioning. A study by 451 Research found that AWS’sinfrastructure is 3.6 times more energy efficient than the median U.S. enterprise data center surveyed. Along withour use of renewable energy, these factors enable AWS to do the same tasks as traditional data centers with an88% lower carbon footprint. And\n",
            "Metadata: {'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}\n",
            "Score: 0.5478576\n",
            "\n",
            "\n",
            "Content: institutionsaround the world are transitioning from in-person to virtual classrooms and are running on AWS to help ensurecontinuity of learning. And governments are leveraging AWS as a secure platform to build out new capabilitiesin their efforts to end this pandemic.\n",
            "Metadata: {'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}\n",
            "Score: 0.5480351\n",
            "\n",
            "\n",
            "Content: scalable, dependable, and highly secure computing power—whether for vital healthcare work, to help studentscontinue learning, or to keep unprecedented numbers of employees online and productive from home—is criticalin this situation. Hospital networks, pharmaceutical companies, and research labs are using AWS to care forpatients, explore treatments, and mitigate the impacts of COVID-19 in many other ways. Academic institutionsaround the world are transitioning from in-person to virtual classrooms and are\n",
            "Metadata: {'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}\n",
            "Score: 0.5809112\n",
            "\n",
            "\n",
            "Content: Service decide where best to allocate resources. InCanada, OTN—one of the world’s largest virtual care networks—is scaling its AWS-powered video service toaccommodate a 4,000% spike in demand to support citizens as the pandemic continues. In Brazil, AWS willprovide the São Paulo State Government with cloud computing infrastructure to guarantee online classes to1 million students in public schools across the state.\n",
            "Metadata: {'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}\n",
            "Score: 0.5957121\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year=2020\n",
        "query = \"How has AWS evolved?\"\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print(query)\n",
        "similarity_query_byyear(query, year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pULzSE3ix_0Z",
        "outputId": "58c7289f-0972-45a9-fb81-01f61bb3a6ae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "How has AWS evolved?\n",
            "\n",
            "Number of Results for the year 2020: 4\n",
            "\n",
            "Content: 100 million smart home devices to Alexa. Amazon Web Services serves millions of customers and ended 2020\n",
            "with a $50 billion annualized run rate. In 1997, we hadn’t invented Prime, Marketplace, Alexa, or AWS.\n",
            "They weren’t even ideas then, and none was preordained. We took great risk with each one and put sweat\n",
            "and ingenuity into each one.\n",
            "Along the way, we’ve created $1.6 trillion of wealth for shareowners. Who are they? Y our Chair is one, and\n",
            "Metadata: {'year': 2020, 'source': 'AMZN-2020-Shareholder-Letter.pdf'}\n",
            "Score: 0.59557676\n",
            "\n",
            "\n",
            "Content: To our shareowners:\n",
            "In Amazon’s 1997 letter to shareholders, our first, I talked about our hope to create an “enduring franchise,”\n",
            "one that would reinvent what it means to serve customers by unlocking the internet’s power. I noted that\n",
            "Amazon had grown from having 158 employees to 614, and that we had surpassed 1.5 million customer\n",
            "accounts. We had just gone public at a split-adjusted stock price of $1.50 per share. I wrote that it was Day 1.\n",
            "Metadata: {'year': 2020, 'source': 'AMZN-2020-Shareholder-Letter.pdf'}\n",
            "Score: 0.61698914\n",
            "\n",
            "\n",
            "Content: their own cost $45 billion from AWS). The difficult part of this estimation exercise is that the direct cost\n",
            "reduction is the smallest portion of the customer benefit of moving to the cloud. The bigger benefit is the\n",
            "increased speed of software development – something that can significantly improve the customer’s\n",
            "competitiveness and top line. We have no reasonable way of estimating that portion of customer value\n",
            "Metadata: {'year': 2020, 'source': 'AMZN-2020-Shareholder-Letter.pdf'}\n",
            "Score: 0.6175883\n",
            "\n",
            "\n",
            "Content: creation.\n",
            "AWS is challenging to estimate because each customer’s workload is so different, but we’ll do it anyway,\n",
            "acknowledging up front that the error bars are high. Direct cost improvements from operating in the cloud\n",
            "versus on premises vary, but a reasonable estimate is 30%. Across AWS’s entire 2020 revenue of $45 billion,\n",
            "that 30% would imply customer value creation of $19 billion (what would have cost them $64 billion on\n",
            "Metadata: {'year': 2020, 'source': 'AMZN-2020-Shareholder-Letter.pdf'}\n",
            "Score: 0.6196256\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year=2021\n",
        "query = \"How has AWS evolved?\"\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print(query)\n",
        "similarity_query_byyear(query, year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x-u0Pb_yG9O",
        "outputId": "09ff45cf-8e4d-4790-b722-cc82767c2871"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "How has AWS evolved?\n",
            "\n",
            "Number of Results for the year 2021: 4\n",
            "\n",
            "Content: customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.5202411\n",
            "\n",
            "\n",
            "Content: from working with colleagues and technology on-premises to working remotely. AWS played amajor role in enabling this business continuity. Whether companies saw extraordinary demand spikes, ordemand diminish quickly with reduced external consumption, the cloud’s elasticity to scale capacity up anddown quickly, as well as AWS’s unusually broad functionality helped millions of companies adjust to thesedifficult circumstances.\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.5373376\n",
            "\n",
            "\n",
            "Content: back and determining what they wanted to change coming out of the pandemic. Many concludedthat they didn’t want to continue managing their technology infrastructure themselves, and made thedecision to accelerate their move to the cloud. This shift by so many companies (along with the economyrecovering) helped re-accelerate AWS’s revenue growth to 37% Y oY in 2021.\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.53964233\n",
            "\n",
            "\n",
            "Content: kept triggering one of the biggest tensions in product development—where to draw the line on functionality inV1. One early meeting in particular—for our core compute service called Elastic Compute Cloud (“EC2”)—was scheduled for an hour, and took three, as we animatedly debated whether we could launch a computeservice without an accompanying persistent block storage companion (a form of network attached storage).\n",
            "Metadata: {'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "Score: 0.5754214\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year=2022\n",
        "query = \"How has AWS evolved?\"\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print(query)\n",
        "similarity_query_byyear(query, year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0cpWFvpJwc4",
        "outputId": "62d2d127-d2e2-44a7-e564-e067a23892dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "How has AWS evolved?\n",
            "\n",
            "Number of Results for the year 2022: 4\n",
            "\n",
            "Content: in AWS. Our new customer pipeline is robust, as are our active migrations. Many companies usediscontinuous periods like this to step back and determine what they strategically want to change, and wefind an increasing number of enterprises opting out of managing their own infrastructure, and preferring tomove to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantlyfor customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.5204745\n",
            "\n",
            "\n",
            "Content: done innovating here,and this long-term investment should prove fruitful for both customers and AWS. AWS is still in the earlystages of its evolution, and has a chance for unusual growth in the next decade.\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.52201253\n",
            "\n",
            "\n",
            "Content: We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.52275383\n",
            "\n",
            "\n",
            "Content: customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and services launchedin 2022), and invest in long-term inventions that change what’s possible.\n",
            "Metadata: {'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "Score: 0.5285984\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt and Completions Examples"
      ],
      "metadata": {
        "id": "yM9zL38DQuKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query1 = \"who is the President of the USA?\"\n",
        "query2 = \"Who won the baseball World Series in 2020? and Who Lost\"\n",
        "\n",
        "device=\"cuda\"\n",
        "def prompt_completion(query):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"%s\"%query}\n",
        "    ]\n",
        "\n",
        "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = encodeds.to(device)\n",
        "\n",
        "    #https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id\n",
        "\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=512, do_sample=True, negative_prompt_attention_mask='attention_mask',\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(generated_ids)\n",
        "    print()\n",
        "    print()\n",
        "    result=decoded[0].replace('<s> [INST] %s [/INST]'%query,\"\")\n",
        "    result=result.replace('</s>',\"\")\n",
        "    print('Prompt: %s'%query)\n",
        "    print('-'*80)\n",
        "    print('Answer: %s'%result)\n",
        "\n",
        "prompt_completion(query)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query1)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query2)\n",
        "\n",
        "query3 = \"what is the 20.5% of 40?\"\n",
        "query4 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "query5 ='Which country has the most natural lakes? Answer with only the country name.'\n",
        "\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query3)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query4)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query5)\n",
        "\n",
        "\n",
        "query6 = \"How AWS has evolved?\"\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query6)\n",
        "\n",
        "query7 = \"Summarize this: Mistral LLM and Langchain intgegration. Overview and Tutorial with practical examples. Mistral LLM is a large language model developed by Mistral AI, a French startup making waves in\\\n",
        "the tech community. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. \\\n",
        "It uses sliding window attention, grouped query attention, and byte-fallback BPE tokenizer to achieve its impressive performance. \\\n",
        "Mistral LLM and Mistral LLM MoE 8x7B are language models developed by Mistral AI, a French startup. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. \\\n",
        "On the other hand, Mistral LLM MoE 8x7B[1c] is an open-weight model that employs a Mixture of Expert (MoE) architecture to generate human-like responses. While there is no direct information about the relationship between Mistral LLM and Mistral LLM MoE 8x7B, it is safe to assume that Mistral LLM MoE 8x7B is an extension of Mistral LLM. \\\n",
        "Mistral LLM MoE 8x7B is a larger model than Mistral LLM, with eight experts, each with seven billion parameters.\\\n",
        "Mistral LLM is designed for various natural language processing tasks, including text generation, summarization, and question-answering.\\\n",
        "It has outperformed other large language models, such as Llama 2 13B, on all benchmarks tested.\\\n",
        "Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs. \\\n",
        "The model can be used to understand images and answer questions about them. It is best at answering general questions about what is present in the pictures, but it is not yet optimized to answer detailed questions about the location of specific objects in an image.\\\n",
        "Mistral LLM is a significant step forward in natural language processing. Its impressive performance and large parameter count make it a powerful [3] tool for developers working on a wide range of NLP tasks.\\\n",
        "In summary, Mistral LLM is a powerful language model that has been shown to outperform other large language models on all benchmarks tested.\\\n",
        "It is available to developers via the gpt-4-vision-preview model and the Chat Completions API and can be used for a wide range of natural language processing tasks.\\\n",
        "Mistral LLM has been benchmarked against other large language models, such as Llama 2 13B, and it has been shown to outperform all benchmarks tested.\\In particular, Mistral 7B outperforms Llama 2 13B on all metrics measured and is on par with Llama 34B.\\\n",
        "It is important to note that the benchmarks used to compare these models can vary widely, and the results may not be directly comparable.\\\n",
        "However, the fact that Mistral LLM has been shown to outperform other large language models on multiple benchmarks is a testament to its impressive performance.\\\n",
        "LangChain is an open-source AI abstraction library that easily integrates large language models (LLMs) like GPT-4/LLaMa 2 into applications.\\\n",
        "LangChain provides a simplified framework for querying LLMs to generate text, code, translations, and more using Python [9]. LangChain is a generic interface for nearly any LLM, providing a centralized development environment to build and integrate LLM applications with external data sources and software workflows.\\LangChain's integration with LLMs like OpenAI, Cohere, and Hugging Face is fundamental to its functionality [11].\\\n",
        "Mistral LLM is a large language model developed by Mistral AI that has been shown to outperform other large language models, such as Llama 2 13B, on all benchmarks tested.\\\n",
        "Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs [12].\\\n",
        "The model can be used to understand images and answer questions about them.\\\n",
        "While there is no direct information about the integration of LangChain and Mistral LLM, LangChain's ability to integrate with nearly any LLM suggests that it can be used with Mistral LLM.\\\n",
        "The combination of LangChain and Mistral LLM could provide a powerful tool for developers working on natural language processing tasks [9-14]. However, it is essential to consider the model's limitations as you explore what use cases it can apply to.\\\n",
        "As an illustration of how the concepts and terms mentioned above work together, we implemented a jyputer notebook thoroughly tested in Google Colab based on GPU T4 devices.\\\n",
        "The notebook covers how we can use Langchain using OpenAI embedding and a PostgreSQL extension called pg_embedding to upload actual documents from AWS S3.\\\n",
        "Also, as a model for LLM, we used mistral/Mistral-7B-v0.1 from the hugging face repository, which created an entire open-source integrated ecosystem to support generative AI solutions.\\\n",
        "I also share a notebook of using MLxDL/Mistral_IN_AWS.ipynb at main · frank-morales2020/MLxDL (github.com)\\\n",
        "You can access this case in my GitHub ML/DL development repository shown below: MLxDL/Mistral_Integration_with_Langchain_PostgreSQL.ipynb at main · frank-morales2020/MLxDL (github.com)\"\n",
        "\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query7)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owzp3YYXo-dp",
        "outputId": "e4dc5660-c73f-4e33-b7b3-cccd76f63f9b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Let's explain this step-by-step:\n",
            "\n",
            "1. You bought an ice cream for 6 kids.\n",
            "2. Each cone cost $1.25.\n",
            "3. So, the total cost of the ice creams would be 6 * $1.25 = $7.50.\n",
            "4. You paid with a $10 bill.\n",
            "5. Therefore, the amount of change you got back would be $10 - $7.50 = $2.50.\n",
            "\n",
            "The final answer is that you got back $2.50.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: who is the President of the USA?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  At my time, in 1994, the President of the United States was Bill Clinton. But I am unable to offer a contemporary response as I do not have access to real-time information.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Who won the baseball World Series in 2020? and Who Lost\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  The Los Angeles Dodgers won the 2020 Major League Baseball World Series. They defeated the Tampa Bay Rays in a seven-game series with a score of 3-2. The Tampa Bay Rays lost the series.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: what is the 20.5% of 40?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  To calculate 20.5% of 40, you need to multiply the number (in this case, 40) by the percentage (in this case, 20.5).\n",
            "\n",
            "For example, let's say you want to find out what 20.5% of 40 is. You can express this as a proportion: 20.5/100. Then, you can multiply the numerator (20.5) by the denominator (100):\n",
            "\n",
            "20.5 x 100 = 20.5%\n",
            "\n",
            "Therefore, 20.5% of 40 is equal to 20.5.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Sure, I'd be happy to explain regularization in machine learning. Regularization is a technique used to prevent overfitting in machine learning models. It is a process of adding a penalty term to the loss function of a model to discourage large weights and encourage simpler models. Overfitting occurs when a model is too complex and fits the training data too well, including the noise and random fluctuations in the data. This can lead to poor performance on new data.\n",
            "\n",
            "Regularization works by adding a penalty term to the loss function, which encourages the model to use smaller weights. The penalty term is usually proportional to the square of the L1 or L2 norm of the weights, which measures the absolute value or Euclidean distance of the weights, respectively. This has the effect of shrinking the weights towards zero, making the model less complex.\n",
            "\n",
            "There are two main types of regularization: L1 and L2 regularization. L1 regularization, also known as Lasso regularization, adds a penalty term that is proportional to the absolute value of the weights. This has the effect of setting some weights to exactly zero, effectively removing the corresponding features from the model. L2 regularization, also known as Ridge regularization, adds a penalty term that is proportional to the square of the Euclidean norm of the weights. This has the effect of shrinking all the weights towards zero, but not all the way to exactly zero.\n",
            "\n",
            "Regularization is a powerful technique that can improve the performance of machine learning models by preventing overfitting and encouraging simpler, more interpretable models.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Which country has the most natural lakes? Answer with only the country name.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Canada\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: How AWS has evolved?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  AWS, or Amazon Web Services, has evolved significantly since its founding in 2007 as a cloud computing platform. Here is an overview of the major ways AWS has evolved over the years:\n",
            "\n",
            "1. Services and offerings: When AWS was founded, it mainly offered on-demand virtual computing services, such as EC2, and storage services, such as S3. Over the years, AWS has expanded its service offerings to include a wide range of cloud-based computing services, such as computing, storage, databases, analytics, networking, security, and machine learning.\n",
            "2. Geographic expansion: AWS has continued to expand its geographic presence, with data centers located in various regions across the world. This has allowed customers to deploy their applications and services on AWS in the region of their choice, closer to their customers and end-users, for improved performance.\n",
            "3. Pricing model: AWS has evolved to offer a cost-effective and flexible pricing model. It now offers prepaid, reserved, and on-demand pricing options, depending on the customer's needs. Additionally, AWS has introduced various cost optimization tools and services, such as AWS Auto Scaling and AWS Lambda, to help customers save money.\n",
            "4. Innovation and new technologies: AWS has been at the forefront of innovation, introducing new technologies and services along the way. Some of these include serverless computing (API Gateway, AWS Lambda, AWS Step Functions), container computing (ECS, Fargate), machine learning (Amazon SageMaker, Amazon Rekognition), and Internet of Things (IoT) services (AWS IoT).\n",
            "5. DevOps and developer experience: AWS has made significant improvements in its developer experience and DevOps capabilities. It now offers a wide range of tools and services for developers, including AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy. AWS has also introduced the AWS CLI, a command-line tool for managing AWS services, making it easier for developers to interact with AWS.\n",
            "\n",
            "Overall, AWS has evolved into a robust and comprehensive cloud computing platform that offers a wide range of services and capabilities, making it easier for organizations to build, deploy, and run their applications and services in the cloud.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Summarize this: Mistral LLM and Langchain intgegration. Overview and Tutorial with practical examples. Mistral LLM is a large language model developed by Mistral AI, a French startup making waves inthe tech community. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. It uses sliding window attention, grouped query attention, and byte-fallback BPE tokenizer to achieve its impressive performance. Mistral LLM and Mistral LLM MoE 8x7B are language models developed by Mistral AI, a French startup. Mistral LLM is a decoder-based language model with 7 billion parameters, which makes it one of the most significant language models available. On the other hand, Mistral LLM MoE 8x7B[1c] is an open-weight model that employs a Mixture of Expert (MoE) architecture to generate human-like responses. While there is no direct information about the relationship between Mistral LLM and Mistral LLM MoE 8x7B, it is safe to assume that Mistral LLM MoE 8x7B is an extension of Mistral LLM. Mistral LLM MoE 8x7B is a larger model than Mistral LLM, with eight experts, each with seven billion parameters.Mistral LLM is designed for various natural language processing tasks, including text generation, summarization, and question-answering.It has outperformed other large language models, such as Llama 2 13B, on all benchmarks tested.Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs. The model can be used to understand images and answer questions about them. It is best at answering general questions about what is present in the pictures, but it is not yet optimized to answer detailed questions about the location of specific objects in an image.Mistral LLM is a significant step forward in natural language processing. Its impressive performance and large parameter count make it a powerful [3] tool for developers working on a wide range of NLP tasks.In summary, Mistral LLM is a powerful language model that has been shown to outperform other large language models on all benchmarks tested.It is available to developers via the gpt-4-vision-preview model and the Chat Completions API and can be used for a wide range of natural language processing tasks.Mistral LLM has been benchmarked against other large language models, such as Llama 2 13B, and it has been shown to outperform all benchmarks tested.\\In particular, Mistral 7B outperforms Llama 2 13B on all metrics measured and is on par with Llama 34B.It is important to note that the benchmarks used to compare these models can vary widely, and the results may not be directly comparable.However, the fact that Mistral LLM has been shown to outperform other large language models on multiple benchmarks is a testament to its impressive performance.LangChain is an open-source AI abstraction library that easily integrates large language models (LLMs) like GPT-4/LLaMa 2 into applications.LangChain provides a simplified framework for querying LLMs to generate text, code, translations, and more using Python [9]. LangChain is a generic interface for nearly any LLM, providing a centralized development environment to build and integrate LLM applications with external data sources and software workflows.\\LangChain's integration with LLMs like OpenAI, Cohere, and Hugging Face is fundamental to its functionality [11].Mistral LLM is a large language model developed by Mistral AI that has been shown to outperform other large language models, such as Llama 2 13B, on all benchmarks tested.Mistral LLM is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which has been updated to support image inputs [12].The model can be used to understand images and answer questions about them.While there is no direct information about the integration of LangChain and Mistral LLM, LangChain's ability to integrate with nearly any LLM suggests that it can be used with Mistral LLM.The combination of LangChain and Mistral LLM could provide a powerful tool for developers working on natural language processing tasks [9-14]. However, it is essential to consider the model's limitations as you explore what use cases it can apply to.As an illustration of how the concepts and terms mentioned above work together, we implemented a jyputer notebook thoroughly tested in Google Colab based on GPU T4 devices.The notebook covers how we can use Langchain using OpenAI embedding and a PostgreSQL extension called pg_embedding to upload actual documents from AWS S3.Also, as a model for LLM, we used mistral/Mistral-7B-v0.1 from the hugging face repository, which created an entire open-source integrated ecosystem to support generative AI solutions.I also share a notebook of using MLxDL/Mistral_IN_AWS.ipynb at main · frank-morales2020/MLxDL (github.com)You can access this case in my GitHub ML/DL development repository shown below: MLxDL/Mistral_Integration_with_Langchain_PostgreSQL.ipynb at main · frank-morales2020/MLxDL (github.com)\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Mistral LLM is a powerful large language model developed by Mistral AI that outperformed other large language models on all benchmarks tested. It is a decoder-based language model with 7 billion parameters, which makes it a significant tool for developers working on a wide range of natural language processing tasks. The model is available to developers via the gpt-4-vision-preview model and the Chat Completions API, which can be used for image inputs. However, it is not yet optimized to answer detailed questions about the location of specific objects in an image.\n",
            "Mistral LLM MoE 8x7B is an open-weight model that employs a Mixture of Expert (MoE) architecture to generate human-like responses. It is larger than Mistral LLM, with eight experts, each with seven billion parameters. LangChain is an open-source AI abstraction library that easily integrates large language models like GPT-4/LLaMa 2 into applications. It provides a simplified framework for querying LLMs to generate text, code, translations, and more using Python. LangChain's integration with LLMs like OpenAI, Cohere, and Hugging Face is fundamental to its functionality. While there is no direct information about the integration of LangChain and Mistral LLM, LangChain's ability to integrate with nearly any LLM suggests that it can be used with Mistral LLM. The combination of LangChain and Mistral LLM could provide a powerful tool for developers working on natural language processing tasks, but it is essential to consider the model's limitations as you explore what use cases it can apply to.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token = os.getenv(\"HF_TOKEN\")\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig, GenerationConfig, pipeline\n",
        "from transformers import AutoTokenizer, MistralForCausalLM\n",
        "\n",
        "#model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",padding_side=\"left\")\n",
        "#tokenizer.pad_token = tokenizer.eos_token # Most LLMs don't have a pad token by default\n",
        "\n",
        "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py\n",
        "\n",
        "# Generate\n",
        "#print()\n",
        "#generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
        "\n",
        "#generate_ids = model.generate(model_inputs, max_new_tokens=512, do_sample=True, negative_prompt_attention_mask='attention_mask',\n",
        "#                pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#response=tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "#print(response)\n",
        "print()\n",
        "\n",
        "#query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query=\"What is your favourite condiment?\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"%s\"%query},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "model_inputs = encodeds.to(device)\n",
        "#model.to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=512, do_sample=True, negative_prompt_attention_mask='attention_mask',\n",
        "                pad_token_id=tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "AY3FuMUBU-9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f6be7f-755c-412a-9049-2295fcb520d5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!  [INST] Do you have mayonnaise recipes? [/INST] Oh, absolutely! Here's a simple recipe for homemade mayonnaise:\n",
            "\n",
            "Ingredients:\n",
            "\n",
            "* 1 large egg yolk\n",
            "* 1 tablespoon Dijon mustard\n",
            "* 1/4 cup olive oil\n",
            "* 1/4 cup champagne vinegar or white wine vinegar\n",
            "* Salt and pepper to taste\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. In a small bowl, whisk the egg yolk and Dijon mustard together until smooth and creamy.\n",
            "2. Slowly pour in the olive oil, whisking constantly as you go, until the mixture is well emulsified.\n",
            "3. Add the vinegar and whisk again until everything is well combined.\n",
            "4. Season to taste with salt and pepper.\n",
            "5. Store the mayonnaise in an airtight container in the fridge. It will keep for about a week.\n",
            "\n",
            "And there you have it! Enjoy your delicious homemade mayonnaise on your favourite sandwiches or salads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers --upgrade\n",
        "#device = \"cuda\"\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "query = \"who is Barack Obama?\"\n",
        "result = llm(query)\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print()"
      ],
      "metadata": {
        "id": "jREtnaDouFMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "9518b4b6-a572-4584-f01a-d5a81a718bdb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>who is Barack Obama?</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>\nA: Q: Barack Obama was the 44th President of the United States, serving from 2009 to 2017. He was born on August 4, 1961, in Honolulu, Hawaii. Prior to his presidency, Obama served as a senator for Illinois and as a member of the Illinois State Senate. He is married to Michelle Obama and has two daughters.</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who is Steve Jobs?\"\n",
        "result = llm(query)\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "vhbJ_lNEs7IJ",
        "outputId": "5068807d-60b7-47fb-bb2e-e593329a6819"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>who is Steve Jobs?</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>\nA: the father of the Macintosh and founder of Apple Inc. He was known for his visionary approach to technology and design, which revolutionized the personal computer industry in the 1980s.</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "#import colab_env\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "#retriever = db.as_retriever(search_type=\"similarity_score_threshold\",search_kwargs='1')\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "# create a chain to answer questions\n",
        "#qa = RetrievalQA.from_chain_type(\n",
        "#     llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "     llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "query = \"How AWS has evolved?\"\n",
        "\n",
        "result = qa(query)\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "print()\n",
        "\n",
        "print()\n",
        "print('CHAIN to answer questions')\n",
        "print(\"-\" * 80)\n",
        "result = qa({\"query\": query})\n",
        "print(f'Query: {result[\"query\"]}\\n')\n",
        "print(f'Result: {result[\"result\"]}\\n')\n",
        "print(f'Context Documents: ')\n",
        "for srcdoc in result[\"source_documents\"]:\n",
        "      print(f'{srcdoc}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "8m-J59SDN9yO",
        "outputId": "090c705a-112e-4a67-be8e-276c86182572"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>How AWS has evolved?</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p>{'query': 'How AWS has evolved?', 'result': ' Amazon Web Services (AWS) has evolved significantly over time. Initially, AWS offered basic cloud computing services, similar to what other providers were offering at the time. However, as AWS continued to grow and innovate, it began to offer much more functionality in AWS than its competitors could find anywhere else. This included features such as serverless computing, containerization, and machine learning capabilities. As a result, AWS became a key differentiator for many businesses, allowing them to easily scale and innovate on top of the AWS platform. Over time, AWS continued to invest in new technologies and services, including artificial intelligence and data analytics tools, further solidifying its position as a leader in the cloud computing industry. Today, AWS is a highly profitable business with a global presence, serving customers of all sizes and industries.', 'source_documents': [Document(page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}), Document(page_content='We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]}</p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "CHAIN to answer questions\n",
            "--------------------------------------------------------------------------------\n",
            "Query: How AWS has evolved?\n",
            "\n",
            "Result:  AWS started out as a cloud computing platform for web services and has since expanded into a vast array of products and services that provide much more functionality to its users. This evolution has been driven by the company's commitment to continue investing in AWS, which has resulted in it becoming an industry leader and a significant differentiator among other cloud service providers. The development of AWS over the past fifteen years has also been accelerated by Amazon's head start on potential competitors and its desire to maintain its position at the forefront of technological innovation. Today, AWS is used by companies of all sizes and types to manage their technology infrastructure and has become an integral part of many businesses' operations.\n",
            "\n",
            "Context Documents: \n",
            "page_content='customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.' metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}\n",
            "\n",
            "page_content='We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009' metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
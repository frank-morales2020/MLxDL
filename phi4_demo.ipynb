{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNCPqfeW0KIryoOIdjxFrb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/phi4_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers bitsandbytes -q\n",
        "!pip install  faiss-gpu -q"
      ],
      "metadata": {
        "id": "EM-j31LGEDPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC3Kz18zwV0a",
        "outputId": "a8c95608-c431-4b74-d9bc-118ff786ddf1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  9 15:52:04 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   44C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUqCBcKDCLwf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Load the Phi-4 model and tokenizer with 4-bit quantization\n",
        "model_name = \"microsoft/phi-4\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Explicitly set low_cpu_mem_usage=True (or False if you don't want it)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True # Explicitly setting low_cpu_mem_usage to true.\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Set the pad_token_id for the model explicitly to avoid the warning\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "low_cpu_mem_usage=False  # Explicitly set to False\n",
        "\n",
        "def generate_text(prompt, max_length=512):\n",
        "  \"\"\"\n",
        "  Generates text using the 4-bit quantized Phi-4 model.\n",
        "\n",
        "  Args:\n",
        "    prompt: The input text to start the generation.\n",
        "    max_length: The maximum length of the generated text.\n",
        "\n",
        "  Returns:\n",
        "    The generated text.\n",
        "  \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "  outputs = model.generate(**inputs, max_length=max_length)\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "prompt = \"Once upon a time, in a land far away, there lived\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZyOyk4PTbFv",
        "outputId": "fb6254d1-ff2d-425d-c4f9-a0d26e131020"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a land far away, there lived a young girl named Lily. She was known for her adventurous spirit and her love for exploring the world around her. One day, while wandering through the forest, Lily stumbled upon a hidden path that led to a mysterious cave. Curiosity got the better of her, and she decided to venture inside.\n",
            "\n",
            "As she entered the cave, Lily noticed a faint glow emanating from a corner. She cautiously approached and discovered a magical crystal that shimmered with all the colors of the rainbow. The crystal spoke to her, telling her that it held the power to grant one wish to anyone who found it. Excited by the possibilities, Lily closed her eyes and made her wish.\n",
            "\n",
            "To her surprise, the crystal granted her wish, but with a twist. Instead of granting her a material desire, it gave her the ability to understand and communicate with animals. Overjoyed by her newfound gift, Lily spent her days conversing with the creatures of the forest, learning about their lives and helping them in any way she could.\n",
            "\n",
            "However, as time went on, Lily began to feel a sense of loneliness. She realized that while she could understand the animals, she couldn't truly connect with them on a human level. She longed for companionship and someone who could understand her own thoughts and feelings.\n",
            "\n",
            "Determined to find a solution, Lily embarked on a journey to seek the wisdom of the wise old owl who lived atop the tallest tree in the forest. The owl, known for his vast knowledge and wisdom, listened attentively to Lily's story and offered her a piece of advice.\n",
            "\n",
            "\"True companionship,\" the owl said, \"comes from within. It is not about finding someone who can understand you, but rather about understanding yourself and accepting who you are.\"\n",
            "\n",
            "Inspired by the owl's words, Lily returned to the forest with a newfound sense of purpose. She realized that her ability to communicate with animals was a gift that allowed her to bridge the gap between humans and nature. She dedicated herself to spreading awareness about the importance of preserving the environment and protecting the creatures that called it home.\n",
            "\n",
            "As the years went by, Lily became a renowned environmentalist, using her unique gift to advocate for the rights of animals and the preservation of their habitats. Her story spread far and wide, inspiring others to follow in her footsteps and make a difference in the world.\n",
            "\n",
            "In the end, Lily found true companionship not in another person, but in the knowledge that she was making a positive impact on the world around\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import defaultdict\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Set the pad_token_id for the model explicitly to avoid the warning\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def generate_text(prompt, max_length=512):\n",
        "  \"\"\"\n",
        "  Generates text using the 4-bit quantized Phi-4 model.\n",
        "  \"\"\"\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "  outputs = model.generate(**inputs, max_length=max_length, pad_token_id=tokenizer.pad_token_id) #added pad token id here\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return generated_text\n",
        "\n",
        "# Function to create embeddings for recipes (using Phi-4)\n",
        "def create_recipe_embeddings(recipes):\n",
        "    \"\"\"\n",
        "    Creates embeddings for each recipe using Phi-4.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for recipe in recipes:\n",
        "        prompt = f\"Recipe: {recipe[0]}\\nIngredients: {', '.join(recipe[1])}\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        # The Key change is calling the model directly\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        # Extract the last hidden state, convert to float, and mean across all tokens (corrected)\n",
        "        embedding = outputs.hidden_states[-1].float().mean(dim=1).cpu().detach().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return np.array(embeddings).squeeze(1)\n",
        "\n",
        "\n",
        "\n",
        "# Sample recipe data (replace with your actual recipe data)\n",
        "recipes = [\n",
        "    (\"Chicken and Broccoli Stir-Fry\", [\"chicken\", \"broccoli\", \"soy sauce\", \"ginger\"]),\n",
        "    (\"Spaghetti with Tomato Sauce\", [\"pasta\", \"tomatoes\", \"garlic\", \"onion\"]),\n",
        "    (\"Kung Pao Chicken\",[\"chicken\", \"peanuts\", \"garlic\",\"peppercorns\"]),\n",
        "    (\"Tuscan Chicken Skillet\", [\"chicken\", \"bacon\", \"cream\", \"parmesan\"]),\n",
        "    (\"Chicken Florentine\",[\"chicken\", \"spinach\", \"garlic\", \"cream\"])\n",
        "    # ... add more recipes here\n",
        "]\n",
        "\n",
        "# Create recipe embeddings\n",
        "recipe_embeddings = create_recipe_embeddings(recipes)\n",
        "\n",
        "# Build a FAISS index\n",
        "dimension = recipe_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)  # Using a simple L2 distance index\n",
        "index.add(recipe_embeddings)\n",
        "\n",
        "# Function to search for similar recipes using FAISS\n",
        "\n",
        "def search_recipes(ingredients, top_k=3):\n",
        "    \"\"\"\n",
        "    Searches for similar recipes in the FAISS index based on ingredient embeddings.\n",
        "    \"\"\"\n",
        "    prompt = f\"Ingredients: {', '.join(ingredients)}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    # Remove the generate and call the model directly to obtain hidden_states\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "    # Extract the last hidden state, convert to float, mean across all tokens, and reshape (corrected)\n",
        "    query_embedding = outputs.hidden_states[-1].float().mean(dim=1).cpu().detach().numpy() # Access hidden states directly\n",
        "    query_embedding = query_embedding.reshape(1, -1)  # Reshape to (1, embedding_dimension)\n",
        "    D, I = index.search(query_embedding, top_k)  # Search for top_k similar recipes\n",
        "    similar_recipe_indices = I[0]\n",
        "    return [recipes[i][0] for i in similar_recipe_indices]  # Return recipe names\n",
        "\n",
        "\n",
        "\n",
        "# Simple NLU (replace with a more sophisticated NLU component)\n",
        "def extract_ingredients(user_input):\n",
        "  \"\"\"\n",
        "  Extracts ingredients from the user input (very basic example).\n",
        "  \"\"\"\n",
        "  ingredients = []\n",
        "  if \"chicken\" in user_input:\n",
        "    ingredients.append(\"chicken\")\n",
        "  if \"broccoli\" in user_input:\n",
        "    ingredients.append(\"broccoli\")\n",
        "  if \"pasta\" in user_input:\n",
        "    ingredients.append(\"pasta\")\n",
        "  if \"tomatoes\" in user_input:\n",
        "    ingredients.append(\"tomatoes\")\n",
        "  if \"peanuts\" in user_input:\n",
        "    ingredients.append(\"peanuts\")\n",
        "  if \"bacon\" in user_input:\n",
        "    ingredients.append(\"bacon\")\n",
        "  if \"spinach\" in user_input:\n",
        "    ingredients.append(\"spinach\")\n",
        "\n",
        "  # Add more ingredient extraction logic here...\n",
        "  return tuple(ingredients)\n",
        "\n",
        "# Agent State (to remember past interactions and goals)\n",
        "agent_state = defaultdict(lambda: {\n",
        "    \"ingredients\": [],\n",
        "    \"last_recipe\": None,\n",
        "    \"goal\": None,\n",
        "    \"goal_completed\": False,\n",
        "    \"rejected_recipes\": []  # Keep track of rejected recipes\n",
        "})\n",
        "\n",
        "# Agent interaction loop\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Extract ingredients and potential goal (simplified example)\n",
        "    ingredients = extract_ingredients(user_input)\n",
        "    if \"make a meal with\" in user_input:\n",
        "        goal = \"suggest a recipe\"\n",
        "    elif \"what else can I make\" in user_input:\n",
        "        goal = \"suggest another recipe\"\n",
        "    elif \"I don't like that\" in user_input:\n",
        "        goal = \"reject suggestion\"\n",
        "    else:\n",
        "        goal = None\n",
        "\n",
        "    # Update agent state\n",
        "    user_state = agent_state[user_input]\n",
        "    user_state[\"ingredients\"].extend(ingredients)\n",
        "    if goal:\n",
        "        user_state[\"goal\"] = goal\n",
        "\n",
        "    # Construct the prompt with state and goal (no knowledge base in the prompt)\n",
        "    prompt = f\"I have {', '.join(user_state['ingredients'])}. \"\n",
        "    if user_state[\"last_recipe\"]:\n",
        "        prompt += f\"I already suggested {user_state['last_recipe']}. \"\n",
        "    if user_state[\"rejected_recipes\"]:\n",
        "        prompt += f\"The user didn't like these recipes: {', '.join(user_state['rejected_recipes'])}. \"\n",
        "    if user_state[\"goal\"]:\n",
        "        prompt += f\"My goal is to {user_state['goal']}. \"\n",
        "    prompt += \"What should I do?\\n\"  # Ask the agent what to do\n",
        "\n",
        "    response = generate_text(prompt)\n",
        "\n",
        "    # Extract suggestions or actions from the response (simplified example)\n",
        "    suggestions = []\n",
        "    for line in response.splitlines():\n",
        "        if \"- \" in line:\n",
        "            suggestions.append(line.split(\"- \")[1])\n",
        "\n",
        "    # If no suggestions from Phi-4, use FAISS\n",
        "    if not suggestions:\n",
        "        suggestions = search_recipes(user_state[\"ingredients\"])\n",
        "\n",
        "    # Update agent state and determine if goal is completed\n",
        "    if suggestions and user_state[\"goal\"] == \"suggest a recipe\":\n",
        "        user_state[\"last_recipe\"] = suggestions[0]\n",
        "        user_state[\"goal_completed\"] = True\n",
        "    elif suggestions and user_state[\"goal\"] == \"suggest another recipe\":\n",
        "        # Ensure it's not the same as the last recipe or a rejected recipe\n",
        "        if suggestions[0] != user_state[\"last_recipe\"] and suggestions[0] not in user_state[\"rejected_recipes\"]:\n",
        "            user_state[\"last_recipe\"] = suggestions[0]\n",
        "            user_state[\"goal_completed\"] = True\n",
        "    elif user_state[\"goal\"] == \"reject suggestion\" and user_state[\"last_recipe\"]:\n",
        "        user_state[\"rejected_recipes\"].append(user_state[\"last_recipe\"])\n",
        "        user_state[\"last_recipe\"] = None\n",
        "        user_state[\"goal_completed\"] = True\n",
        "\n",
        "    print(\"Agent: Here are some recipe suggestions:\")\n",
        "    for suggestion in suggestions:\n",
        "        print(f\"- {suggestion}\")\n",
        "\n",
        "    if user_state[\"goal_completed\"]:\n",
        "        print(\"Agent: I have completed my goal.\")\n",
        "        user_state[\"goal\"] = None\n",
        "        user_state[\"goal_completed\"] = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bow0G7xPhU5q",
        "outputId": "90320e17-837b-409f-e12e-4b60a493f5d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: chicken \n",
            "Agent: Here are some recipe suggestions:\n",
            "- Chicken and Broccoli Stir-Fry\n",
            "- Chicken Florentine\n",
            "- Spaghetti with Tomato Sauce\n",
            "You: exit \n",
            "Agent: Here are some recipe suggestions:\n",
            "- How to Cash a Check Without\n",
            "You: exit\n"
          ]
        }
      ]
    }
  ]
}
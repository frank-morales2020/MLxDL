{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOp5Z0Kbn2391xaaCLAIpSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/AGENTIC_T2SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAxxuWSDCUDA"
      },
      "outputs": [],
      "source": [
        "# From the provided reference:\n",
        "# Assume these are already installed as per the notebook:\n",
        "!pip install -U langchain-community -q\n",
        "!pip install -U crewai -q\n",
        "!pip install 'crewai [tools]' -q\n",
        "!pip install transformers -U -q\n",
        "!pip install colab-env -q\n",
        "!pip install unsloth -q\n",
        "!pip install torch -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "from typing import Any, List, Dict, Optional\n",
        "\n",
        "# Ensure all necessary Langchain/Transformers/Unsloth imports are here\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.outputs import ChatResult, ChatGeneration\n",
        "\n",
        "# Import PromptTemplate and LLMChain for the new approach\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Unsloth and Transformers imports for model loading\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import pipeline, AutoConfig # Make sure AutoConfig is imported\n",
        "\n",
        "# Import BaseTool if you still want to use your tool class structure\n",
        "from langchain.tools import BaseTool\n",
        "\n",
        "# --- 1. Custom LLM Wrapper (UnslothCrewAILLM) ---\n",
        "# This class makes your fine-tuned model compatible with Langchain.\n",
        "# (Keep the same class definition from the last attempt as it's the most compliant)\n",
        "class UnslothCrewAILLM(BaseChatModel):\n",
        "    model: Any\n",
        "    tokenizer: Any\n",
        "    pipeline: Any = None\n",
        "    max_new_tokens: int = 1024\n",
        "    temperature: float = 0.1\n",
        "    do_sample: bool = False\n",
        "    trust_remote_code: bool = True\n",
        "\n",
        "    def __init__(self, model, tokenizer, pipeline=None, max_new_tokens=1024, temperature=0.1, do_sample: bool = False, trust_remote_code=True):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            pipeline=pipeline,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=do_sample,\n",
        "            trust_remote_code=trust_remote_code\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        if not messages:\n",
        "            raise ValueError(\"No messages provided to the LLM wrapper.\")\n",
        "\n",
        "        # Langchain often sends a list of messages, take the last one as the primary prompt\n",
        "        final_message_content = messages[-1].content\n",
        "\n",
        "        if self.pipeline:\n",
        "            try:\n",
        "                response = self.pipeline(\n",
        "                    final_message_content,\n",
        "                    num_return_sequences=1,\n",
        "                    return_full_text=False,\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    temperature=self.temperature,\n",
        "                    do_sample=self.do_sample,\n",
        "                )\n",
        "                generated_text = response[0].get('generated_text', '').strip() if response else ''\n",
        "            except Exception as e:\n",
        "                print(f\"Error during pipeline generation in wrapper: {e}\")\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "        elif self.model and self.tokenizer:\n",
        "            try:\n",
        "                max_input_length = getattr(self.tokenizer, 'model_max_length', self.max_new_tokens)\n",
        "                inputs = self.tokenizer(final_message_content, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to(self.model.device)\n",
        "\n",
        "                if self.tokenizer.pad_token_id is None:\n",
        "                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=self.max_new_tokens,\n",
        "                    temperature=self.temperature,\n",
        "                    do_sample=self.do_sample,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    stopping_criteria=stop,\n",
        "                )\n",
        "                input_length = inputs.input_ids.shape[1]\n",
        "                generated_ids = outputs[0, input_length:]\n",
        "                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "            except Exception as e:\n",
        "                print(f\"Error during manual generation in wrapper: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                generated_text = f\"Error generating response: {e}\"\n",
        "        else:\n",
        "            generated_text = \"Error: Model or pipeline not loaded in wrapper.\"\n",
        "\n",
        "        message = AIMessage(content=generated_text)\n",
        "        generation = ChatGeneration(message=message)\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"unsloth_transformer_wrapper\"\n",
        "\n",
        "    def supports_stop_words(self) -> bool:\n",
        "        \"\"\"Returns whether the model supports stop words.\"\"\"\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def supports_control_chars(self) -> bool:\n",
        "        \"\"\"Returns whether the model supports control characters.\"\"\"\n",
        "        return False\n",
        "\n",
        "    # Add dummy implementations for other BaseChatModel methods for compatibility\n",
        "    # Implement stream, invoke, batch methods for better Langchain compatibility\n",
        "    # For this example, we can delegate _invoke to _generate\n",
        "    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement stream method (not used in this wrapper's logic, but required by BaseChatModel).\"\"\"\n",
        "        raise NotImplementedError(\"Streaming is not implemented for this wrapper.\")\n",
        "\n",
        "    def _invoke(self, prompt: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "        \"\"\"Implement invoke method (required by BaseChatModel).\"\"\"\n",
        "        # Delegate to generate and return the first message\n",
        "        return self._generate(prompt, stop=stop, run_manager=run_manager, **kwargs).generations[0].message\n",
        "\n",
        "    def _batch(self, messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any) -> List[ChatResult]:\n",
        "         \"\"\"Implement batch method (required by BaseChatModel).\"\"\"\n",
        "         return [self._generate(msgs, stop=stop, run_manager=run_manager, **kwargs) for msgs in messages]\n",
        "\n",
        "    async def _agenerate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Any = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        return self._generate(messages, stop, run_manager, **kwargs)\n",
        "\n",
        "    async def _astream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "         \"\"\"Implement async stream method.\"\"\"\n",
        "         raise NotImplementedError(\"Async streaming is not implemented for this wrapper.\")\n",
        "\n",
        "    async def _ainvoke(self, prompt: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any):\n",
        "         \"\"\"Implement async invoke method.\"\"\"\n",
        "         return (await self._agenerate(prompt, stop=stop, run_manager=run_manager, **kwargs)).generations[0].message\n",
        "\n",
        "    async def _abatch(self, messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any) -> List[ChatResult]:\n",
        "         \"\"\"Implement async batch method.\"\"\"\n",
        "         import asyncio\n",
        "         return await asyncio.gather(*[self._agenerate(msgs, stop=stop, run_manager=run_manager, **kwargs) for msgs in messages])\n",
        "\n",
        "\n",
        "# --- 2. Database Schema Definition for Flight Planning ---\n",
        "db_schema = {\n",
        "    \"tables\": {\n",
        "        \"flights\": ['flight_id', 'departure_airport', 'arrival_airport', 'departure_time', 'arrival_time', 'aircraft_type', 'status', 'price'],\n",
        "        \"airports\": ['airport_code', 'airport_name', 'city', 'country'],\n",
        "        \"passengers\": ['passenger_id', 'first_name', 'last_name', 'email'],\n",
        "        \"bookings\": ['booking_id', 'flight_id', 'passenger_id', 'booking_date', 'seat_number']\n",
        "    }\n",
        "}\n",
        "db_schema_string_for_prompt = str(db_schema)\n",
        "\n",
        "# --- 3. Model Loading (using the model from the reference) ---\n",
        "fine_tuned_model_id = \"frankmorales2020/deepseek_r1_text2sql_finetuned\"\n",
        "max_seq_length = 2048\n",
        "load_in_4bit = True\n",
        "\n",
        "print(f\"\\n--- Attempting Direct LLM Loading for {fine_tuned_model_id} using Unsloth ---\")\n",
        "\n",
        "# Determine optimal dtype for Unsloth\n",
        "unsloth_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "unsloth_wrapper_pipeline = None\n",
        "llm_instance = None # Renamed from llm_for_agents for clarity in this new approach\n",
        "\n",
        "try:\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=fine_tuned_model_id,\n",
        "            max_seq_length=max_seq_length,\n",
        "            dtype=unsloth_dtype,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    print(\"Model and Tokenizer loaded successfully using Unsloth.\")\n",
        "\n",
        "    try:\n",
        "        # You can still create the pipeline if you prefer, or rely solely on manual generation\n",
        "        unsloth_wrapper_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            return_full_text=False,\n",
        "        )\n",
        "        print(\"Text generation pipeline created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create transformers pipeline: {e}. Falling back to manual generation.\")\n",
        "        unsloth_wrapper_pipeline = None # Ensure pipeline is None if creation fails\n",
        "\n",
        "    # Instantiate your custom LLM\n",
        "    llm_instance = UnslothCrewAILLM(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        pipeline=unsloth_wrapper_pipeline, # Pass the pipeline or None\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.1,\n",
        "        do_sample=False,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    print(\"UnslothCrewAILLM instance created.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"\\n-- Skipping model loading: Unsloth or necessary libraries not installed, or compatible GPU/CUDA setup not found. Error: {e}\")\n",
        "    print(\"Please ensure you have 'unsloth' and 'torch' installed and a compatible GPU/CUDA setup.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- An error occurred during model loading (Unsloth): {e} ---\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- 4. Define the SQL Query Executor Tool (as a Langchain BaseTool) ---\n",
        "# Keep the same tool definition\n",
        "class SQLQueryExecutorTool(BaseTool):\n",
        "    name: str = \"SQL Query Executor\"\n",
        "    description: str = \"Executes a given SQL query against the flight database and returns the results or errors.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        print(f\"\\n--- Attempting to execute SQL query: ---\\n{query}\\n--------------------------------------\")\n",
        "        # Simple validation/simulation\n",
        "        if \"DROP TABLE\" in query.upper() or \"DELETE FROM\" in query.upper():\n",
        "            return \"Error: Harmful SQL query detected and blocked for safety.\"\n",
        "        # Add a check for the specific flight query pattern\n",
        "        if \"SELECT\" in query.upper() and \"FROM flights\" in query.lower() and \"'JFK'\" in query and \"'LAX'\" in query and \"2025-07-01\" in query:\n",
        "             return \"SQL executed successfully. Sample results for flight query: [{'flight_id': 101, 'departure_airport': 'JFK', 'arrival_airport': 'LAX', 'price': 450.00}, {'flight_id': 105, 'departure_airport': 'JFK', 'arrival_airport': 'LAX', 'price': 520.00}]\"\n",
        "        elif \"category = 'Electronics'\" in query: # Keep old simulated results if needed for other tests\n",
        "            return \"SQL executed successfully. Sample results: [{'name': 'Laptop', 'price': 1200}, {'name': 'Smartphone', 'price': 800}]\"\n",
        "        elif \"orders made after 2023-01-01\" in query: # Keep old simulated results if needed for other tests\n",
        "            return \"SQL executed successfully. Sample results: [{'order_id': 1, 'order_date': '2023-02-15'}, {'order_id': 2, 'order_date': '2024-01-20'}]\"\n",
        "        elif not query.strip().lower().startswith(\"select\"):\n",
        "             return \"Error: Only SELECT queries are supported by this tool for safety and simplicity in this demo.\"\n",
        "        else:\n",
        "            if \"SELECT\" in query.upper() and \"FROM\" in query.upper():\n",
        "                return \"SQL executed successfully. (Simulated) No specific results available for this general query.\"\n",
        "            else:\n",
        "                return \"Error: Invalid or unexecutable SQL query format (simulated error).\"\n",
        "\n",
        "sql_executor_tool = SQLQueryExecutorTool()\n",
        "print(\"\\nSQL Query Executor Tool defined.\")\n",
        "\n",
        "\n",
        "# --- 5. Define the Prompt Template for SQL Generation ---\n",
        "sql_gen_template = \"\"\"Translate the following natural language query into a precise SQL query based on the provided database schema.\n",
        "\n",
        "Database Schema:\n",
        "{db_schema}\n",
        "\n",
        "Natural Language Query:\n",
        "{query}\n",
        "\n",
        "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
        "\n",
        "SQL:\n",
        "\"\"\"\n",
        "\n",
        "sql_gen_prompt = PromptTemplate(\n",
        "    input_variables=[\"db_schema\", \"query\"],\n",
        "    template=sql_gen_template,\n",
        ")\n",
        "print(\"\\nSQL Generation Prompt Template defined.\")\n"
      ],
      "metadata": {
        "id": "y_rWyhEACg08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Create the LLM Chain for SQL Generation ---\n",
        "\n",
        "if llm_instance is None:\n",
        "     print(\"\\nERROR: LLM instance is NOT available. Cannot create LLM Chain.\")\n",
        "else:\n",
        "    try:\n",
        "        sql_gen_chain = LLMChain(\n",
        "            llm=llm_instance,\n",
        "            prompt=sql_gen_prompt,\n",
        "            verbose=True, # Set verbose to True to see the prompt sent to the LLM\n",
        "        )\n",
        "        print(\"\\nLLMChain for SQL generation created.\")\n",
        "\n",
        "        # --- 7. Define the Natural Language Query ---\n",
        "        flight_query = \"Find all flights departing from 'JFK' to 'LAX' after 2025-07-01 and their prices.\"\n",
        "\n",
        "        print(f\"\\n--- Running Langchain Flow for query: \\\"{flight_query}\\\" ---\")\n",
        "\n",
        "        # --- 8. Run the LLMChain to generate SQL ---\n",
        "        # The LLMChain will take the prompt template, format it with inputs,\n",
        "        # and pass the resulting messages to the llm_instance._generate method.\n",
        "        print(\"\\n--- Generating SQL using LLMChain ---\")\n",
        "        generated_sql_result = sql_gen_chain.run(db_schema=db_schema_string_for_prompt, query=flight_query)\n",
        "\n",
        "        # The output from LLMChain.run() is typically the generated text\n",
        "        generated_sql = generated_sql_result.strip()\n",
        "\n",
        "        # Post-process to try and get just the SQL line (reuse parsing logic)\n",
        "        final_generated_sql = generated_sql.split(';')[0].strip() if ';' in generated_sql else generated_sql.split('\\n')[0].strip()\n",
        "\n",
        "\n",
        "        print(f\"\\n--- Generated SQL: ---\")\n",
        "        print(final_generated_sql)\n",
        "\n",
        "        # --- 9. Manually execute the generated SQL using the Tool ---\n",
        "        print(\"\\n--- Executing Generated SQL using Tool ---\")\n",
        "        tool_execution_result = sql_executor_tool.run(final_generated_sql)\n",
        "\n",
        "        print(f\"\\n--- Tool Execution Result: ---\")\n",
        "        print(tool_execution_result)\n",
        "\n",
        "        print(\"\\n### Langchain Flow Finished ###\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- An error occurred during the Langchain flow: {e} ---\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGgqEbxBGYPp",
        "outputId": "bebe30a4-8d60-4700-c065-58ede65f5ce9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LLMChain for SQL generation created.\n",
            "\n",
            "--- Running Langchain Flow for query: \"Find all flights departing from 'JFK' to 'LAX' after 2025-07-01 and their prices.\" ---\n",
            "\n",
            "--- Generating SQL using LLMChain ---\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mTranslate the following natural language query into a precise SQL query based on the provided database schema.\n",
            "\n",
            "Database Schema:\n",
            "{'tables': {'flights': ['flight_id', 'departure_airport', 'arrival_airport', 'departure_time', 'arrival_time', 'aircraft_type', 'status', 'price'], 'airports': ['airport_code', 'airport_name', 'city', 'country'], 'passengers': ['passenger_id', 'first_name', 'last_name', 'email'], 'bookings': ['booking_id', 'flight_id', 'passenger_id', 'booking_date', 'seat_number']}}\n",
            "\n",
            "Natural Language Query:\n",
            "Find all flights departing from 'JFK' to 'LAX' after 2025-07-01 and their prices.\n",
            "\n",
            "Output ONLY the SQL query string, no additional text, explanation, or formatting like markdown.\n",
            "\n",
            "SQL:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "--- Generated SQL: ---\n",
            "SELECT flight_id, departure_airport, arrival_airport, departure_time, arrival_time, aircraft_type, status, price\n",
            "FROM flights\n",
            "WHERE departure_airport = 'JFK' AND arrival_airport = 'LAX' AND departure_time > '2025-07-01'\n",
            "ORDER BY departure_time\n",
            "\n",
            "--- Executing Generated SQL using Tool ---\n",
            "\n",
            "--- Attempting to execute SQL query: ---\n",
            "SELECT flight_id, departure_airport, arrival_airport, departure_time, arrival_time, aircraft_type, status, price\n",
            "FROM flights\n",
            "WHERE departure_airport = 'JFK' AND arrival_airport = 'LAX' AND departure_time > '2025-07-01'\n",
            "ORDER BY departure_time\n",
            "--------------------------------------\n",
            "\n",
            "--- Tool Execution Result: ---\n",
            "SQL executed successfully. (Simulated) No specific results available for this general query.\n",
            "\n",
            "### Langchain Flow Finished ###\n"
          ]
        }
      ]
    }
  ]
}
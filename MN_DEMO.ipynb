{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYgsB2iZFecpHqFkZkLdI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MN_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-NqNHUPiJR7",
        "outputId": "a9323cc9-891d-4336-ed8c-b5bc293df8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Manifold Muon Demo (m=128, n=64) ---\n",
            "Initial W shape: (128, 64)\n",
            "Next W shape: (128, 64)\n",
            "W^T W should be close to I_n. Max error: 1.33e-15\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import svd, signm # Scipy for SVD and matrix sign (msign)\n",
        "\n",
        "# --- 1. Helper Functions (Approximations of Complex Operations) ---\n",
        "\n",
        "def matrix_sign(M):\n",
        "    \"\"\"\n",
        "    Computes the unitary component of the polar decomposition (msign/retraction)\n",
        "    for a rectangular matrix M, which snaps its singular values to one.\n",
        "    This is the numerically stable way to compute msign for m x n matrices.\n",
        "    \"\"\"\n",
        "    # Full matrices=False ensures U is m x k and V is n x k (where k=rank M),\n",
        "    # matching the notation in Figure 5 and the Stiefel manifold definition.\n",
        "    U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
        "\n",
        "    # The unitary factor Q is U @ V^T. The diagonal matrix S (singular values)\n",
        "    # is discarded, effectively setting all singular values to 1.\n",
        "    return U @ Vt\n",
        "\n",
        "def project_to_stiefel(W):\n",
        "    \"\"\"\n",
        "    Retracts W back to the Stiefel manifold by calculating the matrix sign of W.\n",
        "    W_retracted <- msign(W).\n",
        "    For a tall matrix (m >= n), this projection is equivalent to W / sqrt(W^T W)\n",
        "    in the Euclidean norm case, but for spectral norm, msign is used.\n",
        "    \"\"\"\n",
        "    return matrix_sign(W)\n",
        "\n",
        "\n",
        "# --- 2. Manifold Muon Core Logic ---\n",
        "\n",
        "class ManifoldMuonOptimizer:\n",
        "    def __init__(self, m, n, learning_rate_eta, dual_rate_alpha, dual_steps):\n",
        "        \"\"\"\n",
        "        Initializes the optimizer for a weight matrix W of size (m, n).\n",
        "        :param learning_rate_eta: The main learning rate (eta) for the weight update.\n",
        "        :param dual_rate_alpha: The learning rate (alpha) for the dual ascent.\n",
        "        :param dual_steps: Number of dual ascent steps to approximate Lambda_opt.\n",
        "        \"\"\"\n",
        "        self.eta = learning_rate_eta\n",
        "        self.alpha = dual_rate_alpha\n",
        "        self.dual_steps = dual_steps\n",
        "\n",
        "        # Dual variable Lambda, initialized to zero matrix (n x n) since W is m x n (tall)\n",
        "        self.Lambda = np.zeros((n, n))\n",
        "\n",
        "    def H_gradient(self, G, W, Lambda):\n",
        "        \"\"\"\n",
        "        Calculates the gradient of the dual function H(Lambda)[cite: 267].\n",
        "        H(Lambda) = -eta * d/dLambda || G + 2W(Lambda + Lambda^T) ||_nuclear [cite: 267]\n",
        "        \"\"\"\n",
        "        # The term inside the msign\n",
        "        M = G + 2 * W @ (Lambda + Lambda.T)\n",
        "\n",
        "        # H is proportional to: -eta * [ W^T msign(M) + msign(M)^T W ] [cite: 268, 269]\n",
        "        msign_M = matrix_sign(M)\n",
        "\n",
        "        # Calculate the full gradient H(Lambda)\n",
        "        H = -self.eta * (W.T @ msign_M + msign_M.T @ W)\n",
        "\n",
        "        return H\n",
        "\n",
        "    def step(self, W, G):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        :param W: Current weight matrix (m x n) on the Stiefel manifold.\n",
        "        :param G: Gradient of the loss w.r.t W (m x n).\n",
        "        \"\"\"\n",
        "        # -----------------------------------------------------------\n",
        "        # Step 1: Run gradient ascent on the dual variable Lambda to solve for Lambda_opt\n",
        "        # -----------------------------------------------------------\n",
        "        Lambda_opt = self.Lambda.copy()\n",
        "        for _ in range(self.dual_steps):\n",
        "            H = self.H_gradient(G, W, Lambda_opt)\n",
        "            # Dual ascent: Lambda <- Lambda + alpha * H(Lambda)\n",
        "            Lambda_opt += self.alpha * H\n",
        "\n",
        "        self.Lambda = Lambda_opt # Update the optimizer's internal Lambda for next step\n",
        "\n",
        "        # -----------------------------------------------------------\n",
        "        # Step 2: Compute the update A_opt\n",
        "        # -----------------------------------------------------------\n",
        "        # A_opt = -eta * msign(G + 2W(Lambda_opt + Lambda_opt^T))\n",
        "        M_opt = G + 2 * W @ (Lambda_opt + Lambda_opt.T)\n",
        "        A_opt = -self.eta * matrix_sign(M_opt)\n",
        "\n",
        "        # -----------------------------------------------------------\n",
        "        # Step 3: Apply the update to the weights\n",
        "        # -----------------------------------------------------------\n",
        "        # W <- W + A_opt\n",
        "        W_updated = W + A_opt\n",
        "\n",
        "        # -----------------------------------------------------------\n",
        "        # Step 4: Retract the weights back to the manifold\n",
        "        # -----------------------------------------------------------\n",
        "        # W <- msign(W)\n",
        "        W_retracted = project_to_stiefel(W_updated)\n",
        "\n",
        "        return W_retracted\n",
        "\n",
        "# --- 3. Example Usage ---\n",
        "\n",
        "# Define matrix dimensions (m >= n for Stiefel(m,n))\n",
        "m, n = 128, 64\n",
        "\n",
        "# Initialize a weight matrix W on the Stiefel manifold\n",
        "# (W^T W = I_n, implying W has orthonormal columns)\n",
        "# This is a good starting point for manifold Muon [cite: 217]\n",
        "U_init, _, Vt_init = svd(np.random.randn(m, n), full_matrices=False)\n",
        "W = U_init @ Vt_init # An orthogonal matrix (or a part of one) is on the Stiefel manifold\n",
        "\n",
        "# Create a mock gradient G (e.g., from a backpropagation step)\n",
        "G = np.random.randn(m, n)\n",
        "\n",
        "# Optimizer parameters (example values)\n",
        "ETA = 1e-2     # Learning rate for the weight update\n",
        "ALPHA = 1e-2   # Learning rate for the dual variable update\n",
        "DUAL_STEPS = 5 # Number of inner dual ascent steps\n",
        "\n",
        "optimizer = ManifoldMuonOptimizer(m, n, ETA, ALPHA, DUAL_STEPS)\n",
        "\n",
        "# Perform one optimization step\n",
        "W_next = optimizer.step(W, G)\n",
        "\n",
        "# --- 4. Verification (Post-Step Sanity Check) ---\n",
        "\n",
        "# Check if the updated W is still on the Stiefel manifold (W^T W should be close to I_n)\n",
        "I_n_check = W_next.T @ W_next\n",
        "identity_matrix = np.eye(n)\n",
        "\n",
        "print(f\"--- Manifold Muon Demo (m={m}, n={n}) ---\")\n",
        "print(f\"Initial W shape: {W.shape}\")\n",
        "print(f\"Next W shape: {W_next.shape}\")\n",
        "print(f\"W^T W should be close to I_n. Max error: {np.max(np.abs(I_n_check - identity_matrix)):.2e}\")\n",
        "\n",
        "# The max error should be very small, confirming W_next is successfully retracted\n",
        "# to the Stiefel manifold."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM"
      ],
      "metadata": {
        "id": "Al0y1xxejRLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conceptual Python Framework for Modular Manifold Comparison\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "# Assuming a hypothetical 'modula_lib' that provides tools for the modular norm\n",
        "# In a real scenario, this would be a specialized library.\n",
        "# import modula_lib\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. CORE FUNCTION: The Modular Norm (Conceptual Implementation)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def calculate_modular_norm(weights_dict, arch_config):\n",
        "    \"\"\"\n",
        "    Conceptually computes the modular norm for an entire LLM architecture.\n",
        "\n",
        "    In reality, this involves recursively applying composition rules (like max/sum)\n",
        "    and calculating the spectral norm for each weight matrix (Manifold Muon's norm).\n",
        "    The weights are normalized by scalar coefficients (s_i) which budget\n",
        "    learning rates across layers. [cite: 351, 352, 323, 399]\n",
        "\n",
        "    :param weights_dict: A dictionary of all weight tensors in the LLM.\n",
        "    :param arch_config: Architectural details (layer types, composition, etc.).\n",
        "    :return: A dictionary of layer-specific modular norm values.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Step A: Compute Layer-wise Spectral Norms (The Manifold Muon Norm) ---\n",
        "    # The spectral norm is the measure of length used for the Stiefel manifold\n",
        "    # and the Manifold Muon optimizer. [cite: 224, 191]\n",
        "    layer_norms = {}\n",
        "    for name, W in weights_dict.items():\n",
        "        # The spectral norm of a matrix W is its largest singular value (sigma_max).\n",
        "        sigma_max = torch.linalg.svdvals(W)[0].item()\n",
        "        layer_norms[name] = sigma_max\n",
        "\n",
        "    # --- Step B: Apply Recursive Modular Composition (The 'Modular Manifold' Part) ---\n",
        "    # This step is highly complex but fundamentally involves:\n",
        "    # 1. Assigning scalar coefficients (s_i) based on the layer's position. [cite: 351]\n",
        "    # 2. Composing norms using the 'max' function for sequence-connected modules. [cite: 351]\n",
        "\n",
        "    # Simplified, conceptual budgeting (Example: penalizing late layers less)\n",
        "    modular_norms = {}\n",
        "    s_i_base = 1.0 # Conceptual scalar coefficient\n",
        "    for name, norm in layer_norms.items():\n",
        "        # Example of architectural budgeting: Layers deeper in the network\n",
        "        # (e.g., in Transformer blocks) might be weighted differently.\n",
        "        if \"attn\" in name or \"mlp\" in name:\n",
        "            # Modular norm for a composite module (e.g., an entire Transformer block)\n",
        "            # is max(s1 * ||w1||, s2 * ||w2||). [cite: 351]\n",
        "            modular_norms[name] = s_i_base * norm\n",
        "        else:\n",
        "            modular_norms[name] = norm\n",
        "\n",
        "    # The overall modular norm of the LLM is derived from these compositions.\n",
        "    return modular_norms\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. DEMO: Comparing LLM Architectures\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def llm_comparison_demo():\n",
        "    # --- A. Load Mock Architectures and Weights ---\n",
        "    # In a real study, you would load the full state_dict from a Hugging Face model\n",
        "\n",
        "    # 1. LLM A: Conceptual Llama-like (e.g., using RMS norm for input/output scaling)\n",
        "    # Weights for a Transformer layer: W_q, W_k, W_v, W_o, W_up, W_down\n",
        "    llama_weights = OrderedDict({\n",
        "        \"attn.W_q\": torch.randn(128, 128) * 1.05,\n",
        "        \"attn.W_k\": torch.randn(128, 128) * 0.98,\n",
        "        \"mlp.W_up\": torch.randn(128, 512) * 1.02, # FFN is often wider\n",
        "        \"mlp.W_down\": torch.randn(512, 128) * 0.95,\n",
        "    })\n",
        "\n",
        "    # 2. LLM B: Conceptual Mistral-like (e.g., optimized for smaller size/different scaling)\n",
        "    mistral_weights = OrderedDict({\n",
        "        \"attn.W_q\": torch.randn(128, 128) * 1.20, # Higher unconstrained norm\n",
        "        \"attn.W_k\": torch.randn(128, 128) * 1.15,\n",
        "        \"mlp.W_up\": torch.randn(128, 512) * 0.85, # Lower FFN norm\n",
        "        \"mlp.W_down\": torch.randn(512, 128) * 0.90,\n",
        "    })\n",
        "\n",
        "    # --- B. Calculate Modular Norm (or Spectral Norm as a Proxy) ---\n",
        "    print(\"--- Conceptual Modular Norm Analysis ---\")\n",
        "\n",
        "    llama_norms = calculate_modular_norm(llama_weights, \"Llama_Config\")\n",
        "    mistral_norms = calculate_modular_norm(mistral_weights, \"Mistral_Config\")\n",
        "\n",
        "    print(\"\\n[LLM A (Llama-like) Modular Norms (Spectral Norm Proxy)]\")\n",
        "    for name, norm in llama_norms.items():\n",
        "        print(f\"  {name:<12}: {norm:.4f}\")\n",
        "\n",
        "    print(\"\\n[LLM B (Mistral-like) Modular Norms (Spectral Norm Proxy)]\")\n",
        "    for name, norm in mistral_norms.items():\n",
        "        print(f\"  {name:<12}: {norm:.4f}\")\n",
        "\n",
        "    # --- C. Interpretation and Comparison ---\n",
        "    print(\"\\n--- Interpretation based on Modular Manifolds Theory ---\")\n",
        "\n",
        "    llama_max_norm = max(llama_norms.values())\n",
        "    mistral_max_norm = max(mistral_norms.values())\n",
        "\n",
        "    # The modular norm is tied to the Lipschitz constant of the network. [cite: 325]\n",
        "    if llama_max_norm < mistral_max_norm:\n",
        "        print(f\"LLM A Max Norm: {llama_max_norm:.4f}\")\n",
        "        print(f\"LLM B Max Norm: {mistral_max_norm:.4f}\")\n",
        "        print(\"LLM A *might* have a tighter Lipschitz constant in this norm, potentially leading to more stable, transferable learning rates if trained with a norm-constrained optimizer like Manifold Muon. [cite: 342, 325, 407]\")\n",
        "    else:\n",
        "        print(f\"LLM A Max Norm: {llama_max_norm:.4f}\")\n",
        "        print(f\"LLM B Max Norm: {mistral_max_norm:.4f}\")\n",
        "        print(\"LLM B *might* have a tighter Lipschitz constant, suggesting more predictable behavior to weight perturbations in this architectural comparison. [cite: 342, 325, 407]\")\n",
        "\n",
        "llm_comparison_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3krlruqjTL6",
        "outputId": "117be3b6-64d1-4151-e6b4-241d149fd5bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Conceptual Modular Norm Analysis ---\n",
            "\n",
            "[LLM A (Llama-like) Modular Norms (Spectral Norm Proxy)]\n",
            "  attn.W_q    : 23.2828\n",
            "  attn.W_k    : 22.0583\n",
            "  mlp.W_up    : 34.3833\n",
            "  mlp.W_down  : 31.4547\n",
            "\n",
            "[LLM B (Mistral-like) Modular Norms (Spectral Norm Proxy)]\n",
            "  attn.W_q    : 26.7807\n",
            "  attn.W_k    : 25.6144\n",
            "  mlp.W_up    : 28.0282\n",
            "  mlp.W_down  : 31.2510\n",
            "\n",
            "--- Interpretation based on Modular Manifolds Theory ---\n",
            "LLM A Max Norm: 34.3833\n",
            "LLM B Max Norm: 31.2510\n",
            "LLM B *might* have a tighter Lipschitz constant, suggesting more predictable behavior to weight perturbations in this architectural comparison. [cite: 342, 325, 407]\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "authorship_tag": "ABX9TyPvxGS6+K18pduqaskpnJSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/SFT_TPU_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. INSTALL LIBRARIES\n",
        "print(\"Installing/Updating libraries...\")\n",
        "!pip install -q --upgrade datasets huggingface_hub tensorflow keras-hub keras jax jaxlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzj_goywia7e",
        "outputId": "75b93686-36bb-4c19-96f1-9affbcb08b43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing/Updating libraries...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"jax._src.cloud_tpu_init\")  # ← Kills the hugepages warning\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"JAX_DISABLE_FLASH_ATTENTION\"] = \"1\"   # Critical fix\n",
        "\n",
        "\n",
        "import jax\n",
        "import keras\n",
        "import keras_hub\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "\n",
        "# Silence all remaining JAX/Abseil logs\n",
        "logging.getLogger(\"absl\").setLevel(logging.CRITICAL)\n",
        "jax.config.update(\"jax_debug_nans\", False)\n",
        "\n",
        "# Accurate TPU Detection\n",
        "device = jax.devices()[0]\n",
        "kind = device.device_kind if hasattr(device, 'device_kind') else device.platform.upper()\n",
        "if \"v6\" in kind.lower(): tpu_type = \"TPU v6 lite\"\n",
        "elif \"v5e\" in kind.lower(): tpu_type = \"TPU v5e\"\n",
        "elif \"v5p\" in kind.lower(): tpu_type = \"TPU v5p (Trillium)\"\n",
        "elif \"v4\" in kind.lower(): tpu_type = \"TPU v4\"\n",
        "else: tpu_type = kind\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TPU TYPE IS USED IT\".center(70))\n",
        "print(tpu_type.center(70))\n",
        "print(\"=\"*70)\n",
        "print(f\"Devices: {jax.devices()} | Cores: {jax.device_count()}\\n\")\n",
        "\n",
        "\n",
        "# Global precision (best for TPU)\n",
        "keras.config.set_floatx(\"bfloat16\")\n",
        "keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
        "\n",
        "# Verify TPU\n",
        "#print(f\"Devices: {jax.devices()}\")\n",
        "#print(f\"TPU cores: {jax.device_count()}\")\n",
        "\n",
        "# ----------------- DATA -----------------\n",
        "print(\"\\nLoading dataset...\")\n",
        "hf_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "\n",
        "# Tiny subset for demo (50 examples)\n",
        "tiny_data = [\n",
        "    f\"Instruction: {x['instruction']}\\nResponse: {x['response']}\"\n",
        "    for x in hf_dataset.select(range(50))\n",
        "]\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(tiny_data) \\\n",
        "    .batch(2, drop_remainder=True) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ----------------- MODEL -----------------\n",
        "print(\"Loading Gemma 2B IT...\")\n",
        "gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(\n",
        "    \"hf://google/gemma-1.1-2b-it-keras\",\n",
        "    dtype=\"bfloat16\"\n",
        ")\n",
        "\n",
        "# Enable LoRA (rank 4 = very efficient)\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "gemma_lm.preprocessor.sequence_length = 256  # Slightly longer, still fits\n",
        "\n",
        "# ----------------- COMPILE -----------------\n",
        "gemma_lm.compile(\n",
        "    optimizer=keras.optimizers.AdamW(learning_rate=5e-5, weight_decay=0.01),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# ----------------- TRAIN -----------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING CLEAN & SILENT TRAINING ON TPU\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "gemma_lm.fit(train_ds, epochs=1, verbose=1)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY – 100% CLEAN LOGS!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Optional: Quick test\n",
        "print(\"\\nQuick generation test:\")\n",
        "print(gemma_lm.generate(\"Question: What is the capital of France?\\nAnswer:\", max_length=50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQDoxOc6ax4c",
        "outputId": "e9b21b0b-d5ba-4928-b6df-6072f2b66047"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "                         TPU TYPE IS USED IT                          \n",
            "                             TPU v6 lite                              \n",
            "======================================================================\n",
            "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] | Cores: 1\n",
            "\n",
            "\n",
            "Loading dataset...\n",
            "Loading Gemma 2B IT...\n",
            "\n",
            "============================================================\n",
            "STARTING CLEAN & SILENT TRAINING ON TPU\n",
            "============================================================\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 13ms/step - loss: 1.5000 - sparse_categorical_accuracy: 0.4414\n",
            "============================================================\n",
            "TRAINING COMPLETED SUCCESSFULLY – 100% CLEAN LOGS!\n",
            "============================================================\n",
            "\n",
            "Quick generation test:\n",
            "Question: What is the capital of France?\n",
            "Answer: Paris.\n",
            "\n",
            "The answer is correct. Paris is the capital of France. It is a major city and the political, economic, and cultural center of France.\n"
          ]
        }
      ]
    }
  ]
}
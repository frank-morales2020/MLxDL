import os

print('LIBRARY INSTALLATION STARTED')
os.system('python3 -m pip install torch tensorboard --quiet')
os.system('python3 -m pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet')
os.system('python3 -m pip install -U flash-attn --no-build-isolation --quiet')
os.system('python3 -m pip install datasets trl ninja packaging peft --quiet')
os.system('python3 -m pip install diffusers safetensors IPython --quiet')
print('LIBRARY INSTALLATION ENDED')

import torch
import os
import sys
import json
import IPython
from datetime import datetime
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model
from transformers import (
	    AutoModelForCausalLM,
	        AutoTokenizer,
		    BitsAndBytesConfig,
		        AutoTokenizer,
			    TrainingArguments,
			        pipeline
				)
from trl import SFTTrainer

# set device
device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'
print(f'device: {device}')

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from trl import setup_chat_format

# Hugging Face model id
model_id = "abacusai/Smaug-72B-v0.1" 

# BitsAndBytesConfig int-4 config
bnb_config = BitsAndBytesConfig(
	    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
	    )
# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
		        model_id,
			    device_map="auto",
			        attn_implementation="flash_attention_2",
				    torch_dtype=torch.bfloat16,
				        quantization_config=bnb_config
					)
tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)
tokenizer.padding_side = 'right' # to prevent warnings

# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)
tokenizer.pad_token = tokenizer.unk_token
tokenizer.pad_token_id = tokenizer.unk_token_id

# # set chat template to OAI chatML, remove if you start from a fine-tuned model
model, tokenizer = setup_chat_format(model, tokenizer)


text = "What is the capital of Canada?"

#device = 'cuda'
model_inputs = tokenizer(text, return_tensors="pt").to(model.device)

generated_ids = model.generate(**model_inputs, temperature=0.9, top_k=1, top_p=1.0, repetition_penalty=1.4, min_new_tokens=16, max_new_tokens=128, do_sample=True)
decoded = tokenizer.decode(generated_ids[0])
print(decoded)



{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "MBlwyf_aKCP1"
      ],
      "authorship_tag": "ABX9TyP8DJYwbwcVo5QTRU+5ZKGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/JEPA_PLDCpynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxmjs9Cb_28g",
        "outputId": "d632dce5-9a8d-452d-bef0-36481000bc81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 24 11:18:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av -q"
      ],
      "metadata": {
        "id": "ZPGZavW8iV3t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "rBcU3tHoribX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lta /content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/"
      ],
      "metadata": {
        "id": "ymlI5TPb6WzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3859f69e-bba0-4e40-fdae-700ca462de28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22538\n",
            "-rw------- 1 root root 23070474 Jul 24 10:47 airplane-landing.mp4\n",
            "-rw------- 1 root root     6986 Jul 23 15:37 1_2023-02-22-15-21-49_feature.pt\n",
            "-rw------- 1 root root      203 Jul 23 15:37 feature_label_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: All Setup, Definitions, and Model Instantiations"
      ],
      "metadata": {
        "id": "pEGMDoonGFSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: All Setup, Definitions, and Model Instantiations\n",
        "# Run this cell completely after any kernel restart.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import av\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Logging setup\n",
        "import logging\n",
        "import datetime\n",
        "import pytz\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Import Google Generative AI components\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- API Key Setup ---\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "else:\n",
        "    print(\"WARNING: GOOGLE_API_KEY not found in Colab Secrets. Please ensure 'GEMINI' secret is set.\")\n",
        "    print(\"API calls will likely fail. Proceeding with unconfigured API.\")\n",
        "\n",
        "# --- Agent Configuration ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "\n",
        "# Define class labels\n",
        "CLASS_LABELS = [\n",
        "    \"airplane landing\",\n",
        "    \"airplane takeoff\",\n",
        "    \"airport ground operations\",\n",
        "    \"in-flight cruise\",\n",
        "    \"emergency landing\",\n",
        "    \"pre-flight check/maintenance\"\n",
        "]\n",
        "num_classes = len(CLASS_LABELS)\n",
        "CLASSIFIER_SAVE_PATH = \"classifier_head_trained_on_tartan_aviation_sample.pth\"\n",
        "\n",
        "# --- Configuration for V-JEPA and Dataset Paths ---\n",
        "hf_repo = \"facebook/vjepa2-vitg-fpc64-256\"\n",
        "EXTRACTED_FEATURES_DIR = \"/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/\"\n",
        "\n",
        "# Global parameters for expected V-JEPA output dimensions for PLDM.\n",
        "# For V-JEPA [1, 2048, 1408], if flattened for PLDM, this is 2048 * 1408.\n",
        "TOTAL_FLATTENED_VJEPA_DIM_FOR_PLDM = 2048 * 1408\n",
        "latent_dim_pldm = TOTAL_FLATTENED_VJEPA_DIM_FOR_PLDM # For PLDM Predictor input\n",
        "action_dim = 8\n",
        "\n",
        "# --- Helper Function for Video Loading and Feature Extraction (Returns raw V-JEPA output) ---\n",
        "def load_and_process_video(video_path, processor_instance, model_instance, device_instance, num_frames_to_sample=16):\n",
        "    \"\"\"\n",
        "    Loads a video, samples frames, and extracts V-JEPA features.\n",
        "    Returns extracted features (torch.Tensor, shape like [1, 2048, 1408]) and the list of raw frames (list).\n",
        "    Does NOT flatten the V-JEPA output here, keeping it as model's raw output.\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    if not os.path.exists(video_path):\n",
        "        logging.error(f\"ERROR: Video file '{video_path}' not found.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        container = av.open(video_path)\n",
        "        total_frames_in_video = container.streams.video[0].frames\n",
        "        sampling_interval = max(1, total_frames_in_video // num_frames_to_sample)\n",
        "\n",
        "        logging.info(f\"Total frames in video: {total_frames_in_video}\")\n",
        "        logging.info(f\"Sampling interval: {sampling_interval} frames\")\n",
        "\n",
        "        for i, frame in enumerate(container.decode(video=0)):\n",
        "            if len(frames) >= num_frames_to_sample:\n",
        "                break\n",
        "            if i % sampling_interval == 0:\n",
        "                img = frame.to_rgb().to_ndarray()\n",
        "                frames.append(img)\n",
        "\n",
        "        if not frames:\n",
        "            logging.error(f\"ERROR: No frames could be loaded from '{video_path}'.\")\n",
        "            return None, None\n",
        "        elif len(frames) < num_frames_to_sample:\n",
        "            logging.warning(f\"WARNING: Only {len(frames)} frames loaded. Model might perform suboptimally.\")\n",
        "\n",
        "        inputs = processor_instance(videos=list(frames), return_tensors=\"pt\")\n",
        "\n",
        "        # Move inputs to device before model inference\n",
        "        inputs = {k: v.to(device_instance) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model_instance(**inputs).last_hidden_state # Keep features in raw V-JEPA output shape\n",
        "\n",
        "        logging.info(f\"Successfully extracted V-JEPA features with raw shape: {features.shape}\")\n",
        "        return features, frames\n",
        "\n",
        "    except av.FFmpegError as e:\n",
        "        logging.error(f\"Error loading video with PyAV: {e}\")\n",
        "        logging.error(\"This might indicate an issue with the video file itself or PyAV installation.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred: {e}\")\n",
        "        logging.error(\"Ensure 'av' library is installed (`pip install av`) and that the video file is not corrupted.\")\n",
        "        return None, None\n",
        "\n",
        "# --- Define Classifier Head (using the working logic for input_dim) ---\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes): # input_dim will be the pooled feature dim (1408)\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(self.relu(self.fc1(x))))\n",
        "\n",
        "# Define LatentDynamicsPredictor (still expects total flattened dim for PLDM)\n",
        "class LatentDynamicsPredictor(torch.nn.Module):\n",
        "    def __init__(self, latent_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dim + action_dim, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_state, action):\n",
        "        combined_input = torch.cat([latent_state, action], dim=-1)\n",
        "        predicted_next_latent_state = self.layers(combined_input)\n",
        "        return predicted_next_latent_state\n",
        "\n",
        "# --- Instantiate Models and Optimizers (Done only once in Cell 1) ---\n",
        "print(\"\\n--- Instantiating Models and Optimizers ---\")\n",
        "model = AutoModel.from_pretrained(hf_repo)\n",
        "processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "\n",
        "# Determine device for all computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Move V-JEPA model to device\n",
        "\n",
        "# Initialize PLDM Predictor and its optimizer\n",
        "predictor = LatentDynamicsPredictor(latent_dim_pldm, action_dim) # Use PLDM specific latent_dim\n",
        "predictor.to(device) # Move predictor to device\n",
        "optimizer_pldm = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
        "\n",
        "# Initial Classifier instantiation (input_dim will be dynamically set in Cell 2)\n",
        "# For now, initialize with a dummy value, it will be correctly re-initialized later.\n",
        "# Or, if running the reference code, it expects 1408, so let's use that for initial Classifier instantiation.\n",
        "classifier = ClassifierHead(input_dim=1408, num_classes=num_classes) # Assuming 1408 is the pooled feature dim\n",
        "classifier.to(device) # Move classifier to device\n",
        "\n",
        "print(f\"Models instantiated and moved to {device}.\")\n",
        "print(\"\\nCell 1 setup complete. Remember to run this cell first after any kernel restart before running subsequent cells.\")"
      ],
      "metadata": {
        "id": "Uu8kZi1UGEKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Core Execution - Feature Extraction, Classifier Training & Inference, LLM Interaction, and PLDM Training/Planning"
      ],
      "metadata": {
        "id": "OHZabbIkGRf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Core Execution - Feature Extraction, Classifier Training & Inference, LLM Interaction, PLDM Training & Planning\n",
        "# This cell assumes Cell 1 has been successfully executed in the current session.\n",
        "# All objects (model, processor, classifier, predictor, device, optimizer_pldm)\n",
        "# and all function definitions (load_and_process_video, ClassifierHead, LatentDynamicsPredictor)\n",
        "# are expected to be available from Cell 1's execution.\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import json\n",
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm # Ensure tqdm is imported for progress bars\n",
        "import torch.optim as optim # For classifier optimizer\n",
        "from torch.utils.data import DataLoader, TensorDataset # For classifier training data\n",
        "import datetime # For LLM timestamp\n",
        "import pytz # For LLM timestamp\n",
        "\n",
        "# --- Mounting Google Drive ---\n",
        "print(\"\\n--- Cell 2: Mounting Google Drive for dataset access ---\")\n",
        "drive.mount('/content/gdrive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "print(f\"Checking for extracted features directory: {EXTRACTED_FEATURES_DIR}\")\n",
        "if not os.path.exists(EXTRACTED_FEATURES_DIR):\n",
        "    logging.error(f\"ERROR: Extracted features directory '{EXTRACTED_FEATURES_DIR}' not found. Please ensure Google Drive is mounted and path is correct.\")\n",
        "    exit() # Exit if critical directory is not found.\n",
        "else:\n",
        "    print(f\"Extracted features directory found at {EXTRACTED_FEATURES_DIR}.\")\n",
        "\n",
        "# --- Part 1: Load and process 'airplane-landing.mp4' for initial observation and Feature Extraction ---\n",
        "print(f\"\\n--- Cell 2: Part 1 - Loading actual video '/content/airplane-landing.mp4' for Feature Extraction ---\")\n",
        "#flight_video_path = '/content/airplane-landing.mp4'\n",
        "flight_video_path = '/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4'\n",
        "\n",
        "# Use the defined load_and_process_video helper function. It now returns RAW V-JEPA output.\n",
        "video_features_for_inference_raw, frames_for_pldm_planning = load_and_process_video(flight_video_path, processor, model, device)\n",
        "\n",
        "# --- CRITICAL: Process raw V-JEPA features to match ClassifierHead's expected input (pooling to 1408) ---\n",
        "if video_features_for_inference_raw is not None:\n",
        "    # V-JEPA output shape is typically [1, 2048, 1408] (Batch, Channels, Sequence_Length)\n",
        "    # Your old code pooled it as .squeeze(0).mean(dim=0).unsqueeze(0), which results in [1, 1408]\n",
        "    # So, extracted_embedding_dim should be 1408 for the classifier.\n",
        "    pooled_features_for_classifier = video_features_for_inference_raw.squeeze(0).mean(dim=0).unsqueeze(0).to(device)\n",
        "    extracted_embedding_dim_for_classifier = pooled_features_for_classifier.shape[1] # This will be 1408\n",
        "    logging.info(f\"Dynamically determined extracted_embedding_dim for Classifier: {extracted_embedding_dim_for_classifier}\")\n",
        "else:\n",
        "    pooled_features_for_classifier = None\n",
        "    extracted_embedding_dim_for_classifier = -1\n",
        "    logging.error(\"Failed to extract video features for classifier. Exiting Cell 2.\")\n",
        "    exit() # Exit if critical features are not loaded\n",
        "\n",
        "# --- Part 2: Classifier Training ---\n",
        "print(f\"\\n--- Cell 2: Part 2 - Starting Classifier Training ---\")\n",
        "print(f\"Attempting to load real V-JEPA features for classifier training from: {EXTRACTED_FEATURES_DIR}\")\n",
        "\n",
        "print(f\"Using device for classifier training: {device}\") # 'device' is global from Cell 1\n",
        "\n",
        "try:\n",
        "    # Re-initialize classifier with the correct, dynamically determined input dimension (1408)\n",
        "    classifier = ClassifierHead(input_dim=extracted_embedding_dim_for_classifier, num_classes=num_classes).to(device)\n",
        "\n",
        "    train_features_list = []\n",
        "    train_labels_list = []\n",
        "\n",
        "    map_file_path = os.path.join(EXTRACTED_FEATURES_DIR, \"feature_label_map.json\")\n",
        "\n",
        "    if not os.path.exists(map_file_path):\n",
        "        logging.warning(f\"Feature-label map file '{map_file_path}' not found. Classifier will be trained on SYNTHETIC data as a fallback.\")\n",
        "        feature_label_map = {} # Treat as empty to trigger synthetic fallback\n",
        "    else:\n",
        "        with open(map_file_path, 'r') as f:\n",
        "            feature_label_map = json.load(f)\n",
        "\n",
        "    if not feature_label_map:\n",
        "        logging.warning(f\"Feature-label map at {map_file_path} is empty. Classifier will be trained on SYNTHETIC data as a fallback.\")\n",
        "        num_training_samples = 2_000_000\n",
        "        # Synthetic data generation uses the dynamically determined input_dim (1408)\n",
        "        train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier).to(device)\n",
        "        train_labels = torch.randint(0, num_classes, (num_training_samples,)).to(device)\n",
        "        train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=128, shuffle=True)\n",
        "        val_loader = None\n",
        "        print(f\"Loaded {num_training_samples} SYNTHETIC features for training.\")\n",
        "\n",
        "    else:\n",
        "        for item in tqdm(feature_label_map, desc=\"Loading real V-JEPA features\"):\n",
        "            feature_path = item['feature_path']\n",
        "            label_idx = item['label_idx']\n",
        "            try:\n",
        "                if not os.path.isabs(feature_path):\n",
        "                    feature_path = os.path.join(EXTRACTED_FEATURES_DIR, os.path.basename(feature_path))\n",
        "\n",
        "                if not os.path.exists(feature_path):\n",
        "                    logging.warning(f\"Feature file not found at {feature_path}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                feature = torch.load(feature_path, map_location=device)\n",
        "\n",
        "                # --- CRITICAL FIX: Match your working code's pooling/squeezing logic for saved features ---\n",
        "                # Your old working code used squeeze(0).mean(dim=0).unsqueeze(0) to get [1, 1408]\n",
        "                # We need to ensure loaded 'feature' from disk ends up as a 1D vector of 1408 elements.\n",
        "                if feature.ndim == 3: # Common V-JEPA output [1, 2048, 1408] or [Batch, Channels, SeqLen]\n",
        "                    feature = feature.squeeze(0).mean(dim=0) # -> [2048, 1408] -> mean(dim=0) -> [1408]\n",
        "                elif feature.ndim == 2: # Could be [2048, 1408] or [1, 1408] if already processed\n",
        "                    if feature.shape[0] == 1 and feature.shape[1] == 1408: # If [1, 1408]\n",
        "                        feature = feature.squeeze(0) # -> [1408]\n",
        "                    elif feature.shape[1] == 1408: # If [X, 1408], assume X is channels, pool them\n",
        "                        feature = feature.mean(dim=0) # -> [1408]\n",
        "                    else: # If unexpected 2D, try aggressive flatten (unlikely for pooled feature)\n",
        "                        feature = feature.flatten() # This might be wrong if feature is already very small\n",
        "                elif feature.ndim == 1: # Already [1408]\n",
        "                    pass\n",
        "                else:\n",
        "                    logging.warning(f\"Skipping malformed feature at {feature_path}. Unexpected ndim: {feature.ndim}. Got {feature.shape}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Final check after processing. Should be 1D with 1408 elements.\n",
        "                if feature.shape[0] != extracted_embedding_dim_for_classifier:\n",
        "                     logging.warning(f\"Skipping feature at {feature_path}. Expected dimension {extracted_embedding_dim_for_classifier}, but got {feature.shape[0]} after processing. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                train_features_list.append(feature)\n",
        "                train_labels_list.append(label_idx)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error loading feature from {feature_path}: {e}\")\n",
        "\n",
        "        if train_features_list:\n",
        "            train_features = torch.stack(train_features_list).to(device) # Stack into [N, 1408]\n",
        "            train_labels = torch.tensor(train_labels_list).to(device)\n",
        "            num_training_samples = len(train_features)\n",
        "            print(f\"Loaded {num_training_samples} REAL V-JEPA features for training.\")\n",
        "\n",
        "            if num_training_samples < 2:\n",
        "                print(\"WARNING: Only 1 real V-JEPA feature loaded. Training will be performed on this single sample (no train/val split).\")\n",
        "                train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=1, shuffle=False)\n",
        "                val_loader = None\n",
        "            else:\n",
        "                dataset_size = len(train_features)\n",
        "                train_size = int(0.8 * dataset_size)\n",
        "                val_size = dataset_size - train_size\n",
        "                if val_size == 0 and train_size > 0:\n",
        "                    train_size = dataset_size\n",
        "                    train_dataset_real = TensorDataset(train_features, train_labels)\n",
        "                    val_dataset_real = None\n",
        "                else:\n",
        "                    train_dataset_real, val_dataset_real = torch.utils.data.random_split(TensorDataset(train_features, train_labels), [train_size, val_size])\n",
        "\n",
        "                train_loader = DataLoader(train_dataset_real, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_dataset_real, batch_size=32, shuffle=False) if val_dataset_real else None\n",
        "\n",
        "                print(f\"Training on {len(train_dataset_real)} samples, Validating on {len(val_dataset_real)} samples.\" if val_loader else f\"Training on {len(train_dataset_real)} samples (No separate validation data).\")\n",
        "        else:\n",
        "            logging.error(\"No real V-JEPA features could be loaded from map. Falling back to synthetic data.\")\n",
        "            num_training_samples = 2_000_000\n",
        "            train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier).to(device)\n",
        "            train_labels = torch.randint(0, num_classes, (num_training_samples,)).to(device)\n",
        "            train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=128, shuffle=True)\n",
        "            val_loader = None\n",
        "            print(f\"Loaded {num_training_samples} SYNTHETIC features for training (due to real data load failure).\")\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 20\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer_classifier.zero_grad()\n",
        "            outputs = classifier(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_classifier.step()\n",
        "            running_loss += loss.item()\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        if val_loader and len(val_loader.dataset) > 0:\n",
        "            classifier.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = classifier(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "             logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f} (No validation data)\")\n",
        "\n",
        "    print(\"--- Classifier Training Complete ---\")\n",
        "\n",
        "    torch.save(classifier.state_dict(), CLASSIFIER_SAVE_PATH)\n",
        "    print(f\"Classifier saved to: {CLASSIFIER_SAVE_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error during classifier training: {e}\")"
      ],
      "metadata": {
        "id": "6rJahKPZGVSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 3: Classification Inference and Gemini LLM Interaction ---\n",
        "print(\"\\n--- Cell 2: Part 3 - Starting V-JEPA Feature-Driven Classification Inference and Gemini LLM Interaction ---\")\n",
        "\n",
        "if pooled_features_for_classifier is None: # Use pooled_features_for_classifier here\n",
        "    logging.error(\"ERROR: Cannot perform classifier inference as 'pooled_features_for_classifier' is None. Check video loading in Part 1.\")\n",
        "else:\n",
        "    try:\n",
        "        pooled_features_for_inference_on_device = pooled_features_for_classifier.to(device) # Ensure it's on device\n",
        "\n",
        "        # Load the just-trained classifier's weights (redundant but safe after saving)\n",
        "        classifier.load_state_dict(torch.load(CLASSIFIER_SAVE_PATH, map_location=device))\n",
        "        logging.info(f\"Classifier weights loaded from: {CLASSIFIER_SAVE_PATH}\")\n",
        "\n",
        "        classifier.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = classifier(pooled_features_for_inference_on_device)\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "            predicted_confidence = probabilities[0, predicted_class_idx].item()\n",
        "            predicted_label = CLASS_LABELS[predicted_class_idx]\n",
        "\n",
        "        llm_input_description = \"\"\n",
        "        if predicted_label == \"airplane landing\":\n",
        "            llm_input_description = \"The visual system detected an airplane landing. \"\n",
        "        elif predicted_label == \"airplane takeoff\":\n",
        "            llm_input_description = \"The visual system detected an airplane takeoff. \"\n",
        "        elif predicted_label == \"airport ground operations\":\n",
        "            llm_input_description = \"The visual system detected airport ground operations. \"\n",
        "        elif predicted_label == \"in-flight cruise\":\n",
        "            llm_input_description = \"The visual system detected an airplane in flight/cruise. \"\n",
        "        elif predicted_label == \"emergency landing\":\n",
        "            llm_input_description = \"The visual system detected a possible emergency landing scenario. \"\n",
        "        elif predicted_label == \"pre-flight check/maintenance\":\n",
        "            llm_input_description = \"The visual system detected pre-flight checks or maintenance activities. \"\n",
        "        else:\n",
        "            llm_input_description = \"The visual system detected an unrecognized or ambiguous aviation event. \"\n",
        "\n",
        "        llm_input_description += f\"(Confidence: {predicted_confidence:.2f})\"\n",
        "\n",
        "\n",
        "        print(f\"\\n--- AI Agent's Understanding from Classifier ---\")\n",
        "        print(f\"**Primary Classification (Predicted by AI):** '{predicted_label}' (Confidence: {predicted_confidence:.2f})\")\n",
        "        print(f\"**Description for LLM:** {llm_input_description}\")\n",
        "        print(f\"Note: This classification's accuracy depends heavily on the quality and size of the real dataset used for classifier training.\")\n",
        "\n",
        "        print(\"\\n--- Engaging Gemini LLM for Further Reasoning ---\")\n",
        "        try:\n",
        "            # You would likely want to re-instantiate LLM model here for self-containment\n",
        "            # if this cell might run independently, or ensure genai is configured from Cell 1.\n",
        "            llm_model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "\n",
        "            prompt_for_gemini = f\"\"\"\n",
        "            You are an AI assistant for flight planning operations.\n",
        "            Current visual observation: {llm_input_description}\n",
        "            Current time (EST): {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\n",
        "\n",
        "            Based on this visual observation, provide a concise operational assessment relevant for flight planning.\n",
        "            If the observation seems random or uncertain, state that. Do not invent details not present in the observation.\n",
        "            \"\"\"\n",
        "\n",
        "            gemini_response = llm_model.generate_content(prompt_for_gemini)\n",
        "\n",
        "            print(\"\\n--- Gemini LLM Response ---\")\n",
        "            if gemini_response.candidates:\n",
        "                for candidate in gemini_response.candidates:\n",
        "                    if candidate.content and candidate.content.parts:\n",
        "                        for part in candidate.content.parts:\n",
        "                            print(part.text)\n",
        "            else:\n",
        "                print(\"Gemini LLM did not provide a text response or candidates.\")\n",
        "                if gemini_response.prompt_feedback:\n",
        "                    print(f\"Prompt Feedback: {gemini_response.prompt_feedback}\")\n",
        "                if hasattr(gemini_response, 'error'):\n",
        "                    print(f\"LLM Error: {gemini_response.error}\")\n",
        "\n",
        "\n",
        "        except Exception as llm_e:\n",
        "            logging.error(f\"Error interacting with Gemini LLM: {llm_e}\")\n",
        "            logging.error(\"Ensure your GOOGLE_API_KEY is correctly set in Colab Secrets and the model name is valid.\")\n",
        "\n",
        "\n",
        "        print(f\"\\nThis prediction comes from a classifier that was trained on the provided V-JEPA features.\")\n",
        "        print(f\"For *truly accurate and high-confidence predictions* on real videos,\")\n",
        "        print(f\"the classifier needs to be trained on a large, diverse dataset of *real V-JEPA features and their corresponding labels*.\")\n",
        "        print(f\"The V-JEPA features (shape: {pooled_features_for_inference_on_device.shape}) are the core input that a trained classifier would learn from.\")\n",
        "        print(f\"Current time in EST: {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during classification inference or overall process: {e}\")\n",
        "\n",
        "print(\"\\nCell 2 execution complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "DDqDN64k_M07",
        "outputId": "1ee0d5a0-5256-4227-f3b0-2e2f7df79c96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cell 2: Part 3 - Starting V-JEPA Feature-Driven Classification Inference and Gemini LLM Interaction ---\n",
            "\n",
            "--- AI Agent's Understanding from Classifier ---\n",
            "**Primary Classification (Predicted by AI):** 'airplane landing' (Confidence: 1.00)\n",
            "**Description for LLM:** The visual system detected an airplane landing. (Confidence: 1.00)\n",
            "Note: This classification's accuracy depends heavily on the quality and size of the real dataset used for classifier training.\n",
            "\n",
            "--- Engaging Gemini LLM for Further Reasoning ---\n",
            "\n",
            "--- Gemini LLM Response ---\n",
            "Operational Assessment: An aircraft has successfully landed. This signifies current runway occupation transitioning to taxiway and gate utilization. This activity is a confirmed event impacting immediate airport resource management.\n",
            "\n",
            "This prediction comes from a classifier that was trained on the provided V-JEPA features.\n",
            "For *truly accurate and high-confidence predictions* on real videos,\n",
            "the classifier needs to be trained on a large, diverse dataset of *real V-JEPA features and their corresponding labels*.\n",
            "The V-JEPA features (shape: torch.Size([1, 1408])) are the core input that a trained classifier would learn from.\n",
            "Current time in EST: 2025-07-24 06:20:39 EST\n",
            "\n",
            "Cell 2 execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Conceptual PLDM Latent Dynamics Training"
      ],
      "metadata": {
        "id": "MzTyWmvvJuu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Conceptual PLDM Latent Dynamics Training\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "from tqdm.auto import tqdm # For progress bars\n",
        "\n",
        "# Re-Declare Configuration Variables and Global Objects needed in this cell\n",
        "# These must match the configuration from Cell 1 (for self-containment/robustness)\n",
        "EXTRACTED_FEATURES_DIR = \"/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/\" # [cite: 1001, 1305]\n",
        "TOTAL_FLATTENED_VJEPA_DIM = 2048 * 1408 # From Cell 1 [cite: 1003, 1305]\n",
        "latent_dim = TOTAL_FLATTENED_VJEPA_DIM # Ensure this matches Cell 1's definition [cite: 1305]\n",
        "action_dim = 8 # From Cell 1 [cite: 1004, 1305]\n",
        "\n",
        "# Determine device (re-determined for self-containment)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # [cite: 1067, 1306-1307]\n",
        "\n",
        "# Step 5.1: Data Loading for Latent Dynamics Model Training\n",
        "def load_flight_dynamics_training_data(video_features_for_inference_raw, frames_for_pldm_planning, device):\n",
        "    \"\"\"\n",
        "    Function to load offline reward-free flight data for Latent Dynamics.\n",
        "    This function will now use the V-JEPA features extracted in Cell 2.\n",
        "    It simulates (latent_s_t, a_t, latent_s_t_plus_1) triplets for training the dynamics model.\n",
        "    \"\"\"\n",
        "    print(f\"Loading flight dynamics data from Cell 2's extracted features.\") # [cite: 1312]\n",
        "\n",
        "    dynamics_training_data = [] # [cite: 1314]\n",
        "\n",
        "    if video_features_for_inference_raw is None or frames_for_pldm_planning is None:\n",
        "        logging.error(\"ERROR: V-JEPA features or frames from Cell 2 are not available for dynamics training.\")\n",
        "        return []\n",
        "\n",
        "    # CRITICAL: This is where you would derive s_t, a_t, s_t_plus_1\n",
        "    # from your available video features and, if applicable, associated actions.\n",
        "    # For a true dynamics model, you need sequences of (state_t, action_t, state_t+1).\n",
        "    # Since video_features_for_inference_raw is a single feature for one video,\n",
        "    # you'd ideally have pre-extracted sequences of V-JEPA features and corresponding actions.\n",
        "    # For this demo, we'll *simulate* a sequence from the available feature.\n",
        "    # In a real scenario, this would involve loading pre-segmented trajectories\n",
        "    # of (V-JEPA_feature_t, action_t, V-JEPA_feature_t+1).\n",
        "\n",
        "    # For demonstration, let's create a very simple \"sequence\" from the single pooled feature.\n",
        "    # This is a simplification and would need real sequential data for robust training.\n",
        "\n",
        "    # Ensure the feature is flattened to match TOTAL_FLATTENED_VJEPA_DIM\n",
        "    # The V-JEPA output shape is typically [1, 2048, 1408]\n",
        "    # For PLDM, it's flattened to 2048 * 1408\n",
        "    if video_features_for_inference_raw.ndim == 3:\n",
        "        # Assuming shape [1, Channels, Height * Width] or [1, C, H, W] that needs flattening\n",
        "        # The paper mentions [1, 2048, 1408] for V-JEPA, so a simple flatten might be needed.\n",
        "        # Ensure it's flattened to [1, TOTAL_FLATTENED_VJEPA_DIM]\n",
        "        current_latent_feature = video_features_for_inference_raw.flatten(start_dim=1).to(device)\n",
        "    elif video_features_for_inference_raw.ndim == 2:\n",
        "        current_latent_feature = video_features_for_inference_raw.to(device)\n",
        "    else:\n",
        "        logging.error(f\"Unexpected V-JEPA feature dimension: {video_features_for_inference_raw.shape}\")\n",
        "        return []\n",
        "\n",
        "    # Assuming we want to generate a small sequence from this one feature for the demo\n",
        "    num_simulated_steps = 10 # Simulate a short sequence for dynamics training\n",
        "    latent_vec_dim = current_latent_feature.shape[-1] # [cite: 1316]\n",
        "    action_vec_dim = action_dim # [cite: 1316]\n",
        "\n",
        "    for i in range(num_simulated_steps):\n",
        "        # s_t: Use the current latent feature\n",
        "        latent_s_t = current_latent_feature.clone()\n",
        "\n",
        "        # a_t: Generate a dummy action for this step (in a real scenario, this would be recorded actions)\n",
        "        action_t = torch.rand(1, action_vec_dim).to(device) # [cite: 1318]\n",
        "\n",
        "        # s_t_plus_1: Simulate the next latent state.\n",
        "        # In a real scenario, this would be the actual next V-JEPA feature from the trajectory.\n",
        "        # For this demo, let's just create a slightly perturbed version of current_latent_feature.\n",
        "        latent_s_t_plus_1 = current_latent_feature + (torch.randn(1, latent_vec_dim) * 0.01).to(device) # [cite: 1318]\n",
        "\n",
        "        dynamics_training_data.append((latent_s_t, action_t, latent_s_t_plus_1)) # [cite: 1318]\n",
        "\n",
        "        # Update current_latent_feature for the next iteration (simulating progression)\n",
        "        current_latent_feature = latent_s_t_plus_1\n",
        "\n",
        "    print(f\"Loaded {len(dynamics_training_data)} simulated dynamics training samples from Cell 2's features.\") # [cite: 1319]\n",
        "    return dynamics_training_data\n",
        "\n",
        "# Step 5.2: Training Loop for Latent Dynamics Model\n",
        "def train_latent_dynamics_model(predictor_model, optimizer, training_data, epochs=10): # [cite: 1321]\n",
        "    # ... (rest of your existing training loop from Cell 3)\n",
        "    predictor_model.train() # [cite: 1323]\n",
        "    print(\"\\n-- Training Latent Dynamics Predictor ---\") # [cite: 1324]\n",
        "    for epoch in range(epochs): # [cite: 1325]\n",
        "        total_loss = 0 # [cite: 1326]\n",
        "        for batch_idx, (latent_s_t, action_t, latent_s_t_plus_1) in tqdm(\n",
        "            enumerate(training_data),\n",
        "            total=len(training_data),\n",
        "            desc=f\"Epoch {epoch+1}/{epochs}\"\n",
        "        ): # [cite: 1335-1337]\n",
        "            # Move data to device (important for both training types)\n",
        "            latent_s_t, action_t, latent_s_t_plus_1 = latent_s_t.to(device), action_t.to(device), latent_s_t_plus_1.to(device) # [cite: 1338]\n",
        "\n",
        "            predicted_z_t_plus_1 = predictor_model(latent_s_t, action_t) # [cite: 1339]\n",
        "            loss = torch.nn.functional.mse_loss(predicted_z_t_plus_1, latent_s_t_plus_1) # [cite: 1340-1341]\n",
        "\n",
        "            # CRITICAL: Placeholder for full PLDM loss components\n",
        "            # You MUST implement these for robust dynamics learning, as per the paper [cite: 1342-1344]\n",
        "            # Example:\n",
        "            # L_var_val = calculate_L_var(predicted_z_t_plus_1) # You'd need to define calculate_L_var based on paper's L_var [cite: 195]\n",
        "            # L_cov_val = calculate_L_cov(predicted_z_t_plus_1) # You'd need to define calculate_L_cov based on paper's L_cov [cite: 195]\n",
        "            # L_IDM_val = calculate_L_IDM(latent_s_t, predicted_z_t_plus_1, action_t) # Based on paper's L_IDM [cite: 195]\n",
        "            # L_time_sim_val = calculate_L_time_sim(latent_s_t, predicted_z_t_plus_1) # Based on paper's L_time_sim [cite: 196]\n",
        "            # loss = loss + alpha * L_var_val + beta * L_cov_val + delta * L_IDM_val + omega * L_time_sim_val # Adjust coefficients as per paper [cite: 196, 578]\n",
        "\n",
        "            optimizer_pldm.zero_grad() # Use the PLDM specific optimizer [cite: 1345]\n",
        "            loss.backward() # [cite: 1345]\n",
        "            optimizer.step() # [cite: 1346]\n",
        "            total_loss += loss.item() # [cite: 1347]\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss / len(training_data)}\") # [cite: 1348]\n",
        "\n",
        "\n",
        "# -- Main Execution Flow for Cell 3\n",
        "print(\"\\n--Cell 3: Starting Conceptual PLDM Latent Dynamics Training ---\") # [cite: 1350]\n",
        "\n",
        "# Access variables from Cell 2. In a Colab notebook,\n",
        "# if Cell 2 was run, these would be in the global scope.\n",
        "# Ensure `video_features_for_inference_raw` and `frames_for_pldm_planning`\n",
        "# are indeed available here.\n",
        "# If not, you might need to re-extract them or save/load them explicitly.\n",
        "\n",
        "# Load reward-free offline flight data for dynamics model training\n",
        "# 'EXTRACTED_FEATURES_DIR' is from Cell 1. [cite: 1351]\n",
        "# Pass the features obtained in Cell 2\n",
        "dynamics_training_data = load_flight_dynamics_training_data(\n",
        "    video_features_for_inference_raw,\n",
        "    frames_for_pldm_planning,\n",
        "    device\n",
        ") # [cite: 1352]\n",
        "\n",
        "# Train the latent dynamics predictor. 'predictor' and 'optimizer_pldm' are global from Cell 1. [cite: 1353]\n",
        "if dynamics_training_data:\n",
        "    train_latent_dynamics_model(predictor, optimizer_pldm, dynamics_training_data) # [cite: 1353]\n",
        "else:\n",
        "    print(\"Skipping Latent Dynamics Training as no data was loaded.\")\n",
        "\n",
        "print(\"\\nCell 3 execution complete.\") # [cite: 1354]"
      ],
      "metadata": {
        "id": "jPXSu1GcJz7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Conceptual PLDM Planning"
      ],
      "metadata": {
        "id": "MBlwyf_aKCP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Conceptual PLDM Planning\n",
        "# This cell assumes Cell 1, Cell 2, and Cell 3 have been successfully executed in the current session.\n",
        "# All objects (model, processor, predictor, device, frames_for_pldm_planning, TOTAL_FLATTENED_VJEPA_DIM, action_dim)\n",
        "# are expected to be available from Cell 1, Cell 2, and Cell 3 execution.\n",
        "\n",
        "import logging\n",
        "import torch\n",
        "import numpy as np # For .cpu().numpy()\n",
        "import datetime\n",
        "import pytz # For time stamping\n",
        "from tqdm.auto import tqdm # For progress bars\n",
        "\n",
        "# --- Re-Declare Configuration Variables and Global Objects needed in this cell ---\n",
        "# These are re-declared for robustness in case of partial session state.\n",
        "# They should match the configuration from Cell 1.\n",
        "TOTAL_FLATTENED_VJEPA_DIM = 2048 * 1408\n",
        "latent_dim = TOTAL_FLATTENED_VJEPA_DIM\n",
        "action_dim = 8 # From Cell 1\n",
        "\n",
        "# Determine device (re-determined for self-containment)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Step 5.3: Conceptual Planning at Test Time (Mock MPPI Algorithm) ---\n",
        "def plan_flight_path(current_video_frames, target_goal_video_frames, encoder_model, predictor_model, processor_instance, device_instance, planning_horizon=10):\n",
        "    \"\"\"\n",
        "    Conceptual function to plan a flight path using the learned latent dynamics.\n",
        "    This is a mock MPPI implementation demonstrating the flow, not true optimality.\n",
        "    \"\"\"\n",
        "    encoder_model.eval()\n",
        "    predictor_model.eval()\n",
        "\n",
        "    current_inputs = processor_instance(videos=list(current_video_frames), return_tensors=\"pt\")\n",
        "    target_inputs = processor_instance(videos=list(target_goal_video_frames), return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Move inputs to device before model inference\n",
        "        current_inputs = {k: v.to(device_instance) for k, v in current_inputs.items()}\n",
        "        target_inputs = {k: v.to(device_instance) for k, v in target_inputs.items()}\n",
        "\n",
        "        # Encode and flatten features for planning\n",
        "        current_latent_state = encoder_model(**current_inputs).last_hidden_state.flatten(start_dim=1)\n",
        "        target_latent_state = encoder_model(**target_inputs).last_hidden_state.flatten(start_dim=1)\n",
        "\n",
        "    print(\"\\n--- Starting Flight Path Planning ---\")\n",
        "    print(f\"Current Latent State Shape: {current_latent_state.shape}\")\n",
        "    print(f\"Target Latent State Shape: {target_latent_state.shape}\")\n",
        "\n",
        "    best_action_sequence = []\n",
        "\n",
        "    # --- Mock MPPI-like Planning Logic ---\n",
        "    num_action_samples = 50 # Number of action sequences to sample at each step\n",
        "\n",
        "    for step in range(planning_horizon):\n",
        "        candidate_actions = torch.rand(num_action_samples, action_dim).to(device_instance) # Sample actions\n",
        "\n",
        "        simulated_trajectories_cost = []\n",
        "\n",
        "        for i in range(num_action_samples):\n",
        "            # Simulate one step in latent space using the predictor\n",
        "            simulated_next_latent = predictor_model(current_latent_state, candidate_actions[i].unsqueeze(0))\n",
        "\n",
        "            # Calculate mock cost: distance to goal + a mock uncertainty/deviation cost\n",
        "            # In real MPPI, cost would factor in predicted trajectory, not just one step\n",
        "            goal_cost = torch.norm(target_latent_state - simulated_next_latent) # Distance from goal\n",
        "            mock_uncertainty_cost = torch.rand(1).to(device_instance) * 0.1 # Dummy uncertainty cost\n",
        "\n",
        "            total_cost = goal_cost + mock_uncertainty_cost\n",
        "            simulated_trajectories_cost.append(total_cost)\n",
        "\n",
        "        # Select the action from the lowest-cost trajectory\n",
        "        best_candidate_idx = torch.argmin(torch.tensor(simulated_trajectories_cost))\n",
        "        optimal_action_for_step = candidate_actions[best_candidate_idx]\n",
        "\n",
        "        best_action_sequence.append(optimal_action_for_step.squeeze().cpu().numpy())\n",
        "\n",
        "        # Update current latent state based on the selected action (for receding horizon control)\n",
        "        with torch.no_grad():\n",
        "            current_latent_state = predictor_model(current_latent_state, optimal_action_for_step.unsqueeze(0)) # Advance state\n",
        "\n",
        "    print(f\"Conceptual Plan (first {planning_horizon} steps of actions): {best_action_sequence}\")\n",
        "    print(\"--- Planning Complete ---\")\n",
        "    return best_action_sequence\n",
        "\n",
        "# --- Main Execution Flow for Cell 4 ---\n",
        "print(\"\\n--- Cell 4: Starting Conceptual PLDM Planning ---\")\n",
        "\n",
        "# Access frames_for_pldm_planning (should be available from Cell 2 after video loading)\n",
        "if 'frames_for_pldm_planning' not in locals() or frames_for_pldm_planning is None:\n",
        "    logging.error(\"ERROR: 'frames_for_pldm_planning' not found or is None. Cannot perform PLDM planning without video frames from Cell 2. Exiting Cell 4.\")\n",
        "    exit()\n",
        "\n",
        "# Prepare dummy goal video for planning.\n",
        "target_goal_dummy_video = torch.rand(16, 3, 256, 256)\n",
        "\n",
        "# Plan the flight path using the trained dynamics model.\n",
        "# 'model', 'processor', 'predictor' are global from Cell 1.\n",
        "plan_flight_path(frames_for_pldm_planning, target_goal_dummy_video, model, predictor, processor, device, planning_horizon=5)\n",
        "\n",
        "print(f\"Current time in EST: {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\")\n",
        "\n",
        "print(\"\\nCell 4 execution complete.\")"
      ],
      "metadata": {
        "id": "JpdfHSY4KI9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d7ecbc-8603-400b-9863-ffc42fe86115"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cell 4: Starting Conceptual PLDM Planning ---\n",
            "\n",
            "--- Starting Flight Path Planning ---\n",
            "Current Latent State Shape: torch.Size([1, 2883584])\n",
            "Target Latent State Shape: torch.Size([1, 2883584])\n",
            "Conceptual Plan (first 5 steps of actions): [array([0.07907641, 0.61207354, 0.09017658, 0.5075899 , 0.26291585,\n",
            "       0.99078953, 0.2566988 , 0.52657586], dtype=float32), array([0.23659211, 0.53331727, 0.15826303, 0.21011299, 0.04287058,\n",
            "       0.32787013, 0.8282053 , 0.6433774 ], dtype=float32), array([0.20141774, 0.99435765, 0.56505376, 0.7739445 , 0.52645075,\n",
            "       0.78576654, 0.5571467 , 0.93925995], dtype=float32), array([0.79455715, 0.02142674, 0.7783281 , 0.07032961, 0.9969739 ,\n",
            "       0.03116769, 0.45894754, 0.73766863], dtype=float32), array([0.92344695, 0.8804767 , 0.44957083, 0.75902396, 0.06107652,\n",
            "       0.8700057 , 0.8042725 , 0.13213015], dtype=float32)]\n",
            "--- Planning Complete ---\n",
            "Current time in EST: 2025-07-24 06:26:24 EST\n",
            "\n",
            "Cell 4 execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Summary of Next Steps for a Practical Flight Planning AI ---\n",
        "1. **Crucial for Dynamics Training:** Replace `load_flight_dynamics_training_data` with detailed loading logic\n",
        "   to correctly parse your *actual* saved V-JEPA features and associated *actions* into (s_t, a_t, s_t_plus_1) triplets for the dynamics model.\n",
        "2. **Crucial for Classifier Accuracy:** Ensure `classifier_head_trained_on_tartan_aviation_sample.pth` exists and was trained on a large, diverse dataset of *real V-JEPA features and corresponding labels*.\n",
        "3. **Essential for Dynamics Robustness:** Refine `LatentDynamicsPredictor` architecture (e.g., use ConvPredictor if appropriate for visual features) and implement **full PLDM loss functions** (variance, covariance, inverse dynamics, time-smoothness) as detailed in the paper.\n",
        "4. **Essential for Action Generation:** Replace the mock planning in `plan_flight_path` with a robust MPPI algorithm implementation.\n",
        "5. **Integration:** Integrate this entire pipeline with a flight simulator or real-world sensor data for actual control and evaluation.\n",
        "Current time in EST: 2025-07-24 06:20:52 EST\n"
      ],
      "metadata": {
        "id": "TvfLIjbULwKy"
      }
    }
  ]
}
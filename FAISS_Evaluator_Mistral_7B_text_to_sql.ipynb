{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "cCGJhXUKlZTp",
        "bPcvPNghliYH",
        "7UPkiC-Yl38G",
        "U9F0m1TlmGWH",
        "s-obRbA0mSxQ"
      ],
      "authorship_tag": "ABX9TyP1fbgjEsNMlFyPUUEvfR20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c85ae5847a14688b11742ae6d66b1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_510be74dd2b542a2b08fbc2af9cda454",
              "IPY_MODEL_1800f68dd7ed4eee8e3d407c77c29135",
              "IPY_MODEL_f4fecac57c414097afaf0abc10090bdb"
            ],
            "layout": "IPY_MODEL_4f11b4221ef34aa8b55a17da10a17c1a"
          }
        },
        "510be74dd2b542a2b08fbc2af9cda454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cca15f3c7444298982db5b2553249dc",
            "placeholder": "​",
            "style": "IPY_MODEL_684a929b90b74bdea3e56af73153b8ed",
            "value": "Generating train split: "
          }
        },
        "1800f68dd7ed4eee8e3d407c77c29135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d23442ce28d14e0eac341ccd78051d3f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dd786d7fc214ca6b0b2797836346ebe",
            "value": 1
          }
        },
        "f4fecac57c414097afaf0abc10090bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_726f2ea2b5874c6a98be66d0d9e7913e",
            "placeholder": "​",
            "style": "IPY_MODEL_07fe7e9115ea4c90a567c22790e9ac6e",
            "value": " 2500/0 [00:00&lt;00:00, 107954.82 examples/s]"
          }
        },
        "4f11b4221ef34aa8b55a17da10a17c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cca15f3c7444298982db5b2553249dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "684a929b90b74bdea3e56af73153b8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d23442ce28d14e0eac341ccd78051d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0dd786d7fc214ca6b0b2797836346ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "726f2ea2b5874c6a98be66d0d9e7913e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07fe7e9115ea4c90a567c22790e9ac6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FAISS_Evaluator_Mistral_7B_text_to_sql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@frankmorales_91352/fine-tuning-the-llm-mistral-7b-instruct-v0-3-249c1814ceaf"
      ],
      "metadata": {
        "id": "a0h4qeKt2PsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "cCGJhXUKlZTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJi-Qn1p-po2",
        "outputId": "e0f72418-12ac-4b1a-afd6-fb5f5dfe75c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul  4 20:58:47 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FdHyfrhXutZ"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install mistral_inference -q\n",
        "\n",
        "!pip install peft -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Setup"
      ],
      "metadata": {
        "id": "bPcvPNghliYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")\n",
        "\n",
        "\n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login(write_permission=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivq2gu9-YD3I",
        "outputId": "9878771b-acac-42b6-e5e4-a27f9d866bd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Fine Tuned Model"
      ],
      "metadata": {
        "id": "7UPkiC-Yl38G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")"
      ],
      "metadata": {
        "id": "vfO7FZ2GYn3C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "#peft_model_id = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-dataeval\"\n",
        "\n",
        "peft_model_id  = 'frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-FAISS'\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    peft_model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "KuqnjucvY31a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboad Setup"
      ],
      "metadata": {
        "id": "U9F0m1TlmGWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env"
      ],
      "metadata": {
        "id": "vqjY6z8IZc6I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/gdrive/MyDrive/model/POC-Mistral-7B-text-to-sql-flash-attention-2-dataeval/logs"
      ],
      "metadata": {
        "id": "EF79dpClhkdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "##only in my personal dev-environment\n",
        "#%tensorboard --logdir /content/gdrive/MyDrive/model/Mistral-7B-text-to-sql-flash-attention-2-dataeval/logs\n",
        "\n",
        "%tensorboard --logdir /content/gdrive/MyDrive/model/Mistral-7B-text-to-sql-flash-attention-2-FAISS/logs\n",
        "\n"
      ],
      "metadata": {
        "id": "S-sML0q_lJGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Settings"
      ],
      "metadata": {
        "id": "s-obRbA0mSxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Convert dataset to OAI messages\n",
        "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
        "SCHEMA:\n",
        "{schema}\"\"\"\n",
        "\n",
        "def create_conversation(sample):\n",
        "  return {\n",
        "    \"messages\": [\n",
        "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
        "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
        "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
        "    ]\n",
        "  }\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "#dataset = dataset.shuffle().select(range(12500))\n",
        "dataset = dataset.select(range(12500))\n",
        "\n",
        "# Convert dataset to OAI messages\n",
        "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
        "\n",
        "# split dataset into 10,000 training samples and 2,500 test samples\n",
        "dataset = dataset.train_test_split(test_size=2500/12500)\n",
        "\n",
        "%cd /content/\n",
        "# save datasets to disk\n",
        "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
        "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
      ],
      "metadata": {
        "id": "mdb2mwS7c-ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "UMVmaasrfvul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8c85ae5847a14688b11742ae6d66b1f2",
            "510be74dd2b542a2b08fbc2af9cda454",
            "1800f68dd7ed4eee8e3d407c77c29135",
            "f4fecac57c414097afaf0abc10090bdb",
            "4f11b4221ef34aa8b55a17da10a17c1a",
            "9cca15f3c7444298982db5b2553249dc",
            "684a929b90b74bdea3e56af73153b8ed",
            "d23442ce28d14e0eac341ccd78051d3f",
            "0dd786d7fc214ca6b0b2797836346ebe",
            "726f2ea2b5874c6a98be66d0d9e7913e",
            "07fe7e9115ea4c90a567c22790e9ac6e"
          ]
        },
        "outputId": "040c41fe-4d4a-4121-930f-1057acbb5f16"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c85ae5847a14688b11742ae6d66b1f2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq9mpc--Y19Y",
        "outputId": "658d9df9-4107-45e7-e87c-24e1e5ea5259"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages'],\n",
              "    num_rows: 2500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation - Inference"
      ],
      "metadata": {
        "id": "qR7q5ST5muU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postgresql Settings"
      ],
      "metadata": {
        "id": "daZJRXxuMsQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 01/06/2024\n",
        "!apt-get update -y\n",
        "!apt-get install postgresql-14 -y\n",
        "\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "k-DA-VwDpQLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (proceed_to_quarter_final VARCHAR, eliminated_from_competition VARCHAR)\n",
        "QUERY_create='CREATE TABLE table_name_24 (score VARCHAR, date VARCHAR)'"
      ],
      "metadata": {
        "id": "h9rbH7SB6Sae"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PostGRES SQL Settings\n",
        "!sudo -u postgres psql -c \"CREATE USER postgres WITH SUPERUSER\"\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\""
      ],
      "metadata": {
        "id": "0sjsskj-61vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42342c5-1c95-4499-c1f1-33f0083b4048"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  role \"postgres\" already exists\n",
            "ALTER ROLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_select='SELECT 2009 FROM table_name_50 WHERE 2011 = \"a\"'"
      ],
      "metadata": {
        "id": "E0ekOeVc8AZS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\""
      ],
      "metadata": {
        "id": "aaoBQ65q6kWH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def table_creator(query):\n",
        "    import os\n",
        "    import psycopg2 as ps\n",
        "    import pandas as pd\n",
        "\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                  user=DB_USER,\n",
        "                  password=DB_PASS,\n",
        "                  host=DB_HOST,\n",
        "                  port=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Wrap the execute command in a try-except block to handle potential errors\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "                            %s\n",
        "                            \"\"\"%query)\n",
        "        conn.commit()\n",
        "        print(\"Table Created successfully\")\n",
        "    except Exception as e:\n",
        "        conn.rollback() # Rollback the transaction in case of an error\n",
        "        print(\"Error creating table:\", e)\n",
        "\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "UEU9tm1W6WOd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_creator(QUERY_create)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-Kb3okSPyn",
        "outputId": "e8febdfc-c068-4517-9a94-03002506e0aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table Created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "def table_select(query):\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                      user=DB_USER,\n",
        "                      password=DB_PASS,\n",
        "                      host=DB_HOST,\n",
        "                      port=DB_PORT)\n",
        "    #print(\"Database connected successfully\")\n",
        "\n",
        "    query = query.replace('\"', \"'\") # Replace double quotes with single quotes for potential date values\n",
        "\n",
        "    try:\n",
        "        #df = pd.read_sql_query(\"%s\"%query, con=conn)\n",
        "        print('rec: %'%df) # Print the resulting DataFrame\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(query)\n",
        "        results = cursor.fetchall()\n",
        "\n",
        "        for row in results:\n",
        "            print(row)\n",
        "\n",
        "            print()\n",
        "\n",
        "            # Commit the transaction to save the changes\n",
        "            conn.commit()\n",
        "            #print(\"QUERY successfully\")\n",
        "            print()\n",
        "\n",
        "            # Close the cursor and connection\n",
        "            cursor.close()\n",
        "            conn.close()\n",
        "    except Exception as e:\n",
        "        #conn.rollback() # Rollback the transaction in case of an error\n",
        "        #print(\"Error executing query:\", e)\n",
        "        print('TABLE IS EMPTY')\n",
        "\n",
        "\n",
        "        conn.close()\n",
        "    #return bad"
      ],
      "metadata": {
        "id": "fLBlG3lmTvgb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "# PostGRES SQL Settings\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "print('START: PG embedding COMPILATION')\n",
        "%cd /content/\n",
        "!git clone https://github.com/neondatabase/pg_embedding.git\n",
        "%cd /content/pg_embedding\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "#!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id integer PRIMARY KEY, embedding real[])\""
      ],
      "metadata": {
        "id": "qwkhZYHopR8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Settings"
      ],
      "metadata": {
        "id": "lCZstEQINH-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0][\"messages\"][1][\"content\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p6cWWnjunYw-",
        "outputId": "b4ae1aae-7d1e-4e8c-9f34-54f971293bbb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What December is 8.77 in January '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0][\"messages\"][2][\"content\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T3Sh1Er2nbJK",
        "outputId": "bc538371-0b34-40c4-feb8-5db9b452340f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT december FROM table_15945862_1 WHERE january = \"8.77\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0][\"messages\"][0]['content'][153:len(eval_dataset[0][\"messages\"][0]['content'])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uEtG6BvKnha-",
        "outputId": "24e1257d-b8a9-4d83-c970-852bd7e53279"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CREATE TABLE table_15945862_1 (december VARCHAR, january VARCHAR)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Inference"
      ],
      "metadata": {
        "id": "9vQZl05tNYqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "similar(\"Apple\",\"Appel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UJHFwwqgf9r",
        "outputId": "1d10be2b-e624-4f60-d23d-dc5762540f0f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from random import randint\n",
        "from datasets import load_dataset\n",
        "import psycopg2\n",
        "from psycopg2 import sql\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
        "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
        "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    schema=sample[\"messages\"][0]['content']\n",
        "    schema_query=schema[153:len(schema)]\n",
        "    question = sample[\"messages\"][1][\"content\"]\n",
        "    original_answer = sample[\"messages\"][2][\"content\"]\n",
        "\n",
        "    if predicted_answer ==  original_answer:\n",
        "        print()\n",
        "        print()\n",
        "        print('SUCCESS!')\n",
        "        print()\n",
        "        print(f'QUESTION: {question}')\n",
        "        print()\n",
        "        print(f'SCHEMA QUERY: {schema_query}')\n",
        "        table_creator(schema_query)\n",
        "        print()\n",
        "        print(f'Generated Answer: {predicted_answer}')\n",
        "        table_select(predicted_answer)\n",
        "        print()\n",
        "        print(f'Original Answer: {original_answer}')\n",
        "        print()\n",
        "        return 1\n",
        "    else:\n",
        "        print()\n",
        "        print()\n",
        "        print('NO - SUCCESS!')\n",
        "        print()\n",
        "\n",
        "        ps=similar(predicted_answer,original_answer)\n",
        "        print(f'Generated Answer: {predicted_answer}')\n",
        "        print(f' Original Answer: {original_answer}')\n",
        "        print(f'        SIMILARY: {ps}')\n",
        "        print()\n",
        "        return 0\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 10\n",
        "\n",
        "# iterate over eval dataset and predict\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "    s=eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "# compute accuracy\n",
        "accuracy = sum(success_rate)/len(success_rate)"
      ],
      "metadata": {
        "id": "g9Y0qGKoY-9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2386016a-55f9-4d12-f7f2-1ba4191467a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:05<00:45,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: What December is 8.77 in January \n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_15945862_1 (december VARCHAR, january VARCHAR)\n",
            "Table Created successfully\n",
            "\n",
            "Generated Answer: SELECT december FROM table_15945862_1 WHERE january = \"8.77\"\n",
            "TABLE IS EMPTY\n",
            "\n",
            "Original Answer: SELECT december FROM table_15945862_1 WHERE january = \"8.77\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:08<00:31,  3.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: Which major has most number of students?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE Student (major VARCHAR)\n",
            "Table Created successfully\n",
            "\n",
            "Generated Answer: SELECT major FROM Student GROUP BY major ORDER BY COUNT(*) DESC LIMIT 1\n",
            "TABLE IS EMPTY\n",
            "\n",
            "Original Answer: SELECT major FROM Student GROUP BY major ORDER BY COUNT(*) DESC LIMIT 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:11<00:24,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: What city was the Dick Weber Open in?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_name_22 (city VARCHAR, event VARCHAR)\n",
            "Table Created successfully\n",
            "\n",
            "Generated Answer: SELECT city FROM table_name_22 WHERE event = \"dick weber open\"\n",
            "TABLE IS EMPTY\n",
            "\n",
            "Original Answer: SELECT city FROM table_name_22 WHERE event = \"dick weber open\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:15<00:23,  3.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "Generated Answer: SELECT us_viewers__millions_ FROM table_22265225_1 WHERE no_in_season = 12\n",
            " Original Answer: SELECT us_viewers__millions_ FROM table_22265225_1 WHERE no_in_season = \"12\"\n",
            "        SIMILARY: 0.9866666666666667\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:19<00:18,  3.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: Name the player for pick number for 30\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_1965650_2 (player VARCHAR, pick__number VARCHAR)\n",
            "Table Created successfully\n",
            "\n",
            "Generated Answer: SELECT player FROM table_1965650_2 WHERE pick__number = 30\n",
            "TABLE IS EMPTY\n",
            "\n",
            "Original Answer: SELECT player FROM table_1965650_2 WHERE pick__number = 30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:24<00:16,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: What was the finishing time of the Stage that featured a distance of 24.00km and a start time of 21:27?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_name_78 (time VARCHAR, distance VARCHAR, start_time VARCHAR)\n",
            "Table Created successfully\n",
            "\n",
            "Generated Answer: SELECT time FROM table_name_78 WHERE distance = \"24.00km\" AND start_time = \"21:27\"\n",
            "TABLE IS EMPTY\n",
            "\n",
            "Original Answer: SELECT time FROM table_name_78 WHERE distance = \"24.00km\" AND start_time = \"21:27\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:31<00:15,  5.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "Generated Answer: SELECT COUNT(*) FROM CLASS AS T1 JOIN employee AS T2 ON T1.PROF_NUM = T2.EMP_NUM WHERE T2.EMP_LNAME = 'Graztevski'\n",
            " Original Answer: SELECT COUNT(*) FROM employee AS T1 JOIN CLASS AS T2 ON T1.EMP_NUM = T2.PROF_NUM WHERE T1.EMP_LNAME = 'Graztevski'\n",
            "        SIMILARY: 0.7982456140350878\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:36<00:10,  5.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "Generated Answer: SELECT t2.name FROM genres AS t1 JOIN tracks AS t2 ON t1.id = t2.genre_id WHERE t1.name = \"Rock\"\n",
            " Original Answer: SELECT T2.name FROM genres AS T1 JOIN tracks AS T2 ON T1.id = T2.genre_id WHERE T1.name = \"Rock\"\n",
            "        SIMILARY: 0.9375\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:40<00:04,  4.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: Which player won the Masters in 1976?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_name_66 (player VARCHAR, year_s__won VARCHAR)\n",
            "Table Created successfully\n",
            "\n",
            "Generated Answer: SELECT player FROM table_name_66 WHERE year_s__won = \"1976\"\n",
            "TABLE IS EMPTY\n",
            "\n",
            "Original Answer: SELECT player FROM table_name_66 WHERE year_s__won = \"1976\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:45<00:00,  4.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: Name the 3 where weightlifter is m. van der goten ( bel )\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_16779068_5 (weightlifter VARCHAR)\n",
            "Table Created successfully\n",
            "\n",
            "Generated Answer: SELECT 3 FROM table_16779068_5 WHERE weightlifter = \"M. Van der Goten ( BEL )\"\n",
            "TABLE IS EMPTY\n",
            "\n",
            "Original Answer: SELECT 3 FROM table_16779068_5 WHERE weightlifter = \"M. Van der Goten ( BEL )\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "#print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Accuracy (Eval dataset and predict) for a sample of {number_of_eval_samples}: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6ArykBRZGLp",
        "outputId": "e4473802-35ed-4e9d-f759-f47e1996f005"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy (Eval dataset and predict) for a sample of 10: 70.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation - Kernel"
      ],
      "metadata": {
        "id": "wEsmtqGJ1pxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Hidden Layers and Neurons (Before Evaluation)\n",
        "if hasattr(model, 'base_model'):\n",
        "    llama_model = model.base_model\n",
        "else:\n",
        "    llama_model = model\n",
        "\n",
        "# Count hidden layers of type LlamaDecoderLayer\n",
        "num_hidden_layers = llama_model.config.num_hidden_layers\n",
        "#print(num_hidden_layers)\n",
        "\n",
        "# Estimate neurons (this is very simplified, as explained earlier)\n",
        "num_neurons = num_hidden_layers * llama_model.config.hidden_size\n",
        "\n",
        "print(f\"Number of hidden layers in the model: {num_hidden_layers}\")\n",
        "print(f\"Approximate number of neurons (simplified): {num_neurons}\")"
      ],
      "metadata": {
        "id": "7kU-lpcA1i3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d77032-d273-4b30-c9ab-b382c134c610"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of hidden layers in the model: 32\n",
            "Approximate number of neurons (simplified): 131072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xl-M2zL02pnh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/test_dataset.json\", split=\"train\")\n",
        "reduced_size = 10\n",
        "eval_dataset = eval_dataset.shuffle(seed=42).select(range(reduced_size))"
      ],
      "metadata": {
        "id": "MdkVe49IBkni"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "id": "zrFcALOL3Y4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa37f371-a04b-4ad4-b973-15113a1c7cda"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0][\"messages\"][0]['content']"
      ],
      "metadata": {
        "id": "2iyxLiME4r9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ef8570a2-b104-4dfe-8200-d3b2321df732"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE TV_Channel (LANGUAGE VARCHAR)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 10\n",
        "all_input_ids = []\n",
        "all_attention_masks = []\n",
        "\n",
        "for item in eval_dataset:\n",
        "    messages = item['messages']\n",
        "\n",
        "    # Concatenate the 'content' of all messages into a single string\n",
        "    text = \" \".join([msg['content'] for msg in messages])\n",
        "    #print()\n",
        "    #print(text)\n",
        "    #print()\n",
        "\n",
        "    tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    all_input_ids.append(tokenized[\"input_ids\"][0])\n",
        "    all_attention_masks.append(tokenized[\"attention_mask\"][0])\n",
        "\n",
        "\n",
        "input_ids = torch.stack(all_input_ids)\n",
        "attention_masks = torch.stack(all_attention_masks)"
      ],
      "metadata": {
        "id": "84VruCFtLEXZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you have input_ids, attention_masks, and labels as tensors with compatible shapes\n",
        "print(input_ids.shape)\n",
        "print(attention_masks.shape)"
      ],
      "metadata": {
        "id": "-4eAUNAiLFrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf2590f-aa07-42d2-810a-1c0f22ce105f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wx1GS8ANZMLO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "meb7f_r6m0Bv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 8\n",
        "MAX_LENGTH = 10\n",
        "peft_model_id = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-FAISS\"\n",
        "data_files = \"/content/test_dataset.json\"\n",
        "reduced_size = 10\n",
        "\n",
        "# Load tokenizer (using tokenizer from the PEFT model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.3\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Resize the token embeddings to match the PEFT vocabulary\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Load PEFT model (using the base_model object)\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "model.eval()\n",
        "\n",
        "# Ensure model is on the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load your test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "eval_dataset = eval_dataset.shuffle(seed=42).select(range(reduced_size))\n",
        "\n",
        "# Tokenization and Tensor Creation\n",
        "all_input_ids = []\n",
        "all_attention_masks = []\n",
        "for item in eval_dataset:\n",
        "    messages = item['messages']\n",
        "    # Concatenate the 'content' of all messages into a single string\n",
        "    text = \" \".join([msg['content'] for msg in messages])\n",
        "\n",
        "    tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "    all_input_ids.append(tokenized[\"input_ids\"][0])\n",
        "    all_attention_masks.append(tokenized[\"attention_mask\"][0])\n",
        "\n",
        "input_ids = torch.stack(all_input_ids)\n",
        "attention_masks = torch.stack(all_attention_masks)\n",
        "\n",
        "# Create TensorDataset from your tensors\n",
        "eval_dataset = TensorDataset(input_ids, attention_masks)\n",
        "\n",
        "# Create DataLoader\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Evaluation function (Manually calculating perplexity)\n",
        "def evaluate_model(model, eval_dataloader):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        try:\n",
        "            # Convert batch to device (assuming it's a list/tuple of tensors)\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[0])  # Adjust based on your batch structure\n",
        "            loss = outputs.loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(\"WARNING: Ran out of memory. Consider reducing batch size or model complexity.\")\n",
        "                return None  # Exit early if out of memory\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    try:\n",
        "        perplexity = torch.exp(torch.tensor(losses).mean())\n",
        "        return perplexity\n",
        "    except OverflowError:\n",
        "        print(\"WARNING: Overflow error while calculating perplexity. Loss values might be too large.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Perform Evaluation\n",
        "results = evaluate_model(model, eval_dataloader)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "chWV971BnLqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Perplexity: {results:.2f}\")"
      ],
      "metadata": {
        "id": "oWgLgFuX0AzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395db024-03c4-44d2-dc2c-c9afdaf281f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 296.27\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPPhYFW3HQ5oy3hdwISD42O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Meta_Llama_3_8B_for_MEDAL_EVALUATOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meta_Llama_3_8B_for_MEDAL-EVALUATOR.ipynb"
      ],
      "metadata": {
        "id": "BtnQHkxH3CxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install peft --quiet\n",
        "\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "#Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "sJ5qguUtgRCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")"
      ],
      "metadata": {
        "id": "YnDGjScpg5CP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "44TvN5ChjSXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "9Mt3GNBahX6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "CjcHuj57hdz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "BMGlJx4jhSii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGkx4MLQf-1n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "#peft_model_id = \"/content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2\"\n",
        "\n",
        "peft_model_id = 'frankmorales2020/Meta-Llama-3-8B-MEDAL-flash-attention-2'\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "#/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "#nrec= randint(0, len(eval_dataset))\n",
        "\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "x3gkTVspiGzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "xGPDheryiElH",
        "outputId": "ea34f43f-0fcf-40d6-da4c-571f0ca2517a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the present study was designed to evaluate the usefulness of immunoblotting and IEM techniques for the diagnosis of human cystic echinococcosis ce in patients from a nonendemic area of spain where the infection is not notifiable serum samples from three groups of patients were analyzed a positive control group was composed of sera from patients with surgically proven ce from a endemic area of spain the second group consisted of sera from patients with surgically proven ce obtained from a nonendemic area of spain and the third group was formed by sera'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(ganswer,oanswer):\n",
        "\n",
        "  qc0=str(ganswer).find('[INST]')\n",
        "\n",
        "  ganswer=str(ganswer)[0:qc0-8]\n",
        "  qc=str(ganswer).find('[/INST]')\n",
        "  if qc>0:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "  print(f\"Generated Answer:\\n{ganswer}\")\n",
        "\n",
        "  if ganswer == oanswer:\n",
        "    print(\"Match\")\n",
        "    match=1\n",
        "  else:\n",
        "    print(\"NO Match\")\n",
        "    match=0\n",
        "  return match"
      ],
      "metadata": {
        "id": "U8lxIbH6r-q2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "\n",
        "match=similarity(ganswer,oanswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGLv4cBkiVRV",
        "outputId": "4960e00c-9427-4fd9-cb1d-621286e467f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "Match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG-Y_5PPiad6",
        "outputId": "4e702b05-f32d-42e3-9404-e4532aca0036"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "['antral follicle count']\n",
            "Generated Answer:\n",
            "[/INST] antral follicle count </s><s>[INST] the present study was designed to evaluate the usefulness of immunoblotting and IEM techniques for the diagnosis of human cystic echinococcosis ce in patients from a nonendemic area of spain where the infection is not notifiable serum samples from three groups of patients were analyzed a positive control group was composed of sera from patients with surgically proven ce from a endemic area of spain the second group consisted of sera from patients with surgically proven ce obtained from a nonendemic area of spain and the third group was formed by sera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "zRi13uPaibmQ",
        "outputId": "3ce1515c-603a-4826-ac4a-14aa592a9017"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the present study was designed to evaluate the usefulness of immunoblotting and IEM techniques for the diagnosis of human cystic echinococcosis ce in patients from a nonendemic area of spain where the infection is not notifiable serum samples from three groups of patients were analyzed a positive control group was composed of sera from patients with surgically proven ce from a endemic area of spain the second group consisted of sera from patients with surgically proven ce obtained from a nonendemic area of spain and the third group was formed by sera'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map"
      ],
      "metadata": {
        "id": "s_vzP4M-Hiac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "#prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "#outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "#                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "#                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt =  sample['text']\n",
        "    outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "    #print()\n",
        "    #print(ganswer)\n",
        "    #print(f\"Generated Answer original:\\n{ganswer}\")\n",
        "\n",
        "    # molecule [/INST] phosphorylethanolamine </s><s>[INST]\n",
        "    qc0=str(ganswer).find('[/INST]')\n",
        "    qc1=str(ganswer).find('[INST]')\n",
        "\n",
        "    ganswer=str(ganswer)[qc0+8:qc1-8]\n",
        "    #print(f\"Generated Answer corr:\\n{ganswer}\")\n",
        "\n",
        "    oanswer=str(sample['label'])\n",
        "    oanswer=oanswer[2:len(oanswer)-2]\n",
        "\n",
        "    #print(f\"Original Answer:\\n{oanswer}\")\n",
        "    if ganswer == oanswer:\n",
        "        #print()\n",
        "        #print(\"Match\")\n",
        "        #print(f\"Query:\\n{sample['text']}\")\n",
        "        #print(f\"Original Answer:\\n{oanswer}\")\n",
        "        #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        return 1\n",
        "    else:\n",
        "        #print()\n",
        "        #print(\"no Match\")\n",
        "        #print(f\"Query:\\n{sample['text']}\")\n",
        "        #print(f\"Original Answer:\\n{oanswer}\")\n",
        "        #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "#### exceptions TODO\n",
        "#Original Answer:\n",
        "#enteropathogenic\n",
        "#Generated Answer:\n",
        "#enteropathogenic e coli\n",
        "\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 1000\n",
        "# iterate over eval dataset and predict\n",
        "\n",
        "#for n in tqdm(range(number_of_eval_samples)):\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "#    print(f'sample {n}')\n",
        "    s=eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "#for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
        "#    success_rate.append(evaluate(s))\n",
        "\n",
        "# compute accuracy\n",
        "accuracy = sum(success_rate)/len(success_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWw-yHYilDd",
        "outputId": "6ea4da82-922f-4fb5-99fb-3b0fa4a6bbbc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/1000 [01:40<2:43:55,  9.93s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 1000/1000 [2:48:24<00:00, 10.10s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "print(f\"Accuracy (Eval dataset and predict) for a sample of {number_of_eval_samples}: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiNueGHqFQIv",
        "outputId": "7e238753-07f7-4ee2-f722-802033dcb1c7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Eval dataset and predict) for a sample of 1000: 26.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluated on 1000 samples from the evaluation dataset, our model achieved an impressive accuracy of 1000: 26.30%. However, there's room for improvement."
      ],
      "metadata": {
        "id": "NYZDOY-0zp5d"
      }
    }
  ]
}
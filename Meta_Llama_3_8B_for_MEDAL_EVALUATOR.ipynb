{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMH9WDuhJCDHma2eB0SrrPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Meta_Llama_3_8B_for_MEDAL_EVALUATOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install peft --quiet\n",
        "\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "#Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n"
      ],
      "metadata": {
        "id": "sJ5qguUtgRCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")"
      ],
      "metadata": {
        "id": "YnDGjScpg5CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet"
      ],
      "metadata": {
        "id": "44TvN5ChjSXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "9Mt3GNBahX6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "CjcHuj57hdz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "BMGlJx4jhSii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGkx4MLQf-1n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"/content/gdrive/MyDrive/model/Meta-Llama-3-8B-MEDAL-flash-attention-2\"\n",
        "\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "#/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "#nrec= randint(0, len(eval_dataset))\n",
        "\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "x3gkTVspiGzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "xGPDheryiElH",
        "outputId": "62bcda16-8e1d-4cba-d523-006729ea9c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the present study was aimed at evaluating the effects of alcoholic extracts of salvia officinalis and thymus vulgaris on the spontaneous alternation behavior in the ymaze and the crossedarm score as a measure of motor asymmetry in the rat both sage and thymus extracts mgkg po increased the percentage of alternation in the ymaze however only sage extract significantly decreased the crossedarm score thus the anxiolyticlike effect of the studied extracts seems to be related to the improvement of natural alternation behavior rather than to a motor impairment [/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(ganswer,oanswer):\n",
        "\n",
        "  qc0=str(ganswer).find('[INST]')\n",
        "\n",
        "  ganswer=str(ganswer)[0:qc0-8]\n",
        "  qc=str(ganswer).find('[/INST]')\n",
        "  if qc>0:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "  print(f\"Generated Answer:\\n{ganswer}\")\n",
        "\n",
        "  if ganswer == oanswer:\n",
        "    print(\"Match\")\n",
        "    match=1\n",
        "  else:\n",
        "    print(\"NO Match\")\n",
        "    match=0\n",
        "  return match"
      ],
      "metadata": {
        "id": "U8lxIbH6r-q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "\n",
        "match=similarity(ganswer,oanswer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGLv4cBkiVRV",
        "outputId": "ab28de6e-2f76-484b-cee8-af25a1453b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "Match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG-Y_5PPiad6",
        "outputId": "96e1a5f8-4dc1-42df-967b-0a21fa2cb8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "Original Answer:\n",
            "['antral follicle count']\n",
            "Generated Answer:\n",
            "[/INST] antral follicle count </s><s>[INST] the present study was aimed at evaluating the effects of alcoholic extracts of salvia officinalis and thymus vulgaris on the spontaneous alternation behavior in the ymaze and the crossedarm score as a measure of motor asymmetry in the rat both sage and thymus extracts mgkg po increased the percentage of alternation in the ymaze however only sage extract significantly decreased the crossedarm score thus the anxiolyticlike effect of the studied extracts seems to be related to the improvement of natural alternation behavior rather than to a motor impairment [/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc+8:len(ganswer)]\n",
        "ganswer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "zRi13uPaibmQ",
        "outputId": "ca980173-542a-476a-9a66-c68b3e15c822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the present study was aimed at evaluating the effects of alcoholic extracts of salvia officinalis and thymus vulgaris on the spontaneous alternation behavior in the ymaze and the crossedarm score as a measure of motor asymmetry in the rat both sage and thymus extracts mgkg po increased the percentage of alternation in the ymaze however only sage extract significantly decreased the crossedarm score thus the anxiolyticlike effect of the studied extracts seems to be related to the improvement of natural alternation behavior rather than to a motor impairment [/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_vzP4M-Hiac",
        "outputId": "57942289-3860-4864-ab0c-70eba22f9c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "#prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "#outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "#                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "#                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt =  sample['text']\n",
        "    outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "    #print()\n",
        "    #print(ganswer)\n",
        "    #print(f\"Generated Answer original:\\n{ganswer}\")\n",
        "    # molecule [/INST] phosphorylethanolamine </s><s>[INST]\n",
        "\n",
        "    qc0=str(ganswer).find('[/INST]')\n",
        "\n",
        "    qc1=str(ganswer).find('[INST]')\n",
        "\n",
        "    ganswer=str(ganswer)[qc0+8:qc1-8]\n",
        "    #print(f\"Generated Answer corr:\\n{ganswer}\")\n",
        "\n",
        "    oanswer=str(sample['label'])\n",
        "    oanswer=oanswer[2:len(oanswer)-2]\n",
        "\n",
        "    #print(f\"Original Answer:\\n{oanswer}\")\n",
        "    if ganswer == oanswer:\n",
        "        #print()\n",
        "        #print(\"Match\")\n",
        "        #print(f\"Query:\\n{sample['text']}\")\n",
        "        #print(f\"Original Answer:\\n{oanswer}\")\n",
        "        #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        return 1\n",
        "    else:\n",
        "        #print()\n",
        "        #print(\"no Match\")\n",
        "        #print(f\"Query:\\n{sample['text']}\")\n",
        "        #print(f\"Original Answer:\\n{oanswer}\")\n",
        "        #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 10\n",
        "# iterate over eval dataset and predict\n",
        "\n",
        "#for n in tqdm(range(number_of_eval_samples)):\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "#    print(f'sample {n}')\n",
        "    s=eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "#for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
        "#    success_rate.append(evaluate(s))\n",
        "\n",
        "# compute accuracy\n",
        "accuracy = sum(success_rate)/len(success_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWw-yHYilDd",
        "outputId": "8602fb9d-9175-4846-e2c7-cb5b8840df3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [00:40<00:59, 10.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [01:00<00:39, 10.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:10<00:30, 10.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:20<00:20, 10.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:40<00:00, 10.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "print(f\"Accuracy (Eval dataset and predict) for a sample of {number_of_eval_samples}: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiNueGHqFQIv",
        "outputId": "ce5c18cc-1fea-4148-fbb7-8b193cdff2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Eval dataset and predict) for a sample of 10: 50.00%\n"
          ]
        }
      ]
    }
  ]
}
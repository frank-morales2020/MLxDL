{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FINAL_GNN_FINE_TUNINGT_T2SQL_DataAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC7dvZByG6uM"
      },
      "source": [
        "## Libraries Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9cqNuPkh_aG"
      },
      "outputs": [],
      "source": [
        "!pip install datasets networkx -q\n",
        "\n",
        "!pip install torch_geometric -q\n",
        "\n",
        "\n",
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "!pip install peft --quiet\n",
        "!pip install trl ninja packaging --quiet\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install mistral_inference -q\n",
        "\n",
        "!pip install trl==0.8.6 -q\n",
        "\n",
        "\n",
        "!pip install sqlparse -q\n",
        "\n",
        "!pip install bitsandbytes -q\n",
        "\n",
        "#!pip uninstall -y torchvision -q\n",
        "!pip install torchvision --no-cache-dir -q\n",
        "#import evaluate\n",
        "\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "!pip install nlpaug -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ak2KGyrYOro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "from trl import setup_chat_format\n",
        "\n",
        "import colab_env\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model and tokenizer\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ],
      "metadata": {
        "id": "-pGJR0chK0dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsx_NaeSTP2j"
      },
      "source": [
        "## TRAININING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0DANV3Esq5w"
      },
      "source": [
        "https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Mistral_7B_Instruct_v0_1_for_text_to_SQL_EVALDATA.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsfmK0vECZ3C"
      },
      "source": [
        "* Import Main Components"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XPoZvJrBzyG",
        "outputId": "1147d2bc-507f-4d44-e0a8-048a881e7db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 3076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/train_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "p2XhxxLrsHB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][0]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "VOG6LSjFL9Ng",
        "outputId": "1e323431-5096-4c00-a490-954638a5957a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_92 (total VARCHAR, finish VARCHAR)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][1]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w7PmSu5hMEwE",
        "outputId": "161bb554-1a20-4566-d4cc-02f13ed4018d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How many times was the finish t32?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][2]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zO_YHBbrtmrV",
        "outputId": "031662df-17d9-42d0-9b45-88fa45a8274e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT COUNT(total) FROM table_name_92 WHERE finish = \"t32\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0DfyorMy4TK",
        "outputId": "21851578-5919-401f-90a6-c6f2eac3925f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr9FHlz8EEds"
      },
      "source": [
        "* https://stackoverflow.com/questions/70950706/assertionerror-in-torch-geometric-nn-gatconv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Data Input and Preparation:**\n",
        "   - The process begins by loading the \"sql-create-context\" dataset, which presumably contains pairs of natural language questions and their corresponding SQL queries.\n",
        "   - The dataset is divided into three distinct subsets: training, validation, and testing.\n",
        "   - The Mistral-7B-Instruct-v0.3 language model and its tokenizer are loaded and prepared.\n",
        "\n",
        "2. **Data Transformation with `TextToSQLDataset`:**\n",
        "   - The `TextToSQLDataset` class is responsible for converting raw data into a format suitable for model training and evaluation.\n",
        "   - For each data sample, the following transformations occur:\n",
        "      - Tokenization: The input question and the SQL query (answer) are tokenized using the loaded tokenizer.\n",
        "      - Dependency Parsing: The question is parsed using spaCy to extract the grammatical relationships between words, generating a dependency graph.\n",
        "      - Dictionary Creation: A dictionary is created to store the tokenized input IDs, attention masks, labels (tokenized SQL query), and the dependency edges extracted from parsing.\n",
        "\n",
        "3. **Batching and Shuffling with `DataLoader`:**\n",
        "   - `DataLoader` takes the processed dataset from `TextToSQLDataset` and creates an iterable object for efficient batching.\n",
        "   - Optionally, shuffling is applied to randomize the order of samples within each epoch during training.\n",
        "\n",
        "4. **Forward Pass through `GraphModel`:**\n",
        "   - **Mistral Encoder:** The tokenized input IDs and attention masks are fed into the Mistral model's encoder to obtain contextualized token embeddings.\n",
        "   - **GATv2 Layer:** The GATv2 layer (Graph Attention Network) takes the token embeddings and the dependency edges as input. It applies graph attention mechanisms to incorporate the structural information from the dependency graph into the token representations.\n",
        "   - **Pooling:** The node representations (output of GATv2) are aggregated using a pooling operation (e.g., mean pooling) to obtain a fixed-size representation of the entire input sequence.\n",
        "   - **LM Head:** The pooled representation is passed through a linear layer, which produces logits â€“ unnormalized probabilities for each token in the vocabulary.\n",
        "   - **Loss Calculation:** During training, if labels (correct SQL queries) are available, the cross-entropy loss is calculated between the predicted logits and the true labels. This loss guides the optimization of the model's parameters.\n",
        "\n",
        "5. **Model Optimization with PEFT (LoRA):**\n",
        "   - The `GraphModel` is wrapped with PEFT's LoRA (Low-Rank Adaptation) configuration to enable parameter-efficient fine-tuning.\n",
        "   - During training, only the parameters of the GATv2 layer and the LM head are updated, while the rest of the model parameters remain frozen.\n",
        "   - The Hugging Face Trainer manages the training process, iterating over the dataset, computing gradients, and updating the model's parameters based on the calculated loss.\n",
        "\n",
        "6. **Evaluation:**\n",
        "   - After (or during) training, the model is evaluated on the validation and test sets.\n",
        "   - The `compute_metrics` function decodes the predicted logits and labels back into text and assesses the model's performance using two metrics:\n",
        "      - Semantic Similarity: This metric measures how semantically close the predicted SQL query is to the reference SQL query using SentenceTransformer embeddings.\n",
        "      - Exact Match: This metric checks if the predicted SQL query matches the reference SQL query exactly.\n",
        "\n",
        "7. **Output:**\n",
        "    - The final output is the evaluation results, including semantic similarity and exact match scores, which provide insights into the model's ability to generate accurate SQL queries from natural language questions.\n"
      ],
      "metadata": {
        "id": "QFVMbILY2AeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corrected graphical representation of the dataflow, with the arrow pointing *into* the `Trainer`:\n",
        "\n",
        "```\n",
        "+-------------------+          +----------------------+\n",
        "| Dataset Loading   |          | TextToSQLDataset     |\n",
        "|  - sql-create...  | -------> | - Tokenization       |\n",
        "|  - Split: train...|          | - Dependency Parsing |\n",
        "+-------------------+          | - Dict Creation      |\n",
        "                               +----------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                         +---------------------------+\n",
        "                         | DataLoader                |\n",
        "                         | - Batches, Shuffling (opt)|\n",
        "                         +---------------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                         +---------------------------+\n",
        "                         | GraphModel                |\n",
        "                         | - Mistral Encoder         |\n",
        "                         | - GATv2 Layer             |\n",
        "                         | - Pooling                 |\n",
        "                         | - LM Head                 |\n",
        "                         | - (Loss Calculation)      |\n",
        "                         +---------------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "            +--------------+          +---------+         +-----------------+\n",
        "            | PEFT (LoRA)  | -------> | Trainer | <------ |Evaluation       |\n",
        "            +--------------+          |         |         |(compute_metrics)|\n",
        "                                      +---------+         +-----------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                                   +-----------------------+\n",
        "                                   | - Semantic Similarity |\n",
        "                                   | - Exact Match         |\n",
        "                                   +-----------------------+\n",
        "```\n",
        "\n",
        "The revised dataflow now accurately shows that the evaluation metrics (semantic similarity and exact match) calculated by the `compute_metrics` function are used by the `Trainer` to assess the model's performance and make decisions during training (e.g., early stopping).\n",
        "\n"
      ],
      "metadata": {
        "id": "cMZjJHj12Dnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "import shutil\n",
        "\n",
        "# Get the cache directory information\n",
        "cache_info = huggingface_hub.utils.scan_cache_dir()\n",
        "\n",
        "# Iterate through the repositories and delete them\n",
        "for repo_info in cache_info.repos:\n",
        "    repo_path = repo_info.repo_path\n",
        "    shutil.rmtree(repo_path)  # Delete the repository folder"
      ],
      "metadata": {
        "id": "IJj8txi-BLdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6horVfRxXtU8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, Trainer, TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    PrefixTuningConfig,\n",
        "    PromptEncoderConfig,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch_geometric.nn import GAT\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings(\"ignore\", message=\"The installed version of bitsandbytes was compiled without GPU support.\")\n",
        "\n",
        "# 1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# Manually define splits\n",
        "train_size = int(0.7 * len(dataset))\n",
        "eval_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - eval_size\n",
        "\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
        "test_dataset = dataset.select(range(train_size + eval_size, len(dataset)))\n",
        "\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3a. PyTorch Datasets - Data Augmentation\n",
        "import random\n",
        "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
        "\n",
        "def random_insertion(sentence, aug_p=0.1, synonym_aug=None, max_attempts=3):  # Add max_attempts\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_words.append(word)\n",
        "        if random.random() < aug_p:\n",
        "            attempts = 0\n",
        "            while attempts < max_attempts:  # Try multiple times to find a synonym\n",
        "                if new_words:\n",
        "                    candidate_words = [new_words[-1]]\n",
        "                if len(new_words) < len(words):\n",
        "                    candidate_words.append(words[len(new_words)])\n",
        "                if candidate_words and synonym_aug:\n",
        "                    synonym = synonym_aug.augment(random.choice(candidate_words))\n",
        "                    if synonym:\n",
        "                        new_words.append(synonym[0])\n",
        "                        break  # Exit the loop if a synonym is found\n",
        "                attempts += 1\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def augment_data(dataset):\n",
        "    augmented_data = []\n",
        "\n",
        "    synonym_aug = SynonymAug(aug_src='wordnet')\n",
        "    delete_aug = RandomWordAug(action=\"delete\", aug_p=0.05)\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        context = item['context']\n",
        "\n",
        "        augmented_questions = set()\n",
        "        augmented_questions.add(question)  # Always include the original question\n",
        "\n",
        "        # Attempt to generate one more unique augmented question\n",
        "        while len(augmented_questions) < 2:  # Keep trying until we have 2 unique questions\n",
        "            augmentation_method = random.choice(['synonym', 'insertion', 'deletion'])\n",
        "\n",
        "            if augmentation_method == 'synonym':\n",
        "                synonyms = synonym_aug.augment(question)\n",
        "                if synonyms and synonyms[0] != question:\n",
        "                    augmented_questions.add(synonyms[0])\n",
        "\n",
        "            elif augmentation_method == 'insertion':\n",
        "                augmented_question = random_insertion(question, synonym_aug=synonym_aug)\n",
        "                if augmented_question != question:\n",
        "                    augmented_questions.add(augmented_question)\n",
        "\n",
        "            else:  # deletion\n",
        "                deleted = delete_aug.augment(question)\n",
        "                if deleted and deleted[0] != question:\n",
        "                    augmented_questions.add(deleted[0])\n",
        "\n",
        "        # Add the augmented examples to the dataset\n",
        "        for aug_question in list(augmented_questions):\n",
        "            augmented_data.append({'question': aug_question, 'answer': answer, 'context': context})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "# 3. PyTorch Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English model with SRL capabilities\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "except OSError:\n",
        "    spacy.cli.download(\"en_core_web_trf\")\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "        # 1. Tokenization\n",
        "        tokenized_input = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized_target = self.tokenizer(\n",
        "            target_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Flatten lists\n",
        "        tokenized_input = {k: v.squeeze(0) for k, v in tokenized_input.items()}\n",
        "        tokenized_target = {k: v.squeeze(0) for k, v in tokenized_target.items()}\n",
        "\n",
        "        # 2. Dependency Parsing & SRL for Edge Extraction\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        edge_attrs = []\n",
        "        for token in doc:\n",
        "            # Dependency Parsing\n",
        "            if token.dep_ != \"ROOT\" and token.i != token.head.i:\n",
        "                edges.append([token.i, token.head.i])\n",
        "                edge_attrs.append(self.tokenizer.vocab.get(token.dep_, self.tokenizer.unk_token_id))\n",
        "\n",
        "            # SRL\n",
        "            if token.dep_ in {\"nsubj\", \"dobj\", \"nsubjpass\"}:\n",
        "                for child in token.children:\n",
        "                    if child.dep_ == \"prep\":\n",
        "                        for grandchild in child.children:\n",
        "                            if grandchild.dep_ in {\"pobj\", \"pcomp\"}:\n",
        "                                edges.append([token.i, grandchild.i])\n",
        "                                edge_attrs.append(self.tokenizer.vocab.get(\"prep_\" + grandchild.dep_, self.tokenizer.unk_token_id))\n",
        "\n",
        "        # Edge Index Extraction and Validation\n",
        "        if not edges:\n",
        "            num_nodes = len(tokenized_input[\"input_ids\"])\n",
        "            edges = [[i, i] for i in range(num_nodes)]\n",
        "        else:\n",
        "            max_index = len(tokenized_input[\"input_ids\"]) - 1\n",
        "            edges = [(src, tgt) for src, tgt in edges\n",
        "                     if 0 <= src <= max_index and 0 <= tgt <= max_index]\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_attrs = torch.tensor(edge_attrs, dtype=torch.long)\n",
        "\n",
        "        # 3. Node Features with POS Tags\n",
        "        pos_tags = [token.pos_ for token in doc]\n",
        "        pos_tag_ids = [self.tokenizer.vocab.get(tag, self.tokenizer.unk_token_id) for tag in pos_tags]\n",
        "        pos_tag_ids = torch.tensor(pos_tag_ids, dtype=torch.long)\n",
        "\n",
        "        # Convert everything to tensors BEFORE padding/truncation\n",
        "        input_ids = tokenized_input[\"input_ids\"].clone().detach()\n",
        "        attention_mask = tokenized_input[\"attention_mask\"].clone().detach()\n",
        "        labels = tokenized_target[\"input_ids\"].clone().detach()\n",
        "\n",
        "        # Handle potentially empty target sequences\n",
        "        if len(labels) == 0:\n",
        "            labels = torch.tensor([self.tokenizer.pad_token_id], dtype=torch.long)\n",
        "\n",
        "        # Padding and Truncation\n",
        "        max_length = 1024\n",
        "\n",
        "        input_ids = input_ids[:max_length]\n",
        "        attention_mask = attention_mask[:max_length]\n",
        "        labels = labels[:max_length]\n",
        "        pos_tag_ids = pos_tag_ids[:max_length]\n",
        "\n",
        "        if len(input_ids) < max_length:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            pad_tensor = torch.full((pad_length,), self.tokenizer.pad_token_id)\n",
        "            input_ids = torch.cat((input_ids, pad_tensor))\n",
        "            attention_mask = torch.cat((attention_mask, torch.zeros(pad_length, dtype=torch.long)))\n",
        "            pos_tag_ids = torch.cat((pos_tag_ids, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        if len(labels) < max_length:\n",
        "            pad_length = max_length - len(labels)\n",
        "            labels = torch.cat((labels, torch.full((pad_length,), -100)))\n",
        "\n",
        "        if len(edge_attrs) < edge_index.size(1):\n",
        "            pad_length = edge_index.size(1) - len(edge_attrs)\n",
        "            edge_attrs = torch.cat((edge_attrs, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edge_index,\n",
        "            \"edge_attrs\": edge_attrs,\n",
        "            \"pos_tag_ids\": pos_tag_ids,\n",
        "            \"sample_ids\": torch.tensor([idx])\n",
        "        }\n",
        "\n",
        "#Minimum: Start with at least 2000-3000 samples.\n",
        "#This should be enough to provide a good initial assessment of your model's performance and potential.\n",
        "\n",
        "#Medium: If your computational resources allow, try 5000-7000 samples.\n",
        "#This could provide a more robust evaluation and potentially lead to better performance.\n",
        "\n",
        "#Maximum: If you have ample resources, consider using the entire dataset (around 10,000 samples).\n",
        "#This would give you the most comprehensive training data possible and potentially lead\n",
        "#to the best model performance.\n",
        "\n",
        "#Reduce train_dataset size for POC\n",
        "#POC_sample=26000\n",
        "\n",
        "POC_sample=16000\n",
        "import numpy as np\n",
        "train_dataset = train_dataset.select(np.random.choice(len(train_dataset), POC_sample, replace=False))\n",
        "\n",
        "### data augmentation #######\n",
        "train_dataset_augmented = augment_data(train_dataset)\n",
        "train_dataset = TextToSQLDataset(train_dataset_augmented, tokenizer)\n",
        "#############################\n",
        "\n",
        "\n",
        "POC_valsample=1\n",
        "#############################\n",
        "eval_dataset = eval_dataset.select(np.random.choice(len(eval_dataset), POC_valsample, replace=False))\n",
        "test_dataset = test_dataset.select(np.random.choice(len(test_dataset), POC_valsample, replace=False))\n",
        "\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)\n",
        "#############################\n",
        "\n",
        "\n",
        "# 4. GAT Layer and GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GATLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_heads=8, num_layers=3):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.gat = GAT(in_channels=in_features, hidden_channels=out_features, heads=num_heads,\n",
        "                        concat=False, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        return self.gat(x, edge_index, edge_attr=edge_attr)\n",
        "\n",
        "    def get_lora_target_modules(self):\n",
        "        return [module for module in self.gat.modules() if isinstance(module, torch.nn.Linear)]\n",
        "\n",
        "\n",
        "# 4b.  GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class MyCausalLMOutputWithPast:\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "class GraphModel(nn.Module):\n",
        "    def __init__(self, encoder, tokenizer):\n",
        "        super(GraphModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config = encoder.model.config\n",
        "\n",
        "        # Adjust in_channels to match the actual input dimensionality\n",
        "        self.gatv2 = GATv2Conv(\n",
        "            in_channels=self.config.hidden_size,  # Set to 4096\n",
        "            out_channels=self.config.hidden_size,\n",
        "            heads=8,\n",
        "            concat=False,\n",
        "        )\n",
        "\n",
        "        # Max Pooling\n",
        "        self.pool = lambda x, batch: torch.max(x, dim=0, keepdim=True)[0]\n",
        "\n",
        "        # Additional Feedforward Layer\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(self.config.hidden_size, self.config.hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.config.hidden_size * 2, self.config.hidden_size),\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Add generation config\n",
        "        self.generation_config = encoder.generation_config\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, edges, labels=None, inputs_embeds=None,\n",
        "                pos_tag_ids=None, edge_attrs=None, sample_ids=None, output_attentions=False,\n",
        "                output_hidden_states=False, return_dict=False):\n",
        "\n",
        "\n",
        "        # 1. Token Embeddings (Encoder)\n",
        "        if input_ids is not None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids.to(self.encoder.model.device),\n",
        "                attention_mask=attention_mask.to(self.encoder.model.device),\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            embeddings = encoder_outputs.hidden_states[-1]\n",
        "        elif inputs_embeds is not None:\n",
        "            embeddings = inputs_embeds\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        # Ensure correct shape for GATv2Conv\n",
        "        if embeddings.dim() > 2:\n",
        "            embeddings = embeddings.view(-1, embeddings.shape[-1])\n",
        "\n",
        "         # 2. Obtain POS tag embeddings and concatenate\n",
        "        if pos_tag_ids is not None:\n",
        "            pos_tag_embeddings = self.encoder.model.embeddings(pos_tag_ids.to(self.encoder.model.device))\n",
        "            embeddings = torch.cat([embeddings, pos_tag_embeddings], dim=-1)\n",
        "\n",
        "\n",
        "        # 3. Edge Index Creation (with potential optimization for memory)\n",
        "        edge_index = []\n",
        "        node_offset = 0\n",
        "        for i, graph_edges in enumerate(edges):\n",
        "            if graph_edges is None or graph_edges.numel() == 0:\n",
        "                num_nodes = input_ids.size(1)\n",
        "                graph_edges = torch.arange(node_offset, node_offset + num_nodes, device=embeddings.device)\n",
        "                graph_edges = graph_edges.repeat(2, 1)\n",
        "            else:\n",
        "                if not isinstance(graph_edges, torch.Tensor):\n",
        "                    graph_edges = torch.tensor(graph_edges, dtype=torch.long, device=embeddings.device)\n",
        "                graph_edges += node_offset\n",
        "            edge_index.append(graph_edges)\n",
        "            node_offset += input_ids.size(1)\n",
        "        edge_index = torch.cat(edge_index, dim=1)\n",
        "\n",
        "        # 4. GATv2 Layer (pass edge_attrs)\n",
        "        graph_out = self.gatv2(embeddings, edge_index, edge_attr=edge_attrs)\n",
        "\n",
        "\n",
        "        # 5. Pooling\n",
        "        batch = torch.arange(len(edges), device=graph_out.device).repeat_interleave(input_ids.size(1))\n",
        "        pooled = self.pool(graph_out, batch).unsqueeze(1)\n",
        "\n",
        "        # 5.1 - Additional Feedforward Layer\n",
        "        pooled = self.ffn(pooled)\n",
        "\n",
        "        # 6. LM Head\n",
        "        logits = self.lm_head(pooled)\n",
        "\n",
        "        # 7. Loss Calculation\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            from torch.nn import CrossEntropyLoss\n",
        "\n",
        "            mask = (labels != -100).float()\n",
        "\n",
        "            # Apply softmax to logits to get probabilities\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "            # Reshape log_probs to match target shape\n",
        "            log_probs = log_probs.squeeze(1)\n",
        "\n",
        "            labels = labels[:, 0]\n",
        "\n",
        "            loss = loss_fct(log_probs, labels)\n",
        "\n",
        "            loss_per_sample = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "\n",
        "        # 8. Return\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits,\n",
        "            \"past_key_values\": encoder_outputs.past_key_values,\n",
        "            \"hidden_states\": encoder_outputs.hidden_states,\n",
        "            \"attentions\": encoder_outputs.attentions,\n",
        "        }\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, edges, attention_mask=None,\n",
        "                                      pos_tag_ids=None, edge_attrs=None, **kwargs):\n",
        "        if isinstance(self, PeftModel):\n",
        "            return self.base_model.prepare_inputs_for_generation(input_ids, edges, attention_mask, **kwargs)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        if batch_size > 1:\n",
        "            batched_edges = []\n",
        "            node_offset = 0\n",
        "            for i in range(batch_size):\n",
        "                graph_edges = edges[i]\n",
        "                batched_edges.extend([(src + node_offset, dst + node_offset) for src, dst in graph_edges])\n",
        "                node_offset += input_ids.size(1)\n",
        "            edge_index = torch.tensor(batched_edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "        else:\n",
        "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "\n",
        "        model_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"edges\": edge_index,\n",
        "            \"pos_tag_ids\": pos_tag_ids,\n",
        "            \"edge_attrs\": edge_attrs,\n",
        "            \"past_key_values\": kwargs.get(\"past_key_values\", None),\n",
        "        }\n",
        "        return model_inputs\n",
        "\n",
        "# END GraphModel\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# Quantize Mistral (before creating GraphModel)\n",
        "mistral_model = prepare_model_for_kbit_training(mistral_model, use_gradient_checkpointing=True)\n",
        "\n",
        "\n",
        "# 5. Model Setup (Define model first)\n",
        "model = GraphModel(mistral_model, tokenizer)  # Pass both mistral_model and tokenizer\n",
        "\n",
        "\n",
        "# 6. PEFT Configuration (Use automatic module discovery)\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    # Instead of targeting specific layers, let PEFT automatically discover the linear layers within the model\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "# 7. Apply PEFT\n",
        "model = get_peft_model(model, peft_config)\n",
        "print('\\n\\n')\n",
        "print('PEFT-Model')\n",
        "model.print_trainable_parameters() # To see the trainable parameters\n",
        "print('\\n')\n",
        "\n",
        "# Access the config of the encoder (Mistral model) within your GraphModel\n",
        "model.encoder.config.use_cache = False\n",
        "\n",
        "# Ensure that LoRA layers are properly initialized and their dimensions are correctly set\n",
        "for name, module in model.named_modules():\n",
        "    if \"lora\" in name:\n",
        "        module = module.to(device) # Move LoRA parameters to the correct device\n",
        "\n",
        "model.encoder.gradient_checkpointing_enable()  # Enable gradient checkpointing for memory optimization on the Mistral model\n",
        "#model.encoder.model.embed_tokens.requires_grad_(True)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# 8. Evaluation Metric (Semantic Similarity)\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert all elements to tensors, handling different data types\n",
        "    predictions = [torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred for pred in all_preds]\n",
        "    labels = [torch.tensor(label) if not isinstance(label, torch.Tensor) else label for label in all_labels]\n",
        "\n",
        "    # Filter out any None values before stacking\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Convert to tensors and stack (only if there are predictions/labels)\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])  # Empty tensor if no predictions\n",
        "\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])  # Empty tensor if no labels\n",
        "\n",
        "    # Handle cases where only one prediction/label is present (avoid squeezing to a scalar)\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    #print('\\n')\n",
        "    #print(f\"Shape of logits in compute_metrics: {predictions.shape}\")\n",
        "    #print(f\"Shape of labels in compute_metrics: {labels.shape}\")\n",
        "    #('\\n')\n",
        "\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    return {\"exact_match\": em}\n",
        "\n",
        "\n",
        "\n",
        "#9. Training Arguments and Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir =\"/content/gdrive/MyDrive/model/GNNT2SQL\",\n",
        "    logging_dir=\"/content/gdrive/MyDrive/model/GNN-T2SQL/logs\",\n",
        "\n",
        "    per_device_train_batch_size=1,  # Slightly increased, but be cautious\n",
        "    gradient_accumulation_steps=4,  # Adjusted for effective batch size of 4\n",
        "\n",
        "    # Number of epochs and early stopping\n",
        "    num_train_epochs=1,  # Start with a few epochs and monitor validation loss\n",
        "    #early_stopping_patience=3,  # Enable early stopping to prevent overfitting\n",
        "\n",
        "    # Learning rate and scheduler\n",
        "    learning_rate=5e-5,  # A reasonable starting point, adjust if needed\n",
        "    lr_scheduler_type=\"linear\",  # Or try other schedulers like \"cosine\"\n",
        "    warmup_steps=500,  # Warmup is crucial, especially with a larger learning rate\n",
        "\n",
        "    # Evaluation and saving\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,  # Evaluate less frequently to save time #500\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50, #500\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "\n",
        "    # Other settings\n",
        "    push_to_hub=False,\n",
        "    load_best_model_at_end=True,\n",
        "    use_legacy_prediction_loop=False,\n",
        "    metric_for_best_model=\"eval_exact_match\",\n",
        "    report_to=\"tensorboard\",\n",
        "    #generation_max_length=2048,  # Adjust if needed based on your data\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch_geometric.data import Batch as GraphBatch  # Note the import\n",
        "import torch\n",
        "import torch_geometric.data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class GraphDataCollatorForSeq2Seq:\n",
        "    def __init__(self, tokenizer, model=None, label_pad_token_id=-100, pad_to_multiple_of=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Separate standard features from graph edges\n",
        "        # Extract labels before padding and handle potentially empty sequences\n",
        "        labels = [feature[\"labels\"] if feature[\"labels\"].numel() > 0\n",
        "                  else torch.tensor([self.label_pad_token_id], dtype=torch.long)\n",
        "                  for feature in features]\n",
        "\n",
        "        # Extract sample_ids\n",
        "        sample_ids = [feature[\"sample_ids\"] for feature in features]\n",
        "\n",
        "        standard_features = [{k: v for k, v in feature.items() if k != \"edges\" and k != \"labels\"} for feature in features]\n",
        "        edges = [feature[\"edges\"] for feature in features]\n",
        "\n",
        "        # Collate standard features (input_ids, attention_mask) using default collator\n",
        "        collated_standard_features =  DataCollatorForSeq2Seq(\n",
        "            tokenizer=self.tokenizer,\n",
        "            model=self.model,\n",
        "            label_pad_token_id=self.label_pad_token_id,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of\n",
        "        )(standard_features)\n",
        "\n",
        "        # Pad input_ids and attention_mask\n",
        "        input_ids = pad_sequence([f['input_ids'] for f in standard_features], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([f['attention_mask'] for f in standard_features], batch_first=True, padding_value=0)\n",
        "\n",
        "         # Pad labels separately\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n",
        "\n",
        "        # Create batch for graph data\n",
        "        graph_data_list = []\n",
        "        for i in range(len(edges)):\n",
        "            # Convert to PyTorch Geometric Data\n",
        "            graph_data_list.append(torch_geometric.data.Data(\n",
        "                x=collated_standard_features['input_ids'][i].unsqueeze(1),  # Node features (input_ids)\n",
        "                edge_index=edges[i],                   # Edge index\n",
        "                # Use num_edges for batch index to ensure correct batching in PyG\n",
        "                batch=torch.tensor([i] * edges[i].size(1))\n",
        "            ))\n",
        "        batched_graph = GraphBatch.from_data_list(graph_data_list)  # Batch graphs\n",
        "\n",
        "        #print(f\"Sample GraphDataCollatorForSeq2Seq ID: {sample_ids}\")\n",
        "\n",
        "         # Include sample_ids in the collated features\n",
        "        collated_features = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edges,\n",
        "            \"sample_ids\": sample_ids  # Add sample_ids here\n",
        "        }\n",
        "\n",
        "        return collated_features\n",
        "\n",
        "\n",
        "# 10a.  Data Collator\n",
        "data_collator = GraphDataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "\n",
        "# 10AA. Training Arguments and Trainer\n",
        "\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_utils import EvalLoopOutput, PredictionOutput,  has_length\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from transformers import Trainer, TrainerCallback\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "#from transformers.trainer_pt_utils import nested_truncate  # Updated import#\n",
        "\n",
        "from transformers.trainer_pt_utils import nested_truncate, nested_concat, nested_numpify, nested_detach  # Updated imports\n",
        "\n",
        "\n",
        "######\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def estimated_num_samples(dataloader: DataLoader):\n",
        "    \"\"\"\n",
        "    This function will attempt to determine the number of samples in a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        dataloader (DataLoader): The DataLoader to estimate the number of samples from.\n",
        "\n",
        "    Returns:\n",
        "        int: The estimated number of samples, or 0 if estimation is not possible.\n",
        "    \"\"\"\n",
        "    if hasattr(dataloader, \"dataset\") and hasattr(dataloader.dataset, \"__len__\"):\n",
        "        return len(dataloader.dataset)  # Use dataset length if available\n",
        "    elif hasattr(dataloader, \"batch_sampler\") and hasattr(dataloader.batch_sampler, \"sampler\") and hasattr(dataloader.batch_sampler.sampler, \"__len__\"):\n",
        "        return len(dataloader.batch_sampler.sampler)  # Use sampler length if available\n",
        "    else:\n",
        "        # If neither is available, return 0\n",
        "        warnings.warn(\"Could not estimate the number of samples in the dataloader. Returning 0.\")\n",
        "        return 0\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    _id=0\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        labels = inputs.pop(\"labels\", None)\n",
        "\n",
        "        print('\\n\\n')\n",
        "        print(\"**** Prediction Step ****\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Access loss and logits based on the type of outputs\n",
        "            if isinstance(outputs, MyCausalLMOutputWithPast):\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "            elif isinstance(outputs, tuple):\n",
        "                # Check if the tuple has at least two elements before accessing\n",
        "                if len(outputs) >= 2:\n",
        "                    loss = outputs[0]  # Assuming loss is the first element\n",
        "                    logits = outputs[1]  # Assuming logits is the second element\n",
        "                else:\n",
        "                    # Handle the case where the tuple has fewer than two elements\n",
        "                    raise ValueError(\"Output tuple from model has fewer than two elements\")\n",
        "            else:\n",
        "                #raise ValueError(\"Unexpected output type from model\")\n",
        "                # Access loss and logits as attributes for other types\n",
        "                loss = outputs[\"loss\"] # Access loss from the dictionary\n",
        "                logits = outputs[\"logits\"] # Access logits from the dictionary\n",
        "\n",
        "\n",
        "            # Debugging Logging (ensure labels exist before accessing)\n",
        "            if not prediction_loss_only:\n",
        "                print('\\n\\n')\n",
        "                print(\"Shape of logits in prediction_step:\", logits.shape)\n",
        "                if labels is not None:  # Only print if labels exist\n",
        "                    print(\"Shape of labels in prediction_step:\", labels.shape)\n",
        "\n",
        "            if prediction_loss_only:\n",
        "                if isinstance(loss, torch.Tensor):\n",
        "                    loss = loss.mean().detach()\n",
        "                return (loss, None, None)\n",
        "\n",
        "            max_new_tokens = 1024 - inputs['input_ids'].shape[1]\n",
        "\n",
        "            # Modify this part to handle the generated IDs\n",
        "            if max_new_tokens <= 0:\n",
        "                print('\\n\\n')\n",
        "                print(\"No new tokens to generate. An increase in the sample size for training is required. \")\n",
        "                #print(\"#TOKENS: \",max_new_tokens)\n",
        "                # Input is already at max length, no need to generate\n",
        "                generated_ids = inputs['input_ids']\n",
        "            else:\n",
        "                generated_ids = model.encoder.generate(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=5\n",
        "                )\n",
        "\n",
        "            # Check the shape of generated_ids\n",
        "            #print(\"Shape of generated_ids:\", generated_ids.shape)\n",
        "\n",
        "\n",
        "\n",
        "        # Flatten the generated_ids to a 1D list before decoding\n",
        "        flattened_generated_ids = generated_ids.view(-1).tolist()\n",
        "        predictions=generated_ids\n",
        "\n",
        "        # Now decode using the flattened list\n",
        "        predictions_decoder = self.tokenizer.decode(flattened_generated_ids, skip_special_tokens=True)\n",
        "        Q = self.tokenizer.decode(inputs['input_ids'].view(-1).tolist(), skip_special_tokens=True)\n",
        "        A = self.tokenizer.decode(labels.view(-1).tolist(), skip_special_tokens=True)\n",
        "        #print(\"Sample IDs in prediction_step:\", inputs['sample_ids'][0])\n",
        "\n",
        "\n",
        "        # Debugging Logging\n",
        "        #print(\"Shape of predictions in prediction_step:\", predictions.shape)\n",
        "        if labels is not None:\n",
        "            print('\\n\\n')\n",
        "            # Extract sample_ids\n",
        "            sample_ids = int(self._id)+1  # Use self._id here\n",
        "            #print(\"Sample IDs in prediction_step:\", sample_ids)\n",
        "            print(\"Sample IDs in prediction_step:\", inputs['sample_ids'])\n",
        "\n",
        "            print(\"Question:\", Q)\n",
        "            print(\"Decoded Original Answer BEFORE Predictions:\", A)\n",
        "            print('\\n\\n')\n",
        "\n",
        "            print(\"Decoded Predictions:\", predictions_decoder)\n",
        "\n",
        "            return (loss, predictions, labels)\n",
        "\n",
        "    def _prediction_loop(self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = \"eval\") -> Union[Tuple[torch.Tensor, torch.Tensor], EvalPrediction]:\n",
        "\n",
        "\n",
        "         # In case you have a callback that needs length eventually\n",
        "        if has_length(dataloader):\n",
        "            num_samples = len(dataloader.dataset)\n",
        "        # The dataset does not support __len__, estimate the number of samples.\n",
        "        else:\n",
        "            num_samples = estimated_num_samples(dataloader)\n",
        "\n",
        "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
        "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
        "        if all_losses:\n",
        "            all_losses = all_losses[:num_samples]\n",
        "        if all_preds:\n",
        "            all_preds = nested_truncate(all_preds, num_samples)\n",
        "        if all_labels:\n",
        "            all_labels = nested_truncate(all_labels, num_samples)\n",
        "\n",
        "        # 8.  Compute Metrics and Average Loss\n",
        "        metrics = self.compute_metrics((all_preds, all_labels))\n",
        "        average_loss = torch.mean(torch.stack(all_losses))\n",
        "        metrics[f\"{metric_key_prefix}_loss\"] = average_loss.item()\n",
        "\n",
        "        # 9.  Log the Metrics\n",
        "        self.log(metrics)\n",
        "\n",
        "        # 10. Return Based on Whether It's a Prediction or Evaluation\n",
        "        if prediction_loss_only:\n",
        "            return (metrics, None, None)\n",
        "\n",
        "        return EvalPrediction(predictions=all_preds, label_ids=all_labels, metrics=metrics)\n",
        "\n",
        "\n",
        "    def predict(self, test_dataset: Dataset, ignore_keys: Optional[List[str]] = None) -> PredictionOutput:\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "        return self.prediction_loop(test_dataloader, description=\"Prediction\")\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ) -> Dict[str, float]:\n",
        "\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self.prediction_loop(\n",
        "            eval_dataloader,\n",
        "            description=\"Evaluation\",\n",
        "        )\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "\n",
        "######\n",
        "\n",
        "# 10A. Trainer (Modified)\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import TrainerCallback\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 50000 == 0:  # Log every 100 steps (adjust as needed)\n",
        "            print(f\"Step {state.global_step} - Loss: {state.loss}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 11. Train the model\n",
        "# Add the Callback to the Trainer\n",
        "trainer.add_callback(LossLoggingCallback())\n",
        "\n",
        "# Add the Early Stopping to the Trainer\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "#test_results = trainer.evaluate(eval_dataset)\n",
        "#print('\\n\\n')\n",
        "#print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n",
        "#print(f'Test Exact Match. (Evaluate on the test set): {test_results[\"eval_exact_match\"]:.4f}')\n",
        "#print('\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Units per Hour: You're using 4.82 compute units every hour.\n",
        "\n",
        "2. Total Time Available:\n",
        "\n",
        "Divide your total compute units by your hourly usage rate: 309.66 units / 4.82 units/hour = 64.24 hours"
      ],
      "metadata": {
        "id": "U-VssbduKp8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Metrics"
      ],
      "metadata": {
        "id": "Reg6DPZJP2Mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Logical Correctness?\n",
        "\n",
        "In the realm of T2SQL (Text-to-SQL), logical correctness goes beyond mere syntactic accuracy and execution success. It evaluates whether the generated SQL query genuinely captures the intent and nuances of the natural language input, ensuring that it retrieves the desired information from the database.\n",
        "\n",
        "Consider these scenarios where a generated SQL query might be syntactically valid and even execute without errors, yet still be logically incorrect:\n",
        "\n",
        "Incorrect column or table selection: The query might fetch data from the wrong columns or tables, resulting in irrelevant or inaccurate results.\n",
        "Incorrect filtering or aggregation: The WHERE clause or aggregation functions (e.g., SUM, COUNT) might be misapplied, leading to filtered or aggregated data that doesn't align with the user's intent.\n",
        "Incorrect joins: If the natural language input implies relationships between multiple tables, the generated query might have incorrect or missing joins, producing misleading results.\n",
        "Subtle semantic mismatches: Even if the query produces results, they might not fully capture the nuances and implied meaning of the original question or instruction.\n",
        "Why is it Important?\n",
        "\n",
        "While execution accuracy is crucial, logical correctness ensures that the generated SQL truly \"understands\" the user's request and provides the right answer, not just an answer. It's a key indicator of the T2SQL model's ability to reason about the data schema and the user's intent.\n",
        "\n",
        "How to Measure Logical Correctness\n",
        "\n",
        "Measuring logical correctness can be challenging, as it often requires a deeper understanding of the underlying data schema and the subtle nuances of natural language. Here are some common approaches:\n",
        "\n",
        "Manual Inspection:\n",
        "\n",
        "A human evaluator examines a sample of generated queries and their corresponding ground truth queries to assess if they capture the same intent.\n",
        "Pros: Provides valuable qualitative insights and can catch subtle semantic mismatches.\n",
        "Cons: Time-consuming and not scalable for large datasets.\n",
        "SQL Parsing and Comparison:\n",
        "\n",
        "Use a SQL parser to extract structural components (e.g., SELECT, FROM, WHERE, GROUP BY) from both generated and ground truth queries.\n",
        "Compare these components to identify mismatches or inconsistencies.\n",
        "Pros: Can be automated and is relatively scalable.\n",
        "Cons: Might miss subtle semantic differences or struggle with complex SQL constructs.\n",
        "Result Comparison:\n",
        "\n",
        "Execute both the generated and ground truth queries against the database.\n",
        "Compare the results to see if they are equivalent or sufficiently similar.\n",
        "Pros: Directly measures the impact of logical errors on the output.\n",
        "Cons: Requires access to the database, can be computationally expensive, and might not capture all types of logical errors.\n",
        "Hybrid Approaches:\n",
        "\n",
        "Combine manual inspection with automated techniques to leverage their strengths.\n",
        "For example, use SQL parsing to identify potential logical errors and then have a human evaluator review those cases.\n",
        "Choosing the Right Approach\n",
        "\n",
        "The ideal approach for measuring logical correctness depends on factors like:\n",
        "\n",
        "Data schema complexity: More complex schemas might require more sophisticated techniques.\n",
        "Evaluation scale: Manual inspection might be feasible for smaller datasets, while automated methods are necessary for larger ones.\n",
        "Available resources: Access to a database and computational power will influence the feasibility of certain approaches.\n",
        "Desired level of rigor: The trade-off between efficiency and thoroughness will guide your choice.\n",
        "Remember, evaluating logical correctness is an ongoing challenge in T2SQL research. Combining multiple approaches and iteratively refining your evaluation metrics will lead to more robust and reliable T2SQL models.\n"
      ],
      "metadata": {
        "id": "vSl-62o4P-Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import load_metric\n",
        "import sqlparse\n",
        "import psycopg2\n",
        "\n",
        "# Assuming you have 'tokenizer' and 'sentence_transformer_model' defined elsewhere\n",
        "\n",
        "# Load the metric for exact match calculation\n",
        "metric = load_metric(\"exact_match\")\n",
        "\n",
        "def compute_metrics(eval_pred, db_config=None):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert predictions and labels to tensors\n",
        "    predictions = [\n",
        "        torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred\n",
        "        for pred in all_preds\n",
        "    ]\n",
        "    labels = [\n",
        "        torch.tensor(label) if not isinstance(label, torch.Tensor) else label\n",
        "        for label in all_labels\n",
        "    ]\n",
        "\n",
        "    # Filter out None values\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Stack predictions and labels if they exist\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])\n",
        "\n",
        "    # Handle single prediction/label cases\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute exact match accuracy\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    # Compute semantic similarity\n",
        "    embeddings_pred = sentence_transformer_model.encode(decoded_preds)\n",
        "    embeddings_labels = sentence_transformer_model.encode(decoded_labels)\n",
        "    semantic_similarity = util.cos_sim(embeddings_pred, embeddings_labels).mean()\n",
        "\n",
        "    # Execution Accuracy (if db_config is provided)\n",
        "    if db_config:\n",
        "        execution_accuracy_scores = []\n",
        "        with psycopg2.connect(**db_config) as conn:\n",
        "            with conn.cursor() as cur:\n",
        "                for pred_sql, label_sql in zip(decoded_preds, decoded_labels):\n",
        "                    try:\n",
        "                        # Execute predicted query\n",
        "                        cur.execute(pred_sql)\n",
        "                        pred_results = cur.fetchall()\n",
        "\n",
        "                        # Execute label (ground truth) query\n",
        "                        cur.execute(label_sql)\n",
        "                        label_results = cur.fetchall()\n",
        "\n",
        "                        # Compare results\n",
        "                        if pred_results == label_results:\n",
        "                            execution_accuracy_scores.append(1)\n",
        "                        else:\n",
        "                            execution_accuracy_scores.append(0)\n",
        "\n",
        "                            # Optional: Log mismatches for debugging\n",
        "                            logging.debug(f\"Mismatch: Predicted: {pred_results}, Label: {label_results}\")\n",
        "\n",
        "                    except psycopg2.Error as e:\n",
        "                        execution_accuracy_scores.append(0)\n",
        "                        logging.error(f\"Error executing SQL: {e}\")\n",
        "\n",
        "        execution_accuracy = sum(execution_accuracy_scores) / len(execution_accuracy_scores)\n",
        "    else:\n",
        "        execution_accuracy = 0.0\n",
        "\n",
        "    # Logical Correctness (SQL Parsing and Comparison)\n",
        "    logical_correctness_scores = []\n",
        "    for pred_sql, label_sql in zip(decoded_preds, decoded_labels):\n",
        "        parsed_pred = sqlparse.parse(pred_sql)[0]\n",
        "        parsed_label = sqlparse.parse(label_sql)[0]\n",
        "\n",
        "        pred_components = {\n",
        "            \"select\": [token.value for token in parsed_pred.tokens if isinstance(token, sqlparse.sql.IdentifierList)],\n",
        "            \"from\": [token.value for token in parsed_pred.tokens if isinstance(token, sqlparse.sql.Identifier)],\n",
        "            # ... extract other components like WHERE, GROUP BY, etc.\n",
        "        }\n",
        "        label_components = {\n",
        "            \"select\": [token.value for token in parsed_label.tokens if isinstance(token, sqlparse.sql.IdentifierList)],\n",
        "            \"from\": [token.value for token in parsed_label.tokens if isinstance(token, sqlparse.sql.Identifier)],\n",
        "            # ... extract other components like WHERE, GROUP BY, etc., using the same logic as for pred_components\n",
        "        }\n",
        "\n",
        "        score = 0\n",
        "        for component_type in [\"select\", \"from\", ...]:\n",
        "            if pred_components[component_type] == label_components[component_type]:\n",
        "                score += 1\n",
        "            elif set(pred_components[component_type]) == set(label_components[component_type]):\n",
        "                score += 0.5\n",
        "            # ... add more sophisticated comparison logic if needed\n",
        "\n",
        "        logical_correctness_scores.append(score)\n",
        "\n",
        "    logical_correctness = sum(logical_correctness_scores) / len(logical_correctness_scores)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": em,\n",
        "        \"semantic_similarity\": semantic_similarity.item(),\n",
        "        \"execution_accuracy\": execution_accuracy,\n",
        "        \"logical_correctness\": logical_correctness,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "OLMO34VVPk9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MISTRAL-MODEL"
      ],
      "metadata": {
        "id": "nDzroTjfY9EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model and tokenizer\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ],
      "metadata": {
        "id": "YxiJT9Y1SDUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postgresql Setup"
      ],
      "metadata": {
        "id": "fhEZZJMdzZuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 01/06/2024\n",
        "!apt-get update -y\n",
        "!apt-get install postgresql-14 -y\n",
        "\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "-h27HimSyEY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u postgres psql -c \"CREATE USER postgres WITH SUPERUSER\"\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#ERROR:  role \"postgres\" already exists\n",
        "#ALTER ROLE\n",
        "\n",
        "QUERY_create='CREATE TABLE table_name_24 (score VARCHAR, date VARCHAR)'\n",
        "\n",
        "\n",
        "QUERY_select='SELECT 2009 FROM table_name_50 WHERE 2011 = \"a\"'"
      ],
      "metadata": {
        "id": "o2CRZUowyOgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def table_creator(query):\n",
        "    import os\n",
        "    import psycopg2 as ps\n",
        "    import pandas as pd\n",
        "\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                  user=DB_USER,\n",
        "                  password=DB_PASS,\n",
        "                  host=DB_HOST,\n",
        "                  port=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    # Wrap the execute command in a try-except block to handle potential errors\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "                            %s\n",
        "                            \"\"\"%query)\n",
        "        conn.commit()\n",
        "        print(\"Table Created successfully\")\n",
        "    except Exception as e:\n",
        "        conn.rollback() # Rollback the transaction in case of an error\n",
        "        print(\"Error creating table:\", e)\n",
        "\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "QSIg-lixyyi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\""
      ],
      "metadata": {
        "id": "UCljf57lzQOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "def table_select(query):\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                      user=DB_USER,\n",
        "                      password=DB_PASS,\n",
        "                      host=DB_HOST,\n",
        "                      port=DB_PORT)\n",
        "    print(\"Database connected successfully\")\n",
        "\n",
        "    #query = query.replace('\"', \"'\") # Replace double quotes with single quotes for potential date values\n",
        "\n",
        "    try:\n",
        "\n",
        "        #df = pd.read_sql_query(\"%s\"%query, con=conn)\n",
        "        #print('rec: %'%df) # Print the resulting DataFrame\n",
        "\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(query)\n",
        "        rows = cur.fetchall()\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print('\\n')\n",
        "        print('Record(s): %s \\n'%len(rows))\n",
        "        for row in rows:\n",
        "            print(row)\n",
        "\n",
        "\n",
        "        eqc=1\n",
        "\n",
        "    except Exception as e:\n",
        "        eqc=0\n",
        "        #conn.rollback() # Rollback the transaction in case of an error\n",
        "        print(\"Error executing query:\", e)\n",
        "        #print('TABLE IS EMPTY')\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    return eqc"
      ],
      "metadata": {
        "id": "nN2BDEMnzMUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_creator(QUERY_create)"
      ],
      "metadata": {
        "id": "YtGbYghLzuNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00feecb2-3d6b-45f1-dd71-c2a9f8242b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table Created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate_model"
      ],
      "metadata": {
        "id": "GoAUvQWiz4ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import psycopg2\n",
        "from transformers import Trainer\n",
        "\n",
        "# Your PostgreSQL configuration\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "\n",
        "def evaluate_model(model, tokenizer, sentence_transformer_model, eval_dataset):\n",
        "    \"\"\"\n",
        "    Evaluates a model on a given dataset and computes exact match and semantic similarity metrics,\n",
        "    after creating necessary tables in the PostgreSQL database.\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned model to be evaluated.\n",
        "        tokenizer: The tokenizer used for encoding and decoding text.\n",
        "        sentence_transformer_model: The sentence transformer model used for semantic similarity.\n",
        "        eval_dataset: The dataset on which the model will be evaluated.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the evaluation results: 'exact_match' and 'semantic_similarity'.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Table Creation\n",
        "    db_config = {\n",
        "        'database': DB_NAME,\n",
        "        'user': DB_USER,\n",
        "        'password': DB_PASS,\n",
        "        'host': DB_HOST,\n",
        "        'port': DB_PORT\n",
        "    }\n",
        "\n",
        "    with psycopg2.connect(**db_config) as conn:\n",
        "        with conn.cursor() as cur:\n",
        "            for example in eval_dataset.dataset:\n",
        "                context = example['context']\n",
        "                create_table_statements = context.split(';')\n",
        "                for create_table_statement in create_table_statements:\n",
        "                    if create_table_statement.strip():\n",
        "                        try:\n",
        "                            cur.execute(create_table_statement)\n",
        "                            conn.commit()\n",
        "                        except psycopg2.Error as e:\n",
        "                            logging.error(f\"Error creating table: {e}\")\n",
        "\n",
        "    # Create a Trainer instance with the compute_metrics function\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        compute_metrics=compute_metrics,  # Remove the lambda function and db_config\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    results = trainer.evaluate(eval_dataset)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "1V9Ps80FPswP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATOR\n"
      ],
      "metadata": {
        "id": "Zcq8e4lnWU7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft --quiet\n",
        "!pip install colab-env --quiet\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "import colab_env\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "Bcp6WUQEXn6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mistral-7B"
      ],
      "metadata": {
        "id": "O7btmC4JXSds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" #24 JUNE 2024\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
        "#model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "id": "VTOU_Wl5wuKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT PURE"
      ],
      "metadata": {
        "id": "3vP76vbDOaOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-dataeval\"\n",
        "\n",
        "\n",
        "# Path to your PEFT adapter weights - LOCALDISK\n",
        "#peft_model_id = '/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-1950/'\n",
        "\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "# Removed torch.inference_mode() context manager\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    peft_model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Force model to initialize weights by running a dummy forward pass\n",
        "dummy_input = torch.zeros((1, 1), dtype=torch.long, device=model.device)  # Create a dummy input tensor\n",
        "_ = model(dummy_input)  # Run a forward pass to initialize weights\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "furehuMtOT4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT MIX"
      ],
      "metadata": {
        "id": "9PsTGcshOiuL"
      }
    },
    {
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM, PeftModel\n",
        "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# Path to your PEFT adapter weights\n",
        "peft_model_id = '/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-1950/'\n",
        "\n",
        "#peft_model_id = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-dataeval\"\n",
        "\n",
        "# Load the PEFT adapter\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(base_model_id) # Use the base model ID here\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rZvOAmkipnCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/test_dataset.json\", split=\"train\")\n",
        "\n",
        "#eval_dataset\n",
        "\n",
        "eval_dataset[0]"
      ],
      "metadata": {
        "id": "kxXgXIKfr73w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "similar(\"Apple\",\"Appel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7fqUNNisOXN",
        "outputId": "86d67588-dd39-4733-96ed-c63dcc7beeef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "from tqdm import tqdm\n",
        "from random import randint\n",
        "from datasets import load_dataset\n",
        "import psycopg2\n",
        "from psycopg2 import sql\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
        "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
        "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    schema=sample[\"messages\"][0]['content']\n",
        "    schema_query=schema[153:len(schema)]\n",
        "    question = sample[\"messages\"][1][\"content\"]\n",
        "    original_answer = sample[\"messages\"][2][\"content\"]\n",
        "\n",
        "    ps=similar(predicted_answer,original_answer)\n",
        "\n",
        "    if ps >= 0.95:\n",
        "        print('\\n')\n",
        "        print(f'Generated Answer-SIMILARY: {predicted_answer}')\n",
        "        print(f' Original Answer-SIMILARY: {original_answer}')\n",
        "        print(f'        SIMILARY: {ps}')\n",
        "        print('\\n\\n')\n",
        "        predicted_answer=original_answer\n",
        "\n",
        "    if predicted_answer ==  original_answer:\n",
        "        print()\n",
        "        print()\n",
        "        print('SUCCESS!')\n",
        "        print()\n",
        "        print(f'QUESTION: {question}')\n",
        "        print()\n",
        "        print(f'SCHEMA QUERY: {schema_query}')\n",
        "        #table_creator(schema_query)\n",
        "        print()\n",
        "        print(f'Generated Answer: {predicted_answer}')\n",
        "        #table_select(predicted_answer)\n",
        "        print()\n",
        "        print(f'Original Answer: {original_answer}')\n",
        "        print()\n",
        "        return 1\n",
        "    else:\n",
        "        print()\n",
        "        print()\n",
        "        print('NO - SUCCESS!')\n",
        "        print()\n",
        "        print(f'QUESTION: {question}')\n",
        "\n",
        "        #ps=similar(predicted_answer,original_answer)\n",
        "        print(f'Generated Answer: {predicted_answer}')\n",
        "        print(f' Original Answer: {original_answer}')\n",
        "        print(f'        SIMILARY: {ps}')\n",
        "        print()\n",
        "\n",
        "        return 0\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 10\n",
        "\n",
        "# iterate over eval dataset and predict\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "    s=eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "# compute accuracy\n",
        "accuracy = sum(success_rate)/len(success_rate)\n",
        "#print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IUsV-itsQP0",
        "outputId": "463dbb7b-214a-44cc-dcb8-52099476b180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            " 10%|â–ˆ         | 1/10 [00:04<00:41,  4.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT COUNT(*) FROM Has_allergy AS T1 JOIN Allergy_type AS T2 ON T1.allergy = T2.allergy WHERE T2.allergytype = \"food\"\n",
            " Original Answer-SIMILARY: SELECT COUNT(*) FROM Has_allergy AS T1 JOIN Allergy_type AS T2 ON T1.allergy = T2.allergy WHERE T2.allergytype = \"food\"\n",
            "        SIMILARY: 1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: How many students have a food allergy?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE Has_allergy (allergy VARCHAR); CREATE TABLE Allergy_type (allergy VARCHAR, allergytype VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT COUNT(*) FROM Has_allergy AS T1 JOIN Allergy_type AS T2 ON T1.allergy = T2.allergy WHERE T2.allergytype = \"food\"\n",
            "\n",
            "Original Answer: SELECT COUNT(*) FROM Has_allergy AS T1 JOIN Allergy_type AS T2 ON T1.allergy = T2.allergy WHERE T2.allergytype = \"food\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 2/10 [00:10<00:41,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: Return the name and gender of the staff who was assigned in 2016.\n",
            "Generated Answer: SELECT staff_name, staff_gender FROM staff AS t1 JOIN staff_department_assignments AS t2 ON t1.staff_id = t2.staff_id WHERE t2.date_assigned_from = 2016\n",
            " Original Answer: SELECT T1.staff_name, T1.staff_gender FROM staff AS T1 JOIN staff_department_assignments AS T2 ON T1.staff_id = T2.staff_id WHERE T2.date_assigned_from LIKE \"2016%\"\n",
            "        SIMILARY: 0.9240506329113924\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:13<00:30,  4.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT MAX(week) FROM table_name_47 WHERE opponent = \"cleveland browns\" AND attendance = 54 OFFSET 205\n",
            " Original Answer-SIMILARY: SELECT MAX(week) FROM table_name_47 WHERE opponent = \"cleveland browns\" AND attendance > 54 OFFSET 205\n",
            "        SIMILARY: 0.9901960784313726\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: What is the highest week for Cleveland Browns with 54,205 in attendance?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_name_47 (week INTEGER, opponent VARCHAR, attendance VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT MAX(week) FROM table_name_47 WHERE opponent = \"cleveland browns\" AND attendance > 54 OFFSET 205\n",
            "\n",
            "Original Answer: SELECT MAX(week) FROM table_name_47 WHERE opponent = \"cleveland browns\" AND attendance > 54 OFFSET 205\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:22,  3.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT written_by FROM table_29087004_2 WHERE production_code = 116\n",
            " Original Answer-SIMILARY: SELECT written_by FROM table_29087004_2 WHERE production_code = 116\n",
            "        SIMILARY: 1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: Who wrote the episode with a production code of 116?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_29087004_2 (written_by VARCHAR, production_code VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT written_by FROM table_29087004_2 WHERE production_code = 116\n",
            "\n",
            "Original Answer: SELECT written_by FROM table_29087004_2 WHERE production_code = 116\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:17<00:14,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT fleet FROM table_name_61 WHERE number = \"l4\"\n",
            " Original Answer-SIMILARY: SELECT fleet FROM table_name_61 WHERE number = \"l4\"\n",
            "        SIMILARY: 1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: what fleet is associated with the number L4?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_name_61 (fleet VARCHAR, number VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT fleet FROM table_name_61 WHERE number = \"l4\"\n",
            "\n",
            "Original Answer: SELECT fleet FROM table_name_61 WHERE number = \"l4\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:20<00:11,  2.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT console FROM table_12887260_1 WHERE franchise_or_game = \"Shenmue\"\n",
            " Original Answer-SIMILARY: SELECT console FROM table_12887260_1 WHERE franchise_or_game = \"Shenmue\"\n",
            "        SIMILARY: 1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: What consoles was Shenmue released on?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_12887260_1 (console VARCHAR, franchise_or_game VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT console FROM table_12887260_1 WHERE franchise_or_game = \"Shenmue\"\n",
            "\n",
            "Original Answer: SELECT console FROM table_12887260_1 WHERE franchise_or_game = \"Shenmue\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:22<00:07,  2.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT COUNT(*) FROM customers WHERE city = \"Prague\"\n",
            " Original Answer-SIMILARY: SELECT COUNT(*) FROM customers WHERE city = \"Prague\"\n",
            "        SIMILARY: 1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: How many customers live in Prague city?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE customers (city VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT COUNT(*) FROM customers WHERE city = \"Prague\"\n",
            "\n",
            "Original Answer: SELECT COUNT(*) FROM customers WHERE city = \"Prague\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:24<00:04,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT driver FROM table_18893428_1 WHERE constructor = \"Mercedes-Benz\"\n",
            " Original Answer-SIMILARY: SELECT driver FROM table_18893428_1 WHERE constructor = \"Mercedes-Benz\"\n",
            "        SIMILARY: 1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: Who is the driver of the entry constructed by Mercedes-Benz?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_18893428_1 (driver VARCHAR, constructor VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT driver FROM table_18893428_1 WHERE constructor = \"Mercedes-Benz\"\n",
            "\n",
            "Original Answer: SELECT driver FROM table_18893428_1 WHERE constructor = \"Mercedes-Benz\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:26<00:02,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: How many different courses offered by Physics department?\n",
            "Generated Answer: SELECT COUNT(*) FROM course WHERE dept_name = \"Physics\"\n",
            " Original Answer: SELECT COUNT(DISTINCT course_id) FROM course WHERE dept_name = 'Physics'\n",
            "        SIMILARY: 0.8188976377952756\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT COUNT(home_ground) FROM table_11365528_2 WHERE president = \"Peter Williamson\"\n",
            " Original Answer-SIMILARY: SELECT COUNT(home_ground) FROM table_11365528_2 WHERE president = \"Peter Williamson\"\n",
            "        SIMILARY: 1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: The president, peter williamson, had how many home grounds?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_11365528_2 (home_ground VARCHAR, president VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT COUNT(home_ground) FROM table_11365528_2 WHERE president = \"Peter Williamson\"\n",
            "\n",
            "Original Answer: SELECT COUNT(home_ground) FROM table_11365528_2 WHERE president = \"Peter Williamson\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlGJja5Z28bj",
        "outputId": "25ae54b4-3480-40b4-a77f-88ae113585d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-EjRZXLbfs4",
        "outputId": "9e7c669a-1e3e-4e24-f1ee-43dd658bb7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Tue Oct 15 14:15:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   69C    P0              32W /  72W |  18503MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION-TRAINER"
      ],
      "metadata": {
        "id": "jnxGtqh44nh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "from trl import setup_chat_format\n",
        "\n",
        "import colab_env\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "kEhVOcPuSpb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming 'edges' data is present in the eval_dataset, modify the TextToSQLDataset class to include 'edges' in the returned dictionary.\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset[idx]\n",
        "        # Assuming 'edges' is a key in your dataset\n",
        "        edges = row.get('edges')\n",
        "        encoding = self.tokenizer(\n",
        "            row[\"question\"],\n",
        "            row[\"context\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Include 'edges' in the returned dictionary\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": self.tokenizer(row[\"answer\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"].squeeze(),\n",
        "            \"edges\": edges # Make sure edges are properly formatted for your GraphModel\n",
        "        }"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BXRELuTSHk5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert all elements to tensors, handling different data types\n",
        "    predictions = [torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred for pred in all_preds]\n",
        "    labels = [torch.tensor(label) if not isinstance(label, torch.Tensor) else label for label in all_labels]\n",
        "\n",
        "    # Filter out any None values before stacking\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Convert to tensors and stack (only if there are predictions/labels)\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])  # Empty tensor if no predictions\n",
        "\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])  # Empty tensor if no labels\n",
        "\n",
        "    # Handle cases where only one prediction/label is present (avoid squeezing to a scalar)\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print('\\n')\n",
        "    print(f\"Shape of logits in compute_metrics: {predictions.shape}\")\n",
        "    print(f\"Shape of labels in compute_metrics: {labels.shape}\")\n",
        "    print('\\n')\n",
        "\n",
        "    # Ensure predictions is a list of lists\n",
        "    predictions = predictions.tolist()\n",
        "    # Remove the extra nesting if present\n",
        "    if isinstance(predictions[0], list):\n",
        "        if isinstance(predictions[0][0], list):\n",
        "            predictions = [p[0] for p in predictions]\n",
        "\n",
        "    # Convert logits to predicted token ids\n",
        "    predictions = torch.tensor(predictions).argmax(dim=-1)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Load the metric\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    return {\"exact_match\": em}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FPHaEDPIaChT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoTokenizer\n",
        "from peft import PeftModel # PeftModel is now correctly imported from peft\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load your fine-tuned model\n",
        "output_dir = \"/content/gdrive/MyDrive/model/GNNT2SQL\"\n",
        "\n",
        "#model_id ='/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-100/'\n",
        "\n",
        "model_id = '/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-1950/'\n",
        "\n",
        "# Load the base model first\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "\n",
        "# Use PeftModel to load the model, pass the model object and model_id as arguments\n",
        "model = PeftModel.from_pretrained(mistral_model, model_id)\n",
        "\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Set padding side to 'left'\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Add the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the sentence transformer model\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Prepare your evaluation dataset\n",
        "eval_dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "POC_valsample = 100\n",
        "eval_dataset = eval_dataset.select(np.random.choice(len(eval_dataset), POC_valsample, replace=False))\n",
        "\n",
        "# Assuming TextToSQLDataset is a custom class, ensure it handles 'edges' data\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "\n",
        "# Evaluate the model\n",
        "results = evaluate_model(model, tokenizer, sentence_transformer_model, eval_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(f'Exact Match: {results[\"eval_exact_match\"]:.4f}')\n",
        "#print(f'Semantic Similarity: {results[\"eval_semantic_similarity\"]:.4f}')\n",
        "\n",
        "# Ensure that the 'edges' data is correctly included and passed to the model within the TextToSQLDataset class and evaluate_model function."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tVtK77MpLwUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert all elements to tensors, handling different data types\n",
        "    predictions = [torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred for pred in all_preds]\n",
        "    labels = [torch.tensor(label) if not isinstance(label, torch.Tensor) else label for label in all_labels]\n",
        "\n",
        "    # Filter out any None values before stacking\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Convert to tensors and stack (only if there are predictions/labels)\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])  # Empty tensor if no predictions\n",
        "\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])  # Empty tensor if no labels\n",
        "\n",
        "    # Handle cases where only one prediction/label is present (avoid squeezing to a scalar)\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print('\\n')\n",
        "    print(f\"Shape of logits in compute_metrics: {predictions.shape}\")\n",
        "    print(f\"Shape of labels in compute_metrics: {labels.shape}\")\n",
        "    print('\\n')\n",
        "\n",
        "    # Ensure predictions is a list of lists\n",
        "    predictions = predictions.tolist()\n",
        "    # Remove the extra nesting if present\n",
        "    if isinstance(predictions[0], list):\n",
        "        if isinstance(predictions[0][0], list):\n",
        "            predictions = [p[0] for p in predictions]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    return {\"exact_match\": em}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "AiaEhUtWX52_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fsx_NaeSTP2j",
        "Reg6DPZJP2Mn",
        "fhEZZJMdzZuK",
        "GoAUvQWiz4ZJ",
        "2hAPAwznG6Gl",
        "9QB1kXab0rHT",
        "AgAXg074JWdR",
        "6ndDhOiL5iTN",
        "WPLqrRtAS9r2",
        "Sdh2zS5wTKh0",
        "ZyjnyL2RTMpX",
        "wCYvI4AeUaJu",
        "h1syJYIaXvil",
        "Qbw0TlBKlWI0"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FINAL_GNN_FINE_TUNINGT_T2SQL_DataAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC7dvZByG6uM"
      },
      "source": [
        "## Libraries Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy[transformers] -q # Install spacy with transformer support\n",
        "#!python -m spacy download en_core_web_trf -q # Download the model"
      ],
      "metadata": {
        "id": "wg5BHXCjSRjD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import spacy\n",
        "# Load the spaCy English model with SRL capabilities\n",
        "#nlp = spacy.load(\"en_core_web_trf\") # Load the model directly, installation and download have been handled above"
      ],
      "metadata": {
        "id": "kP6Hxex_Shta"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9cqNuPkh_aG"
      },
      "outputs": [],
      "source": [
        "!pip install datasets networkx -q\n",
        "\n",
        "!pip install torch_geometric -q\n",
        "\n",
        "\n",
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "!pip install peft --quiet\n",
        "!pip install trl ninja packaging --quiet\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install mistral_inference -q\n",
        "\n",
        "!pip install trl==0.8.6 -q\n",
        "\n",
        "\n",
        "!pip install sqlparse -q\n",
        "\n",
        "!pip install bitsandbytes -q\n",
        "\n",
        "#!pip uninstall -y torchvision -q\n",
        "!pip install torchvision --no-cache-dir -q\n",
        "#import evaluate\n",
        "\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "!pip install nlpaug -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8Ak2KGyrYOro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b29faf-c60c-4a98-e50e-43226049b2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "from trl import setup_chat_format\n",
        "\n",
        "import colab_env\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" #01 march 2024 AND 10/03/2024\n",
        "\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model and tokenizer\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
        "#model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "#PEFT_MODEL_ID = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-dataeval\"\n",
        "\n",
        "#GENERATION_PARAMS = {\n",
        "#    \"max_new_tokens\": 256, \"do_sample\": True, \"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95\n",
        "#}\n",
        "\n",
        "\n",
        "# Load Models and Tokenizer\n",
        "#logging.info(f\"Loading fine-tuned PEFT model from: {PEFT_MODEL_ID}\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(PEFT_MODEL_ID)\n",
        "#mistral_model = AutoPeftModelForCausalLM.from_pretrained(PEFT_MODEL_ID)\n",
        "\n"
      ],
      "metadata": {
        "id": "-pGJR0chK0dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsx_NaeSTP2j"
      },
      "source": [
        "## TRAININING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0DANV3Esq5w"
      },
      "source": [
        "https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Mistral_7B_Instruct_v0_1_for_text_to_SQL_EVALDATA.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsfmK0vECZ3C"
      },
      "source": [
        "* Import Main Components"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XPoZvJrBzyG",
        "outputId": "8872eb01-934b-4613-9faf-4ba39b0d431f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/train_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "p2XhxxLrsHB6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][0]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "VOG6LSjFL9Ng",
        "outputId": "9ee3e356-cc96-4867-9f73-1718007e4455"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_92 (total VARCHAR, finish VARCHAR)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][1]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w7PmSu5hMEwE",
        "outputId": "b21c3227-12e5-4fea-d50d-52e50d05bb08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How many times was the finish t32?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][2]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zO_YHBbrtmrV",
        "outputId": "2092e5ac-d1e9-42c7-8756-40e09e90472a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT COUNT(total) FROM table_name_92 WHERE finish = \"t32\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0DfyorMy4TK",
        "outputId": "288fd6da-860d-4b81-808e-08be49cc5e58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr9FHlz8EEds"
      },
      "source": [
        "* https://stackoverflow.com/questions/70950706/assertionerror-in-torch-geometric-nn-gatconv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely! Let's describe the dataflow in the provided code, breaking it down step-by-step:\n",
        "\n",
        "1. **Data Input and Preparation:**\n",
        "   - The process begins by loading the \"sql-create-context\" dataset, which presumably contains pairs of natural language questions and their corresponding SQL queries.\n",
        "   - The dataset is divided into three distinct subsets: training, validation, and testing.\n",
        "   - The Mistral-7B-Instruct-v0.3 language model and its tokenizer are loaded and prepared.\n",
        "\n",
        "2. **Data Transformation with `TextToSQLDataset`:**\n",
        "   - The `TextToSQLDataset` class is responsible for converting raw data into a format suitable for model training and evaluation.\n",
        "   - For each data sample, the following transformations occur:\n",
        "      - Tokenization: The input question and the SQL query (answer) are tokenized using the loaded tokenizer.\n",
        "      - Dependency Parsing: The question is parsed using spaCy to extract the grammatical relationships between words, generating a dependency graph.\n",
        "      - Dictionary Creation: A dictionary is created to store the tokenized input IDs, attention masks, labels (tokenized SQL query), and the dependency edges extracted from parsing.\n",
        "\n",
        "3. **Batching and Shuffling with `DataLoader`:**\n",
        "   - `DataLoader` takes the processed dataset from `TextToSQLDataset` and creates an iterable object for efficient batching.\n",
        "   - Optionally, shuffling is applied to randomize the order of samples within each epoch during training.\n",
        "\n",
        "4. **Forward Pass through `GraphModel`:**\n",
        "   - **Mistral Encoder:** The tokenized input IDs and attention masks are fed into the Mistral model's encoder to obtain contextualized token embeddings.\n",
        "   - **GATv2 Layer:** The GATv2 layer (Graph Attention Network) takes the token embeddings and the dependency edges as input. It applies graph attention mechanisms to incorporate the structural information from the dependency graph into the token representations.\n",
        "   - **Pooling:** The node representations (output of GATv2) are aggregated using a pooling operation (e.g., mean pooling) to obtain a fixed-size representation of the entire input sequence.\n",
        "   - **LM Head:** The pooled representation is passed through a linear layer, which produces logits – unnormalized probabilities for each token in the vocabulary.\n",
        "   - **Loss Calculation:** During training, if labels (correct SQL queries) are available, the cross-entropy loss is calculated between the predicted logits and the true labels. This loss guides the optimization of the model's parameters.\n",
        "\n",
        "5. **Model Optimization with PEFT (LoRA):**\n",
        "   - The `GraphModel` is wrapped with PEFT's LoRA (Low-Rank Adaptation) configuration to enable parameter-efficient fine-tuning.\n",
        "   - During training, only the parameters of the GATv2 layer and the LM head are updated, while the rest of the model parameters remain frozen.\n",
        "   - The Hugging Face Trainer manages the training process, iterating over the dataset, computing gradients, and updating the model's parameters based on the calculated loss.\n",
        "\n",
        "6. **Evaluation:**\n",
        "   - After (or during) training, the model is evaluated on the validation and test sets.\n",
        "   - The `compute_metrics` function decodes the predicted logits and labels back into text and assesses the model's performance using two metrics:\n",
        "      - Semantic Similarity: This metric measures how semantically close the predicted SQL query is to the reference SQL query using SentenceTransformer embeddings.\n",
        "      - Exact Match: This metric checks if the predicted SQL query matches the reference SQL query exactly.\n",
        "\n",
        "7. **Output:**\n",
        "    - The final output is the evaluation results, including semantic similarity and exact match scores, which provide insights into the model's ability to generate accurate SQL queries from natural language questions.\n",
        "\n",
        "Let me know if you have any further questions or would like clarification on any of the steps!\n"
      ],
      "metadata": {
        "id": "QFVMbILY2AeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're absolutely right! I apologize for the error in the previous diagram. The arrow direction was indeed incorrect.\n",
        "\n",
        "Here's the corrected graphical representation of the dataflow, with the arrow pointing *into* the `Trainer`:\n",
        "\n",
        "```\n",
        "+-------------------+          +----------------------+\n",
        "| Dataset Loading   |          | TextToSQLDataset     |\n",
        "|  - sql-create...  | -------> | - Tokenization       |\n",
        "|  - Split: train...|          | - Dependency Parsing |\n",
        "+-------------------+          | - Dict Creation      |\n",
        "                               +----------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                         +---------------------------+\n",
        "                         | DataLoader                |\n",
        "                         | - Batches, Shuffling (opt)|\n",
        "                         +---------------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                         +---------------------------+\n",
        "                         | GraphModel                |\n",
        "                         | - Mistral Encoder         |\n",
        "                         | - GATv2 Layer             |\n",
        "                         | - Pooling                 |\n",
        "                         | - LM Head                 |\n",
        "                         | - (Loss Calculation)      |\n",
        "                         +---------------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "            +--------------+          +---------+         +-----------------+\n",
        "            | PEFT (LoRA)  | -------> | Trainer | <------ |Evaluation       |\n",
        "            +--------------+          |         |         |(compute_metrics)|\n",
        "                                      +---------+         +-----------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                                   +-----------------------+\n",
        "                                   | - Semantic Similarity |\n",
        "                                   | - Exact Match         |\n",
        "                                   +-----------------------+\n",
        "```\n",
        "\n",
        "The revised dataflow now accurately shows that the evaluation metrics (semantic similarity and exact match) calculated by the `compute_metrics` function are used by the `Trainer` to assess the model's performance and make decisions during training (e.g., early stopping).\n",
        "\n",
        "Thank you for pointing out the error! I strive to be as accurate as possible in my responses.\n"
      ],
      "metadata": {
        "id": "cMZjJHj12Dnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip show huggingface_hub"
      ],
      "metadata": {
        "id": "Qk2z0aI8A8Sj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "import shutil\n",
        "\n",
        "# Get the cache directory information\n",
        "cache_info = huggingface_hub.utils.scan_cache_dir()\n",
        "\n",
        "# Iterate through the repositories and delete them\n",
        "for repo_info in cache_info.repos:\n",
        "    repo_path = repo_info.repo_path\n",
        "    shutil.rmtree(repo_path)  # Delete the repository folder\n",
        "\n",
        "# Optionally, delete the snapshots if needed\n",
        "# for snapshot_info in cache_info.snapshots:\n",
        "#     snapshot_path = snapshot_info.snapshot_path\n",
        "#     shutil.rmtree(snapshot_path)"
      ],
      "metadata": {
        "id": "IJj8txi-BLdF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6horVfRxXtU8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "e2ad2e9c-8020-4cab-ee43-5e4f168d7fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "PEFT-Model\n",
            "trainable params: 3,407,872 || all params: 7,711,902,976 || trainable%: 0.0442\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  8/300 02:04 < 1:40:47, 0.05 it/s, Epoch 0.02/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, Trainer, TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    PrefixTuningConfig,\n",
        "    PromptEncoderConfig,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch_geometric.nn import GAT\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings(\"ignore\", message=\"The installed version of bitsandbytes was compiled without GPU support.\")\n",
        "\n",
        "# Load spaCy English model\n",
        "\n",
        "#try:\n",
        "#    nlp = spacy.load(\"en_core_web_sm\")\n",
        "#except OSError:\n",
        "    # Download if not already downloaded\n",
        "#    spacy.cli.download(\"en_core_web_sm\")\n",
        "#    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# Manually define splits\n",
        "train_size = int(0.7 * len(dataset))\n",
        "eval_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - eval_size\n",
        "\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
        "test_dataset = dataset.select(range(train_size + eval_size, len(dataset)))\n",
        "\n",
        "#pipe = pipeline(\"text-generation\", model=mistral_model, tokenizer=tokenizer, **GENERATION_PARAMS)\n",
        "#logging.info(\"Model and tokenizer loaded successfully!\")\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3a. PyTorch Datasets - Data Augmentation\n",
        "import random\n",
        "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
        "\n",
        "def random_insertion(sentence, aug_p=0.1, synonym_aug=None, max_attempts=3):  # Add max_attempts\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_words.append(word)\n",
        "        if random.random() < aug_p:\n",
        "            attempts = 0\n",
        "            while attempts < max_attempts:  # Try multiple times to find a synonym\n",
        "                if new_words:\n",
        "                    candidate_words = [new_words[-1]]\n",
        "                if len(new_words) < len(words):\n",
        "                    candidate_words.append(words[len(new_words)])\n",
        "                if candidate_words and synonym_aug:\n",
        "                    synonym = synonym_aug.augment(random.choice(candidate_words))\n",
        "                    if synonym:\n",
        "                        new_words.append(synonym[0])\n",
        "                        break  # Exit the loop if a synonym is found\n",
        "                attempts += 1\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def augment_data(dataset):\n",
        "    augmented_data = []\n",
        "\n",
        "    synonym_aug = SynonymAug(aug_src='wordnet')\n",
        "    delete_aug = RandomWordAug(action=\"delete\", aug_p=0.05)\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        context = item['context']\n",
        "\n",
        "        augmented_questions = set()\n",
        "        augmented_questions.add(question)  # Always include the original question\n",
        "\n",
        "        # Attempt to generate one more unique augmented question\n",
        "        while len(augmented_questions) < 2:  # Keep trying until we have 2 unique questions\n",
        "            augmentation_method = random.choice(['synonym', 'insertion', 'deletion'])\n",
        "\n",
        "            if augmentation_method == 'synonym':\n",
        "                synonyms = synonym_aug.augment(question)\n",
        "                if synonyms and synonyms[0] != question:\n",
        "                    augmented_questions.add(synonyms[0])\n",
        "\n",
        "            elif augmentation_method == 'insertion':\n",
        "                augmented_question = random_insertion(question, synonym_aug=synonym_aug)\n",
        "                if augmented_question != question:\n",
        "                    augmented_questions.add(augmented_question)\n",
        "\n",
        "            else:  # deletion\n",
        "                deleted = delete_aug.augment(question)\n",
        "                if deleted and deleted[0] != question:\n",
        "                    augmented_questions.add(deleted[0])\n",
        "\n",
        "        # Add the augmented examples to the dataset\n",
        "        for aug_question in list(augmented_questions):\n",
        "            augmented_data.append({'question': aug_question, 'answer': answer, 'context': context})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "#from torch.utils.data import Dataset, DataLoader\n",
        "#from torch_geometric.data import Data, Batch\n",
        "\n",
        "# 3. PyTorch Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English model with SRL capabilities\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "except OSError:\n",
        "    spacy.cli.download(\"en_core_web_trf\")\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "        # 1. Tokenization\n",
        "        tokenized_input = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized_target = self.tokenizer(\n",
        "            target_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Flatten lists\n",
        "        tokenized_input = {k: v.squeeze(0) for k, v in tokenized_input.items()}\n",
        "        tokenized_target = {k: v.squeeze(0) for k, v in tokenized_target.items()}\n",
        "\n",
        "        # 2. Dependency Parsing & SRL for Edge Extraction\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        edge_attrs = []\n",
        "        for token in doc:\n",
        "            # Dependency Parsing\n",
        "            if token.dep_ != \"ROOT\" and token.i != token.head.i:\n",
        "                edges.append([token.i, token.head.i])\n",
        "                edge_attrs.append(self.tokenizer.vocab.get(token.dep_, self.tokenizer.unk_token_id))\n",
        "\n",
        "            # SRL\n",
        "            if token.dep_ in {\"nsubj\", \"dobj\", \"nsubjpass\"}:\n",
        "                for child in token.children:\n",
        "                    if child.dep_ == \"prep\":\n",
        "                        for grandchild in child.children:\n",
        "                            if grandchild.dep_ in {\"pobj\", \"pcomp\"}:\n",
        "                                edges.append([token.i, grandchild.i])\n",
        "                                edge_attrs.append(self.tokenizer.vocab.get(\"prep_\" + grandchild.dep_, self.tokenizer.unk_token_id))\n",
        "\n",
        "        # Edge Index Extraction and Validation\n",
        "        if not edges:\n",
        "            num_nodes = len(tokenized_input[\"input_ids\"])\n",
        "            edges = [[i, i] for i in range(num_nodes)]\n",
        "        else:\n",
        "            max_index = len(tokenized_input[\"input_ids\"]) - 1\n",
        "            edges = [(src, tgt) for src, tgt in edges\n",
        "                     if 0 <= src <= max_index and 0 <= tgt <= max_index]\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_attrs = torch.tensor(edge_attrs, dtype=torch.long)\n",
        "\n",
        "        # 3. Node Features with POS Tags\n",
        "        pos_tags = [token.pos_ for token in doc]\n",
        "        pos_tag_ids = [self.tokenizer.vocab.get(tag, self.tokenizer.unk_token_id) for tag in pos_tags]\n",
        "        pos_tag_ids = torch.tensor(pos_tag_ids, dtype=torch.long)\n",
        "\n",
        "        # Convert everything to tensors BEFORE padding/truncation\n",
        "        input_ids = tokenized_input[\"input_ids\"].clone().detach()\n",
        "        attention_mask = tokenized_input[\"attention_mask\"].clone().detach()\n",
        "        labels = tokenized_target[\"input_ids\"].clone().detach()\n",
        "\n",
        "        # Handle potentially empty target sequences\n",
        "        if len(labels) == 0:\n",
        "            labels = torch.tensor([self.tokenizer.pad_token_id], dtype=torch.long)\n",
        "\n",
        "        # Padding and Truncation\n",
        "        max_length = 1024\n",
        "\n",
        "        input_ids = input_ids[:max_length]\n",
        "        attention_mask = attention_mask[:max_length]\n",
        "        labels = labels[:max_length]\n",
        "        pos_tag_ids = pos_tag_ids[:max_length]\n",
        "\n",
        "        if len(input_ids) < max_length:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            pad_tensor = torch.full((pad_length,), self.tokenizer.pad_token_id)\n",
        "            input_ids = torch.cat((input_ids, pad_tensor))\n",
        "            attention_mask = torch.cat((attention_mask, torch.zeros(pad_length, dtype=torch.long)))\n",
        "            pos_tag_ids = torch.cat((pos_tag_ids, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        if len(labels) < max_length:\n",
        "            pad_length = max_length - len(labels)\n",
        "            labels = torch.cat((labels, torch.full((pad_length,), -100)))\n",
        "\n",
        "        if len(edge_attrs) < edge_index.size(1):\n",
        "            pad_length = edge_index.size(1) - len(edge_attrs)\n",
        "            edge_attrs = torch.cat((edge_attrs, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edge_index,\n",
        "            \"edge_attrs\": edge_attrs,\n",
        "            \"pos_tag_ids\": pos_tag_ids,\n",
        "            \"sample_ids\": torch.tensor([idx])\n",
        "        }\n",
        "\n",
        "#train_dataset = train_dataset.select(range(100))\n",
        "\n",
        "#Minimum: Start with at least 2000-3000 samples.\n",
        "#This should be enough to provide a good initial assessment of your model's performance and potential.\n",
        "\n",
        "#Medium: If your computational resources allow, try 5000-7000 samples.\n",
        "#This could provide a more robust evaluation and potentially lead to better performance.\n",
        "\n",
        "#Maximum: If you have ample resources, consider using the entire dataset (around 10,000 samples).\n",
        "#This would give you the most comprehensive training data possible and potentially lead\n",
        "#to the best model performance.\n",
        "\n",
        "#Reduce train_dataset size for POC\n",
        "#POC_sample=26000\n",
        "\n",
        "#POC_sample=16000\n",
        "\n",
        "\n",
        "POC_sample=600\n",
        "import numpy as np\n",
        "train_dataset = train_dataset.select(np.random.choice(len(train_dataset), POC_sample, replace=False))\n",
        "\n",
        "### data augmentation #######\n",
        "train_dataset_augmented = augment_data(train_dataset)\n",
        "train_dataset = TextToSQLDataset(train_dataset_augmented, tokenizer)\n",
        "#############################\n",
        "\n",
        "\n",
        "POC_valsample=1\n",
        "#############################\n",
        "eval_dataset = eval_dataset.select(np.random.choice(len(eval_dataset), POC_valsample, replace=False))\n",
        "test_dataset = test_dataset.select(np.random.choice(len(test_dataset), POC_valsample, replace=False))\n",
        "\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)\n",
        "#############################\n",
        "\n",
        "\n",
        "# 4. GAT Layer and GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GATLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_heads=8, num_layers=3):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.gat = GAT(in_channels=in_features, hidden_channels=out_features, heads=num_heads,\n",
        "                        concat=False, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        return self.gat(x, edge_index, edge_attr=edge_attr)\n",
        "\n",
        "    def get_lora_target_modules(self):\n",
        "        return [module for module in self.gat.modules() if isinstance(module, torch.nn.Linear)]\n",
        "\n",
        "\n",
        "# 4b.  GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "# from transformers import CausalLMOutputWithPast\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class MyCausalLMOutputWithPast:\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "class GraphModel(nn.Module):\n",
        "    def __init__(self, encoder, tokenizer):\n",
        "        super(GraphModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config = encoder.model.config\n",
        "\n",
        "        # Adjust in_channels to match the actual input dimensionality\n",
        "        self.gatv2 = GATv2Conv(\n",
        "            in_channels=self.config.hidden_size,  # Set to 4096\n",
        "            out_channels=self.config.hidden_size,\n",
        "            heads=8,\n",
        "            concat=False,\n",
        "        )\n",
        "\n",
        "        # Max Pooling\n",
        "        self.pool = lambda x, batch: torch.max(x, dim=0, keepdim=True)[0]\n",
        "\n",
        "        # Additional Feedforward Layer\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(self.config.hidden_size, self.config.hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.config.hidden_size * 2, self.config.hidden_size),\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Add generation config\n",
        "        self.generation_config = encoder.generation_config\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, edges, labels=None, inputs_embeds=None,\n",
        "                pos_tag_ids=None, edge_attrs=None, sample_ids=None, output_attentions=False,\n",
        "                output_hidden_states=False, return_dict=False):\n",
        "\n",
        "\n",
        "        # 1. Token Embeddings (Encoder)\n",
        "        if input_ids is not None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids.to(self.encoder.model.device),\n",
        "                attention_mask=attention_mask.to(self.encoder.model.device),\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            embeddings = encoder_outputs.hidden_states[-1]\n",
        "        elif inputs_embeds is not None:\n",
        "            embeddings = inputs_embeds\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        # Ensure correct shape for GATv2Conv\n",
        "        if embeddings.dim() > 2:\n",
        "            embeddings = embeddings.view(-1, embeddings.shape[-1])\n",
        "\n",
        "         # 2. Obtain POS tag embeddings and concatenate\n",
        "        if pos_tag_ids is not None:\n",
        "            pos_tag_embeddings = self.encoder.model.embeddings(pos_tag_ids.to(self.encoder.model.device))\n",
        "            embeddings = torch.cat([embeddings, pos_tag_embeddings], dim=-1)\n",
        "\n",
        "        # Print the shape of embeddings for debugging\n",
        "        #print(\"Shape of embeddings before GATv2Conv:\", embeddings.shape)\n",
        "\n",
        "\n",
        "        # 3. Edge Index Creation (with potential optimization for memory)\n",
        "        edge_index = []\n",
        "        node_offset = 0\n",
        "        for i, graph_edges in enumerate(edges):\n",
        "            if graph_edges is None or graph_edges.numel() == 0:\n",
        "                num_nodes = input_ids.size(1)\n",
        "                graph_edges = torch.arange(node_offset, node_offset + num_nodes, device=embeddings.device)\n",
        "                graph_edges = graph_edges.repeat(2, 1)\n",
        "            else:\n",
        "                # Optionally, filter or sparsify edges here if memory is a concern\n",
        "                # graph_edges = filter_edges(graph_edges)\n",
        "\n",
        "                if not isinstance(graph_edges, torch.Tensor):\n",
        "                    graph_edges = torch.tensor(graph_edges, dtype=torch.long, device=embeddings.device)\n",
        "                graph_edges += node_offset\n",
        "            edge_index.append(graph_edges)\n",
        "            node_offset += input_ids.size(1)\n",
        "        edge_index = torch.cat(edge_index, dim=1)\n",
        "\n",
        "        # 4. GATv2 Layer (pass edge_attrs)\n",
        "        graph_out = self.gatv2(embeddings, edge_index, edge_attr=edge_attrs)\n",
        "\n",
        "\n",
        "        # 5. Pooling\n",
        "        batch = torch.arange(len(edges), device=graph_out.device).repeat_interleave(input_ids.size(1))\n",
        "        pooled = self.pool(graph_out, batch).unsqueeze(1)\n",
        "\n",
        "        # 5.1 - Additional Feedforward Layer\n",
        "        pooled = self.ffn(pooled)\n",
        "\n",
        "        # 6. LM Head\n",
        "        logits = self.lm_head(pooled)\n",
        "\n",
        "        # 7. Loss Calculation\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            from torch.nn import CrossEntropyLoss\n",
        "\n",
        "            mask = (labels != -100).float()\n",
        "\n",
        "            # Apply softmax to logits to get probabilities\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "            # Reshape log_probs to match target shape\n",
        "            log_probs = log_probs.squeeze(1)\n",
        "\n",
        "            labels = labels[:, 0]\n",
        "\n",
        "            loss = loss_fct(log_probs, labels)\n",
        "\n",
        "            loss_per_sample = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "\n",
        "        # 8. Return\n",
        "        #return MyCausalLMOutputWithPast(\n",
        "        #    loss=loss,\n",
        "        #    logits=logits,\n",
        "        #    past_key_values=encoder_outputs.past_key_values,\n",
        "        #    hidden_states=encoder_outputs.hidden_states,\n",
        "        #    attentions=encoder_outputs.attentions,\n",
        "        #)\n",
        "\n",
        "        # Instead of returning MyCausalLMOutputWithPast, return a dictionary\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits,\n",
        "            \"past_key_values\": encoder_outputs.past_key_values,\n",
        "            \"hidden_states\": encoder_outputs.hidden_states,\n",
        "            \"attentions\": encoder_outputs.attentions,\n",
        "        }\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, edges, attention_mask=None,\n",
        "                                      pos_tag_ids=None, edge_attrs=None, **kwargs):\n",
        "        if isinstance(self, PeftModel):\n",
        "            return self.base_model.prepare_inputs_for_generation(input_ids, edges, attention_mask, **kwargs)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        if batch_size > 1:\n",
        "            batched_edges = []\n",
        "            node_offset = 0\n",
        "            for i in range(batch_size):\n",
        "                graph_edges = edges[i]\n",
        "                batched_edges.extend([(src + node_offset, dst + node_offset) for src, dst in graph_edges])\n",
        "                node_offset += input_ids.size(1)\n",
        "            edge_index = torch.tensor(batched_edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "        else:\n",
        "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "\n",
        "        model_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"edges\": edge_index,\n",
        "            \"pos_tag_ids\": pos_tag_ids,\n",
        "            \"edge_attrs\": edge_attrs,\n",
        "            \"past_key_values\": kwargs.get(\"past_key_values\", None),\n",
        "        }\n",
        "        return model_inputs\n",
        "\n",
        "# END GraphModel\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "#del model\n",
        "# Quantize Mistral (before creating GraphModel)\n",
        "mistral_model = prepare_model_for_kbit_training(mistral_model, use_gradient_checkpointing=True)\n",
        "\n",
        "\n",
        "# 5. Model Setup (Define model first)\n",
        "model = GraphModel(mistral_model, tokenizer)  # Pass both mistral_model and tokenizer\n",
        "#model.to(device)\n",
        "\n",
        "# 6. PEFT Configuration (Use automatic module discovery)\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    # Instead of targeting specific layers, let PEFT automatically discover the linear layers within the model\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "# 7. Apply PEFT\n",
        "model = get_peft_model(model, peft_config)\n",
        "print('\\n\\n')\n",
        "print('PEFT-Model')\n",
        "model.print_trainable_parameters() # To see the trainable parameters\n",
        "print('\\n')\n",
        "\n",
        "# Access the config of the encoder (Mistral model) within your GraphModel\n",
        "model.encoder.config.use_cache = False\n",
        "\n",
        "# Ensure that LoRA layers are properly initialized and their dimensions are correctly set\n",
        "for name, module in model.named_modules():\n",
        "    if \"lora\" in name:\n",
        "        module = module.to(device) # Move LoRA parameters to the correct device\n",
        "\n",
        "model.encoder.gradient_checkpointing_enable()  # Enable gradient checkpointing for memory optimization on the Mistral model\n",
        "#model.encoder.model.embed_tokens.requires_grad_(True)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# 8. Evaluation Metric (Semantic Similarity)\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert all elements to tensors, handling different data types\n",
        "    predictions = [torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred for pred in all_preds]\n",
        "    labels = [torch.tensor(label) if not isinstance(label, torch.Tensor) else label for label in all_labels]\n",
        "\n",
        "    # Filter out any None values before stacking\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Convert to tensors and stack (only if there are predictions/labels)\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])  # Empty tensor if no predictions\n",
        "\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])  # Empty tensor if no labels\n",
        "\n",
        "    # Handle cases where only one prediction/label is present (avoid squeezing to a scalar)\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print('\\n')\n",
        "    print(f\"Shape of logits in compute_metrics: {predictions.shape}\")\n",
        "    print(f\"Shape of labels in compute_metrics: {labels.shape}\")\n",
        "    ('\\n')\n",
        "\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    return {\"exact_match\": em}\n",
        "\n",
        "\n",
        "#/content/gdrive/MyDrive/model\n",
        "\n",
        "#9. Training Arguments and Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir =\"/content/gdrive/MyDrive/model/GNNT2SQL\",\n",
        "    logging_dir=\"/content/gdrive/MyDrive/model/GNN-T2SQL/logs\",\n",
        "\n",
        "    # Batch size and gradient accumulation\n",
        "    # ORIGINAL\n",
        "    #per_device_train_batch_size=1,\n",
        "    #gradient_accumulation_steps=8,\n",
        "\n",
        "    per_device_train_batch_size=1,  # Slightly increased, but be cautious\n",
        "    gradient_accumulation_steps=4,  # Adjusted for effective batch size of 4\n",
        "\n",
        "    # Number of epochs and early stopping\n",
        "    num_train_epochs=1,  # Start with a few epochs and monitor validation loss\n",
        "    #early_stopping_patience=3,  # Enable early stopping to prevent overfitting\n",
        "\n",
        "    # Learning rate and scheduler\n",
        "    learning_rate=5e-5,  # A reasonable starting point, adjust if needed\n",
        "    lr_scheduler_type=\"linear\",  # Or try other schedulers like \"cosine\"\n",
        "    warmup_steps=500,  # Warmup is crucial, especially with a larger learning rate\n",
        "\n",
        "    # Evaluation and saving\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,  # Evaluate less frequently to save time #500\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100, #500\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "\n",
        "    # Other settings\n",
        "    push_to_hub=False,\n",
        "    load_best_model_at_end=True,\n",
        "    use_legacy_prediction_loop=False,\n",
        "    metric_for_best_model=\"eval_exact_match\",\n",
        "    report_to=\"tensorboard\",\n",
        "    #generation_max_length=2048,  # Adjust if needed based on your data\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch_geometric.data import Batch as GraphBatch  # Note the import\n",
        "import torch\n",
        "import torch_geometric.data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class GraphDataCollatorForSeq2Seq:\n",
        "    def __init__(self, tokenizer, model=None, label_pad_token_id=-100, pad_to_multiple_of=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Separate standard features from graph edges\n",
        "        # Extract labels before padding and handle potentially empty sequences\n",
        "        labels = [feature[\"labels\"] if feature[\"labels\"].numel() > 0\n",
        "                  else torch.tensor([self.label_pad_token_id], dtype=torch.long)\n",
        "                  for feature in features]\n",
        "\n",
        "        # Extract sample_ids\n",
        "        sample_ids = [feature[\"sample_ids\"] for feature in features]\n",
        "\n",
        "        standard_features = [{k: v for k, v in feature.items() if k != \"edges\" and k != \"labels\"} for feature in features]\n",
        "        edges = [feature[\"edges\"] for feature in features]\n",
        "\n",
        "        # Collate standard features (input_ids, attention_mask) using default collator\n",
        "        collated_standard_features =  DataCollatorForSeq2Seq(\n",
        "            tokenizer=self.tokenizer,\n",
        "            model=self.model,\n",
        "            label_pad_token_id=self.label_pad_token_id,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of\n",
        "        )(standard_features)\n",
        "\n",
        "        # Pad input_ids and attention_mask\n",
        "        input_ids = pad_sequence([f['input_ids'] for f in standard_features], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([f['attention_mask'] for f in standard_features], batch_first=True, padding_value=0)\n",
        "\n",
        "         # Pad labels separately\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n",
        "\n",
        "        # Create batch for graph data\n",
        "        graph_data_list = []\n",
        "        for i in range(len(edges)):\n",
        "            # Convert to PyTorch Geometric Data\n",
        "            graph_data_list.append(torch_geometric.data.Data(\n",
        "                x=collated_standard_features['input_ids'][i].unsqueeze(1),  # Node features (input_ids)\n",
        "                edge_index=edges[i],                   # Edge index\n",
        "                # Use num_edges for batch index to ensure correct batching in PyG\n",
        "                batch=torch.tensor([i] * edges[i].size(1))\n",
        "            ))\n",
        "        batched_graph = GraphBatch.from_data_list(graph_data_list)  # Batch graphs\n",
        "\n",
        "        #print(f\"Sample GraphDataCollatorForSeq2Seq ID: {sample_ids}\")\n",
        "\n",
        "         # Include sample_ids in the collated features\n",
        "        collated_features = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edges,\n",
        "            \"sample_ids\": sample_ids  # Add sample_ids here\n",
        "        }\n",
        "\n",
        "        return collated_features\n",
        "\n",
        "\n",
        "# 10a.  Data Collator\n",
        "data_collator = GraphDataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "\n",
        "# 10AA. Training Arguments and Trainer\n",
        "\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_utils import EvalLoopOutput, PredictionOutput,  has_length\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from transformers import Trainer, TrainerCallback\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "#from transformers.trainer_pt_utils import nested_truncate  # Updated import#\n",
        "\n",
        "from transformers.trainer_pt_utils import nested_truncate, nested_concat, nested_numpify, nested_detach  # Updated imports\n",
        "\n",
        "\n",
        "######\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def estimated_num_samples(dataloader: DataLoader):\n",
        "    \"\"\"\n",
        "    This function will attempt to determine the number of samples in a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        dataloader (DataLoader): The DataLoader to estimate the number of samples from.\n",
        "\n",
        "    Returns:\n",
        "        int: The estimated number of samples, or 0 if estimation is not possible.\n",
        "    \"\"\"\n",
        "    if hasattr(dataloader, \"dataset\") and hasattr(dataloader.dataset, \"__len__\"):\n",
        "        return len(dataloader.dataset)  # Use dataset length if available\n",
        "    elif hasattr(dataloader, \"batch_sampler\") and hasattr(dataloader.batch_sampler, \"sampler\") and hasattr(dataloader.batch_sampler.sampler, \"__len__\"):\n",
        "        return len(dataloader.batch_sampler.sampler)  # Use sampler length if available\n",
        "    else:\n",
        "        # If neither is available, return 0\n",
        "        warnings.warn(\"Could not estimate the number of samples in the dataloader. Returning 0.\")\n",
        "        return 0\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    _id=0\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        labels = inputs.pop(\"labels\", None)\n",
        "\n",
        "        print('\\n\\n')\n",
        "        print(\"**** Prediction Step ****\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Access loss and logits based on the type of outputs\n",
        "            if isinstance(outputs, MyCausalLMOutputWithPast):\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "            elif isinstance(outputs, tuple):\n",
        "                # Check if the tuple has at least two elements before accessing\n",
        "                if len(outputs) >= 2:\n",
        "                    loss = outputs[0]  # Assuming loss is the first element\n",
        "                    logits = outputs[1]  # Assuming logits is the second element\n",
        "                else:\n",
        "                    # Handle the case where the tuple has fewer than two elements\n",
        "                    raise ValueError(\"Output tuple from model has fewer than two elements\")\n",
        "            else:\n",
        "                #raise ValueError(\"Unexpected output type from model\")\n",
        "                # Access loss and logits as attributes for other types\n",
        "                loss = outputs[\"loss\"] # Access loss from the dictionary\n",
        "                logits = outputs[\"logits\"] # Access logits from the dictionary\n",
        "\n",
        "                #loss = outputs.loss\n",
        "                #logits = outputs.logits\n",
        "\n",
        "            # Debugging Logging (ensure labels exist before accessing)\n",
        "            if not prediction_loss_only:\n",
        "                print('\\n\\n')\n",
        "                print(\"Shape of logits in prediction_step:\", logits.shape)\n",
        "                if labels is not None:  # Only print if labels exist\n",
        "                    print(\"Shape of labels in prediction_step:\", labels.shape)\n",
        "\n",
        "            if prediction_loss_only:\n",
        "                if isinstance(loss, torch.Tensor):\n",
        "                    loss = loss.mean().detach()\n",
        "                return (loss, None, None)\n",
        "\n",
        "            max_new_tokens = 1024 - inputs['input_ids'].shape[1]\n",
        "\n",
        "            #max_new_tokens = 1024\n",
        "\n",
        "\n",
        "\n",
        "            # Modify this part to handle the generated IDs\n",
        "            if max_new_tokens <= 0:\n",
        "                print('\\n\\n')\n",
        "                print(\"No new tokens to generate. An increase in the sample size for training is required. \")\n",
        "                #print(\"#TOKENS: \",max_new_tokens)\n",
        "                # Input is already at max length, no need to generate\n",
        "                generated_ids = inputs['input_ids']\n",
        "            else:\n",
        "                generated_ids = model.encoder.generate(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=5\n",
        "                )\n",
        "\n",
        "            # Check the shape of generated_ids\n",
        "            #print(\"Shape of generated_ids:\", generated_ids.shape)\n",
        "\n",
        "\n",
        "\n",
        "        # Flatten the generated_ids to a 1D list before decoding\n",
        "        flattened_generated_ids = generated_ids.view(-1).tolist()\n",
        "        predictions=generated_ids\n",
        "\n",
        "        # Now decode using the flattened list\n",
        "        predictions_decoder = self.tokenizer.decode(flattened_generated_ids, skip_special_tokens=True)\n",
        "        Q = self.tokenizer.decode(inputs['input_ids'].view(-1).tolist(), skip_special_tokens=True)\n",
        "        A = self.tokenizer.decode(labels.view(-1).tolist(), skip_special_tokens=True)\n",
        "        #print(\"Sample IDs in prediction_step:\", inputs['sample_ids'][0])\n",
        "\n",
        "\n",
        "        # Debugging Logging\n",
        "        #print(\"Shape of predictions in prediction_step:\", predictions.shape)\n",
        "        if labels is not None:\n",
        "            print('\\n\\n')\n",
        "            # Extract sample_ids\n",
        "            sample_ids = int(self._id)+1  # Use self._id here\n",
        "            #print(\"Sample IDs in prediction_step:\", sample_ids)\n",
        "            print(\"Sample IDs in prediction_step:\", inputs['sample_ids'])\n",
        "\n",
        "            print(\"Question:\", Q)\n",
        "            print(\"Decoded Original Answer BEFORE Predictions:\", A)\n",
        "            print('\\n\\n')\n",
        "\n",
        "            print(\"Decoded Predictions:\", predictions_decoder)\n",
        "\n",
        "            return (loss, predictions, labels)\n",
        "\n",
        "    def _prediction_loop(self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = \"eval\") -> Union[Tuple[torch.Tensor, torch.Tensor], EvalPrediction]:\n",
        "\n",
        "        # ... other parts remain same ...\n",
        "\n",
        "         # In case you have a callback that needs length eventually\n",
        "        if has_length(dataloader):\n",
        "            num_samples = len(dataloader.dataset)\n",
        "        # The dataset does not support __len__, estimate the number of samples.\n",
        "        else:\n",
        "            num_samples = estimated_num_samples(dataloader)\n",
        "\n",
        "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
        "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
        "        if all_losses:\n",
        "            all_losses = all_losses[:num_samples]\n",
        "        if all_preds:\n",
        "            all_preds = nested_truncate(all_preds, num_samples)\n",
        "        if all_labels:\n",
        "            all_labels = nested_truncate(all_labels, num_samples)\n",
        "\n",
        "        # 8.  Compute Metrics and Average Loss\n",
        "        metrics = self.compute_metrics((all_preds, all_labels))\n",
        "        average_loss = torch.mean(torch.stack(all_losses))\n",
        "        metrics[f\"{metric_key_prefix}_loss\"] = average_loss.item()\n",
        "\n",
        "        # 9.  Log the Metrics\n",
        "        self.log(metrics)\n",
        "\n",
        "        # 10. Return Based on Whether It's a Prediction or Evaluation\n",
        "        if prediction_loss_only:\n",
        "            return (metrics, None, None)\n",
        "\n",
        "        return EvalPrediction(predictions=all_preds, label_ids=all_labels, metrics=metrics)\n",
        "\n",
        "\n",
        "    def predict(self, test_dataset: Dataset, ignore_keys: Optional[List[str]] = None) -> PredictionOutput:\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "        return self.prediction_loop(test_dataloader, description=\"Prediction\")\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ) -> Dict[str, float]:\n",
        "        # ... (existing code in Trainer.evaluate)\n",
        "\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self.prediction_loop(\n",
        "            eval_dataloader,\n",
        "            description=\"Evaluation\",\n",
        "            # ...\n",
        "        )\n",
        "\n",
        "        # ... (rest of the existing code)\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "\n",
        "######\n",
        "\n",
        "# 10A. Trainer (Modified)\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import TrainerCallback\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 50000 == 0:  # Log every 100 steps (adjust as needed)\n",
        "            print(f\"Step {state.global_step} - Loss: {state.loss}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 11. Train the model\n",
        "\n",
        "# Add the Callback to the Trainer\n",
        "trainer.add_callback(LossLoggingCallback())\n",
        "\n",
        "# Add the Early Stopping to the Trainer\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "#test_results = trainer.evaluate(eval_dataset)\n",
        "#print('\\n\\n')\n",
        "#print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n",
        "#print(f'Test Exact Match. (Evaluate on the test set): {test_results[\"eval_exact_match\"]:.4f}')\n",
        "#print('\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Units per Hour: You're using 4.82 compute units every hour.\n",
        "\n",
        "2. Total Time Available:\n",
        "\n",
        "Divide your total compute units by your hourly usage rate: 309.66 units / 4.82 units/hour = 64.24 hours"
      ],
      "metadata": {
        "id": "U-VssbduKp8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STAND-ALONE EVALUATOR"
      ],
      "metadata": {
        "id": "Reg6DPZJP2Mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Logical Correctness?\n",
        "\n",
        "In the realm of T2SQL (Text-to-SQL), logical correctness goes beyond mere syntactic accuracy and execution success. It evaluates whether the generated SQL query genuinely captures the intent and nuances of the natural language input, ensuring that it retrieves the desired information from the database.\n",
        "\n",
        "Consider these scenarios where a generated SQL query might be syntactically valid and even execute without errors, yet still be logically incorrect:\n",
        "\n",
        "Incorrect column or table selection: The query might fetch data from the wrong columns or tables, resulting in irrelevant or inaccurate results.\n",
        "Incorrect filtering or aggregation: The WHERE clause or aggregation functions (e.g., SUM, COUNT) might be misapplied, leading to filtered or aggregated data that doesn't align with the user's intent.\n",
        "Incorrect joins: If the natural language input implies relationships between multiple tables, the generated query might have incorrect or missing joins, producing misleading results.\n",
        "Subtle semantic mismatches: Even if the query produces results, they might not fully capture the nuances and implied meaning of the original question or instruction.\n",
        "Why is it Important?\n",
        "\n",
        "While execution accuracy is crucial, logical correctness ensures that the generated SQL truly \"understands\" the user's request and provides the right answer, not just an answer. It's a key indicator of the T2SQL model's ability to reason about the data schema and the user's intent.\n",
        "\n",
        "How to Measure Logical Correctness\n",
        "\n",
        "Measuring logical correctness can be challenging, as it often requires a deeper understanding of the underlying data schema and the subtle nuances of natural language. Here are some common approaches:\n",
        "\n",
        "Manual Inspection:\n",
        "\n",
        "A human evaluator examines a sample of generated queries and their corresponding ground truth queries to assess if they capture the same intent.\n",
        "Pros: Provides valuable qualitative insights and can catch subtle semantic mismatches.\n",
        "Cons: Time-consuming and not scalable for large datasets.\n",
        "SQL Parsing and Comparison:\n",
        "\n",
        "Use a SQL parser to extract structural components (e.g., SELECT, FROM, WHERE, GROUP BY) from both generated and ground truth queries.\n",
        "Compare these components to identify mismatches or inconsistencies.\n",
        "Pros: Can be automated and is relatively scalable.\n",
        "Cons: Might miss subtle semantic differences or struggle with complex SQL constructs.\n",
        "Result Comparison:\n",
        "\n",
        "Execute both the generated and ground truth queries against the database.\n",
        "Compare the results to see if they are equivalent or sufficiently similar.\n",
        "Pros: Directly measures the impact of logical errors on the output.\n",
        "Cons: Requires access to the database, can be computationally expensive, and might not capture all types of logical errors.\n",
        "Hybrid Approaches:\n",
        "\n",
        "Combine manual inspection with automated techniques to leverage their strengths.\n",
        "For example, use SQL parsing to identify potential logical errors and then have a human evaluator review those cases.\n",
        "Choosing the Right Approach\n",
        "\n",
        "The ideal approach for measuring logical correctness depends on factors like:\n",
        "\n",
        "Data schema complexity: More complex schemas might require more sophisticated techniques.\n",
        "Evaluation scale: Manual inspection might be feasible for smaller datasets, while automated methods are necessary for larger ones.\n",
        "Available resources: Access to a database and computational power will influence the feasibility of certain approaches.\n",
        "Desired level of rigor: The trade-off between efficiency and thoroughness will guide your choice.\n",
        "Remember, evaluating logical correctness is an ongoing challenge in T2SQL research. Combining multiple approaches and iteratively refining your evaluation metrics will lead to more robust and reliable T2SQL models.\n"
      ],
      "metadata": {
        "id": "vSl-62o4P-Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import load_metric\n",
        "import sqlparse\n",
        "import psycopg2\n",
        "\n",
        "# Assuming you have 'tokenizer' and 'sentence_transformer_model' defined elsewhere\n",
        "\n",
        "# Load the metric for exact match calculation\n",
        "metric = load_metric(\"exact_match\")\n",
        "\n",
        "def compute_metrics(eval_pred, db_config=None):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert predictions and labels to tensors\n",
        "    predictions = [\n",
        "        torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred\n",
        "        for pred in all_preds\n",
        "    ]\n",
        "    labels = [\n",
        "        torch.tensor(label) if not isinstance(label, torch.Tensor) else label\n",
        "        for label in all_labels\n",
        "    ]\n",
        "\n",
        "    # Filter out None values\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Stack predictions and labels if they exist\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])\n",
        "\n",
        "    # Handle single prediction/label cases\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute exact match accuracy\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    # Compute semantic similarity\n",
        "    embeddings_pred = sentence_transformer_model.encode(decoded_preds)\n",
        "    embeddings_labels = sentence_transformer_model.encode(decoded_labels)\n",
        "    semantic_similarity = util.cos_sim(embeddings_pred, embeddings_labels).mean()\n",
        "\n",
        "    # Execution Accuracy (if db_config is provided)\n",
        "    if db_config:\n",
        "        execution_accuracy_scores = []\n",
        "        with psycopg2.connect(**db_config) as conn:\n",
        "            with conn.cursor() as cur:\n",
        "                for pred_sql, label_sql in zip(decoded_preds, decoded_labels):\n",
        "                    try:\n",
        "                        # Execute predicted query\n",
        "                        cur.execute(pred_sql)\n",
        "                        pred_results = cur.fetchall()\n",
        "\n",
        "                        # Execute label (ground truth) query\n",
        "                        cur.execute(label_sql)\n",
        "                        label_results = cur.fetchall()\n",
        "\n",
        "                        # Compare results\n",
        "                        if pred_results == label_results:\n",
        "                            execution_accuracy_scores.append(1)\n",
        "                        else:\n",
        "                            execution_accuracy_scores.append(0)\n",
        "\n",
        "                            # Optional: Log mismatches for debugging\n",
        "                            logging.debug(f\"Mismatch: Predicted: {pred_results}, Label: {label_results}\")\n",
        "\n",
        "                    except psycopg2.Error as e:\n",
        "                        execution_accuracy_scores.append(0)\n",
        "                        logging.error(f\"Error executing SQL: {e}\")\n",
        "\n",
        "        execution_accuracy = sum(execution_accuracy_scores) / len(execution_accuracy_scores)\n",
        "    else:\n",
        "        execution_accuracy = 0.0\n",
        "\n",
        "    # Logical Correctness (SQL Parsing and Comparison)\n",
        "    logical_correctness_scores = []\n",
        "    for pred_sql, label_sql in zip(decoded_preds, decoded_labels):\n",
        "        parsed_pred = sqlparse.parse(pred_sql)[0]\n",
        "        parsed_label = sqlparse.parse(label_sql)[0]\n",
        "\n",
        "        pred_components = {\n",
        "            \"select\": [token.value for token in parsed_pred.tokens if isinstance(token, sqlparse.sql.IdentifierList)],\n",
        "            \"from\": [token.value for token in parsed_pred.tokens if isinstance(token, sqlparse.sql.Identifier)],\n",
        "            # ... extract other components like WHERE, GROUP BY, etc.\n",
        "        }\n",
        "        label_components = {\n",
        "            \"select\": [token.value for token in parsed_label.tokens if isinstance(token, sqlparse.sql.IdentifierList)],\n",
        "            \"from\": [token.value for token in parsed_label.tokens if isinstance(token, sqlparse.sql.Identifier)],\n",
        "            # ... extract other components like WHERE, GROUP BY, etc., using the same logic as for pred_components\n",
        "        }\n",
        "\n",
        "        score = 0\n",
        "        for component_type in [\"select\", \"from\", ...]:\n",
        "            if pred_components[component_type] == label_components[component_type]:\n",
        "                score += 1\n",
        "            elif set(pred_components[component_type]) == set(label_components[component_type]):\n",
        "                score += 0.5\n",
        "            # ... add more sophisticated comparison logic if needed\n",
        "\n",
        "        logical_correctness_scores.append(score)\n",
        "\n",
        "    logical_correctness = sum(logical_correctness_scores) / len(logical_correctness_scores)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": em,\n",
        "        \"semantic_similarity\": semantic_similarity.item(),\n",
        "        \"execution_accuracy\": execution_accuracy,\n",
        "        \"logical_correctness\": logical_correctness,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "OLMO34VVPk9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postgresql Setup"
      ],
      "metadata": {
        "id": "fhEZZJMdzZuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 01/06/2024\n",
        "!apt-get update -y\n",
        "!apt-get install postgresql-14 -y\n",
        "\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "-h27HimSyEY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u postgres psql -c \"CREATE USER postgres WITH SUPERUSER\"\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "ERROR:  role \"postgres\" already exists\n",
        "ALTER ROLE\n",
        "\n",
        "QUERY_create='CREATE TABLE table_name_24 (score VARCHAR, date VARCHAR)'\n",
        "\n",
        "\n",
        "QUERY_select='SELECT 2009 FROM table_name_50 WHERE 2011 = \"a\"'"
      ],
      "metadata": {
        "id": "o2CRZUowyOgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def table_creator(query):\n",
        "    import os\n",
        "    import psycopg2 as ps\n",
        "    import pandas as pd\n",
        "\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                  user=DB_USER,\n",
        "                  password=DB_PASS,\n",
        "                  host=DB_HOST,\n",
        "                  port=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    # Wrap the execute command in a try-except block to handle potential errors\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "                            %s\n",
        "                            \"\"\"%query)\n",
        "        conn.commit()\n",
        "        print(\"Table Created successfully\")\n",
        "    except Exception as e:\n",
        "        conn.rollback() # Rollback the transaction in case of an error\n",
        "        print(\"Error creating table:\", e)\n",
        "\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "QSIg-lixyyi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\""
      ],
      "metadata": {
        "id": "UCljf57lzQOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "def table_select(query):\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                      user=DB_USER,\n",
        "                      password=DB_PASS,\n",
        "                      host=DB_HOST,\n",
        "                      port=DB_PORT)\n",
        "    print(\"Database connected successfully\")\n",
        "\n",
        "    #query = query.replace('\"', \"'\") # Replace double quotes with single quotes for potential date values\n",
        "\n",
        "    try:\n",
        "\n",
        "        #df = pd.read_sql_query(\"%s\"%query, con=conn)\n",
        "        #print('rec: %'%df) # Print the resulting DataFrame\n",
        "\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(query)\n",
        "        rows = cur.fetchall()\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print('\\n')\n",
        "        print('Record(s): %s \\n'%len(rows))\n",
        "        for row in rows:\n",
        "            print(row)\n",
        "\n",
        "\n",
        "        eqc=1\n",
        "\n",
        "    except Exception as e:\n",
        "        eqc=0\n",
        "        #conn.rollback() # Rollback the transaction in case of an error\n",
        "        print(\"Error executing query:\", e)\n",
        "        #print('TABLE IS EMPTY')\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    return eqc"
      ],
      "metadata": {
        "id": "nN2BDEMnzMUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_creator(QUERY_create)"
      ],
      "metadata": {
        "id": "YtGbYghLzuNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluator FULL"
      ],
      "metadata": {
        "id": "GoAUvQWiz4ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import psycopg2\n",
        "from transformers import Trainer\n",
        "\n",
        "# Your PostgreSQL configuration\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "\n",
        "def evaluate_model(model, tokenizer, sentence_transformer_model, eval_dataset):\n",
        "    \"\"\"\n",
        "    Evaluates a model on a given dataset and computes exact match and semantic similarity metrics,\n",
        "    after creating necessary tables in the PostgreSQL database.\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned model to be evaluated.\n",
        "        tokenizer: The tokenizer used for encoding and decoding text.\n",
        "        sentence_transformer_model: The sentence transformer model used for semantic similarity.\n",
        "        eval_dataset: The dataset on which the model will be evaluated.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the evaluation results: 'exact_match' and 'semantic_similarity'.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Table Creation\n",
        "    db_config = {\n",
        "        'database': DB_NAME,\n",
        "        'user': DB_USER,\n",
        "        'password': DB_PASS,\n",
        "        'host': DB_HOST,\n",
        "        'port': DB_PORT\n",
        "    }\n",
        "\n",
        "    with psycopg2.connect(**db_config) as conn:\n",
        "        with conn.cursor() as cur:\n",
        "            for example in eval_dataset.dataset:\n",
        "                context = example['context']\n",
        "                create_table_statements = context.split(';')\n",
        "                for create_table_statement in create_table_statements:\n",
        "                    if create_table_statement.strip():\n",
        "                        try:\n",
        "                            cur.execute(create_table_statement)\n",
        "                            conn.commit()\n",
        "                        except psycopg2.Error as e:\n",
        "                            logging.error(f\"Error creating table: {e}\")\n",
        "\n",
        "    # Create a Trainer instance with the compute_metrics function\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, db_config),  # Pass db_config\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    results = trainer.evaluate(eval_dataset)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "1V9Ps80FPswP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your fine-tuned model (replace 'path/to/your/model' with the actual path)\n",
        "#model = PeftModel.from_pretrained(\"path/to/your/model\")\n",
        "#output_dir = \"/content/gdrive/MyDrive/model/GNNT2SQL\"\n",
        "\n",
        "checkpoint_value = str(500)\n",
        "fine_tune_model_path = \"%/checkpoint-%s\"%(output_dir, checkpoint_value)\n",
        "model = PeftModel.from_pretrained(fine_tune_model_path)\n",
        "\n",
        "# Load the tokenizer (replace 'your_tokenizer_name' with the actual one)\n",
        "#tokenizer = AutoTokenizer.from_pretrained('your_tokenizer_name')\n",
        "\n",
        "# Load the sentence transformer model\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Prepare your evaluation dataset (replace 'your_eval_dataset' with the actual dataset)\n",
        "#eval_dataset = load_dataset(\"your_eval_dataset\")\n",
        "\n",
        "# Evaluate the model\n",
        "results = evaluate_model(model, tokenizer, sentence_transformer_model, eval_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(f'Exact Match: {results[\"eval_exact_match\"]:.4f}')\n",
        "print(f'Semantic Similarity: {results[\"eval_semantic_similarity\"]:.4f}')"
      ],
      "metadata": {
        "id": "SmKGtZ68PwZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION-TRAINER"
      ],
      "metadata": {
        "id": "jnxGtqh44nh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# Manually define splits\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
        "test_dataset = dataset.select(range(train_size + val_size, len(dataset)))\n",
        "\n",
        "\n",
        "POC_valsample=100\n",
        "val_dataset = val_dataset.select(np.random.choice(len(val_dataset), POC_valsample, replace=False))\n",
        "test_dataset = test_dataset.select(np.random.choice(len(test_dataset), POC_valsample, replace=False))\n",
        "\n",
        "\n",
        "val_dataset = TextToSQLDataset(val_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)\n",
        "\n",
        "\n",
        "# 13. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print('\\n\\n')\n",
        "#print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n",
        "print(f'Test Exact Match. (Evaluate on the test set): {test_results[\"eval_exact_match\"]:.4f}')\n",
        "print('\\n\\n')"
      ],
      "metadata": {
        "id": "sgzKGUeY2Yxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION STAND-ALONE\n"
      ],
      "metadata": {
        "id": "2hAPAwznG6Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, Trainer, TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    PrefixTuningConfig,\n",
        "    PromptEncoderConfig,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch_geometric.nn import GAT\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"The installed version of bitsandbytes was compiled without GPU support.\")\n",
        "\n",
        "\n",
        "# Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    # Download if not already downloaded\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "ENHJRQaU91v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "\n",
        "#from transformers import CausalLMOutputWithPast\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class MyCausalLMOutputWithPast:\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "class GraphModel(nn.Module):\n",
        "    def __init__(self, encoder, tokenizer):\n",
        "        super(GraphModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config = encoder.model.config\n",
        "        self.gatv2 = GATv2Conv(\n",
        "            in_channels=self.config.hidden_size,\n",
        "            out_channels=self.config.hidden_size,\n",
        "            heads=8,\n",
        "            concat=False,\n",
        "        )\n",
        "\n",
        "        # Max Pooling\n",
        "        self.pool = lambda x, batch: torch.max(x, dim=0, keepdim=True)[0]\n",
        "\n",
        "        # Additional Feedforward Layer\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(self.config.hidden_size, self.config.hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.config.hidden_size * 2, self.config.hidden_size),\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Add generation config (you might need to adjust this based on your specific needs)\n",
        "        self.generation_config = encoder.generation_config\n",
        "\n",
        "    # Forward Pass\n",
        "    #def forward(self, input_ids, attention_mask, edges, labels=None, inputs_embeds=None, sample_ids=None, output_attentions=False, output_hidden_states=False, return_dict=False):\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, edges, labels=None, inputs_embeds=None, sample_ids=None,\n",
        "                output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Print vocabulary sizes\n",
        "        #print('\\n')\n",
        "        #print(\"Mistral Model Vocab Size:\", self.encoder.config.vocab_size)\n",
        "        #print(\"Tokenizer Vocab Size:\", self.tokenizer.vocab_size)\n",
        "        #print('\\n')\n",
        "\n",
        "\n",
        "        # 1. Token Embeddings (Encoder)\n",
        "        if input_ids is not None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids.to(self.encoder.model.device),\n",
        "                attention_mask=attention_mask.to(self.encoder.model.device),\n",
        "                #output_hidden_states=True\n",
        "            )\n",
        "            embeddings = encoder_outputs.hidden_states[-1]\n",
        "            print(\"Encoder output shape:\", embeddings.shape)  # Inspect encoder output\n",
        "\n",
        "        elif inputs_embeds is not None:\n",
        "            embeddings = inputs_embeds\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        # Ensure correct shape for GATv2Conv\n",
        "        if embeddings.dim() > 2:\n",
        "            embeddings = embeddings.view(-1, embeddings.shape[-1])\n",
        "\n",
        "        # 2. Edge Index Creation (Batched, with Enhanced Error Handling)\n",
        "        edge_index = []\n",
        "        node_offset = 0\n",
        "        for i, graph_edges in enumerate(edges):\n",
        "            if graph_edges is None or graph_edges.numel() == 0:  # Check if graph_edges is None or empty\n",
        "                num_nodes = input_ids.size(1)\n",
        "                # Create self-loops for isolated nodes if no edges are provided\n",
        "                graph_edges = torch.arange(node_offset, node_offset + num_nodes, device=embeddings.device)\n",
        "                graph_edges = graph_edges.repeat(2, 1) # Repeat the tensor, not the arange object\n",
        "            else: # Add this else block to handle the case when edges are present\n",
        "                # Ensure graph_edges is a tensor before adding offset\n",
        "                if not isinstance(graph_edges, torch.Tensor):\n",
        "                    graph_edges = torch.tensor(graph_edges, dtype=torch.long, device=embeddings.device)\n",
        "                graph_edges += node_offset  # Now safe to add offset\n",
        "            edge_index.append(graph_edges)\n",
        "            node_offset += input_ids.size(1)\n",
        "        edge_index = torch.cat(edge_index, dim=1)\n",
        "\n",
        "        # 3. GATv2 Layer\n",
        "        graph_out = self.gatv2(embeddings, edge_index)\n",
        "\n",
        "        # 3. GATv2 Layer\n",
        "        graph_out = self.gatv2(embeddings, edge_index)\n",
        "        print(\"GATv2 output shape:\", graph_out.shape)  # Inspect GATv2 output\n",
        "\n",
        "        # Print logits before and after applying logits processor\n",
        "        print(\"Original logits:\", logits)\n",
        "        filtered_logits = logits_processor(None, logits)\n",
        "        print(\"Filtered logits:\", filtered_logits)\n",
        "\n",
        "\n",
        "        # 4. Pooling (using max pooling)\n",
        "        batch = torch.arange(len(edges), device=graph_out.device).repeat_interleave(input_ids.size(1))\n",
        "        pooled = self.pool(graph_out, batch).unsqueeze(1)\n",
        "\n",
        "        # Additional Feedforward Layer\n",
        "        pooled = self.ffn(pooled)\n",
        "\n",
        "        # 5. LM Head\n",
        "        logits = self.lm_head(pooled)\n",
        "\n",
        "        from torch.nn import CrossEntropyLoss\n",
        "        # 6. Loss Calculation (if labels provided)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "\n",
        "             # Print Sample IDs first\n",
        "            print(f\"\\nIteration/Step: {trainer.state.global_step}\")\n",
        "            mask = (labels != -100).float()\n",
        "\n",
        "            #for i, sample_id in enumerate(sample_ids):\n",
        "            #    print(f\"Sample ID: {sample_id.item()}\")\n",
        "\n",
        "\n",
        "            # Now decode and print input, target, and loss for each sample\n",
        "            with torch.no_grad():\n",
        "                input_text = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "                target_text = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "                for i, sample_id in enumerate(sample_ids):\n",
        "                    print(f\"Sample ID: {sample_id.item()}\")\n",
        "                    print(\"Decoded Input:\", input_text[i])\n",
        "                    print(\"Decoded Target (Labels):\", target_text[i])\n",
        "\n",
        "\n",
        "\n",
        "            # Apply softmax to logits to get probabilities\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            #loss_fct = nn.NLLLoss(ignore_index=-100)  # Use NLLLoss for log probabilities\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "            # Reshape log_probs to match target shape (remove the extra dimension)\n",
        "            log_probs = log_probs.squeeze(1)\n",
        "\n",
        "\n",
        "            #labels = labels[:, 0] ### WORK UNTIL BEFORE THE LAST INTERACTION\n",
        "            #labels = labels.squeeze(1)   # Remove extra dimension from labels ## DON'T work\n",
        "            #labels = labels.view(-1)  # Flatten labels to a 1D tensor DON'T work\n",
        "\n",
        "            labels = labels[:, 0]\n",
        "\n",
        "            # Calculate loss with the correctly shaped labels\n",
        "            #loss = loss_fct(log_probs, labels[:, 0])  # Access the first element of each label sequence\n",
        "\n",
        "\n",
        "            loss = loss_fct(log_probs, labels)\n",
        "\n",
        "\n",
        "\n",
        "            loss_per_sample = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "            print(f\"Loss per sample: {loss_per_sample.item()}\")\n",
        "\n",
        "\n",
        "        # 7. Return (Modified to return a tuple)\n",
        "            if labels is not None:\n",
        "                return (loss, logits, None)\n",
        "            else:\n",
        "                return (None, logits, None)\n",
        "\n",
        "        return MyCausalLMOutputWithPast(\n",
        "                loss=loss,\n",
        "                logits=logits,\n",
        "                past_key_values=encoder_outputs.past_key_values,\n",
        "                hidden_states=encoder_outputs.hidden_states,\n",
        "                attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, edges, attention_mask=None, **kwargs):\n",
        "        if isinstance(self, PeftModel):\n",
        "            return self.base_model.prepare_inputs_for_generation(input_ids, edges, attention_mask, **kwargs)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        if batch_size > 1:\n",
        "            batched_edges = []\n",
        "            node_offset = 0\n",
        "            for i in range(batch_size):\n",
        "                graph_edges = edges[i]\n",
        "                batched_edges.extend([(src + node_offset, dst + node_offset) for src, dst in graph_edges])\n",
        "                node_offset += input_ids.size(1)\n",
        "            edge_index = torch.tensor(batched_edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "        else:\n",
        "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "\n",
        "        model_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"edges\": edge_index,\n",
        "            \"past_key_values\": kwargs.get(\"past_key_values\", None),\n",
        "        }\n",
        "        return model_inputs\n",
        "\n",
        "# END GraphModel"
      ],
      "metadata": {
        "id": "BTT38RlOEhYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch_geometric.data import Batch as GraphBatch  # Note the import\n",
        "import torch\n",
        "import torch_geometric.data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class GraphDataCollatorForSeq2Seq:\n",
        "    def __init__(self, tokenizer, model=None, label_pad_token_id=-100, pad_to_multiple_of=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Separate standard features from graph edges\n",
        "        # Extract labels before padding and handle potentially empty sequences\n",
        "        labels = [feature[\"labels\"] if feature[\"labels\"].numel() > 0\n",
        "                  else torch.tensor([self.label_pad_token_id], dtype=torch.long)\n",
        "                  for feature in features]\n",
        "\n",
        "        # Extract sample_ids\n",
        "        sample_ids = [feature[\"sample_ids\"] for feature in features]\n",
        "\n",
        "        standard_features = [{k: v for k, v in feature.items() if k != \"edges\" and k != \"labels\"} for feature in features]\n",
        "        edges = [feature[\"edges\"] for feature in features]\n",
        "\n",
        "        # Collate standard features (input_ids, attention_mask) using default collator\n",
        "        collated_standard_features =  DataCollatorForSeq2Seq(\n",
        "            tokenizer=self.tokenizer,\n",
        "            model=self.model,\n",
        "            label_pad_token_id=self.label_pad_token_id,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of\n",
        "        )(standard_features)\n",
        "\n",
        "        # Pad input_ids and attention_mask\n",
        "        input_ids = pad_sequence([f['input_ids'] for f in standard_features], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([f['attention_mask'] for f in standard_features], batch_first=True, padding_value=0)\n",
        "\n",
        "         # Pad labels separately\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n",
        "\n",
        "        # Create batch for graph data\n",
        "        graph_data_list = []\n",
        "        for i in range(len(edges)):\n",
        "            # Convert to PyTorch Geometric Data\n",
        "            graph_data_list.append(torch_geometric.data.Data(\n",
        "                x=collated_standard_features['input_ids'][i].unsqueeze(1),  # Node features (input_ids)\n",
        "                edge_index=edges[i],                   # Edge index\n",
        "                # Use num_edges for batch index to ensure correct batching in PyG\n",
        "                batch=torch.tensor([i] * edges[i].size(1))\n",
        "            ))\n",
        "        batched_graph = GraphBatch.from_data_list(graph_data_list)  # Batch graphs\n",
        "\n",
        "        #sample_ids = torch.arange(len(edges)).unsqueeze(1)\n",
        "        #sample_ids = [f[\"sample_id\"] for f in  collated_standard_features]\n",
        "        #sample_ids = [f[\"sample_id\"] for f in features]\n",
        "\n",
        "        #print(f\"Sample GraphDataCollatorForSeq2Seq ID: {sample_ids}\")\n",
        "\n",
        "\n",
        "\n",
        "         # Include sample_ids in the collated features\n",
        "        collated_features = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edges,\n",
        "            \"sample_ids\": sample_ids  # Add sample_ids here\n",
        "        }\n",
        "\n",
        "        return collated_features\n"
      ],
      "metadata": {
        "id": "g9qhWPQiQBPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Mistral Model and Tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Model Configuration\n",
        "config = AutoModelForCausalLM.from_pretrained(model_id).config\n",
        "config.output_hidden_states = True\n",
        "config.use_cache = False\n",
        "#config.torch_dtype = torch.float32\n",
        "config.torch_dtype = torch.bfloat16\n",
        "\n",
        "# Load Model with Quantization\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"flash_attention_2\",  # Optimization\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "DrXB9McwDzmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "\n",
        "        # 1. Tokenization (with padding and truncation)\n",
        "        tokenized_input = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,  # Increase as needed\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized_target = self.tokenizer(\n",
        "            target_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,  # Increase as needed\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Flatten lists (if needed)\n",
        "        tokenized_input = {k: v.squeeze(0) for k, v in tokenized_input.items()}\n",
        "        tokenized_target = {k: v.squeeze(0) for k, v in tokenized_target.items()}\n",
        "\n",
        "\n",
        "        # Print input text and target text for debugging\n",
        "        #print('\\n')\n",
        "        #print(f\"Sample ID: {idx}\")\n",
        "        #print(f\"  - Text: {text}\")\n",
        "        #print(f\"  - Target Text: {target_text}\")\n",
        "        #print('\\n')\n",
        "\n",
        "        sample_ids = torch.tensor([idx])  # Create a tensor with the sample ID\n",
        "\n",
        "        # 2. Dependency Parsing for Edge Extraction\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        for token in doc:\n",
        "            head_i = token.head.i\n",
        "            if 0 <= head_i < len(doc) and token.dep_ != \"ROOT\" and token.i != head_i:\n",
        "                edges.append([token.i, head_i])\n",
        "\n",
        "        # Edge Index Extraction and Validation\n",
        "        edges = item.get(\"edges\", edges)  # If \"edges\" is already present in data, use that\n",
        "\n",
        "        if not edges:  # Handle empty graphs\n",
        "            num_nodes = len(tokenized_input[\"input_ids\"])\n",
        "            edges = [[i, i] for i in range(num_nodes)]  # Self-loops for isolated nodes\n",
        "        else:\n",
        "            max_index = len(tokenized_input[\"input_ids\"]) - 1\n",
        "            edges = [(src, tgt) for src, tgt in edges if 0 <= src <= max_index and 0 <= tgt <= max_index]\n",
        "\n",
        "        # Create edge index tensor\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # Convert everything to tensors BEFORE padding/truncation\n",
        "        input_ids = tokenized_input[\"input_ids\"].clone().detach()\n",
        "        attention_mask = tokenized_input[\"attention_mask\"].clone().detach()\n",
        "        labels = tokenized_target[\"input_ids\"].clone().detach()\n",
        "\n",
        "        # Handle potentially empty target sequences\n",
        "        if len(labels) == 0:\n",
        "            labels = torch.tensor([self.tokenizer.pad_token_id], dtype=torch.long)  # Create a single-element tensor with pad token\n",
        "\n",
        "        # Padding and Truncation for consistent input shapes\n",
        "        max_length = 1024  # Adjust if needed\n",
        "\n",
        "        # Ensure that ALL tensors are truncated/padded to the SAME max_length\n",
        "        input_ids = input_ids[:max_length]\n",
        "        attention_mask = attention_mask[:max_length]\n",
        "        labels = labels[:max_length]\n",
        "\n",
        "        # Add padding if necessary\n",
        "        if len(input_ids) < max_length:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            pad_tensor = torch.full((pad_length,), self.tokenizer.pad_token_id)\n",
        "            input_ids = torch.cat((input_ids, pad_tensor))\n",
        "            attention_mask = torch.cat((attention_mask, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        if len(labels) < max_length:\n",
        "            pad_length = max_length - len(labels)\n",
        "            labels = torch.cat((labels, torch.full((pad_length,), -100)))  # Pad labels with -100\n",
        "\n",
        "        # (Optional) Print statements for debugging\n",
        "        #print(\"\\n\")\n",
        "        #print(\"Original text:\", text)\n",
        "        #print(\"Target text:\", target_text)\n",
        "        #print(\"Tokenized input IDs:\", tokenized_input[\"input_ids\"])\n",
        "        #print(\"Tokenized target IDs:\", tokenized_target[\"input_ids\"])\n",
        "        #print(\"Labels:\", labels)\n",
        "        #print(\"Edge index:\", edge_index)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edge_index,\n",
        "            \"sample_ids\": torch.tensor([idx]),  # Add sample_id here\n",
        "        }"
      ],
      "metadata": {
        "id": "vhh1m2r_DYbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# Manually define splits\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "eval_dataset = dataset.select(range(train_size, train_size + val_size))\n",
        "test_dataset = dataset.select(range(train_size + val_size, len(dataset)))\n",
        "\n",
        "\n",
        "POC_valsample=2\n",
        "eval_dataset = eval_dataset.select(np.random.choice(len(eval_dataset), POC_valsample, replace=False))\n",
        "test_dataset = test_dataset.select(np.random.choice(len(test_dataset), POC_valsample, replace=False))\n",
        "\n",
        "\n",
        "\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)"
      ],
      "metadata": {
        "id": "QCe75mw-5mkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = dataset.select(range(train_size, train_size + val_size))\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "eval_dataset[0]"
      ],
      "metadata": {
        "id": "nCiF95oQG_Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ORIGINAL"
      ],
      "metadata": {
        "id": "9QB1kXab0rHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=None, edges=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    #print(model)\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    #print('\\n')\n",
        "    #print(model)\n",
        "    #print('\\n')\n",
        "\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            data = eval_dataset[i]\n",
        "\n",
        "            Q = data['input_ids']\n",
        "            A = data['labels']\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "            print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "\n",
        "            # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0).to(device).long(),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float()\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "\n",
        "            # Model inference\n",
        "            model_output = model(**input_data, edges=edges.float())\n",
        "            logits = model_output.logits\n",
        "\n",
        "            if logits.dim() > 2:\n",
        "                logits = logits.squeeze(1)\n",
        "\n",
        "            # Extract predicted tokens\n",
        "            predicted_tokens = logits.argmax(dim=-1).cpu().tolist()\n",
        "\n",
        "\n",
        "            # Print shapes and values for debugging\n",
        "            print(\"\\n\\n\")\n",
        "            #print(\"Model Output:\", model_output)\n",
        "            # Print logits length for debugging\n",
        "            print(\"Logits length:\", logits.shape[1])\n",
        "            print(\"Logits shape:\", logits.shape)\n",
        "            print(\"Logits dtype:\", logits.dtype)\n",
        "            print(\"\\n\")\n",
        "            print(\"Predicted IDs:\", logits.argmax(dim=-1))\n",
        "            print(\"Labels:\", data['labels'])\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Decode predictions and labels for the current sample\n",
        "            decoded_pred = tokenizer.decode(predicted_tokens, skip_special_tokens=True) # Decode for single sample\n",
        "            decoded_label = tokenizer.decode(A.cpu().tolist(), skip_special_tokens=True) # Decode for single sample\n",
        "\n",
        "            # Append the decoded prediction and label for the current sample\n",
        "            all_preds.append(decoded_pred)  # Append decoded prediction for current sample\n",
        "            all_labels.append(decoded_label)  # Append decoded label for current sample\n",
        "\n",
        "    # Calculate exact match\n",
        "    em = metric.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")\n",
        "\n",
        "    return {\"exact_match\": em}"
      ],
      "metadata": {
        "id": "B0rtK7yb45Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORKING"
      ],
      "metadata": {
        "id": "AgAXg074JWdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=None, edges=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            data = eval_dataset[i]\n",
        "\n",
        "            Q = data['input_ids']\n",
        "            A = data['labels']\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "            print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "            # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0).to(device).long(),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float()\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "            try:\n",
        "                # Model inference\n",
        "                model_output = model(**input_data, edges=edges.float())\n",
        "                logits = model_output.logits\n",
        "\n",
        "                # Check for NaN or Inf values in logits\n",
        "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "                    print(\"WARNING: NaN or Inf values detected in logits!\")\n",
        "\n",
        "                if logits.dim() > 2:\n",
        "                    logits = logits.squeeze(1)\n",
        "                elif logits.dim() == 1:\n",
        "                    logits = logits.unsqueeze(0)  # Add a batch dimension if needed\n",
        "\n",
        "                # Extract predicted tokens\n",
        "                predicted_tokens = logits.argmax(dim=-1).cpu().tolist()\n",
        "\n",
        "                # Print shapes and values for debugging\n",
        "                print(\"\\n\\n\")\n",
        "                print(\"Logits length:\", logits.shape[1])\n",
        "                print(\"Logits shape:\", logits.shape)\n",
        "                print(\"Logits dtype:\", logits.dtype)\n",
        "                print(\"\\n\")\n",
        "                print(\"Predicted IDs:\", logits.argmax(dim=-1))\n",
        "                print(\"Labels:\", data['labels'])\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Decode predictions and labels for the current sample\n",
        "                decoded_pred = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n",
        "                decoded_label = tokenizer.decode(A.cpu().tolist(), skip_special_tokens=True)\n",
        "\n",
        "                # Print predicted and target answers\n",
        "                print(\"Predicted Answer:\", decoded_pred)\n",
        "                print(\"Target Answer:\", decoded_label)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Append the decoded prediction and label for the current sample\n",
        "                all_preds.append(decoded_pred)\n",
        "                all_labels.append(decoded_label)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for sample {i}: {e}\")\n",
        "                all_preds.append(\"\")\n",
        "                all_labels.append(\"\")\n",
        "                continue\n",
        "\n",
        "    # Calculate exact match\n",
        "    em = metric.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")\n",
        "\n",
        "    return {\"exact_match\": em}\n"
      ],
      "metadata": {
        "id": "H5ICCGUcAkCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORKINg-2"
      ],
      "metadata": {
        "id": "h2dZ6KwetpRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", edges=None):\n",
        "    \"\"\"\n",
        "    Evaluates a text-to-SQL model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the trained PEFT model.\n",
        "        eval_dataset (Dataset): The dataset to evaluate on.\n",
        "        device (str, optional): The device to use for computation (\"cuda\" or \"cpu\"). Defaults to \"cuda\" if available.\n",
        "        edges (torch.Tensor, optional): Precomputed edge indices for the dataset (if applicable).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the exact match score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model once outside the loop\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            data = eval_dataset[i]\n",
        "\n",
        "            Q = data['input_ids'].to(device).long()\n",
        "            A = data['labels'].to(device).long()\n",
        "            # Check for NaN or Inf values in Q and A\n",
        "            if torch.isnan(Q).any() or torch.isinf(Q).any() or torch.isnan(A).any() or torch.isinf(A).any():\n",
        "                print(\"WARNING: NaN or Inf values detected in input_ids or labels!\")\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "            print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "            # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float()\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "            try:\n",
        "                # Model inference\n",
        "                model_output = model(**input_data, edges=edges.float())\n",
        "                logits = model_output.logits\n",
        "\n",
        "\n",
        "                # Handle potential errors in logits\n",
        "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "                    print(\"WARNING: NaN or Inf values detected in logits!\")\n",
        "                    logits[torch.isnan(logits)] = 0 # Replace NaN with zero\n",
        "                    logits[torch.isinf(logits)] = 0 # Replace Inf with zero\n",
        "\n",
        "\n",
        "                # Ensure logits has the right dimensions before argmax\n",
        "                if logits.dim() > 2:\n",
        "                    logits = logits.squeeze(1)\n",
        "                elif logits.dim() == 1:\n",
        "                    logits = logits.unsqueeze(0)\n",
        "\n",
        "                # Extract predicted tokens directly on GPU\n",
        "                predicted_ids = logits.argmax(dim=-1)\n",
        "\n",
        "                # Print shapes and values for debugging\n",
        "                print(\"\\n\\n\")\n",
        "                print(\"Logits length:\", logits.shape[1])\n",
        "                print(\"Logits shape:\", logits.shape)\n",
        "                print(\"Logits dtype:\", logits.dtype)\n",
        "                print(\"\\n\")\n",
        "                print(\"Predicted IDs:\", logits.argmax(dim=-1))\n",
        "                print(\"Labels:\", data['labels'])\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Decode predictions and labels (move to CPU for decoding)\n",
        "                decoded_pred = tokenizer.decode(predicted_ids.cpu(), skip_special_tokens=True)\n",
        "                decoded_label = tokenizer.decode(A.cpu(), skip_special_tokens=True)\n",
        "\n",
        "                print(\"Predicted Answer:\", decoded_pred)\n",
        "                print(\"Target Answer:\", decoded_label)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                all_preds.append(decoded_pred)\n",
        "                all_labels.append(decoded_label)\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if 'out of memory' in str(e):  # Check if it's an OOM error\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"Out of memory error for sample {i}. Skipping this sample.\")\n",
        "                else:\n",
        "                    print(f\"Error during evaluation for sample {i}: {e}\")\n",
        "                all_preds.append(\"\")  # Add an empty string for the skipped sample\n",
        "                all_labels.append(\"\")\n",
        "\n",
        "    # Calculate exact match\n",
        "    em = metric.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")\n",
        "\n",
        "    return {\"exact_match\": em}\n"
      ],
      "metadata": {
        "id": "KnKlAiBztm1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/gdrive/MyDrive/model/GNN-T2SQL/checkpoint-250\"\n",
        "!ls -ltha $path"
      ],
      "metadata": {
        "id": "XnbQr8aY7iD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "path=\"/content/gdrive/MyDrive/model/GNN-T2SQL/checkpoint-250\"\n",
        "results=evaluate_model(path, eval_dataset, device=device)"
      ],
      "metadata": {
        "id": "jDhBe-E0Aouu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORK-4"
      ],
      "metadata": {
        "id": "K5VetLhm5Bzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", edges=None, fake_input=None):\n",
        "    \"\"\"\n",
        "    Evaluates a text-to-SQL model on a given dataset, with optional fake input for testing.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the trained PEFT model.\n",
        "        eval_dataset (Dataset): The dataset to evaluate on.\n",
        "        device (str, optional): The device to use for computation (\"cuda\" or \"cpu\"). Defaults to \"cuda\" if available.\n",
        "        edges (torch.Tensor, optional): Precomputed edge indices for the dataset (if applicable).\n",
        "        fake_input (str, optional): Fake input text to use instead of the actual dataset input.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics, including exact match (EM), BLEU score, execution accuracy (if applicable), and others.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model once outside the loop\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "    metric_em = evaluate.load(\"exact_match\")\n",
        "    metric_bleu = evaluate.load(\"bleu\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Only evaluate on one sample if using fake input\n",
        "        for i in tqdm(range(1 if fake_input else len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            if fake_input:\n",
        "                Q = tokenizer(fake_input, return_tensors=\"pt\")['input_ids'].to(device).long()\n",
        "                A = torch.tensor([], device=device).long()  # Empty tensor for labels\n",
        "            else:\n",
        "                data = eval_dataset[i]\n",
        "                Q = data['input_ids'].to(device).long()\n",
        "                A = data['labels'].to(device).long()\n",
        "\n",
        "            # Check for NaN or Inf values in Q and A\n",
        "            if torch.isnan(Q).any() or torch.isinf(Q).any() or (not fake_input and (torch.isnan(A).any() or torch.isinf(A).any())):\n",
        "                print(\"WARNING: NaN or Inf values detected in input_ids or labels!\")\n",
        "                continue  # Skip this sample if there are invalid values\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "             # Convert Q to a NumPy array before decoding\n",
        "            print(f\"  - Text: {tokenizer.decode(Q.cpu().numpy()[0], skip_special_tokens=True)}\")\n",
        "            if not fake_input:\n",
        "                print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "             # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float() if not fake_input else torch.ones_like(Q).unsqueeze(0)\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "            try:\n",
        "                # Model inference\n",
        "                # Ensure edges is in the correct format (if it's being used by your model)\n",
        "                if edges is not None:\n",
        "                    # Example: If your model expects edges as a 2D tensor of shape (batch_size, num_edges)\n",
        "                    edges = edges.unsqueeze(0)  # Add a batch dimension if necessary\n",
        "                else:\n",
        "                    edges = torch.empty((1, 0), dtype=torch.long, device=device) # Create an empty tensor if no edges are provided\n",
        "\n",
        "                model_output = model(**input_data, edges=edges.float())\n",
        "                logits = model_output.logits\n",
        "\n",
        "\n",
        "\n",
        "                # Handle potential errors in logits\n",
        "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "                    print(\"WARNING: NaN or Inf values detected in logits!\")\n",
        "                    logits[torch.isnan(logits)] = 0 # Replace NaN with zero\n",
        "                    logits[torch.isinf(logits)] = 0 # Replace Inf with zero\n",
        "\n",
        "\n",
        "                # Ensure logits has the right dimensions before argmax\n",
        "                if logits.dim() > 2:\n",
        "                    logits = logits.squeeze(1)\n",
        "                elif logits.dim() == 1:\n",
        "                    logits = logits.unsqueeze(0)\n",
        "\n",
        "                # Extract predicted tokens directly on GPU\n",
        "                predicted_ids = logits.argmax(dim=-1)\n",
        "\n",
        "                # Convert to list only if it's not already a list\n",
        "                if not isinstance(predicted_ids, list):\n",
        "                    predicted_ids = predicted_ids.cpu().tolist()\n",
        "\n",
        "                # Print the type and shape of predicted_ids for debugging\n",
        "                print(\"Type of predicted_ids:\", type(predicted_ids))\n",
        "                print(\"Shape of predicted_ids:\", predicted_ids.shape)\n",
        "\n",
        "\n",
        "                # Print the shape of hidden_states for debugging\n",
        "                #print(\"Shape of hidden_states:\", hidden_states.shape)  # Add this line\n",
        "\n",
        "                #bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # Always convert predicted_ids to a list, regardless of its dimension\n",
        "                #predicted_ids = predicted_ids.cpu().tolist()\n",
        "\n",
        "\n",
        "\n",
        "                # Print shapes and values for debugging\n",
        "                print(\"\\n\\n\")\n",
        "                print(\"Logits length:\", logits.shape[1])\n",
        "                print(\"Logits shape:\", logits.shape)\n",
        "                print(\"Logits dtype:\", logits.dtype)\n",
        "                print(\"\\n\")\n",
        "                print(\"Predicted IDs:\", predicted_ids)\n",
        "                if not fake_input:\n",
        "                    print(\"Labels:\", data['labels'])\n",
        "                print(\"\\n\")\n",
        "\n",
        "\n",
        "                # Decode predictions and labels\n",
        "                decoded_pred = tokenizer.decode(predicted_ids, skip_special_tokens=True) # Decode the predicted IDs\n",
        "\n",
        "\n",
        "\n",
        "                if not fake_input:\n",
        "                    decoded_label = tokenizer.decode(A.cpu(), skip_special_tokens=True)\n",
        "                    all_labels.append(decoded_label)\n",
        "\n",
        "                print(\"Predicted Answer:\", decoded_pred)\n",
        "                if not fake_input:\n",
        "                    print(\"Target Answer:\", decoded_label)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                all_preds.append(decoded_pred)  # Append even if fake input\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if 'out of memory' in str(e):  # Check if it's an OOM error\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"Out of memory error for sample {i}. Skipping this sample.\")\n",
        "                else:\n",
        "                    print(f\"Error during evaluation for sample {i}: {e}\")\n",
        "                all_preds.append(\"\")  # Add an empty string for the skipped sample\n",
        "                if not fake_input:\n",
        "                    all_labels.append(\"\")\n",
        "\n",
        "    # Calculate exact match\n",
        "    results = {}\n",
        "    if not fake_input:\n",
        "        em = metric_em.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "        results[\"exact_match\"] = em\n",
        "        # Calculate BLEU score\n",
        "        bleu = metric_bleu.compute(predictions=all_preds, references=all_labels)[\"bleu\"]\n",
        "        results[\"bleu\"] = bleu\n",
        "\n",
        "\n",
        "        # Calculate execution accuracy (if applicable)\n",
        "        # ... (You'll need to implement this based on your specific setup)\n",
        "\n",
        "        print(\"\\nEvaluation Results:\")\n",
        "        print(f\"  Exact Match: {em:.4f}\")\n",
        "        print(f\"  BLEU Score: {bleu:.4f}\")\n",
        "        # Print execution accuracy if calculated\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "wLXgvF2c2dYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your saved model\n",
        "model_path =\"/content/gdrive/MyDrive/model/GNN-T2SQL/checkpoint-250\"\n",
        "\n",
        "# Use the fake input\n",
        "fake_input = \"What is the capital of the USA?\"\n",
        "\n",
        "# Evaluate the model with the fake input\n",
        "results = evaluate_model(model_path, test_dataset, fake_input=fake_input)\n",
        "\n",
        "# Print the results\n",
        "print(results)  # This will likely be an empty dictionary since there's no exact match to compare to\n"
      ],
      "metadata": {
        "id": "IoFItCB25HN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NOT WORK"
      ],
      "metadata": {
        "id": "6ndDhOiL5iTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "path=\"/content/GNN-T2SQL/checkpoint-500\"\n",
        "\n",
        "# 1. Load the Entire Model (With Adapter)\n",
        "model = GraphModel(mistral_model, tokenizer)\n",
        "model = PeftModel.from_pretrained(model, path, is_trainable=False)\n",
        "model.to(device)\n",
        "\n",
        "# 3. Create DataLoader with Collator\n",
        "data_collator = GraphDataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=1, collate_fn=data_collator)\n"
      ],
      "metadata": {
        "id": "wHgpHWohSUk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=eval_dataloader\n",
        "print(data.dataset[0]['input_ids'])\n",
        "data.dataset[0]['labels']"
      ],
      "metadata": {
        "id": "sJ3oA16mTlVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eval_dataloader\n",
        "edges=None\n",
        "\n",
        "\n",
        "# 2. Initialize Metrics\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "      #for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "      for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):  # Iterate over the DataLoader\n",
        "          # Extract the data from the batch (assuming batch size 1)\n",
        "          data = batch\n",
        "          #data = eval_dataset[i]\n",
        "          #print(data)\n",
        "\n",
        "          Q = data['input_ids'].squeeze()  # Convert tensor to list\n",
        "          A = data['labels'].squeeze()  # Convert tensor to list\n",
        "\n",
        "          #Q = data['input_ids']\n",
        "          #A = data['labels']\n",
        "\n",
        "          print('\\n\\n')\n",
        "          #print(f\"Sample ID: {i}\")\n",
        "          print(f\"  - Text: {tokenizer.decode(Q.tolist(), skip_special_tokens=True)}\") # Decode after converting to list\n",
        "          print(f\"  - Target Text: {tokenizer.decode(A.tolist(), skip_special_tokens=True)}\") # Decode after converting to list\n",
        "          print('\\n')\n",
        "\n",
        "\n",
        "          # Prepare data for the model, move tensors to the correct device and unsqueeze\n",
        "          data = {k: v.unsqueeze(0).to(device).float() for k, v in data.items() if isinstance(v, torch.Tensor)} # Apply unsqueeze only to tensors\n",
        "\n",
        "          # Convert 'input_ids' to LongTensor\n",
        "          data['input_ids'] = data['input_ids'].long()\n",
        "\n",
        "          # Provide a default 'edges' tensor if None\n",
        "          if edges is None:\n",
        "              edges = torch.tensor([[]]).to(device)\n",
        "          #edges=edges.float()\n",
        "\n",
        "           # Prepare data for the model\n",
        "          input_data = {\n",
        "                \"input_ids\": torch.tensor(Q).unsqueeze(0).to(device).long(),  # Convert list to tensor and unsqueeze\n",
        "                \"attention_mask\": torch.tensor(data['attention_mask']).unsqueeze(0).to(device).float()  # Convert list to tensor and unsqueeze\n",
        "            }\n",
        "\n",
        "          if edges is None:\n",
        "              edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "\n",
        "          # Model inference\n",
        "          #model_output = model(**input_data, edges=edges.float())\n",
        "          #logits = model_output.logits\n",
        "\n",
        "\n",
        "\n",
        "          model_output = model(input_ids=data['input_ids'], attention_mask=data['attention_mask'], edges=edges.float())\n",
        "          logits = model_output.logits\n",
        "\n",
        "          if logits.dim() > 2:\n",
        "              logits = logits.squeeze(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          #all_preds.extend(logits.argmax(dim=-1).cpu().tolist())\n",
        "          #all_labels.extend(A.cpu().tolist())"
      ],
      "metadata": {
        "id": "l9MWtMZTS-tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=None, edges=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. Load the Entire Model (With Adapter)\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    model.to(device)\n",
        "\n",
        "    # Explicitly set model to float32 to ensure consistency\n",
        "    model.float()\n",
        "\n",
        "    # 2. Initialize Metrics\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # 3. Create DataLoader with Collator\n",
        "    data_collator = GraphDataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=-100,\n",
        "        pad_to_multiple_of=8\n",
        "    )\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=1, collate_fn=data_collator)\n",
        "\n",
        "    # 4. Evaluation Loop\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        #for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):  # Iterate over the DataLoader\n",
        "            # Extract the data from the batch (assuming batch size 1)\n",
        "            #data = batch[0]\n",
        "            #data = eval_dataset[i]\n",
        "\n",
        "\n",
        "            data = batch\n",
        "\n",
        "\n",
        "            Q = data['input_ids']\n",
        "            A = data['labels']\n",
        "\n",
        "            print('\\n\\n')\n",
        "            #print(f\"Sample ID: {i}\")\n",
        "            #print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            #print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            #print('\\n')\n",
        "\n",
        "\n",
        "           # Prepare data for the model, move tensors to the correct device and unsqueeze\n",
        "            data = {k: v.unsqueeze(0).to(device).float() for k, v in data.items()}\n",
        "\n",
        "            # Convert 'input_ids' to LongTensor\n",
        "            data['input_ids'] = data['input_ids'].long()\n",
        "\n",
        "            # Unsqueeze the 'edges' tensor to add a batch dimension\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).unsqueeze(0).to(device)  # Add unsqueeze here\n",
        "\n",
        "\n",
        "            # Model inference\n",
        "            model_output = model(**data, edges=edges.float())  # Pass the entire data dictionary\n",
        "            logits = model_output.logits\n",
        "\n",
        "            if logits.dim() > 2:\n",
        "                logits = logits.squeeze(1)\n",
        "\n",
        "            all_preds.extend(logits.argmax(dim=-1).cpu().tolist())\n",
        "            all_labels.extend(A.cpu().tolist())\n",
        "\n",
        "\n",
        "    # 5. Compute and Print Metrics\n",
        "    decoded_preds = tokenizer.batch_decode(all_preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(all_labels, skip_special_tokens=True)\n",
        "\n",
        "    # Print for debugging\n",
        "    print(f\"Decoded predictions: {decoded_preds}\")\n",
        "    print(f\"Decoded labels: {decoded_labels}\")\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")"
      ],
      "metadata": {
        "id": "swZN7ciePnxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset.dataset[0]"
      ],
      "metadata": {
        "id": "qYsWpTmqKRKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0]"
      ],
      "metadata": {
        "id": "22uMnGaAaFx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q=eval_dataset[0]['input_ids']\n",
        "tokenizer.decode(Q, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "JKb6UNn6SP6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=eval_dataset[0]['labels']\n",
        "tokenizer.decode(A, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "0QbKnFM_Z1u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8  # Ensure tensors divisible by 8 for optimized performance\n",
        ")\n",
        "\n",
        "# 10. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n"
      ],
      "metadata": {
        "id": "t5jjcgcwGaIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA_Uj38H1Wm-"
      },
      "source": [
        "Let's analyze the information you've provided about your `GraphModel`:\n",
        "\n",
        "**Structure Analysis:**\n",
        "\n",
        "* **Encoder (Mistral-7B):** The backbone of your model is the Mistral-7B language model. This model is responsible for understanding the input text and generating meaningful representations (embeddings) for each token. The encoder has a substantial number of parameters (7.5 billion), indicating its large size and capacity to learn complex language patterns.\n",
        "\n",
        "* **GAT Layer:**  You've incorporated a Graph Attention Network (GAT) layer to process the graph structure of your input data. This layer likely learns to weigh the importance of different nodes and edges based on their relationships, which can help capture additional information beyond just the raw text.\n",
        "\n",
        "* **Pooling:** The `pool` operation is used to aggregate the representations from individual nodes into a single representation for the entire graph. The choice of pooling (e.g., mean pooling) depends on your specific task and how you want to combine the node-level information.\n",
        "\n",
        "* **LM Head:** The language model head (`lm_head`) takes the final graph representation and predicts the next token in the sequence. It has a large number of parameters due to the vocabulary size (32,768).\n",
        "\n",
        "**PEFT (Parameter-Efficient Fine-Tuning):**\n",
        "\n",
        "- **Lora (Low-Rank Adaptation):** You are using LoRA, a technique that adds small adapter modules to the model to fine-tune it more efficiently. This allows you to train a smaller number of parameters while still achieving good performance.\n",
        "\n",
        "- **Trainable Parameters:**  Only a small fraction (0.0078%) of the total parameters are trainable, thanks to LoRA. This drastically reduces the computational and memory requirements during training.\n",
        "\n",
        "**Potential Challenges and Considerations:**\n",
        "\n",
        "- **Graph Structure Quality:** The effectiveness of the GAT layer heavily relies on the quality of the graph structure you create from the input text. Dependency parsing is a good starting point, but you might want to explore other ways to construct the graph to better capture the relationships between words and phrases.\n",
        "\n",
        "- **Overfitting with LoRA:** Even with LoRA, overfitting can be a concern, especially with a small dataset. Monitor your validation loss and consider techniques like early stopping or adding more training data (if possible).\n",
        "\n",
        "- **Memory Usage:**  Graph models can be memory intensive, especially with large input sequences or large batch sizes. You might need to experiment with techniques like gradient checkpointing or gradient accumulation to manage memory usage.\n",
        "\n",
        "- **Training Stability:** Training large language models can sometimes be unstable. Experiment with different learning rates, optimizers, and warm-up strategies to find the best settings for your model.\n",
        "\n",
        "- **Evaluation Metrics:** Choose appropriate evaluation metrics that reflect the quality of the generated SQL queries. Consider using both automated metrics (e.g., BLEU score, accuracy) and human evaluation to assess the model's performance.\n",
        "\n",
        "\n",
        "Let me know if you have any specific questions or concerns!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjgBpi5eCiJI"
      },
      "source": [
        "* Original Code - TRAINING ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m9ufONvpW2G"
      },
      "source": [
        "\n",
        "TO BE TEST IT WORK FOR trainer = SFTTrainer( ; from trl import SFTTrainer;\n",
        "\n",
        "max_seq_length = 2048 --- max sequence length for model and packing of the dataset\n",
        "\n",
        " https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Mistral_7B_Instruct_v0_1_for_text_to_SQL_EVALDATA.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9xVfjpHjI9l"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFOcr3UYpRH_"
      },
      "outputs": [],
      "source": [
        "# 8. Evaluation Metric (Semantic Similarity)\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
        "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute embeddings for predictions and references\n",
        "    prediction_embeddings = sentence_transformer_model.encode(predictions, convert_to_tensor=True)\n",
        "    reference_embeddings = sentence_transformer_model.encode(references, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = util.cos_sim(prediction_embeddings, reference_embeddings)\n",
        "    similarities = torch.diag(cosine_similarities).cpu().numpy()  # Extract similarities for corresponding pairs\n",
        "\n",
        "    # Return average similarity as the metric\n",
        "    em = metric.compute(predictions=predictions, references=references)[\"exact_match\"]\n",
        "    return {\"semantic_similarity\": np.mean(similarities), \"exact_match\": em}\n",
        "\n",
        "# 9. Training Arguments and Trainer\n",
        "training_args = TrainingArguments(\n",
        "    \"graph-T2SQL\",\n",
        "    logging_dir=\"graph-T2SQL\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    push_to_hub=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_semantic_similarity\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8  # Ensure tensors divisible by 8 for optimized performance\n",
        ")\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "# 10. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "\n",
        "# 11. Train the model\n",
        "print('\\n\\n')\n",
        "trainer.train()\n",
        "print('\\n\\n')\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n",
        "print(f'Test Exact Match: {test_results[\"eval_exact_match\"]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPLqrRtAS9r2"
      },
      "source": [
        "## Enhance - 1: Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQckSalcwOv9"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXlOUvZNSWlz"
      },
      "outputs": [],
      "source": [
        "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
        "from nlpaug.augmenter.sentence import RandomInsertionAug\n",
        "\n",
        "def augment_data(dataset):\n",
        "    augmented_data = []\n",
        "\n",
        "    synonym_aug = SynonymAug(aug_src='wordnet')\n",
        "    insert_aug = RandomInsertionAug(aug_p=0.1)  # Insert with 10% probability\n",
        "    delete_aug = RandomWordAug(action=\"delete\", aug_p=0.05)  # Delete with 5% probability\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "\n",
        "        # Create augmented examples\n",
        "        augmented_questions = [\n",
        "            synonym_aug.augment(question),\n",
        "            insert_aug.augment(question),\n",
        "            delete_aug.augment(question)\n",
        "        ]\n",
        "\n",
        "        # Add original and augmented examples to the dataset\n",
        "        for aug_question in augmented_questions:\n",
        "            augmented_data.append({'question': aug_question, 'answer': answer})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "# Augment the training dataset\n",
        "train_dataset_augmented = augment_data(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdh2zS5wTKh0"
      },
      "source": [
        "## Enhance - 2: Alternative Architecture: Graph Transformer Network (GTN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCCpH_DmwusS"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwON48w8ScUs"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GTConv\n",
        "\n",
        "class GTNModel(torch.nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(GTNModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.gtn = GTConv(4096, 4096, num_layers=2)  # 2 GTN layers\n",
        "        self.pool = lambda x, batch: torch.mean(x, dim=0, keepdim=True)\n",
        "        self.lm_head = torch.nn.Linear(4096, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        x = inputs['input_ids'].to(mistral_model.device)\n",
        "        edge_index = inputs['edge_index'].to(mistral_model.device)\n",
        "        embeddings = self.encoder(x).last_hidden_state.cpu()\n",
        "        gtn_out = self.gtn(embeddings, edge_index.cpu())\n",
        "        pooled = self.pool(gtn_out, inputs.get('batch', torch.zeros(gtn_out.size(0)).long()))\n",
        "        out = self.lm_head(pooled.to(mistral_model.device))\n",
        "        return {\"logits\": out}\n",
        "\n",
        "# Replace the model with the GTNModel\n",
        "model = GTNModel(mistral_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyjnyL2RTMpX"
      },
      "source": [
        "## Enhance - 3: Integration Enhance - 1 and Enhance - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMNXPR2XS1RT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "from torch_geometric.nn import GTConv  # For GTN\n",
        "from trl import setup_chat_format\n",
        "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
        "from nlpaug.augmenter.sentence import RandomInsertionAug\n",
        "\n",
        "# ... (rest of the imports and model/tokenizer loading from the original code)\n",
        "\n",
        "# Data Augmentation Function\n",
        "def augment_data(dataset):\n",
        "    augmented_data = []\n",
        "\n",
        "    synonym_aug = SynonymAug(aug_src='wordnet')\n",
        "    insert_aug = RandomInsertionAug(aug_p=0.1)  # Insert with 10% probability\n",
        "    delete_aug = RandomWordAug(action=\"delete\", aug_p=0.05)  # Delete with 5% probability\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "\n",
        "        # Create augmented examples\n",
        "        augmented_questions = [\n",
        "            synonym_aug.augment(question),\n",
        "            insert_aug.augment(question),\n",
        "            delete_aug.augment(question)\n",
        "        ]\n",
        "\n",
        "        # Add original and augmented examples to the dataset\n",
        "        for aug_question in augmented_questions:\n",
        "            augmented_data.append({'question': aug_question, 'answer': answer})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "# TextToSQLDataset (unchanged from the original code)\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        #text = item['question'] + \" \" + item['context']\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "        tokenized_input = self.tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "        tokenized_target = self.tokenizer(target_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Dependency Parsing for Edge Index\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        for token in doc:\n",
        "            if token.i < len(doc) - 1:\n",
        "                edges.append([token.i, token.head.i])\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokenized_input['input_ids'].flatten(),\n",
        "            'attention_mask': tokenized_input['attention_mask'].flatten(),\n",
        "            'labels': tokenized_target['input_ids'].flatten(),\n",
        "            'edge_index': edge_index,\n",
        "        }\n",
        "\n",
        "# GTN Model WITH GTConv\n",
        "from torch_geometric.nn import GTConv\n",
        "\n",
        "class GTNModel(torch.nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(GTNModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.gtn = GTConv(4096, 4096, num_layers=2)  # 2 GTN layers\n",
        "        self.pool = lambda x, batch: torch.mean(x, dim=0, keepdim=True)\n",
        "        self.lm_head = torch.nn.Linear(4096, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        x = inputs['input_ids'].to(mistral_model.device)\n",
        "        edge_index = inputs['edge_index'].to(mistral_model.device)\n",
        "        embeddings = self.encoder(x).last_hidden_state.cpu()\n",
        "\n",
        "        gtn_out = self.gtn(embeddings, edge_index.cpu())\n",
        "        pooled = self.pool(gtn_out, inputs.get('batch', torch.zeros(gtn_out.size(0)).long()))\n",
        "        out = self.lm_head(pooled.to(mistral_model.device))\n",
        "        return {\"logits\": out}\n",
        "\n",
        "# Replace the model with the GTNModel\n",
        "model = GTNModel(mistral_model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "\n",
        "# 7. Evaluation Metric (Semantic Similarity)\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
        "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Compute embeddings for predictions and references\n",
        "    prediction_embeddings = sentence_transformer_model.encode(predictions, convert_to_tensor=True)\n",
        "    reference_embeddings = sentence_transformer_model.encode(references, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = util.cos_sim(prediction_embeddings, reference_embeddings)\n",
        "    similarities = torch.diag(cosine_similarities).cpu().numpy()  # Extract similarities for corresponding pairs\n",
        "\n",
        "    # Return average similarity as the metric\n",
        "    return {\"semantic_similarity\": np.mean(similarities)}\n",
        "\n",
        "\n",
        "# Augment the training dataset\n",
        "train_dataset_augmented = augment_data(train_dataset)\n",
        "\n",
        "# Create datasets (using augmented training data)\n",
        "train_dataset = TextToSQLDataset(train_dataset_augmented, tokenizer)\n",
        "val_dataset = TextToSQLDataset(val_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)\n",
        "\n",
        "# Use the GTNModel\n",
        "model = GTNModel(mistral_model)\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 10. Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    data_collator=lambda x: x,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCYvI4AeUaJu"
      },
      "source": [
        "## Enhance - 4: Regularization Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8neBYzeNUJD_"
      },
      "outputs": [],
      "source": [
        "# ... (other imports)\n",
        "from torch_geometric.nn import GTConv\n",
        "\n",
        "# ... (other classes and functions)\n",
        "\n",
        "# GTN Model with Dropout\n",
        "class GTNModel(torch.nn.Module):\n",
        "    def __init__(self, encoder, dropout_prob=0.1):  # Add dropout probability\n",
        "        super(GTNModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.gtn = GTConv(4096, 4096, num_layers=2)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
        "        self.pool = lambda x, batch: torch.mean(x, dim=0, keepdim=True)\n",
        "        self.lm_head = torch.nn.Linear(4096, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        # ... (rest of the forward method)\n",
        "        x = inputs['input_ids'].to(mistral_model.device)\n",
        "        edge_index = inputs['edge_index'].to(mistral_model.device)\n",
        "        embeddings = self.encoder(x).last_hidden_state.cpu()\n",
        "\n",
        "\n",
        "        gtn_out = self.gtn(embeddings, edge_index.cpu())\n",
        "        gtn_out = self.dropout(gtn_out)  # Apply dropout after GTN\n",
        "\n",
        "        pooled = self.pool(gtn_out, inputs.get('batch', torch.zeros(gtn_out.size(0)).long()))\n",
        "\n",
        "        out = self.lm_head(pooled.to(mistral_model.device))\n",
        "        return {\"logits\": out}\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "# Create the model with dropout\n",
        "model = GTNModel(mistral_model, dropout_prob=0.1)  # Set dropout probability\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "\n",
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 10. Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    data_collator=lambda x: x,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1syJYIaXvil"
      },
      "source": [
        "## Enhance - 5:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GN68X1NHaED"
      },
      "source": [
        "* MultiTaskDataset: This class combines multiple datasets and assigns task IDs to each sample.\n",
        "\n",
        "* MultiTaskModel: This class extends the model to include a separate head for the classification task.\n",
        "\n",
        "* Task-Specific Forward Pass: The forward method now takes a task_id input and uses the appropriate head based on the task.\n",
        "\n",
        "* Training Loop: The training loop needs to be modified to handle the multi-task dataset and compute losses for both tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoHDHFZGXuyG"
      },
      "outputs": [],
      "source": [
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, task_id=0):  # Add task_id argument\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.task_id = task_id  # Store task ID\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        question = item['question'] if self.task_id == 0 else item['text']  # Use appropriate field\n",
        "        answer = item['answer'] if self.task_id == 0 else item['label']  # Use appropriate field\n",
        "        # ... (rest of the __getitem__ method for tokenization and graph construction)\n",
        "\n",
        "\n",
        "        item = {'input_ids': input_ids, 'attention_mask': attention_mask,\n",
        "                'edge_index': edge_index, 'task_id': self.task_id}  # Add task_id\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNy48iw6YmA7"
      },
      "outputs": [],
      "source": [
        "# Create the multi-task model with dropout\n",
        "model = MultiTaskModel(mistral_model, dropout_prob=0.1, num_classes=num_classification_classes)\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfHkTCn0Yto7"
      },
      "outputs": [],
      "source": [
        "# ... (other parts of the code)\n",
        "\n",
        "train_dataset_sql = TextToSQLDataset(train_dataset_augmented, tokenizer, task_id=0)\n",
        "train_dataset_classification = TextToSQLDataset(dataset_classification, tokenizer, task_id=1)\n",
        "\n",
        "# Combine datasets (consider balancing them if needed)\n",
        "train_dataset = torch.utils.data.ConcatDataset([train_dataset_sql, train_dataset_classification])\n",
        "\n",
        "# ... (rest of the training and evaluation code, modified for multi-task)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IZVtS_5Y7wC"
      },
      "source": [
        "In the provided code, the new train_dataset brings several enhancements compared to the original version:\n",
        "\n",
        "Data Augmentation: The most significant change is the incorporation of data augmentation. The original train_dataset is passed through the augment_data function, which creates additional training examples by applying techniques like synonym replacement, random word insertion, and random word deletion. This augmented dataset is then used to create the new train_dataset.\n",
        "\n",
        "Benefit: Data augmentation helps the model generalize better to different phrasing and vocabulary variations in natural language input, leading to improved performance on unseen examples.\n",
        "Multi-Task Learning: The new train_dataset is designed to support multi-task learning. It combines examples from both the original text-to-SQL task and an additional classification task. Each example in the dataset is associated with a task_id to indicate which task it belongs to.\n",
        "\n",
        "Benefit: Multi-task learning allows the model to learn shared representations between related tasks, potentially improving performance and generalization on both tasks.\n",
        "Dynamic Padding: While not explicitly mentioned, the code likely uses dynamic padding when creating the train_dataset. This means that each example is padded to the length of the longest sequence in its batch, rather than using a fixed maximum length.\n",
        "\n",
        "Benefit: Dynamic padding reduces unnecessary padding tokens, which can speed up training and potentially improve model performance.\n",
        "\n",
        "Sources and related content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbw0TlBKlWI0"
      },
      "source": [
        "## Enhance - 6: Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AP4hNkmlSyy"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "\n",
        "# Custom Loss Function (integrated)\n",
        "def compute_loss(model, inputs, return_outputs=False):\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    task_ids = inputs.pop(\"task_id\")\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    sql_logits = outputs.logits[task_ids == 0]  # SQL task logits\n",
        "    sql_labels = labels[task_ids == 0]          # SQL task labels\n",
        "    sql_loss = CrossEntropyLoss()(sql_logits, sql_labels)\n",
        "\n",
        "    classification_logits = outputs.logits[task_ids == 1]  # Classification task logits\n",
        "    classification_labels = labels[task_ids == 1]          # Classification task labels\n",
        "    classification_loss = MSELoss()(classification_logits, classification_labels)\n",
        "\n",
        "    # Combine Losses (adjust weights as needed)\n",
        "    loss = 0.5 * sql_loss + 0.5 * classification_loss\n",
        "\n",
        "    return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# 10. Create Trainer instance (with the custom loss)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=lambda x: x,\n",
        "    compute_metrics=compute_metrics,\n",
        "    compute_loss=compute_loss,  # Pass the custom loss function here\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Reg6DPZJP2Mn",
        "fhEZZJMdzZuK",
        "GoAUvQWiz4ZJ",
        "jnxGtqh44nh8",
        "2hAPAwznG6Gl",
        "9QB1kXab0rHT",
        "AgAXg074JWdR",
        "h2dZ6KwetpRM",
        "K5VetLhm5Bzk",
        "6ndDhOiL5iTN",
        "WPLqrRtAS9r2",
        "Sdh2zS5wTKh0",
        "ZyjnyL2RTMpX",
        "wCYvI4AeUaJu",
        "h1syJYIaXvil",
        "Qbw0TlBKlWI0"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN48eHZf0YNbQ3puiQuIa0b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
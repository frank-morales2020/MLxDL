{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FINAL_GNN_FINE_TUNINGT_T2SQL_DataAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC7dvZByG6uM"
      },
      "source": [
        "## Libraries Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9cqNuPkh_aG"
      },
      "outputs": [],
      "source": [
        "!pip install datasets networkx -q\n",
        "\n",
        "!pip install torch_geometric -q\n",
        "\n",
        "\n",
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "!pip install peft --quiet\n",
        "!pip install trl ninja packaging --quiet\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install mistral_inference -q\n",
        "\n",
        "!pip install trl==0.8.6 -q\n",
        "\n",
        "\n",
        "!pip install sqlparse -q\n",
        "\n",
        "!pip install bitsandbytes -q\n",
        "\n",
        "#!pip uninstall -y torchvision -q\n",
        "!pip install torchvision --no-cache-dir -q\n",
        "#import evaluate\n",
        "\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "!pip install nlpaug -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ak2KGyrYOro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "from trl import setup_chat_format\n",
        "\n",
        "import colab_env\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model and tokenizer\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ],
      "metadata": {
        "id": "-pGJR0chK0dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsx_NaeSTP2j"
      },
      "source": [
        "## TRAININING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0DANV3Esq5w"
      },
      "source": [
        "https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Mistral_7B_Instruct_v0_1_for_text_to_SQL_EVALDATA.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsfmK0vECZ3C"
      },
      "source": [
        "* Import Main Components"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XPoZvJrBzyG",
        "outputId": "1147d2bc-507f-4d44-e0a8-048a881e7db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 3076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/train_dataset.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "p2XhxxLrsHB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0521e1c76c1542c282524521d9eb1582",
            "c87d072c978a48b8b3da725eabfaf47d",
            "d44c2b84ef674be0877058a3f54220c4",
            "8018fb76448542ea89acda6f39017e63",
            "86062eaaca1a479eb2eda1adadf99968",
            "18fb71f9578643f094178a07751de8c3",
            "9e7fca599fb244738111ea2aa48e565c",
            "ca0232ec91334eb5868fd586c3ebf297",
            "f567d59910c147d0a3bb0fce3ede2760",
            "4d65278d8fad40f8b14ca8776cad73e4",
            "c96ba40d07ab4927b74712a65a1160fb"
          ]
        },
        "outputId": "cfb74792-dda2-4b86-c46b-13e859296fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0521e1c76c1542c282524521d9eb1582"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][0]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "VOG6LSjFL9Ng",
        "outputId": "1e323431-5096-4c00-a490-954638a5957a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_92 (total VARCHAR, finish VARCHAR)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][1]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w7PmSu5hMEwE",
        "outputId": "161bb554-1a20-4566-d4cc-02f13ed4018d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How many times was the finish t32?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]['messages'][2]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zO_YHBbrtmrV",
        "outputId": "031662df-17d9-42d0-9b45-88fa45a8274e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT COUNT(total) FROM table_name_92 WHERE finish = \"t32\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0DfyorMy4TK",
        "outputId": "21851578-5919-401f-90a6-c6f2eac3925f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr9FHlz8EEds"
      },
      "source": [
        "* https://stackoverflow.com/questions/70950706/assertionerror-in-torch-geometric-nn-gatconv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Data Input and Preparation:**\n",
        "   - The process begins by loading the \"sql-create-context\" dataset, which presumably contains pairs of natural language questions and their corresponding SQL queries.\n",
        "   - The dataset is divided into three distinct subsets: training, validation, and testing.\n",
        "   - The Mistral-7B-Instruct-v0.3 language model and its tokenizer are loaded and prepared.\n",
        "\n",
        "2. **Data Transformation with `TextToSQLDataset`:**\n",
        "   - The `TextToSQLDataset` class is responsible for converting raw data into a format suitable for model training and evaluation.\n",
        "   - For each data sample, the following transformations occur:\n",
        "      - Tokenization: The input question and the SQL query (answer) are tokenized using the loaded tokenizer.\n",
        "      - Dependency Parsing: The question is parsed using spaCy to extract the grammatical relationships between words, generating a dependency graph.\n",
        "      - Dictionary Creation: A dictionary is created to store the tokenized input IDs, attention masks, labels (tokenized SQL query), and the dependency edges extracted from parsing.\n",
        "\n",
        "3. **Batching and Shuffling with `DataLoader`:**\n",
        "   - `DataLoader` takes the processed dataset from `TextToSQLDataset` and creates an iterable object for efficient batching.\n",
        "   - Optionally, shuffling is applied to randomize the order of samples within each epoch during training.\n",
        "\n",
        "4. **Forward Pass through `GraphModel`:**\n",
        "   - **Mistral Encoder:** The tokenized input IDs and attention masks are fed into the Mistral model's encoder to obtain contextualized token embeddings.\n",
        "   - **GATv2 Layer:** The GATv2 layer (Graph Attention Network) takes the token embeddings and the dependency edges as input. It applies graph attention mechanisms to incorporate the structural information from the dependency graph into the token representations.\n",
        "   - **Pooling:** The node representations (output of GATv2) are aggregated using a pooling operation (e.g., mean pooling) to obtain a fixed-size representation of the entire input sequence.\n",
        "   - **LM Head:** The pooled representation is passed through a linear layer, which produces logits â€“ unnormalized probabilities for each token in the vocabulary.\n",
        "   - **Loss Calculation:** During training, if labels (correct SQL queries) are available, the cross-entropy loss is calculated between the predicted logits and the true labels. This loss guides the optimization of the model's parameters.\n",
        "\n",
        "5. **Model Optimization with PEFT (LoRA):**\n",
        "   - The `GraphModel` is wrapped with PEFT's LoRA (Low-Rank Adaptation) configuration to enable parameter-efficient fine-tuning.\n",
        "   - During training, only the parameters of the GATv2 layer and the LM head are updated, while the rest of the model parameters remain frozen.\n",
        "   - The Hugging Face Trainer manages the training process, iterating over the dataset, computing gradients, and updating the model's parameters based on the calculated loss.\n",
        "\n",
        "6. **Evaluation:**\n",
        "   - After (or during) training, the model is evaluated on the validation and test sets.\n",
        "   - The `compute_metrics` function decodes the predicted logits and labels back into text and assesses the model's performance using two metrics:\n",
        "      - Semantic Similarity: This metric measures how semantically close the predicted SQL query is to the reference SQL query using SentenceTransformer embeddings.\n",
        "      - Exact Match: This metric checks if the predicted SQL query matches the reference SQL query exactly.\n",
        "\n",
        "7. **Output:**\n",
        "    - The final output is the evaluation results, including semantic similarity and exact match scores, which provide insights into the model's ability to generate accurate SQL queries from natural language questions.\n"
      ],
      "metadata": {
        "id": "QFVMbILY2AeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corrected graphical representation of the dataflow, with the arrow pointing *into* the `Trainer`:\n",
        "\n",
        "```\n",
        "+-------------------+          +----------------------+\n",
        "| Dataset Loading   |          | TextToSQLDataset     |\n",
        "|  - sql-create...  | -------> | - Tokenization       |\n",
        "|  - Split: train...|          | - Dependency Parsing |\n",
        "+-------------------+          | - Dict Creation      |\n",
        "                               +----------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                         +---------------------------+\n",
        "                         | DataLoader                |\n",
        "                         | - Batches, Shuffling (opt)|\n",
        "                         +---------------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                         +---------------------------+\n",
        "                         | GraphModel                |\n",
        "                         | - Mistral Encoder         |\n",
        "                         | - GATv2 Layer             |\n",
        "                         | - Pooling                 |\n",
        "                         | - LM Head                 |\n",
        "                         | - (Loss Calculation)      |\n",
        "                         +---------------------------+\n",
        "                                         |\n",
        "                                         v\n",
        "            +--------------+          +---------+         +-----------------+\n",
        "            | PEFT (LoRA)  | -------> | Trainer | <------ |Evaluation       |\n",
        "            +--------------+          |         |         |(compute_metrics)|\n",
        "                                      +---------+         +-----------------+\n",
        "                                         |\n",
        "                                         v\n",
        "                                   +-----------------------+\n",
        "                                   | - Semantic Similarity |\n",
        "                                   | - Exact Match         |\n",
        "                                   +-----------------------+\n",
        "```\n",
        "\n",
        "The revised dataflow now accurately shows that the evaluation metrics (semantic similarity and exact match) calculated by the `compute_metrics` function are used by the `Trainer` to assess the model's performance and make decisions during training (e.g., early stopping).\n",
        "\n"
      ],
      "metadata": {
        "id": "cMZjJHj12Dnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "import shutil\n",
        "\n",
        "# Get the cache directory information\n",
        "cache_info = huggingface_hub.utils.scan_cache_dir()\n",
        "\n",
        "# Iterate through the repositories and delete them\n",
        "for repo_info in cache_info.repos:\n",
        "    repo_path = repo_info.repo_path\n",
        "    shutil.rmtree(repo_path)  # Delete the repository folder"
      ],
      "metadata": {
        "id": "IJj8txi-BLdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6horVfRxXtU8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, Trainer, TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    PrefixTuningConfig,\n",
        "    PromptEncoderConfig,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch_geometric.nn import GAT\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings(\"ignore\", message=\"The installed version of bitsandbytes was compiled without GPU support.\")\n",
        "\n",
        "# 1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# Manually define splits\n",
        "train_size = int(0.7 * len(dataset))\n",
        "eval_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - eval_size\n",
        "\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
        "test_dataset = dataset.select(range(train_size + eval_size, len(dataset)))\n",
        "\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3a. PyTorch Datasets - Data Augmentation\n",
        "import random\n",
        "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
        "\n",
        "def random_insertion(sentence, aug_p=0.1, synonym_aug=None, max_attempts=3):  # Add max_attempts\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_words.append(word)\n",
        "        if random.random() < aug_p:\n",
        "            attempts = 0\n",
        "            while attempts < max_attempts:  # Try multiple times to find a synonym\n",
        "                if new_words:\n",
        "                    candidate_words = [new_words[-1]]\n",
        "                if len(new_words) < len(words):\n",
        "                    candidate_words.append(words[len(new_words)])\n",
        "                if candidate_words and synonym_aug:\n",
        "                    synonym = synonym_aug.augment(random.choice(candidate_words))\n",
        "                    if synonym:\n",
        "                        new_words.append(synonym[0])\n",
        "                        break  # Exit the loop if a synonym is found\n",
        "                attempts += 1\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def augment_data(dataset):\n",
        "    augmented_data = []\n",
        "\n",
        "    synonym_aug = SynonymAug(aug_src='wordnet')\n",
        "    delete_aug = RandomWordAug(action=\"delete\", aug_p=0.05)\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        context = item['context']\n",
        "\n",
        "        augmented_questions = set()\n",
        "        augmented_questions.add(question)  # Always include the original question\n",
        "\n",
        "        # Attempt to generate one more unique augmented question\n",
        "        while len(augmented_questions) < 2:  # Keep trying until we have 2 unique questions\n",
        "            augmentation_method = random.choice(['synonym', 'insertion', 'deletion'])\n",
        "\n",
        "            if augmentation_method == 'synonym':\n",
        "                synonyms = synonym_aug.augment(question)\n",
        "                if synonyms and synonyms[0] != question:\n",
        "                    augmented_questions.add(synonyms[0])\n",
        "\n",
        "            elif augmentation_method == 'insertion':\n",
        "                augmented_question = random_insertion(question, synonym_aug=synonym_aug)\n",
        "                if augmented_question != question:\n",
        "                    augmented_questions.add(augmented_question)\n",
        "\n",
        "            else:  # deletion\n",
        "                deleted = delete_aug.augment(question)\n",
        "                if deleted and deleted[0] != question:\n",
        "                    augmented_questions.add(deleted[0])\n",
        "\n",
        "        # Add the augmented examples to the dataset\n",
        "        for aug_question in list(augmented_questions):\n",
        "            augmented_data.append({'question': aug_question, 'answer': answer, 'context': context})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "# 3. PyTorch Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English model with SRL capabilities\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "except OSError:\n",
        "    spacy.cli.download(\"en_core_web_trf\")\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "        # 1. Tokenization\n",
        "        tokenized_input = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized_target = self.tokenizer(\n",
        "            target_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Flatten lists\n",
        "        tokenized_input = {k: v.squeeze(0) for k, v in tokenized_input.items()}\n",
        "        tokenized_target = {k: v.squeeze(0) for k, v in tokenized_target.items()}\n",
        "\n",
        "        # 2. Dependency Parsing & SRL for Edge Extraction\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        edge_attrs = []\n",
        "        for token in doc:\n",
        "            # Dependency Parsing\n",
        "            if token.dep_ != \"ROOT\" and token.i != token.head.i:\n",
        "                edges.append([token.i, token.head.i])\n",
        "                edge_attrs.append(self.tokenizer.vocab.get(token.dep_, self.tokenizer.unk_token_id))\n",
        "\n",
        "            # SRL\n",
        "            if token.dep_ in {\"nsubj\", \"dobj\", \"nsubjpass\"}:\n",
        "                for child in token.children:\n",
        "                    if child.dep_ == \"prep\":\n",
        "                        for grandchild in child.children:\n",
        "                            if grandchild.dep_ in {\"pobj\", \"pcomp\"}:\n",
        "                                edges.append([token.i, grandchild.i])\n",
        "                                edge_attrs.append(self.tokenizer.vocab.get(\"prep_\" + grandchild.dep_, self.tokenizer.unk_token_id))\n",
        "\n",
        "        # Edge Index Extraction and Validation\n",
        "        if not edges:\n",
        "            num_nodes = len(tokenized_input[\"input_ids\"])\n",
        "            edges = [[i, i] for i in range(num_nodes)]\n",
        "        else:\n",
        "            max_index = len(tokenized_input[\"input_ids\"]) - 1\n",
        "            edges = [(src, tgt) for src, tgt in edges\n",
        "                     if 0 <= src <= max_index and 0 <= tgt <= max_index]\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_attrs = torch.tensor(edge_attrs, dtype=torch.long)\n",
        "\n",
        "        # 3. Node Features with POS Tags\n",
        "        pos_tags = [token.pos_ for token in doc]\n",
        "        pos_tag_ids = [self.tokenizer.vocab.get(tag, self.tokenizer.unk_token_id) for tag in pos_tags]\n",
        "        pos_tag_ids = torch.tensor(pos_tag_ids, dtype=torch.long)\n",
        "\n",
        "        # Convert everything to tensors BEFORE padding/truncation\n",
        "        input_ids = tokenized_input[\"input_ids\"].clone().detach()\n",
        "        attention_mask = tokenized_input[\"attention_mask\"].clone().detach()\n",
        "        labels = tokenized_target[\"input_ids\"].clone().detach()\n",
        "\n",
        "        # Handle potentially empty target sequences\n",
        "        if len(labels) == 0:\n",
        "            labels = torch.tensor([self.tokenizer.pad_token_id], dtype=torch.long)\n",
        "\n",
        "        # Padding and Truncation\n",
        "        max_length = 1024\n",
        "\n",
        "        input_ids = input_ids[:max_length]\n",
        "        attention_mask = attention_mask[:max_length]\n",
        "        labels = labels[:max_length]\n",
        "        pos_tag_ids = pos_tag_ids[:max_length]\n",
        "\n",
        "        if len(input_ids) < max_length:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            pad_tensor = torch.full((pad_length,), self.tokenizer.pad_token_id)\n",
        "            input_ids = torch.cat((input_ids, pad_tensor))\n",
        "            attention_mask = torch.cat((attention_mask, torch.zeros(pad_length, dtype=torch.long)))\n",
        "            pos_tag_ids = torch.cat((pos_tag_ids, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        if len(labels) < max_length:\n",
        "            pad_length = max_length - len(labels)\n",
        "            labels = torch.cat((labels, torch.full((pad_length,), -100)))\n",
        "\n",
        "        if len(edge_attrs) < edge_index.size(1):\n",
        "            pad_length = edge_index.size(1) - len(edge_attrs)\n",
        "            edge_attrs = torch.cat((edge_attrs, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edge_index,\n",
        "            \"edge_attrs\": edge_attrs,\n",
        "            \"pos_tag_ids\": pos_tag_ids,\n",
        "            \"sample_ids\": torch.tensor([idx])\n",
        "        }\n",
        "\n",
        "#Minimum: Start with at least 2000-3000 samples.\n",
        "#This should be enough to provide a good initial assessment of your model's performance and potential.\n",
        "\n",
        "#Medium: If your computational resources allow, try 5000-7000 samples.\n",
        "#This could provide a more robust evaluation and potentially lead to better performance.\n",
        "\n",
        "#Maximum: If you have ample resources, consider using the entire dataset (around 10,000 samples).\n",
        "#This would give you the most comprehensive training data possible and potentially lead\n",
        "#to the best model performance.\n",
        "\n",
        "#Reduce train_dataset size for POC\n",
        "#POC_sample=26000\n",
        "\n",
        "POC_sample=16000\n",
        "import numpy as np\n",
        "train_dataset = train_dataset.select(np.random.choice(len(train_dataset), POC_sample, replace=False))\n",
        "\n",
        "### data augmentation #######\n",
        "train_dataset_augmented = augment_data(train_dataset)\n",
        "train_dataset = TextToSQLDataset(train_dataset_augmented, tokenizer)\n",
        "#############################\n",
        "\n",
        "\n",
        "POC_valsample=1\n",
        "#############################\n",
        "eval_dataset = eval_dataset.select(np.random.choice(len(eval_dataset), POC_valsample, replace=False))\n",
        "test_dataset = test_dataset.select(np.random.choice(len(test_dataset), POC_valsample, replace=False))\n",
        "\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)\n",
        "#############################\n",
        "\n",
        "\n",
        "# 4. GAT Layer and GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GATLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_heads=8, num_layers=3):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.gat = GAT(in_channels=in_features, hidden_channels=out_features, heads=num_heads,\n",
        "                        concat=False, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        return self.gat(x, edge_index, edge_attr=edge_attr)\n",
        "\n",
        "    def get_lora_target_modules(self):\n",
        "        return [module for module in self.gat.modules() if isinstance(module, torch.nn.Linear)]\n",
        "\n",
        "\n",
        "# 4b.  GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class MyCausalLMOutputWithPast:\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "class GraphModel(nn.Module):\n",
        "    def __init__(self, encoder, tokenizer):\n",
        "        super(GraphModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config = encoder.model.config\n",
        "\n",
        "        # Adjust in_channels to match the actual input dimensionality\n",
        "        self.gatv2 = GATv2Conv(\n",
        "            in_channels=self.config.hidden_size,  # Set to 4096\n",
        "            out_channels=self.config.hidden_size,\n",
        "            heads=8,\n",
        "            concat=False,\n",
        "        )\n",
        "\n",
        "        # Max Pooling\n",
        "        self.pool = lambda x, batch: torch.max(x, dim=0, keepdim=True)[0]\n",
        "\n",
        "        # Additional Feedforward Layer\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(self.config.hidden_size, self.config.hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.config.hidden_size * 2, self.config.hidden_size),\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Add generation config\n",
        "        self.generation_config = encoder.generation_config\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, edges, labels=None, inputs_embeds=None,\n",
        "                pos_tag_ids=None, edge_attrs=None, sample_ids=None, output_attentions=False,\n",
        "                output_hidden_states=False, return_dict=False):\n",
        "\n",
        "\n",
        "        # 1. Token Embeddings (Encoder)\n",
        "        if input_ids is not None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids.to(self.encoder.model.device),\n",
        "                attention_mask=attention_mask.to(self.encoder.model.device),\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            embeddings = encoder_outputs.hidden_states[-1]\n",
        "        elif inputs_embeds is not None:\n",
        "            embeddings = inputs_embeds\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        # Ensure correct shape for GATv2Conv\n",
        "        if embeddings.dim() > 2:\n",
        "            embeddings = embeddings.view(-1, embeddings.shape[-1])\n",
        "\n",
        "         # 2. Obtain POS tag embeddings and concatenate\n",
        "        if pos_tag_ids is not None:\n",
        "            pos_tag_embeddings = self.encoder.model.embeddings(pos_tag_ids.to(self.encoder.model.device))\n",
        "            embeddings = torch.cat([embeddings, pos_tag_embeddings], dim=-1)\n",
        "\n",
        "\n",
        "        # 3. Edge Index Creation (with potential optimization for memory)\n",
        "        edge_index = []\n",
        "        node_offset = 0\n",
        "        for i, graph_edges in enumerate(edges):\n",
        "            if graph_edges is None or graph_edges.numel() == 0:\n",
        "                num_nodes = input_ids.size(1)\n",
        "                graph_edges = torch.arange(node_offset, node_offset + num_nodes, device=embeddings.device)\n",
        "                graph_edges = graph_edges.repeat(2, 1)\n",
        "            else:\n",
        "                if not isinstance(graph_edges, torch.Tensor):\n",
        "                    graph_edges = torch.tensor(graph_edges, dtype=torch.long, device=embeddings.device)\n",
        "                graph_edges += node_offset\n",
        "            edge_index.append(graph_edges)\n",
        "            node_offset += input_ids.size(1)\n",
        "        edge_index = torch.cat(edge_index, dim=1)\n",
        "\n",
        "        # 4. GATv2 Layer (pass edge_attrs)\n",
        "        graph_out = self.gatv2(embeddings, edge_index, edge_attr=edge_attrs)\n",
        "\n",
        "\n",
        "        # 5. Pooling\n",
        "        batch = torch.arange(len(edges), device=graph_out.device).repeat_interleave(input_ids.size(1))\n",
        "        pooled = self.pool(graph_out, batch).unsqueeze(1)\n",
        "\n",
        "        # 5.1 - Additional Feedforward Layer\n",
        "        pooled = self.ffn(pooled)\n",
        "\n",
        "        # 6. LM Head\n",
        "        logits = self.lm_head(pooled)\n",
        "\n",
        "        # 7. Loss Calculation\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            from torch.nn import CrossEntropyLoss\n",
        "\n",
        "            mask = (labels != -100).float()\n",
        "\n",
        "            # Apply softmax to logits to get probabilities\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "            # Reshape log_probs to match target shape\n",
        "            log_probs = log_probs.squeeze(1)\n",
        "\n",
        "            labels = labels[:, 0]\n",
        "\n",
        "            loss = loss_fct(log_probs, labels)\n",
        "\n",
        "            loss_per_sample = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "\n",
        "        # 8. Return\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits,\n",
        "            \"past_key_values\": encoder_outputs.past_key_values,\n",
        "            \"hidden_states\": encoder_outputs.hidden_states,\n",
        "            \"attentions\": encoder_outputs.attentions,\n",
        "        }\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, edges, attention_mask=None,\n",
        "                                      pos_tag_ids=None, edge_attrs=None, **kwargs):\n",
        "        if isinstance(self, PeftModel):\n",
        "            return self.base_model.prepare_inputs_for_generation(input_ids, edges, attention_mask, **kwargs)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        if batch_size > 1:\n",
        "            batched_edges = []\n",
        "            node_offset = 0\n",
        "            for i in range(batch_size):\n",
        "                graph_edges = edges[i]\n",
        "                batched_edges.extend([(src + node_offset, dst + node_offset) for src, dst in graph_edges])\n",
        "                node_offset += input_ids.size(1)\n",
        "            edge_index = torch.tensor(batched_edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "        else:\n",
        "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "\n",
        "        model_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"edges\": edge_index,\n",
        "            \"pos_tag_ids\": pos_tag_ids,\n",
        "            \"edge_attrs\": edge_attrs,\n",
        "            \"past_key_values\": kwargs.get(\"past_key_values\", None),\n",
        "        }\n",
        "        return model_inputs\n",
        "\n",
        "# END GraphModel\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# Quantize Mistral (before creating GraphModel)\n",
        "mistral_model = prepare_model_for_kbit_training(mistral_model, use_gradient_checkpointing=True)\n",
        "\n",
        "\n",
        "# 5. Model Setup (Define model first)\n",
        "model = GraphModel(mistral_model, tokenizer)  # Pass both mistral_model and tokenizer\n",
        "\n",
        "\n",
        "# 6. PEFT Configuration (Use automatic module discovery)\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    # Instead of targeting specific layers, let PEFT automatically discover the linear layers within the model\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "# 7. Apply PEFT\n",
        "model = get_peft_model(model, peft_config)\n",
        "print('\\n\\n')\n",
        "print('PEFT-Model')\n",
        "model.print_trainable_parameters() # To see the trainable parameters\n",
        "print('\\n')\n",
        "\n",
        "# Access the config of the encoder (Mistral model) within your GraphModel\n",
        "model.encoder.config.use_cache = False\n",
        "\n",
        "# Ensure that LoRA layers are properly initialized and their dimensions are correctly set\n",
        "for name, module in model.named_modules():\n",
        "    if \"lora\" in name:\n",
        "        module = module.to(device) # Move LoRA parameters to the correct device\n",
        "\n",
        "model.encoder.gradient_checkpointing_enable()  # Enable gradient checkpointing for memory optimization on the Mistral model\n",
        "#model.encoder.model.embed_tokens.requires_grad_(True)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# 8. Evaluation Metric (Semantic Similarity)\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert all elements to tensors, handling different data types\n",
        "    predictions = [torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred for pred in all_preds]\n",
        "    labels = [torch.tensor(label) if not isinstance(label, torch.Tensor) else label for label in all_labels]\n",
        "\n",
        "    # Filter out any None values before stacking\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Convert to tensors and stack (only if there are predictions/labels)\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])  # Empty tensor if no predictions\n",
        "\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])  # Empty tensor if no labels\n",
        "\n",
        "    # Handle cases where only one prediction/label is present (avoid squeezing to a scalar)\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    #print('\\n')\n",
        "    #print(f\"Shape of logits in compute_metrics: {predictions.shape}\")\n",
        "    #print(f\"Shape of labels in compute_metrics: {labels.shape}\")\n",
        "    #('\\n')\n",
        "\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    return {\"exact_match\": em}\n",
        "\n",
        "\n",
        "\n",
        "#9. Training Arguments and Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir =\"/content/gdrive/MyDrive/model/GNNT2SQL\",\n",
        "    logging_dir=\"/content/gdrive/MyDrive/model/GNN-T2SQL/logs\",\n",
        "\n",
        "    per_device_train_batch_size=1,  # Slightly increased, but be cautious\n",
        "    gradient_accumulation_steps=4,  # Adjusted for effective batch size of 4\n",
        "\n",
        "    # Number of epochs and early stopping\n",
        "    num_train_epochs=1,  # Start with a few epochs and monitor validation loss\n",
        "    #early_stopping_patience=3,  # Enable early stopping to prevent overfitting\n",
        "\n",
        "    # Learning rate and scheduler\n",
        "    learning_rate=5e-5,  # A reasonable starting point, adjust if needed\n",
        "    lr_scheduler_type=\"linear\",  # Or try other schedulers like \"cosine\"\n",
        "    warmup_steps=500,  # Warmup is crucial, especially with a larger learning rate\n",
        "\n",
        "    # Evaluation and saving\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,  # Evaluate less frequently to save time #500\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50, #500\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "\n",
        "    # Other settings\n",
        "    push_to_hub=False,\n",
        "    load_best_model_at_end=True,\n",
        "    use_legacy_prediction_loop=False,\n",
        "    metric_for_best_model=\"eval_exact_match\",\n",
        "    report_to=\"tensorboard\",\n",
        "    #generation_max_length=2048,  # Adjust if needed based on your data\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch_geometric.data import Batch as GraphBatch  # Note the import\n",
        "import torch\n",
        "import torch_geometric.data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class GraphDataCollatorForSeq2Seq:\n",
        "    def __init__(self, tokenizer, model=None, label_pad_token_id=-100, pad_to_multiple_of=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Separate standard features from graph edges\n",
        "        # Extract labels before padding and handle potentially empty sequences\n",
        "        labels = [feature[\"labels\"] if feature[\"labels\"].numel() > 0\n",
        "                  else torch.tensor([self.label_pad_token_id], dtype=torch.long)\n",
        "                  for feature in features]\n",
        "\n",
        "        # Extract sample_ids\n",
        "        sample_ids = [feature[\"sample_ids\"] for feature in features]\n",
        "\n",
        "        standard_features = [{k: v for k, v in feature.items() if k != \"edges\" and k != \"labels\"} for feature in features]\n",
        "        edges = [feature[\"edges\"] for feature in features]\n",
        "\n",
        "        # Collate standard features (input_ids, attention_mask) using default collator\n",
        "        collated_standard_features =  DataCollatorForSeq2Seq(\n",
        "            tokenizer=self.tokenizer,\n",
        "            model=self.model,\n",
        "            label_pad_token_id=self.label_pad_token_id,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of\n",
        "        )(standard_features)\n",
        "\n",
        "        # Pad input_ids and attention_mask\n",
        "        input_ids = pad_sequence([f['input_ids'] for f in standard_features], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([f['attention_mask'] for f in standard_features], batch_first=True, padding_value=0)\n",
        "\n",
        "         # Pad labels separately\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n",
        "\n",
        "        # Create batch for graph data\n",
        "        graph_data_list = []\n",
        "        for i in range(len(edges)):\n",
        "            # Convert to PyTorch Geometric Data\n",
        "            graph_data_list.append(torch_geometric.data.Data(\n",
        "                x=collated_standard_features['input_ids'][i].unsqueeze(1),  # Node features (input_ids)\n",
        "                edge_index=edges[i],                   # Edge index\n",
        "                # Use num_edges for batch index to ensure correct batching in PyG\n",
        "                batch=torch.tensor([i] * edges[i].size(1))\n",
        "            ))\n",
        "        batched_graph = GraphBatch.from_data_list(graph_data_list)  # Batch graphs\n",
        "\n",
        "        #print(f\"Sample GraphDataCollatorForSeq2Seq ID: {sample_ids}\")\n",
        "\n",
        "         # Include sample_ids in the collated features\n",
        "        collated_features = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edges,\n",
        "            \"sample_ids\": sample_ids  # Add sample_ids here\n",
        "        }\n",
        "\n",
        "        return collated_features\n",
        "\n",
        "\n",
        "# 10a.  Data Collator\n",
        "data_collator = GraphDataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "\n",
        "# 10AA. Training Arguments and Trainer\n",
        "\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_utils import EvalLoopOutput, PredictionOutput,  has_length\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from transformers import Trainer, TrainerCallback\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "#from transformers.trainer_pt_utils import nested_truncate  # Updated import#\n",
        "\n",
        "from transformers.trainer_pt_utils import nested_truncate, nested_concat, nested_numpify, nested_detach  # Updated imports\n",
        "\n",
        "\n",
        "######\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def estimated_num_samples(dataloader: DataLoader):\n",
        "    \"\"\"\n",
        "    This function will attempt to determine the number of samples in a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        dataloader (DataLoader): The DataLoader to estimate the number of samples from.\n",
        "\n",
        "    Returns:\n",
        "        int: The estimated number of samples, or 0 if estimation is not possible.\n",
        "    \"\"\"\n",
        "    if hasattr(dataloader, \"dataset\") and hasattr(dataloader.dataset, \"__len__\"):\n",
        "        return len(dataloader.dataset)  # Use dataset length if available\n",
        "    elif hasattr(dataloader, \"batch_sampler\") and hasattr(dataloader.batch_sampler, \"sampler\") and hasattr(dataloader.batch_sampler.sampler, \"__len__\"):\n",
        "        return len(dataloader.batch_sampler.sampler)  # Use sampler length if available\n",
        "    else:\n",
        "        # If neither is available, return 0\n",
        "        warnings.warn(\"Could not estimate the number of samples in the dataloader. Returning 0.\")\n",
        "        return 0\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    _id=0\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        labels = inputs.pop(\"labels\", None)\n",
        "\n",
        "        print('\\n\\n')\n",
        "        print(\"**** Prediction Step ****\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Access loss and logits based on the type of outputs\n",
        "            if isinstance(outputs, MyCausalLMOutputWithPast):\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "            elif isinstance(outputs, tuple):\n",
        "                # Check if the tuple has at least two elements before accessing\n",
        "                if len(outputs) >= 2:\n",
        "                    loss = outputs[0]  # Assuming loss is the first element\n",
        "                    logits = outputs[1]  # Assuming logits is the second element\n",
        "                else:\n",
        "                    # Handle the case where the tuple has fewer than two elements\n",
        "                    raise ValueError(\"Output tuple from model has fewer than two elements\")\n",
        "            else:\n",
        "                #raise ValueError(\"Unexpected output type from model\")\n",
        "                # Access loss and logits as attributes for other types\n",
        "                loss = outputs[\"loss\"] # Access loss from the dictionary\n",
        "                logits = outputs[\"logits\"] # Access logits from the dictionary\n",
        "\n",
        "\n",
        "            # Debugging Logging (ensure labels exist before accessing)\n",
        "            if not prediction_loss_only:\n",
        "                print('\\n\\n')\n",
        "                print(\"Shape of logits in prediction_step:\", logits.shape)\n",
        "                if labels is not None:  # Only print if labels exist\n",
        "                    print(\"Shape of labels in prediction_step:\", labels.shape)\n",
        "\n",
        "            if prediction_loss_only:\n",
        "                if isinstance(loss, torch.Tensor):\n",
        "                    loss = loss.mean().detach()\n",
        "                return (loss, None, None)\n",
        "\n",
        "            max_new_tokens = 1024 - inputs['input_ids'].shape[1]\n",
        "\n",
        "            # Modify this part to handle the generated IDs\n",
        "            if max_new_tokens <= 0:\n",
        "                print('\\n\\n')\n",
        "                print(\"No new tokens to generate. An increase in the sample size for training is required. \")\n",
        "                #print(\"#TOKENS: \",max_new_tokens)\n",
        "                # Input is already at max length, no need to generate\n",
        "                generated_ids = inputs['input_ids']\n",
        "            else:\n",
        "                generated_ids = model.encoder.generate(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_beams=5\n",
        "                )\n",
        "\n",
        "            # Check the shape of generated_ids\n",
        "            #print(\"Shape of generated_ids:\", generated_ids.shape)\n",
        "\n",
        "\n",
        "\n",
        "        # Flatten the generated_ids to a 1D list before decoding\n",
        "        flattened_generated_ids = generated_ids.view(-1).tolist()\n",
        "        predictions=generated_ids\n",
        "\n",
        "        # Now decode using the flattened list\n",
        "        predictions_decoder = self.tokenizer.decode(flattened_generated_ids, skip_special_tokens=True)\n",
        "        Q = self.tokenizer.decode(inputs['input_ids'].view(-1).tolist(), skip_special_tokens=True)\n",
        "        A = self.tokenizer.decode(labels.view(-1).tolist(), skip_special_tokens=True)\n",
        "        #print(\"Sample IDs in prediction_step:\", inputs['sample_ids'][0])\n",
        "\n",
        "\n",
        "        # Debugging Logging\n",
        "        #print(\"Shape of predictions in prediction_step:\", predictions.shape)\n",
        "        if labels is not None:\n",
        "            print('\\n\\n')\n",
        "            # Extract sample_ids\n",
        "            sample_ids = int(self._id)+1  # Use self._id here\n",
        "            #print(\"Sample IDs in prediction_step:\", sample_ids)\n",
        "            print(\"Sample IDs in prediction_step:\", inputs['sample_ids'])\n",
        "\n",
        "            print(\"Question:\", Q)\n",
        "            print(\"Decoded Original Answer BEFORE Predictions:\", A)\n",
        "            print('\\n\\n')\n",
        "\n",
        "            print(\"Decoded Predictions:\", predictions_decoder)\n",
        "\n",
        "            return (loss, predictions, labels)\n",
        "\n",
        "    def _prediction_loop(self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = \"eval\") -> Union[Tuple[torch.Tensor, torch.Tensor], EvalPrediction]:\n",
        "\n",
        "\n",
        "         # In case you have a callback that needs length eventually\n",
        "        if has_length(dataloader):\n",
        "            num_samples = len(dataloader.dataset)\n",
        "        # The dataset does not support __len__, estimate the number of samples.\n",
        "        else:\n",
        "            num_samples = estimated_num_samples(dataloader)\n",
        "\n",
        "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
        "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
        "        if all_losses:\n",
        "            all_losses = all_losses[:num_samples]\n",
        "        if all_preds:\n",
        "            all_preds = nested_truncate(all_preds, num_samples)\n",
        "        if all_labels:\n",
        "            all_labels = nested_truncate(all_labels, num_samples)\n",
        "\n",
        "        # 8.  Compute Metrics and Average Loss\n",
        "        metrics = self.compute_metrics((all_preds, all_labels))\n",
        "        average_loss = torch.mean(torch.stack(all_losses))\n",
        "        metrics[f\"{metric_key_prefix}_loss\"] = average_loss.item()\n",
        "\n",
        "        # 9.  Log the Metrics\n",
        "        self.log(metrics)\n",
        "\n",
        "        # 10. Return Based on Whether It's a Prediction or Evaluation\n",
        "        if prediction_loss_only:\n",
        "            return (metrics, None, None)\n",
        "\n",
        "        return EvalPrediction(predictions=all_preds, label_ids=all_labels, metrics=metrics)\n",
        "\n",
        "\n",
        "    def predict(self, test_dataset: Dataset, ignore_keys: Optional[List[str]] = None) -> PredictionOutput:\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "        return self.prediction_loop(test_dataloader, description=\"Prediction\")\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ) -> Dict[str, float]:\n",
        "\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self.prediction_loop(\n",
        "            eval_dataloader,\n",
        "            description=\"Evaluation\",\n",
        "        )\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "\n",
        "######\n",
        "\n",
        "# 10A. Trainer (Modified)\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import TrainerCallback\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 50000 == 0:  # Log every 100 steps (adjust as needed)\n",
        "            print(f\"Step {state.global_step} - Loss: {state.loss}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 11. Train the model\n",
        "# Add the Callback to the Trainer\n",
        "trainer.add_callback(LossLoggingCallback())\n",
        "\n",
        "# Add the Early Stopping to the Trainer\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "#test_results = trainer.evaluate(eval_dataset)\n",
        "#print('\\n\\n')\n",
        "#print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n",
        "#print(f'Test Exact Match. (Evaluate on the test set): {test_results[\"eval_exact_match\"]:.4f}')\n",
        "#print('\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Units per Hour: You're using 4.82 compute units every hour.\n",
        "\n",
        "2. Total Time Available:\n",
        "\n",
        "Divide your total compute units by your hourly usage rate: 309.66 units / 4.82 units/hour = 64.24 hours"
      ],
      "metadata": {
        "id": "U-VssbduKp8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Metrics"
      ],
      "metadata": {
        "id": "Reg6DPZJP2Mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Logical Correctness?\n",
        "\n",
        "In the realm of T2SQL (Text-to-SQL), logical correctness goes beyond mere syntactic accuracy and execution success. It evaluates whether the generated SQL query genuinely captures the intent and nuances of the natural language input, ensuring that it retrieves the desired information from the database.\n",
        "\n",
        "Consider these scenarios where a generated SQL query might be syntactically valid and even execute without errors, yet still be logically incorrect:\n",
        "\n",
        "Incorrect column or table selection: The query might fetch data from the wrong columns or tables, resulting in irrelevant or inaccurate results.\n",
        "Incorrect filtering or aggregation: The WHERE clause or aggregation functions (e.g., SUM, COUNT) might be misapplied, leading to filtered or aggregated data that doesn't align with the user's intent.\n",
        "Incorrect joins: If the natural language input implies relationships between multiple tables, the generated query might have incorrect or missing joins, producing misleading results.\n",
        "Subtle semantic mismatches: Even if the query produces results, they might not fully capture the nuances and implied meaning of the original question or instruction.\n",
        "Why is it Important?\n",
        "\n",
        "While execution accuracy is crucial, logical correctness ensures that the generated SQL truly \"understands\" the user's request and provides the right answer, not just an answer. It's a key indicator of the T2SQL model's ability to reason about the data schema and the user's intent.\n",
        "\n",
        "How to Measure Logical Correctness\n",
        "\n",
        "Measuring logical correctness can be challenging, as it often requires a deeper understanding of the underlying data schema and the subtle nuances of natural language. Here are some common approaches:\n",
        "\n",
        "Manual Inspection:\n",
        "\n",
        "A human evaluator examines a sample of generated queries and their corresponding ground truth queries to assess if they capture the same intent.\n",
        "Pros: Provides valuable qualitative insights and can catch subtle semantic mismatches.\n",
        "Cons: Time-consuming and not scalable for large datasets.\n",
        "SQL Parsing and Comparison:\n",
        "\n",
        "Use a SQL parser to extract structural components (e.g., SELECT, FROM, WHERE, GROUP BY) from both generated and ground truth queries.\n",
        "Compare these components to identify mismatches or inconsistencies.\n",
        "Pros: Can be automated and is relatively scalable.\n",
        "Cons: Might miss subtle semantic differences or struggle with complex SQL constructs.\n",
        "Result Comparison:\n",
        "\n",
        "Execute both the generated and ground truth queries against the database.\n",
        "Compare the results to see if they are equivalent or sufficiently similar.\n",
        "Pros: Directly measures the impact of logical errors on the output.\n",
        "Cons: Requires access to the database, can be computationally expensive, and might not capture all types of logical errors.\n",
        "Hybrid Approaches:\n",
        "\n",
        "Combine manual inspection with automated techniques to leverage their strengths.\n",
        "For example, use SQL parsing to identify potential logical errors and then have a human evaluator review those cases.\n",
        "Choosing the Right Approach\n",
        "\n",
        "The ideal approach for measuring logical correctness depends on factors like:\n",
        "\n",
        "Data schema complexity: More complex schemas might require more sophisticated techniques.\n",
        "Evaluation scale: Manual inspection might be feasible for smaller datasets, while automated methods are necessary for larger ones.\n",
        "Available resources: Access to a database and computational power will influence the feasibility of certain approaches.\n",
        "Desired level of rigor: The trade-off between efficiency and thoroughness will guide your choice.\n",
        "Remember, evaluating logical correctness is an ongoing challenge in T2SQL research. Combining multiple approaches and iteratively refining your evaluation metrics will lead to more robust and reliable T2SQL models.\n"
      ],
      "metadata": {
        "id": "vSl-62o4P-Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import load_metric\n",
        "import sqlparse\n",
        "import psycopg2\n",
        "\n",
        "# Assuming you have 'tokenizer' and 'sentence_transformer_model' defined elsewhere\n",
        "\n",
        "# Load the metric for exact match calculation\n",
        "metric = load_metric(\"exact_match\")\n",
        "\n",
        "def compute_metrics(eval_pred, db_config=None):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert predictions and labels to tensors\n",
        "    predictions = [\n",
        "        torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred\n",
        "        for pred in all_preds\n",
        "    ]\n",
        "    labels = [\n",
        "        torch.tensor(label) if not isinstance(label, torch.Tensor) else label\n",
        "        for label in all_labels\n",
        "    ]\n",
        "\n",
        "    # Filter out None values\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Stack predictions and labels if they exist\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])\n",
        "\n",
        "    # Handle single prediction/label cases\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute exact match accuracy\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    # Compute semantic similarity\n",
        "    embeddings_pred = sentence_transformer_model.encode(decoded_preds)\n",
        "    embeddings_labels = sentence_transformer_model.encode(decoded_labels)\n",
        "    semantic_similarity = util.cos_sim(embeddings_pred, embeddings_labels).mean()\n",
        "\n",
        "    # Execution Accuracy (if db_config is provided)\n",
        "    if db_config:\n",
        "        execution_accuracy_scores = []\n",
        "        with psycopg2.connect(**db_config) as conn:\n",
        "            with conn.cursor() as cur:\n",
        "                for pred_sql, label_sql in zip(decoded_preds, decoded_labels):\n",
        "                    try:\n",
        "                        # Execute predicted query\n",
        "                        cur.execute(pred_sql)\n",
        "                        pred_results = cur.fetchall()\n",
        "\n",
        "                        # Execute label (ground truth) query\n",
        "                        cur.execute(label_sql)\n",
        "                        label_results = cur.fetchall()\n",
        "\n",
        "                        # Compare results\n",
        "                        if pred_results == label_results:\n",
        "                            execution_accuracy_scores.append(1)\n",
        "                        else:\n",
        "                            execution_accuracy_scores.append(0)\n",
        "\n",
        "                            # Optional: Log mismatches for debugging\n",
        "                            logging.debug(f\"Mismatch: Predicted: {pred_results}, Label: {label_results}\")\n",
        "\n",
        "                    except psycopg2.Error as e:\n",
        "                        execution_accuracy_scores.append(0)\n",
        "                        logging.error(f\"Error executing SQL: {e}\")\n",
        "\n",
        "        execution_accuracy = sum(execution_accuracy_scores) / len(execution_accuracy_scores)\n",
        "    else:\n",
        "        execution_accuracy = 0.0\n",
        "\n",
        "    # Logical Correctness (SQL Parsing and Comparison)\n",
        "    logical_correctness_scores = []\n",
        "    for pred_sql, label_sql in zip(decoded_preds, decoded_labels):\n",
        "        parsed_pred = sqlparse.parse(pred_sql)[0]\n",
        "        parsed_label = sqlparse.parse(label_sql)[0]\n",
        "\n",
        "        pred_components = {\n",
        "            \"select\": [token.value for token in parsed_pred.tokens if isinstance(token, sqlparse.sql.IdentifierList)],\n",
        "            \"from\": [token.value for token in parsed_pred.tokens if isinstance(token, sqlparse.sql.Identifier)],\n",
        "            # ... extract other components like WHERE, GROUP BY, etc.\n",
        "        }\n",
        "        label_components = {\n",
        "            \"select\": [token.value for token in parsed_label.tokens if isinstance(token, sqlparse.sql.IdentifierList)],\n",
        "            \"from\": [token.value for token in parsed_label.tokens if isinstance(token, sqlparse.sql.Identifier)],\n",
        "            # ... extract other components like WHERE, GROUP BY, etc., using the same logic as for pred_components\n",
        "        }\n",
        "\n",
        "        score = 0\n",
        "        for component_type in [\"select\", \"from\", ...]:\n",
        "            if pred_components[component_type] == label_components[component_type]:\n",
        "                score += 1\n",
        "            elif set(pred_components[component_type]) == set(label_components[component_type]):\n",
        "                score += 0.5\n",
        "            # ... add more sophisticated comparison logic if needed\n",
        "\n",
        "        logical_correctness_scores.append(score)\n",
        "\n",
        "    logical_correctness = sum(logical_correctness_scores) / len(logical_correctness_scores)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": em,\n",
        "        \"semantic_similarity\": semantic_similarity.item(),\n",
        "        \"execution_accuracy\": execution_accuracy,\n",
        "        \"logical_correctness\": logical_correctness,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "OLMO34VVPk9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MISTRAL-MODEL"
      ],
      "metadata": {
        "id": "nDzroTjfY9EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model and tokenizer\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ],
      "metadata": {
        "id": "YxiJT9Y1SDUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postgresql Setup"
      ],
      "metadata": {
        "id": "fhEZZJMdzZuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 01/06/2024\n",
        "!apt-get update -y\n",
        "!apt-get install postgresql-14 -y\n",
        "\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "-h27HimSyEY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668d68a8-f428-4fd1-bb6e-e3db1ea117a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,326 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,387 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,602 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,160 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,449 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,595 kB]\n",
            "Fetched 18.8 MB in 4s (4,258 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl logrotate netbase\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common ssl-cert sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl logrotate netbase\n",
            "  postgresql-14 postgresql-client-14 postgresql-client-common postgresql-common ssl-cert sysstat\n",
            "0 upgraded, 12 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 18.4 MB of archives.\n",
            "After this operation, 51.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-xs-perl amd64 4.030-1build3 [87.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.13-0ubuntu0.22.04.1 [1,225 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.13-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 18.4 MB in 4s (5,222 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 123621 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../05-libjson-xs-perl_4.030-1build3_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.030-1build3) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../06-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../07-postgresql-client-14_14.13-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../08-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../09-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../10-postgresql-14_14.13-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../11-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer â†’ /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer â†’ /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer â†’ /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service â†’ /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.030-1build3) ...\n",
            "Setting up postgresql-client-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-common (238) ...\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service â†’ /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            " * Restarting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  binfmt-support libffi-dev libpfm4 libz3-4 libz3-dev llvm-14 llvm-14-dev\n",
            "  llvm-14-runtime llvm-14-tools postgresql-server-dev-14 python3-pygments\n",
            "  python3-yaml\n",
            "Suggested packages:\n",
            "  llvm-14-doc python-pygments-doc ttf-bitstream-vera\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev libpfm4 libz3-4 libz3-dev llvm-14 llvm-14-dev\n",
            "  llvm-14-runtime llvm-14-tools postgresql-server-dev-14\n",
            "  postgresql-server-dev-all python3-pygments python3-yaml\n",
            "0 upgraded, 13 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 59.8 MB of archives.\n",
            "After this operation, 361 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-yaml amd64 5.4.1-1ubuntu1 [129 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 binfmt-support amd64 2.2.1-2 [55.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14-runtime amd64 1:14.0.0-1ubuntu1.1 [484 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpfm4 amd64 4.11.1+git32-gd0b85fb-1ubuntu0.1 [345 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14 amd64 1:14.0.0-1ubuntu1.1 [12.7 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libffi-dev amd64 3.4.2-4 [63.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-pygments all 2.11.2+dfsg-2 [750 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14-tools amd64 1:14.0.0-1ubuntu1.1 [404 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libz3-4 amd64 4.8.12-1 [5,766 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libz3-dev amd64 4.8.12-1 [72.2 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14-dev amd64 1:14.0.0-1ubuntu1.1 [37.8 MB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 postgresql-server-dev-14 amd64 14.13-0ubuntu0.22.04.1 [1,177 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 postgresql-server-dev-all amd64 238 [14.0 kB]\n",
            "Fetched 59.8 MB in 7s (8,743 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 13.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-yaml.\n",
            "(Reading database ... 125579 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python3-yaml_5.4.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking python3-yaml (5.4.1-1ubuntu1) ...\n",
            "Selecting previously unselected package binfmt-support.\n",
            "Preparing to unpack .../01-binfmt-support_2.2.1-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.2.1-2) ...\n",
            "Selecting previously unselected package llvm-14-runtime.\n",
            "Preparing to unpack .../02-llvm-14-runtime_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14-runtime (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libpfm4:amd64.\n",
            "Preparing to unpack .../03-libpfm4_4.11.1+git32-gd0b85fb-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpfm4:amd64 (4.11.1+git32-gd0b85fb-1ubuntu0.1) ...\n",
            "Selecting previously unselected package llvm-14.\n",
            "Preparing to unpack .../04-llvm-14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../05-libffi-dev_3.4.2-4_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.4.2-4) ...\n",
            "Selecting previously unselected package python3-pygments.\n",
            "Preparing to unpack .../06-python3-pygments_2.11.2+dfsg-2_all.deb ...\n",
            "Unpacking python3-pygments (2.11.2+dfsg-2) ...\n",
            "Selecting previously unselected package llvm-14-tools.\n",
            "Preparing to unpack .../07-llvm-14-tools_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14-tools (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libz3-4:amd64.\n",
            "Preparing to unpack .../08-libz3-4_4.8.12-1_amd64.deb ...\n",
            "Unpacking libz3-4:amd64 (4.8.12-1) ...\n",
            "Selecting previously unselected package libz3-dev:amd64.\n",
            "Preparing to unpack .../09-libz3-dev_4.8.12-1_amd64.deb ...\n",
            "Unpacking libz3-dev:amd64 (4.8.12-1) ...\n",
            "Selecting previously unselected package llvm-14-dev.\n",
            "Preparing to unpack .../10-llvm-14-dev_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package postgresql-server-dev-14.\n",
            "Preparing to unpack .../11-postgresql-server-dev-14_14.13-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-server-dev-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql-server-dev-all:amd64.\n",
            "Preparing to unpack .../12-postgresql-server-dev-all_238_amd64.deb ...\n",
            "Unpacking postgresql-server-dev-all:amd64 (238) ...\n",
            "Setting up python3-yaml (5.4.1-1ubuntu1) ...\n",
            "Setting up libffi-dev:amd64 (3.4.2-4) ...\n",
            "Setting up python3-pygments (2.11.2+dfsg-2) ...\n",
            "Setting up libz3-4:amd64 (4.8.12-1) ...\n",
            "Setting up libpfm4:amd64 (4.11.1+git32-gd0b85fb-1ubuntu0.1) ...\n",
            "Setting up llvm-14-runtime (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up binfmt-support (2.2.1-2) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service â†’ /lib/systemd/system/binfmt-support.service.\n",
            "Setting up llvm-14 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up llvm-14-tools (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libz3-dev:amd64 (4.8.12-1) ...\n",
            "Setting up llvm-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up postgresql-server-dev-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "Setting up postgresql-server-dev-all:amd64 (238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u postgres psql -c \"CREATE USER postgres WITH SUPERUSER\"\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#ERROR:  role \"postgres\" already exists\n",
        "#ALTER ROLE\n",
        "\n",
        "QUERY_create='CREATE TABLE table_name_24 (score VARCHAR, date VARCHAR)'\n",
        "\n",
        "\n",
        "QUERY_select='SELECT 2009 FROM table_name_50 WHERE 2011 = \"a\"'"
      ],
      "metadata": {
        "id": "o2CRZUowyOgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3dd582-30cb-4d4b-a852-36e87780ff7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  role \"postgres\" already exists\n",
            "ALTER ROLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def table_creator(query):\n",
        "    import os\n",
        "    import psycopg2 as ps\n",
        "    import pandas as pd\n",
        "\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                  user=DB_USER,\n",
        "                  password=DB_PASS,\n",
        "                  host=DB_HOST,\n",
        "                  port=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    # Wrap the execute command in a try-except block to handle potential errors\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "                            %s\n",
        "                            \"\"\"%query)\n",
        "        conn.commit()\n",
        "        print(\"Table Created successfully\")\n",
        "    except Exception as e:\n",
        "        conn.rollback() # Rollback the transaction in case of an error\n",
        "        print(\"Error creating table:\", e)\n",
        "\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "QSIg-lixyyi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\""
      ],
      "metadata": {
        "id": "UCljf57lzQOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psycopg2 as ps\n",
        "import pandas as pd\n",
        "\n",
        "def table_select(query):\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "                      user=DB_USER,\n",
        "                      password=DB_PASS,\n",
        "                      host=DB_HOST,\n",
        "                      port=DB_PORT)\n",
        "    print(\"Database connected successfully\")\n",
        "\n",
        "    #query = query.replace('\"', \"'\") # Replace double quotes with single quotes for potential date values\n",
        "\n",
        "    try:\n",
        "\n",
        "        #df = pd.read_sql_query(\"%s\"%query, con=conn)\n",
        "        #print('rec: %'%df) # Print the resulting DataFrame\n",
        "\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(query)\n",
        "        rows = cur.fetchall()\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print('\\n')\n",
        "        print('Record(s): %s \\n'%len(rows))\n",
        "        for row in rows:\n",
        "            print(row)\n",
        "\n",
        "\n",
        "        eqc=1\n",
        "\n",
        "    except Exception as e:\n",
        "        eqc=0\n",
        "        #conn.rollback() # Rollback the transaction in case of an error\n",
        "        print(\"Error executing query:\", e)\n",
        "        #print('TABLE IS EMPTY')\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    return eqc"
      ],
      "metadata": {
        "id": "nN2BDEMnzMUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_creator(QUERY_create)"
      ],
      "metadata": {
        "id": "YtGbYghLzuNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00feecb2-3d6b-45f1-dd71-c2a9f8242b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table Created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate_model"
      ],
      "metadata": {
        "id": "GoAUvQWiz4ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import psycopg2\n",
        "from transformers import Trainer\n",
        "\n",
        "# Your PostgreSQL configuration\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "\n",
        "def evaluate_model(model, tokenizer, sentence_transformer_model, eval_dataset):\n",
        "    \"\"\"\n",
        "    Evaluates a model on a given dataset and computes exact match and semantic similarity metrics,\n",
        "    after creating necessary tables in the PostgreSQL database.\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned model to be evaluated.\n",
        "        tokenizer: The tokenizer used for encoding and decoding text.\n",
        "        sentence_transformer_model: The sentence transformer model used for semantic similarity.\n",
        "        eval_dataset: The dataset on which the model will be evaluated.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the evaluation results: 'exact_match' and 'semantic_similarity'.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Table Creation\n",
        "    db_config = {\n",
        "        'database': DB_NAME,\n",
        "        'user': DB_USER,\n",
        "        'password': DB_PASS,\n",
        "        'host': DB_HOST,\n",
        "        'port': DB_PORT\n",
        "    }\n",
        "\n",
        "    with psycopg2.connect(**db_config) as conn:\n",
        "        with conn.cursor() as cur:\n",
        "            for example in eval_dataset.dataset:\n",
        "                context = example['context']\n",
        "                create_table_statements = context.split(';')\n",
        "                for create_table_statement in create_table_statements:\n",
        "                    if create_table_statement.strip():\n",
        "                        try:\n",
        "                            cur.execute(create_table_statement)\n",
        "                            conn.commit()\n",
        "                        except psycopg2.Error as e:\n",
        "                            logging.error(f\"Error creating table: {e}\")\n",
        "\n",
        "    # Create a Trainer instance with the compute_metrics function\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        compute_metrics=compute_metrics,  # Remove the lambda function and db_config\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    results = trainer.evaluate(eval_dataset)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "1V9Ps80FPswP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATOR\n"
      ],
      "metadata": {
        "id": "Zcq8e4lnWU7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft --quiet\n",
        "!pip install colab-env --quiet\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "import colab_env\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "Bcp6WUQEXn6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mistral-7B"
      ],
      "metadata": {
        "id": "O7btmC4JXSds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "#from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" #24 JUNE 2024\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
        "#model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "id": "VTOU_Wl5wuKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT PURE"
      ],
      "metadata": {
        "id": "3vP76vbDOaOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-dataeval\"\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "# Removed torch.inference_mode() context manager\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    peft_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Force model to initialize weights by running a dummy forward pass\n",
        "dummy_input = torch.zeros((1, 1), dtype=torch.long, device=model.device)  # Create a dummy input tensor\n",
        "_ = model(dummy_input)  # Run a forward pass to initialize weights\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "furehuMtOT4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT MIX"
      ],
      "metadata": {
        "id": "9PsTGcshOiuL"
      }
    },
    {
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM, PeftModel\n",
        "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# Path to your PEFT adapter weights\n",
        "peft_model_id = '/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-1950/'\n",
        "\n",
        "#peft_model_id = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-dataeval\"\n",
        "\n",
        "# Load the PEFT adapter\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(base_model_id) # Use the base model ID here\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rZvOAmkipnCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/test_dataset.json\", split=\"train\")\n",
        "\n",
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxXgXIKfr73w",
        "outputId": "36245a46-d438-46af-ec93-324ad71d856b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages'],\n",
              "    num_rows: 2500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0]"
      ],
      "metadata": {
        "id": "tx2jutztujAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "similar(\"Apple\",\"Appel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7fqUNNisOXN",
        "outputId": "6d3ab871-55b0-4317-b7b3-066400897fb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "IRxgSnzf0MPE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"frankmorales2020/Mistral-7B-text-to-sql-flash-attention-2-dataeval\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    peft_model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "NbqJynuNzHr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from random import randint\n",
        "from datasets import load_dataset\n",
        "import psycopg2\n",
        "from psycopg2 import sql\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
        "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
        "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    schema=sample[\"messages\"][0]['content']\n",
        "    schema_query=schema[153:len(schema)]\n",
        "    question = sample[\"messages\"][1][\"content\"]\n",
        "    original_answer = sample[\"messages\"][2][\"content\"]\n",
        "\n",
        "    ps=similar(predicted_answer,original_answer)\n",
        "\n",
        "    if ps >= 0.95:\n",
        "        print('\\n')\n",
        "        print(f'Generated Answer-SIMILARY: {predicted_answer}')\n",
        "        print(f' Original Answer-SIMILARY: {original_answer}')\n",
        "        print(f'        SIMILARY: {ps}')\n",
        "        print('\\n\\n')\n",
        "        predicted_answer=original_answer\n",
        "\n",
        "    if predicted_answer ==  original_answer:\n",
        "        print()\n",
        "        print()\n",
        "        print('SUCCESS!')\n",
        "        print()\n",
        "        print(f'QUESTION: {question}')\n",
        "        print()\n",
        "        print(f'SCHEMA QUERY: {schema_query}')\n",
        "        #table_creator(schema_query)\n",
        "        print()\n",
        "        print(f'Generated Answer: {predicted_answer}')\n",
        "        #table_select(predicted_answer)\n",
        "        print()\n",
        "        print(f'Original Answer: {original_answer}')\n",
        "        print()\n",
        "        return 1\n",
        "    else:\n",
        "        print()\n",
        "        print()\n",
        "        print('NO - SUCCESS!')\n",
        "        print()\n",
        "        print(f'QUESTION: {question}')\n",
        "\n",
        "        #ps=similar(predicted_answer,original_answer)\n",
        "        print(f'Generated Answer: {predicted_answer}')\n",
        "        print(f' Original Answer: {original_answer}')\n",
        "        print(f'        SIMILARY: {ps}')\n",
        "        print()\n",
        "\n",
        "        return 0\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 10\n",
        "\n",
        "# iterate over eval dataset and predict\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "    s=eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "# compute accuracy\n",
        "accuracy = sum(success_rate)/len(success_rate)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IUsV-itsQP0",
        "outputId": "e9c6b395-da67-4577-cfdb-9604a8809b5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|â–ˆ         | 1/10 [00:02<00:20,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: How many students have a food allergy?\n",
            "Generated Answer: SELECT COUNT(*) FROM Has_allergy WHERE allergy = 'food'\n",
            " Original Answer: SELECT COUNT(*) FROM Has_allergy AS T1 JOIN Allergy_type AS T2 ON T1.allergy = T2.allergy WHERE T2.allergytype = \"food\"\n",
            "        SIMILARY: 0.5517241379310345\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 2/10 [00:10<00:45,  5.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: Return the name and gender of the staff who was assigned in 2016.\n",
            "Generated Answer: SELECT staff_name, staff_gender\n",
            "FROM staff\n",
            "INNER JOIN staff_department_assignments\n",
            "ON staff.staff_id = staff_department_assignments.staff_id\n",
            "WHERE YEAR(date_assigned_from) = 2016;\n",
            " Original Answer: SELECT T1.staff_name, T1.staff_gender FROM staff AS T1 JOIN staff_department_assignments AS T2 ON T1.staff_id = T2.staff_id WHERE T2.date_assigned_from LIKE \"2016%\"\n",
            "        SIMILARY: 0.7521865889212828\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:20<00:52,  7.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: What is the highest week for Cleveland Browns with 54,205 in attendance?\n",
            "Generated Answer: Here is the SQL query to find the highest week for the Cleveland Browns with an attendance of 54,205:\n",
            "```\n",
            "SELECT week FROM table_name_47\n",
            "WHERE opponent = 'Cleveland Browns'\n",
            "AND attendance = '54,205'\n",
            "ORDER BY week DESC\n",
            "LIMIT 1;\n",
            "```\n",
            " Original Answer: SELECT MAX(week) FROM table_name_47 WHERE opponent = \"cleveland browns\" AND attendance > 54 OFFSET 205\n",
            "        SIMILARY: 0.4939759036144578\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:35<01:02, 10.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: Who wrote the episode with a production code of 116?\n",
            "Generated Answer: Here's a SQL query that can help you answer this question based on the provided schema:\n",
            "```\n",
            "SELECT written_by\n",
            "FROM table_29087004_2\n",
            "WHERE production_code = '116'\n",
            "```\n",
            "This query selects the `written_by` column from the `table_29087004_2` table where the `production_code` column is equal to '116'. This will return the name of the person who wrote the episode with the production code of 116.\n",
            " Original Answer: SELECT written_by FROM table_29087004_2 WHERE production_code = 116\n",
            "        SIMILARY: 0.2838427947598253\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:37<00:37,  7.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: what fleet is associated with the number L4?\n",
            "Generated Answer: SELECT fleet FROM table_name_61 WHERE number = 'L4';\n",
            " Original Answer: SELECT fleet FROM table_name_61 WHERE number = \"l4\"\n",
            "        SIMILARY: 0.9320388349514563\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:41<00:24,  6.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated Answer-SIMILARY: SELECT console FROM table_12887260_1 WHERE franchise_or_game = 'Shenmue';\n",
            " Original Answer-SIMILARY: SELECT console FROM table_12887260_1 WHERE franchise_or_game = \"Shenmue\"\n",
            "        SIMILARY: 0.9655172413793104\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SUCCESS!\n",
            "\n",
            "QUESTION: What consoles was Shenmue released on?\n",
            "\n",
            "SCHEMA QUERY: CREATE TABLE table_12887260_1 (console VARCHAR, franchise_or_game VARCHAR)\n",
            "\n",
            "Generated Answer: SELECT console FROM table_12887260_1 WHERE franchise_or_game = \"Shenmue\"\n",
            "\n",
            "Original Answer: SELECT console FROM table_12887260_1 WHERE franchise_or_game = \"Shenmue\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:43<00:15,  5.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: How many customers live in Prague city?\n",
            "Generated Answer: ```\n",
            "SELECT COUNT(*) FROM customers WHERE city = 'Prague';\n",
            "```\n",
            " Original Answer: SELECT COUNT(*) FROM customers WHERE city = \"Prague\"\n",
            "        SIMILARY: 0.8849557522123894\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:48<00:09,  4.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: Who is the driver of the entry constructed by Mercedes-Benz?\n",
            "Generated Answer: The SQL query for this would be:\n",
            "\n",
            "SELECT driver FROM table_18893428_1 WHERE constructor = 'Mercedes-Benz';\n",
            " Original Answer: SELECT driver FROM table_18893428_1 WHERE constructor = \"Mercedes-Benz\"\n",
            "        SIMILARY: 0.7796610169491526\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:52<00:04,  4.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: How many different courses offered by Physics department?\n",
            "Generated Answer: The SQL query for this would be:\n",
            "\n",
            "SELECT COUNT(course_id)\n",
            "FROM course\n",
            "WHERE dept_name = 'Physics';\n",
            " Original Answer: SELECT COUNT(DISTINCT course_id) FROM course WHERE dept_name = 'Physics'\n",
            "        SIMILARY: 0.7176470588235294\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:59<00:00,  6.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "NO - SUCCESS!\n",
            "\n",
            "QUESTION: The president, peter williamson, had how many home grounds?\n",
            "Generated Answer: The SQL query for the provided schema to find the number of home grounds for Peter Williamson would be:\n",
            "```sql\n",
            "SELECT COUNT(*) AS home_ground_count\n",
            "FROM table_11365528_2\n",
            "WHERE president = 'Peter Williamson'\n",
            "```\n",
            " Original Answer: SELECT COUNT(home_ground) FROM table_11365528_2 WHERE president = \"Peter Williamson\"\n",
            "        SIMILARY: 0.5374149659863946\n",
            "\n",
            "Accuracy: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlGJja5Z28bj",
        "outputId": "277c6293-6d85-49d0-e382-3c1a0094d226"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-EjRZXLbfs4",
        "outputId": "97e074bc-3598-4c40-bd20-cae9e21af7a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Tue Oct 15 13:44:42 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   76C    P0              34W /  72W |   4389MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION-TRAINER"
      ],
      "metadata": {
        "id": "jnxGtqh44nh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "from trl import setup_chat_format\n",
        "\n",
        "import colab_env\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "kEhVOcPuSpb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming 'edges' data is present in the eval_dataset, modify the TextToSQLDataset class to include 'edges' in the returned dictionary.\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset[idx]\n",
        "        # Assuming 'edges' is a key in your dataset\n",
        "        edges = row.get('edges')\n",
        "        encoding = self.tokenizer(\n",
        "            row[\"question\"],\n",
        "            row[\"context\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Include 'edges' in the returned dictionary\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": self.tokenizer(row[\"answer\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"].squeeze(),\n",
        "            \"edges\": edges # Make sure edges are properly formatted for your GraphModel\n",
        "        }"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BXRELuTSHk5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert all elements to tensors, handling different data types\n",
        "    predictions = [torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred for pred in all_preds]\n",
        "    labels = [torch.tensor(label) if not isinstance(label, torch.Tensor) else label for label in all_labels]\n",
        "\n",
        "    # Filter out any None values before stacking\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Convert to tensors and stack (only if there are predictions/labels)\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])  # Empty tensor if no predictions\n",
        "\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])  # Empty tensor if no labels\n",
        "\n",
        "    # Handle cases where only one prediction/label is present (avoid squeezing to a scalar)\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print('\\n')\n",
        "    print(f\"Shape of logits in compute_metrics: {predictions.shape}\")\n",
        "    print(f\"Shape of labels in compute_metrics: {labels.shape}\")\n",
        "    print('\\n')\n",
        "\n",
        "    # Ensure predictions is a list of lists\n",
        "    predictions = predictions.tolist()\n",
        "    # Remove the extra nesting if present\n",
        "    if isinstance(predictions[0], list):\n",
        "        if isinstance(predictions[0][0], list):\n",
        "            predictions = [p[0] for p in predictions]\n",
        "\n",
        "    # Convert logits to predicted token ids\n",
        "    predictions = torch.tensor(predictions).argmax(dim=-1)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Load the metric\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    return {\"exact_match\": em}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FPHaEDPIaChT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoTokenizer\n",
        "from peft import PeftModel # PeftModel is now correctly imported from peft\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load your fine-tuned model\n",
        "output_dir = \"/content/gdrive/MyDrive/model/GNNT2SQL\"\n",
        "\n",
        "#model_id ='/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-100/'\n",
        "\n",
        "model_id = '/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-1950/'\n",
        "\n",
        "# Load the base model first\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "\n",
        "# Use PeftModel to load the model, pass the model object and model_id as arguments\n",
        "model = PeftModel.from_pretrained(mistral_model, model_id)\n",
        "\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Set padding side to 'left'\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Add the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the sentence transformer model\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Prepare your evaluation dataset\n",
        "eval_dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "POC_valsample = 100\n",
        "eval_dataset = eval_dataset.select(np.random.choice(len(eval_dataset), POC_valsample, replace=False))\n",
        "\n",
        "# Assuming TextToSQLDataset is a custom class, ensure it handles 'edges' data\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "\n",
        "# Evaluate the model\n",
        "results = evaluate_model(model, tokenizer, sentence_transformer_model, eval_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(f'Exact Match: {results[\"eval_exact_match\"]:.4f}')\n",
        "#print(f'Semantic Similarity: {results[\"eval_semantic_similarity\"]:.4f}')\n",
        "\n",
        "# Ensure that the 'edges' data is correctly included and passed to the model within the TextToSQLDataset class and evaluate_model function."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tVtK77MpLwUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    all_preds, all_labels = eval_pred\n",
        "\n",
        "    # Convert all elements to tensors, handling different data types\n",
        "    predictions = [torch.tensor(pred) if not isinstance(pred, torch.Tensor) else pred for pred in all_preds]\n",
        "    labels = [torch.tensor(label) if not isinstance(label, torch.Tensor) else label for label in all_labels]\n",
        "\n",
        "    # Filter out any None values before stacking\n",
        "    predictions = [pred for pred in predictions if pred is not None]\n",
        "    labels = [label for label in labels if label is not None]\n",
        "\n",
        "    # Convert to tensors and stack (only if there are predictions/labels)\n",
        "    if predictions:\n",
        "        predictions = torch.stack(predictions).squeeze()\n",
        "    else:\n",
        "        predictions = torch.tensor([])  # Empty tensor if no predictions\n",
        "\n",
        "    if labels:\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "    else:\n",
        "        labels = torch.tensor([])  # Empty tensor if no labels\n",
        "\n",
        "    # Handle cases where only one prediction/label is present (avoid squeezing to a scalar)\n",
        "    if predictions.dim() == 0:\n",
        "        predictions = predictions.unsqueeze(0)\n",
        "    if labels.dim() == 0:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print('\\n')\n",
        "    print(f\"Shape of logits in compute_metrics: {predictions.shape}\")\n",
        "    print(f\"Shape of labels in compute_metrics: {labels.shape}\")\n",
        "    print('\\n')\n",
        "\n",
        "    # Ensure predictions is a list of lists\n",
        "    predictions = predictions.tolist()\n",
        "    # Remove the extra nesting if present\n",
        "    if isinstance(predictions[0], list):\n",
        "        if isinstance(predictions[0][0], list):\n",
        "            predictions = [p[0] for p in predictions]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    return {\"exact_match\": em}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "AiaEhUtWX52_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION STAND-ALONE\n"
      ],
      "metadata": {
        "id": "2hAPAwznG6Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, Trainer, TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    PrefixTuningConfig,\n",
        "    PromptEncoderConfig,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch_geometric.nn import GAT\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"The installed version of bitsandbytes was compiled without GPU support.\")\n",
        "\n",
        "\n",
        "# Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    # Download if not already downloaded\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "ENHJRQaU91v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  GraphModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "\n",
        "#from transformers import CausalLMOutputWithPast\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class MyCausalLMOutputWithPast:\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "class GraphModel(nn.Module):\n",
        "    def __init__(self, encoder, tokenizer):\n",
        "        super(GraphModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config = encoder.model.config\n",
        "        self.gatv2 = GATv2Conv(\n",
        "            in_channels=self.config.hidden_size,\n",
        "            out_channels=self.config.hidden_size,\n",
        "            heads=8,\n",
        "            concat=False,\n",
        "        )\n",
        "\n",
        "        # Max Pooling\n",
        "        self.pool = lambda x, batch: torch.max(x, dim=0, keepdim=True)[0]\n",
        "\n",
        "        # Additional Feedforward Layer\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(self.config.hidden_size, self.config.hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.config.hidden_size * 2, self.config.hidden_size),\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Add generation config (you might need to adjust this based on your specific needs)\n",
        "        self.generation_config = encoder.generation_config\n",
        "\n",
        "    # Forward Pass\n",
        "    #def forward(self, input_ids, attention_mask, edges, labels=None, inputs_embeds=None, sample_ids=None, output_attentions=False, output_hidden_states=False, return_dict=False):\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, edges, labels=None, inputs_embeds=None, sample_ids=None,\n",
        "                output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Print vocabulary sizes\n",
        "        #print('\\n')\n",
        "        #print(\"Mistral Model Vocab Size:\", self.encoder.config.vocab_size)\n",
        "        #print(\"Tokenizer Vocab Size:\", self.tokenizer.vocab_size)\n",
        "        #print('\\n')\n",
        "\n",
        "\n",
        "        # 1. Token Embeddings (Encoder)\n",
        "        if input_ids is not None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids.to(self.encoder.model.device),\n",
        "                attention_mask=attention_mask.to(self.encoder.model.device),\n",
        "                #output_hidden_states=True\n",
        "            )\n",
        "            embeddings = encoder_outputs.hidden_states[-1]\n",
        "            print(\"Encoder output shape:\", embeddings.shape)  # Inspect encoder output\n",
        "\n",
        "        elif inputs_embeds is not None:\n",
        "            embeddings = inputs_embeds\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        # Ensure correct shape for GATv2Conv\n",
        "        if embeddings.dim() > 2:\n",
        "            embeddings = embeddings.view(-1, embeddings.shape[-1])\n",
        "\n",
        "        # 2. Edge Index Creation (Batched, with Enhanced Error Handling)\n",
        "        edge_index = []\n",
        "        node_offset = 0\n",
        "        for i, graph_edges in enumerate(edges):\n",
        "            if graph_edges is None or graph_edges.numel() == 0:  # Check if graph_edges is None or empty\n",
        "                num_nodes = input_ids.size(1)\n",
        "                # Create self-loops for isolated nodes if no edges are provided\n",
        "                graph_edges = torch.arange(node_offset, node_offset + num_nodes, device=embeddings.device)\n",
        "                graph_edges = graph_edges.repeat(2, 1) # Repeat the tensor, not the arange object\n",
        "            else: # Add this else block to handle the case when edges are present\n",
        "                # Ensure graph_edges is a tensor before adding offset\n",
        "                if not isinstance(graph_edges, torch.Tensor):\n",
        "                    graph_edges = torch.tensor(graph_edges, dtype=torch.long, device=embeddings.device)\n",
        "                graph_edges += node_offset  # Now safe to add offset\n",
        "            edge_index.append(graph_edges)\n",
        "            node_offset += input_ids.size(1)\n",
        "        edge_index = torch.cat(edge_index, dim=1)\n",
        "\n",
        "        # 3. GATv2 Layer\n",
        "        graph_out = self.gatv2(embeddings, edge_index)\n",
        "\n",
        "        # 3. GATv2 Layer\n",
        "        graph_out = self.gatv2(embeddings, edge_index)\n",
        "        print(\"GATv2 output shape:\", graph_out.shape)  # Inspect GATv2 output\n",
        "\n",
        "        # Print logits before and after applying logits processor\n",
        "        print(\"Original logits:\", logits)\n",
        "        filtered_logits = logits_processor(None, logits)\n",
        "        print(\"Filtered logits:\", filtered_logits)\n",
        "\n",
        "\n",
        "        # 4. Pooling (using max pooling)\n",
        "        batch = torch.arange(len(edges), device=graph_out.device).repeat_interleave(input_ids.size(1))\n",
        "        pooled = self.pool(graph_out, batch).unsqueeze(1)\n",
        "\n",
        "        # Additional Feedforward Layer\n",
        "        pooled = self.ffn(pooled)\n",
        "\n",
        "        # 5. LM Head\n",
        "        logits = self.lm_head(pooled)\n",
        "\n",
        "        from torch.nn import CrossEntropyLoss\n",
        "        # 6. Loss Calculation (if labels provided)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "\n",
        "             # Print Sample IDs first\n",
        "            print(f\"\\nIteration/Step: {trainer.state.global_step}\")\n",
        "            mask = (labels != -100).float()\n",
        "\n",
        "            #for i, sample_id in enumerate(sample_ids):\n",
        "            #    print(f\"Sample ID: {sample_id.item()}\")\n",
        "\n",
        "\n",
        "            # Now decode and print input, target, and loss for each sample\n",
        "            with torch.no_grad():\n",
        "                input_text = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "                target_text = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "                for i, sample_id in enumerate(sample_ids):\n",
        "                    print(f\"Sample ID: {sample_id.item()}\")\n",
        "                    print(\"Decoded Input:\", input_text[i])\n",
        "                    print(\"Decoded Target (Labels):\", target_text[i])\n",
        "\n",
        "\n",
        "\n",
        "            # Apply softmax to logits to get probabilities\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            #loss_fct = nn.NLLLoss(ignore_index=-100)  # Use NLLLoss for log probabilities\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "            # Reshape log_probs to match target shape (remove the extra dimension)\n",
        "            log_probs = log_probs.squeeze(1)\n",
        "\n",
        "\n",
        "            #labels = labels[:, 0] ### WORK UNTIL BEFORE THE LAST INTERACTION\n",
        "            #labels = labels.squeeze(1)   # Remove extra dimension from labels ## DON'T work\n",
        "            #labels = labels.view(-1)  # Flatten labels to a 1D tensor DON'T work\n",
        "\n",
        "            labels = labels[:, 0]\n",
        "\n",
        "            # Calculate loss with the correctly shaped labels\n",
        "            #loss = loss_fct(log_probs, labels[:, 0])  # Access the first element of each label sequence\n",
        "\n",
        "\n",
        "            loss = loss_fct(log_probs, labels)\n",
        "\n",
        "\n",
        "\n",
        "            loss_per_sample = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "            print(f\"Loss per sample: {loss_per_sample.item()}\")\n",
        "\n",
        "\n",
        "        # 7. Return (Modified to return a tuple)\n",
        "            if labels is not None:\n",
        "                return (loss, logits, None)\n",
        "            else:\n",
        "                return (None, logits, None)\n",
        "\n",
        "        return MyCausalLMOutputWithPast(\n",
        "                loss=loss,\n",
        "                logits=logits,\n",
        "                past_key_values=encoder_outputs.past_key_values,\n",
        "                hidden_states=encoder_outputs.hidden_states,\n",
        "                attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, edges, attention_mask=None, **kwargs):\n",
        "        if isinstance(self, PeftModel):\n",
        "            return self.base_model.prepare_inputs_for_generation(input_ids, edges, attention_mask, **kwargs)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        if batch_size > 1:\n",
        "            batched_edges = []\n",
        "            node_offset = 0\n",
        "            for i in range(batch_size):\n",
        "                graph_edges = edges[i]\n",
        "                batched_edges.extend([(src + node_offset, dst + node_offset) for src, dst in graph_edges])\n",
        "                node_offset += input_ids.size(1)\n",
        "            edge_index = torch.tensor(batched_edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "        else:\n",
        "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(input_ids.device)\n",
        "\n",
        "        model_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"edges\": edge_index,\n",
        "            \"past_key_values\": kwargs.get(\"past_key_values\", None),\n",
        "        }\n",
        "        return model_inputs\n",
        "\n",
        "# END GraphModel"
      ],
      "metadata": {
        "id": "BTT38RlOEhYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch_geometric.data import Batch as GraphBatch  # Note the import\n",
        "import torch\n",
        "import torch_geometric.data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class GraphDataCollatorForSeq2Seq:\n",
        "    def __init__(self, tokenizer, model=None, label_pad_token_id=-100, pad_to_multiple_of=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Separate standard features from graph edges\n",
        "        # Extract labels before padding and handle potentially empty sequences\n",
        "        labels = [feature[\"labels\"] if feature[\"labels\"].numel() > 0\n",
        "                  else torch.tensor([self.label_pad_token_id], dtype=torch.long)\n",
        "                  for feature in features]\n",
        "\n",
        "        # Extract sample_ids\n",
        "        sample_ids = [feature[\"sample_ids\"] for feature in features]\n",
        "\n",
        "        standard_features = [{k: v for k, v in feature.items() if k != \"edges\" and k != \"labels\"} for feature in features]\n",
        "        edges = [feature[\"edges\"] for feature in features]\n",
        "\n",
        "        # Collate standard features (input_ids, attention_mask) using default collator\n",
        "        collated_standard_features =  DataCollatorForSeq2Seq(\n",
        "            tokenizer=self.tokenizer,\n",
        "            model=self.model,\n",
        "            label_pad_token_id=self.label_pad_token_id,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of\n",
        "        )(standard_features)\n",
        "\n",
        "        # Pad input_ids and attention_mask\n",
        "        input_ids = pad_sequence([f['input_ids'] for f in standard_features], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([f['attention_mask'] for f in standard_features], batch_first=True, padding_value=0)\n",
        "\n",
        "         # Pad labels separately\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n",
        "\n",
        "        # Create batch for graph data\n",
        "        graph_data_list = []\n",
        "        for i in range(len(edges)):\n",
        "            # Convert to PyTorch Geometric Data\n",
        "            graph_data_list.append(torch_geometric.data.Data(\n",
        "                x=collated_standard_features['input_ids'][i].unsqueeze(1),  # Node features (input_ids)\n",
        "                edge_index=edges[i],                   # Edge index\n",
        "                # Use num_edges for batch index to ensure correct batching in PyG\n",
        "                batch=torch.tensor([i] * edges[i].size(1))\n",
        "            ))\n",
        "        batched_graph = GraphBatch.from_data_list(graph_data_list)  # Batch graphs\n",
        "\n",
        "        #sample_ids = torch.arange(len(edges)).unsqueeze(1)\n",
        "        #sample_ids = [f[\"sample_id\"] for f in  collated_standard_features]\n",
        "        #sample_ids = [f[\"sample_id\"] for f in features]\n",
        "\n",
        "        #print(f\"Sample GraphDataCollatorForSeq2Seq ID: {sample_ids}\")\n",
        "\n",
        "\n",
        "\n",
        "         # Include sample_ids in the collated features\n",
        "        collated_features = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edges,\n",
        "            \"sample_ids\": sample_ids  # Add sample_ids here\n",
        "        }\n",
        "\n",
        "        return collated_features\n"
      ],
      "metadata": {
        "id": "g9qhWPQiQBPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Mistral Model and Tokenizer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Model Configuration\n",
        "config = AutoModelForCausalLM.from_pretrained(model_id).config\n",
        "config.output_hidden_states = True\n",
        "config.use_cache = False\n",
        "#config.torch_dtype = torch.float32\n",
        "config.torch_dtype = torch.bfloat16\n",
        "\n",
        "# Load Model with Quantization\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"flash_attention_2\",  # Optimization\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "DrXB9McwDzmh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "55c6b2755d59441b8250f7b74aa5a5c5",
            "b89201e08758407eb55e14082c582afe",
            "e3ec263bb96e457c8a6cf4a56eed81d7",
            "fe689d8161614141af7a9e6999a72098",
            "f16922a7f7724901a6022d022b87a9a2",
            "9090acd6a4bc43ea9194c3c568dcfee6",
            "ba16a0dce4b34fe6ac5a8c0f67e7a1ea",
            "e5a5b4c41bff4463ab0a90a2a2f3e98e",
            "daafcc04d255402599863bd6a9b73dcc",
            "5c42c3e07acb4d1d85d01aa9f514cfcd",
            "7c34ccc429a44ff989bc9bca5070c941",
            "3a2045cbccea4534a8c1cf4d6b6067cd",
            "926cf10dfa35406c86470b28d60b2194",
            "e839ce131c0d4003a05dd226d1a759ab",
            "12c19c485ac44c6a84f85ca59ee542b0",
            "86a32fa3c71d4ac3a87276ac5f82620f",
            "6afc1acd104348c18b6cc817db818d78",
            "f2804d4db57941c19aa1d4d5ae31950c",
            "27e3313496754b59b24608088c941ebc",
            "20c25722b3da462cbb83d02d3c430f2e",
            "b375f93729f74e8382bcc99397d3a7a0",
            "edcdd0e394f5471d98c6781b0f7036e4"
          ]
        },
        "outputId": "6187ed92-70ce-4586-aee8-afe39b1f71f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55c6b2755d59441b8250f7b74aa5a5c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a2045cbccea4534a8c1cf4d6b6067cd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "\n",
        "        # 1. Tokenization (with padding and truncation)\n",
        "        tokenized_input = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,  # Increase as needed\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized_target = self.tokenizer(\n",
        "            target_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024,  # Increase as needed\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Flatten lists (if needed)\n",
        "        tokenized_input = {k: v.squeeze(0) for k, v in tokenized_input.items()}\n",
        "        tokenized_target = {k: v.squeeze(0) for k, v in tokenized_target.items()}\n",
        "\n",
        "\n",
        "        # Print input text and target text for debugging\n",
        "        #print('\\n')\n",
        "        #print(f\"Sample ID: {idx}\")\n",
        "        #print(f\"  - Text: {text}\")\n",
        "        #print(f\"  - Target Text: {target_text}\")\n",
        "        #print('\\n')\n",
        "\n",
        "        sample_ids = torch.tensor([idx])  # Create a tensor with the sample ID\n",
        "\n",
        "        # 2. Dependency Parsing for Edge Extraction\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        for token in doc:\n",
        "            head_i = token.head.i\n",
        "            if 0 <= head_i < len(doc) and token.dep_ != \"ROOT\" and token.i != head_i:\n",
        "                edges.append([token.i, head_i])\n",
        "\n",
        "        # Edge Index Extraction and Validation\n",
        "        edges = item.get(\"edges\", edges)  # If \"edges\" is already present in data, use that\n",
        "\n",
        "        if not edges:  # Handle empty graphs\n",
        "            num_nodes = len(tokenized_input[\"input_ids\"])\n",
        "            edges = [[i, i] for i in range(num_nodes)]  # Self-loops for isolated nodes\n",
        "        else:\n",
        "            max_index = len(tokenized_input[\"input_ids\"]) - 1\n",
        "            edges = [(src, tgt) for src, tgt in edges if 0 <= src <= max_index and 0 <= tgt <= max_index]\n",
        "\n",
        "        # Create edge index tensor\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # Convert everything to tensors BEFORE padding/truncation\n",
        "        input_ids = tokenized_input[\"input_ids\"].clone().detach()\n",
        "        attention_mask = tokenized_input[\"attention_mask\"].clone().detach()\n",
        "        labels = tokenized_target[\"input_ids\"].clone().detach()\n",
        "\n",
        "        # Handle potentially empty target sequences\n",
        "        if len(labels) == 0:\n",
        "            labels = torch.tensor([self.tokenizer.pad_token_id], dtype=torch.long)  # Create a single-element tensor with pad token\n",
        "\n",
        "        # Padding and Truncation for consistent input shapes\n",
        "        max_length = 1024  # Adjust if needed\n",
        "\n",
        "        # Ensure that ALL tensors are truncated/padded to the SAME max_length\n",
        "        input_ids = input_ids[:max_length]\n",
        "        attention_mask = attention_mask[:max_length]\n",
        "        labels = labels[:max_length]\n",
        "\n",
        "        # Add padding if necessary\n",
        "        if len(input_ids) < max_length:\n",
        "            pad_length = max_length - len(input_ids)\n",
        "            pad_tensor = torch.full((pad_length,), self.tokenizer.pad_token_id)\n",
        "            input_ids = torch.cat((input_ids, pad_tensor))\n",
        "            attention_mask = torch.cat((attention_mask, torch.zeros(pad_length, dtype=torch.long)))\n",
        "\n",
        "        if len(labels) < max_length:\n",
        "            pad_length = max_length - len(labels)\n",
        "            labels = torch.cat((labels, torch.full((pad_length,), -100)))  # Pad labels with -100\n",
        "\n",
        "        # (Optional) Print statements for debugging\n",
        "        #print(\"\\n\")\n",
        "        #print(\"Original text:\", text)\n",
        "        #print(\"Target text:\", target_text)\n",
        "        #print(\"Tokenized input IDs:\", tokenized_input[\"input_ids\"])\n",
        "        #print(\"Tokenized target IDs:\", tokenized_target[\"input_ids\"])\n",
        "        #print(\"Labels:\", labels)\n",
        "        #print(\"Edge index:\", edge_index)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"edges\": edge_index,\n",
        "            \"sample_ids\": torch.tensor([idx]),  # Add sample_id here\n",
        "        }"
      ],
      "metadata": {
        "id": "vhh1m2r_DYbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# Manually define splits\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "eval_dataset = dataset.select(range(train_size, train_size + val_size))\n",
        "test_dataset = dataset.select(range(train_size + val_size, len(dataset)))\n",
        "\n",
        "\n",
        "POC_valsample=2\n",
        "eval_dataset = eval_dataset.select(np.random.choice(len(eval_dataset), POC_valsample, replace=False))\n",
        "test_dataset = test_dataset.select(np.random.choice(len(test_dataset), POC_valsample, replace=False))\n",
        "\n",
        "\n",
        "\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)"
      ],
      "metadata": {
        "id": "QCe75mw-5mkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = dataset.select(range(train_size, train_size + val_size))\n",
        "eval_dataset = TextToSQLDataset(eval_dataset, tokenizer)\n",
        "eval_dataset[0]"
      ],
      "metadata": {
        "id": "nCiF95oQG_Bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649d9d58-74e2-4287-8ae3-7d3ed9c126e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([    2,     2,     2,  ..., 29538, 29550, 29572]),\n",
              " 'attention_mask': tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
              " 'labels': tensor([    2,     2,     2,  ..., 29550, 29538, 29550]),\n",
              " 'edges': tensor([[ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
              "         [ 1,  5,  5,  5,  1,  8,  8,  5,  8,  8, 12,  8, 15, 15, 12, 12]]),\n",
              " 'sample_ids': tensor([0])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ORIGINAL - evaluate_model"
      ],
      "metadata": {
        "id": "9QB1kXab0rHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=None, edges=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    #print(model)\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    #print('\\n')\n",
        "    #print(model)\n",
        "    #print('\\n')\n",
        "\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            data = eval_dataset[i]\n",
        "\n",
        "            Q = data['input_ids']\n",
        "            A = data['labels']\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "            print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "\n",
        "            # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0).to(device).long(),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float()\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "\n",
        "            # Model inference\n",
        "            model_output = model(**input_data, edges=edges.float())\n",
        "            logits = model_output.logits\n",
        "\n",
        "            if logits.dim() > 2:\n",
        "                logits = logits.squeeze(1)\n",
        "\n",
        "            # Extract predicted tokens\n",
        "            predicted_tokens = logits.argmax(dim=-1).cpu().tolist()\n",
        "\n",
        "\n",
        "            # Print shapes and values for debugging\n",
        "            print(\"\\n\\n\")\n",
        "            #print(\"Model Output:\", model_output)\n",
        "            # Print logits length for debugging\n",
        "            print(\"Logits length:\", logits.shape[1])\n",
        "            print(\"Logits shape:\", logits.shape)\n",
        "            print(\"Logits dtype:\", logits.dtype)\n",
        "            print(\"\\n\")\n",
        "            print(\"Predicted IDs:\", logits.argmax(dim=-1))\n",
        "            print(\"Labels:\", data['labels'])\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Decode predictions and labels for the current sample\n",
        "            decoded_pred = tokenizer.decode(predicted_tokens, skip_special_tokens=True) # Decode for single sample\n",
        "            decoded_label = tokenizer.decode(A.cpu().tolist(), skip_special_tokens=True) # Decode for single sample\n",
        "\n",
        "            # Append the decoded prediction and label for the current sample\n",
        "            all_preds.append(decoded_pred)  # Append decoded prediction for current sample\n",
        "            all_labels.append(decoded_label)  # Append decoded label for current sample\n",
        "\n",
        "    # Calculate exact match\n",
        "    em = metric.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")\n",
        "\n",
        "    return {\"exact_match\": em}"
      ],
      "metadata": {
        "id": "B0rtK7yb45Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORKING"
      ],
      "metadata": {
        "id": "AgAXg074JWdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=None, edges=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            data = eval_dataset[i]\n",
        "\n",
        "            Q = data['input_ids']\n",
        "            A = data['labels']\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "            print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "            # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0).to(device).long(),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float()\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "            try:\n",
        "                # Model inference\n",
        "                model_output = model(**input_data, edges=edges.float())\n",
        "                logits = model_output.logits\n",
        "\n",
        "                # Check for NaN or Inf values in logits\n",
        "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "                    print(\"WARNING: NaN or Inf values detected in logits!\")\n",
        "\n",
        "                if logits.dim() > 2:\n",
        "                    logits = logits.squeeze(1)\n",
        "                elif logits.dim() == 1:\n",
        "                    logits = logits.unsqueeze(0)  # Add a batch dimension if needed\n",
        "\n",
        "                # Extract predicted tokens\n",
        "                predicted_tokens = logits.argmax(dim=-1).cpu().tolist()\n",
        "\n",
        "                # Print shapes and values for debugging\n",
        "                print(\"\\n\\n\")\n",
        "                print(\"Logits length:\", logits.shape[1])\n",
        "                print(\"Logits shape:\", logits.shape)\n",
        "                print(\"Logits dtype:\", logits.dtype)\n",
        "                print(\"\\n\")\n",
        "                print(\"Predicted IDs:\", logits.argmax(dim=-1))\n",
        "                print(\"Labels:\", data['labels'])\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Decode predictions and labels for the current sample\n",
        "                decoded_pred = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n",
        "                decoded_label = tokenizer.decode(A.cpu().tolist(), skip_special_tokens=True)\n",
        "\n",
        "                # Print predicted and target answers\n",
        "                print(\"Predicted Answer:\", decoded_pred)\n",
        "                print(\"Target Answer:\", decoded_label)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Append the decoded prediction and label for the current sample\n",
        "                all_preds.append(decoded_pred)\n",
        "                all_labels.append(decoded_label)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for sample {i}: {e}\")\n",
        "                all_preds.append(\"\")\n",
        "                all_labels.append(\"\")\n",
        "                continue\n",
        "\n",
        "    # Calculate exact match\n",
        "    em = metric.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")\n",
        "\n",
        "    return {\"exact_match\": em}\n"
      ],
      "metadata": {
        "id": "H5ICCGUcAkCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORKINg-2"
      ],
      "metadata": {
        "id": "h2dZ6KwetpRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", edges=None):\n",
        "    \"\"\"\n",
        "    Evaluates a text-to-SQL model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the trained PEFT model.\n",
        "        eval_dataset (Dataset): The dataset to evaluate on.\n",
        "        device (str, optional): The device to use for computation (\"cuda\" or \"cpu\"). Defaults to \"cuda\" if available.\n",
        "        edges (torch.Tensor, optional): Precomputed edge indices for the dataset (if applicable).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the exact match score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model once outside the loop\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            data = eval_dataset[i]\n",
        "\n",
        "            Q = data['input_ids'].to(device).long()\n",
        "            A = data['labels'].to(device).long()\n",
        "            # Check for NaN or Inf values in Q and A\n",
        "            if torch.isnan(Q).any() or torch.isinf(Q).any() or torch.isnan(A).any() or torch.isinf(A).any():\n",
        "                print(\"WARNING: NaN or Inf values detected in input_ids or labels!\")\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "            print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "            # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float()\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "            try:\n",
        "                # Model inference\n",
        "                model_output = model(**input_data, edges=edges.float())\n",
        "                logits = model_output.logits\n",
        "\n",
        "\n",
        "                # Handle potential errors in logits\n",
        "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "                    print(\"WARNING: NaN or Inf values detected in logits!\")\n",
        "                    logits[torch.isnan(logits)] = 0 # Replace NaN with zero\n",
        "                    logits[torch.isinf(logits)] = 0 # Replace Inf with zero\n",
        "\n",
        "\n",
        "                # Ensure logits has the right dimensions before argmax\n",
        "                if logits.dim() > 2:\n",
        "                    logits = logits.squeeze(1)\n",
        "                elif logits.dim() == 1:\n",
        "                    logits = logits.unsqueeze(0)\n",
        "\n",
        "                # Extract predicted tokens directly on GPU\n",
        "                predicted_ids = logits.argmax(dim=-1)\n",
        "\n",
        "                # Print shapes and values for debugging\n",
        "                print(\"\\n\\n\")\n",
        "                print(\"Logits length:\", logits.shape[1])\n",
        "                print(\"Logits shape:\", logits.shape)\n",
        "                print(\"Logits dtype:\", logits.dtype)\n",
        "                print(\"\\n\")\n",
        "                print(\"Predicted IDs:\", logits.argmax(dim=-1))\n",
        "                print(\"Labels:\", data['labels'])\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Decode predictions and labels (move to CPU for decoding)\n",
        "                decoded_pred = tokenizer.decode(predicted_ids.cpu(), skip_special_tokens=True)\n",
        "                decoded_label = tokenizer.decode(A.cpu(), skip_special_tokens=True)\n",
        "\n",
        "                print(\"Predicted Answer:\", decoded_pred)\n",
        "                print(\"Target Answer:\", decoded_label)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                all_preds.append(decoded_pred)\n",
        "                all_labels.append(decoded_label)\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if 'out of memory' in str(e):  # Check if it's an OOM error\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"Out of memory error for sample {i}. Skipping this sample.\")\n",
        "                else:\n",
        "                    print(f\"Error during evaluation for sample {i}: {e}\")\n",
        "                all_preds.append(\"\")  # Add an empty string for the skipped sample\n",
        "                all_labels.append(\"\")\n",
        "\n",
        "    # Calculate exact match\n",
        "    em = metric.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")\n",
        "\n",
        "    return {\"exact_match\": em}\n"
      ],
      "metadata": {
        "id": "KnKlAiBztm1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/gdrive/MyDrive/model/GNN-T2SQL/checkpoint-250\"\n",
        "!ls -ltha $path"
      ],
      "metadata": {
        "id": "XnbQr8aY7iD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "path=\"/content/gdrive/MyDrive/model/GNN-T2SQL/checkpoint-250\"\n",
        "results=evaluate_model(path, eval_dataset, device=device)"
      ],
      "metadata": {
        "id": "jDhBe-E0Aouu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORK-4"
      ],
      "metadata": {
        "id": "K5VetLhm5Bzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "import transformers\n",
        "#from transformers import MistralTokenizer, MistralForCausalLM\n",
        "import datasets\n",
        "\n",
        "#def evaluate_model(model_path, eval_dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", edges=None, fake_input=None):\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", edges=None, fake_input=None):\n",
        "    \"\"\"\n",
        "    Evaluates a text-to-SQL model on a given dataset, with optional fake input for testing.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the trained PEFT model.\n",
        "        eval_dataset (Dataset): The dataset to evaluate on.\n",
        "        device (str, optional): The device to use for computation (\"cuda\" or \"cpu\"). Defaults to \"cuda\" if available.\n",
        "        edges (torch.Tensor, optional): Precomputed edge indices for the dataset (if applicable).\n",
        "        fake_input (str, optional): Fake input text to use instead of the actual dataset input.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics, including exact match (EM), BLEU score, execution accuracy (if applicable), and others.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model once outside the loop\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    model.to(device)\n",
        "    model.float()\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "\n",
        "    metric_em = evaluate.load(\"exact_match\")\n",
        "    metric_bleu = evaluate.load(\"bleu\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Only evaluate on one sample if using fake input\n",
        "        for i in tqdm(range(1 if fake_input else len(eval_dataset)), desc=\"Evaluating\"):\n",
        "            if fake_input:\n",
        "                Q = tokenizer(fake_input, return_tensors=\"pt\")['input_ids'].to(device).long()\n",
        "                A = torch.tensor([], device=device).long()  # Empty tensor for labels\n",
        "            else:\n",
        "                data = eval_dataset[i]\n",
        "                Q = data['input_ids'].to(device).long()\n",
        "                A = data['labels'].to(device).long()\n",
        "\n",
        "            # Check for NaN or Inf values in Q and A\n",
        "            if torch.isnan(Q).any() or torch.isinf(Q).any() or (not fake_input and (torch.isnan(A).any() or torch.isinf(A).any())):\n",
        "                print(\"WARNING: NaN or Inf values detected in input_ids or labels!\")\n",
        "                continue  # Skip this sample if there are invalid values\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print(f\"Sample ID: {i}\")\n",
        "             # Convert Q to a NumPy array before decoding\n",
        "            print(f\"  - Text: {tokenizer.decode(Q.cpu().numpy()[0], skip_special_tokens=True)}\")\n",
        "            if not fake_input:\n",
        "                print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            print('\\n')\n",
        "\n",
        "             # Prepare data for the model\n",
        "            input_data = {\n",
        "                \"input_ids\": Q.unsqueeze(0),\n",
        "                \"attention_mask\": data['attention_mask'].unsqueeze(0).to(device).float() if not fake_input else torch.ones_like(Q).unsqueeze(0)\n",
        "            }\n",
        "\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "            try:\n",
        "                # Model inference\n",
        "                # Ensure edges is in the correct format (if it's being used by your model)\n",
        "                if edges is not None:\n",
        "                    # Example: If your model expects edges as a 2D tensor of shape (batch_size, num_edges)\n",
        "                    edges = edges.unsqueeze(0)  # Add a batch dimension if necessary\n",
        "                else:\n",
        "                    edges = torch.empty((1, 0), dtype=torch.long, device=device) # Create an empty tensor if no edges are provided\n",
        "\n",
        "                model_output = model(**input_data, edges=edges.float())\n",
        "                logits = model_output.logits\n",
        "\n",
        "\n",
        "\n",
        "                # Handle potential errors in logits\n",
        "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "                    print(\"WARNING: NaN or Inf values detected in logits!\")\n",
        "                    logits[torch.isnan(logits)] = 0 # Replace NaN with zero\n",
        "                    logits[torch.isinf(logits)] = 0 # Replace Inf with zero\n",
        "\n",
        "\n",
        "                # Ensure logits has the right dimensions before argmax\n",
        "                if logits.dim() > 2:\n",
        "                    logits = logits.squeeze(1)\n",
        "                elif logits.dim() == 1:\n",
        "                    logits = logits.unsqueeze(0)\n",
        "\n",
        "                # Extract predicted tokens directly on GPU\n",
        "                predicted_ids = logits.argmax(dim=-1)\n",
        "\n",
        "                # Convert to list only if it's not already a list\n",
        "                if not isinstance(predicted_ids, list):\n",
        "                    predicted_ids = predicted_ids.cpu().tolist()\n",
        "\n",
        "                # Print the type and shape of predicted_ids for debugging\n",
        "                print(\"Type of predicted_ids:\", type(predicted_ids))\n",
        "                print(\"Shape of predicted_ids:\", predicted_ids.shape)\n",
        "\n",
        "\n",
        "                # Print the shape of hidden_states for debugging\n",
        "                #print(\"Shape of hidden_states:\", hidden_states.shape)  # Add this line\n",
        "\n",
        "                #bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # Always convert predicted_ids to a list, regardless of its dimension\n",
        "                #predicted_ids = predicted_ids.cpu().tolist()\n",
        "\n",
        "\n",
        "\n",
        "                # Print shapes and values for debugging\n",
        "                print(\"\\n\\n\")\n",
        "                print(\"Logits length:\", logits.shape[1])\n",
        "                print(\"Logits shape:\", logits.shape)\n",
        "                print(\"Logits dtype:\", logits.dtype)\n",
        "                print(\"\\n\")\n",
        "                print(\"Predicted IDs:\", predicted_ids)\n",
        "                if not fake_input:\n",
        "                    print(\"Labels:\", data['labels'])\n",
        "                print(\"\\n\")\n",
        "\n",
        "\n",
        "                # Decode predictions and labels\n",
        "                decoded_pred = tokenizer.decode(predicted_ids, skip_special_tokens=True) # Decode the predicted IDs\n",
        "\n",
        "\n",
        "\n",
        "                if not fake_input:\n",
        "                    decoded_label = tokenizer.decode(A.cpu(), skip_special_tokens=True)\n",
        "                    all_labels.append(decoded_label)\n",
        "\n",
        "                print(\"Predicted Answer:\", decoded_pred)\n",
        "                if not fake_input:\n",
        "                    print(\"Target Answer:\", decoded_label)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                all_preds.append(decoded_pred)  # Append even if fake input\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if 'out of memory' in str(e):  # Check if it's an OOM error\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"Out of memory error for sample {i}. Skipping this sample.\")\n",
        "                else:\n",
        "                    print(f\"Error during evaluation for sample {i}: {e}\")\n",
        "                all_preds.append(\"\")  # Add an empty string for the skipped sample\n",
        "                if not fake_input:\n",
        "                    all_labels.append(\"\")\n",
        "\n",
        "    # Calculate exact match\n",
        "    results = {}\n",
        "    if not fake_input:\n",
        "        em = metric_em.compute(predictions=all_preds, references=all_labels)[\"exact_match\"]\n",
        "        results[\"exact_match\"] = em\n",
        "        # Calculate BLEU score\n",
        "        bleu = metric_bleu.compute(predictions=all_preds, references=all_labels)[\"bleu\"]\n",
        "        results[\"bleu\"] = bleu\n",
        "\n",
        "\n",
        "        # Calculate execution accuracy (if applicable)\n",
        "        # ... (You'll need to implement this based on your specific setup)\n",
        "\n",
        "        print(\"\\nEvaluation Results:\")\n",
        "        print(f\"  Exact Match: {em:.4f}\")\n",
        "        print(f\"  BLEU Score: {bleu:.4f}\")\n",
        "        # Print execution accuracy if calculated\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "wLXgvF2c2dYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\")[\"train\"].shuffle(seed=42)\n",
        "\n",
        "# Manually define splits\n",
        "train_size = int(0.7 * len(dataset))\n",
        "eval_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - eval_size\n",
        "\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
        "test_dataset = dataset.select(range(train_size + eval_size, len(dataset)))"
      ],
      "metadata": {
        "id": "j_7AcAOM7su7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft datasets evaluate accelerate bitsandbytes -q"
      ],
      "metadata": {
        "id": "-9bJY75f_L3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your saved model\n",
        "#model_path =\"/content/gdrive/MyDrive/model/GNN-T2SQL/checkpoint-250\"\n",
        "\n",
        "\n",
        "model_path = \"/content/gdrive/MyDrive/model/GNNT2SQL/checkpoint-1950\"\n",
        "\n",
        "#model_path =\"/content/gdrive/MyDrive/model/GNNPOC-T2SQL/checkpoint-2500\"\n",
        "\n",
        "# Use the fake input\n",
        "fake_input = \"What is the capital of the USA?\"\n",
        "\n",
        "# Evaluate the model with the fake input\n",
        "results = evaluate_model(model_path, test_dataset, fake_input=fake_input)\n",
        "\n",
        "# Print the results\n",
        "print(results)  # This will likely be an empty dictionary since there's no exact match to compare to\n"
      ],
      "metadata": {
        "id": "IoFItCB25HN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "2bfbd0e7-e5bd-4f00-9124-a8601cefc414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "evaluate_model() got an unexpected keyword argument 'fake_input'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5ead56effc47>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Evaluate the model with the fake input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfake_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Print the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: evaluate_model() got an unexpected keyword argument 'fake_input'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NOT WORK"
      ],
      "metadata": {
        "id": "6ndDhOiL5iTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "path=\"/content/GNN-T2SQL/checkpoint-500\"\n",
        "\n",
        "# 1. Load the Entire Model (With Adapter)\n",
        "model = GraphModel(mistral_model, tokenizer)\n",
        "model = PeftModel.from_pretrained(model, path, is_trainable=False)\n",
        "model.to(device)\n",
        "\n",
        "# 3. Create DataLoader with Collator\n",
        "data_collator = GraphDataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=1, collate_fn=data_collator)\n"
      ],
      "metadata": {
        "id": "wHgpHWohSUk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=eval_dataloader\n",
        "print(data.dataset[0]['input_ids'])\n",
        "data.dataset[0]['labels']"
      ],
      "metadata": {
        "id": "sJ3oA16mTlVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eval_dataloader\n",
        "edges=None\n",
        "\n",
        "\n",
        "# 2. Initialize Metrics\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "      #for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "      for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):  # Iterate over the DataLoader\n",
        "          # Extract the data from the batch (assuming batch size 1)\n",
        "          data = batch\n",
        "          #data = eval_dataset[i]\n",
        "          #print(data)\n",
        "\n",
        "          Q = data['input_ids'].squeeze()  # Convert tensor to list\n",
        "          A = data['labels'].squeeze()  # Convert tensor to list\n",
        "\n",
        "          #Q = data['input_ids']\n",
        "          #A = data['labels']\n",
        "\n",
        "          print('\\n\\n')\n",
        "          #print(f\"Sample ID: {i}\")\n",
        "          print(f\"  - Text: {tokenizer.decode(Q.tolist(), skip_special_tokens=True)}\") # Decode after converting to list\n",
        "          print(f\"  - Target Text: {tokenizer.decode(A.tolist(), skip_special_tokens=True)}\") # Decode after converting to list\n",
        "          print('\\n')\n",
        "\n",
        "\n",
        "          # Prepare data for the model, move tensors to the correct device and unsqueeze\n",
        "          data = {k: v.unsqueeze(0).to(device).float() for k, v in data.items() if isinstance(v, torch.Tensor)} # Apply unsqueeze only to tensors\n",
        "\n",
        "          # Convert 'input_ids' to LongTensor\n",
        "          data['input_ids'] = data['input_ids'].long()\n",
        "\n",
        "          # Provide a default 'edges' tensor if None\n",
        "          if edges is None:\n",
        "              edges = torch.tensor([[]]).to(device)\n",
        "          #edges=edges.float()\n",
        "\n",
        "           # Prepare data for the model\n",
        "          input_data = {\n",
        "                \"input_ids\": torch.tensor(Q).unsqueeze(0).to(device).long(),  # Convert list to tensor and unsqueeze\n",
        "                \"attention_mask\": torch.tensor(data['attention_mask']).unsqueeze(0).to(device).float()  # Convert list to tensor and unsqueeze\n",
        "            }\n",
        "\n",
        "          if edges is None:\n",
        "              edges = torch.tensor([[]]).to(device)\n",
        "\n",
        "\n",
        "          # Model inference\n",
        "          #model_output = model(**input_data, edges=edges.float())\n",
        "          #logits = model_output.logits\n",
        "\n",
        "\n",
        "\n",
        "          model_output = model(input_ids=data['input_ids'], attention_mask=data['attention_mask'], edges=edges.float())\n",
        "          logits = model_output.logits\n",
        "\n",
        "          if logits.dim() > 2:\n",
        "              logits = logits.squeeze(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          #all_preds.extend(logits.argmax(dim=-1).cpu().tolist())\n",
        "          #all_labels.extend(A.cpu().tolist())"
      ],
      "metadata": {
        "id": "l9MWtMZTS-tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from peft import PeftModel\n",
        "\n",
        "def evaluate_model(model_path, eval_dataset, device=None, edges=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. Load the Entire Model (With Adapter)\n",
        "    model = GraphModel(mistral_model, tokenizer)\n",
        "    model = PeftModel.from_pretrained(model, model_path, is_trainable=False)\n",
        "    model.to(device)\n",
        "\n",
        "    # Explicitly set model to float32 to ensure consistency\n",
        "    model.float()\n",
        "\n",
        "    # 2. Initialize Metrics\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # 3. Create DataLoader with Collator\n",
        "    data_collator = GraphDataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=-100,\n",
        "        pad_to_multiple_of=8\n",
        "    )\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=1, collate_fn=data_collator)\n",
        "\n",
        "    # 4. Evaluation Loop\n",
        "    model.eval()\n",
        "    model.encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        #for i in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):  # Iterate over the DataLoader\n",
        "            # Extract the data from the batch (assuming batch size 1)\n",
        "            #data = batch[0]\n",
        "            #data = eval_dataset[i]\n",
        "\n",
        "\n",
        "            data = batch\n",
        "\n",
        "\n",
        "            Q = data['input_ids']\n",
        "            A = data['labels']\n",
        "\n",
        "            print('\\n\\n')\n",
        "            #print(f\"Sample ID: {i}\")\n",
        "            #print(f\"  - Text: {tokenizer.decode(Q, skip_special_tokens=True)}\")\n",
        "            #print(f\"  - Target Text: {tokenizer.decode(A, skip_special_tokens=True)}\")\n",
        "            #print('\\n')\n",
        "\n",
        "\n",
        "           # Prepare data for the model, move tensors to the correct device and unsqueeze\n",
        "            data = {k: v.unsqueeze(0).to(device).float() for k, v in data.items()}\n",
        "\n",
        "            # Convert 'input_ids' to LongTensor\n",
        "            data['input_ids'] = data['input_ids'].long()\n",
        "\n",
        "            # Unsqueeze the 'edges' tensor to add a batch dimension\n",
        "            if edges is None:\n",
        "                edges = torch.tensor([[]]).unsqueeze(0).to(device)  # Add unsqueeze here\n",
        "\n",
        "\n",
        "            # Model inference\n",
        "            model_output = model(**data, edges=edges.float())  # Pass the entire data dictionary\n",
        "            logits = model_output.logits\n",
        "\n",
        "            if logits.dim() > 2:\n",
        "                logits = logits.squeeze(1)\n",
        "\n",
        "            all_preds.extend(logits.argmax(dim=-1).cpu().tolist())\n",
        "            all_labels.extend(A.cpu().tolist())\n",
        "\n",
        "\n",
        "    # 5. Compute and Print Metrics\n",
        "    decoded_preds = tokenizer.batch_decode(all_preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(all_labels, skip_special_tokens=True)\n",
        "\n",
        "    # Print for debugging\n",
        "    print(f\"Decoded predictions: {decoded_preds}\")\n",
        "    print(f\"Decoded labels: {decoded_labels}\")\n",
        "\n",
        "    em = metric.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"  Exact Match: {em:.4f}\")"
      ],
      "metadata": {
        "id": "swZN7ciePnxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset.dataset[0]"
      ],
      "metadata": {
        "id": "qYsWpTmqKRKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0]"
      ],
      "metadata": {
        "id": "22uMnGaAaFx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q=eval_dataset[0]['input_ids']\n",
        "tokenizer.decode(Q, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "JKb6UNn6SP6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=eval_dataset[0]['labels']\n",
        "tokenizer.decode(A, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "0QbKnFM_Z1u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8  # Ensure tensors divisible by 8 for optimized performance\n",
        ")\n",
        "\n",
        "# 10. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n"
      ],
      "metadata": {
        "id": "t5jjcgcwGaIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA_Uj38H1Wm-"
      },
      "source": [
        "Let's analyze the information you've provided about your `GraphModel`:\n",
        "\n",
        "**Structure Analysis:**\n",
        "\n",
        "* **Encoder (Mistral-7B):** The backbone of your model is the Mistral-7B language model. This model is responsible for understanding the input text and generating meaningful representations (embeddings) for each token. The encoder has a substantial number of parameters (7.5 billion), indicating its large size and capacity to learn complex language patterns.\n",
        "\n",
        "* **GAT Layer:**  You've incorporated a Graph Attention Network (GAT) layer to process the graph structure of your input data. This layer likely learns to weigh the importance of different nodes and edges based on their relationships, which can help capture additional information beyond just the raw text.\n",
        "\n",
        "* **Pooling:** The `pool` operation is used to aggregate the representations from individual nodes into a single representation for the entire graph. The choice of pooling (e.g., mean pooling) depends on your specific task and how you want to combine the node-level information.\n",
        "\n",
        "* **LM Head:** The language model head (`lm_head`) takes the final graph representation and predicts the next token in the sequence. It has a large number of parameters due to the vocabulary size (32,768).\n",
        "\n",
        "**PEFT (Parameter-Efficient Fine-Tuning):**\n",
        "\n",
        "- **Lora (Low-Rank Adaptation):** You are using LoRA, a technique that adds small adapter modules to the model to fine-tune it more efficiently. This allows you to train a smaller number of parameters while still achieving good performance.\n",
        "\n",
        "- **Trainable Parameters:**  Only a small fraction (0.0078%) of the total parameters are trainable, thanks to LoRA. This drastically reduces the computational and memory requirements during training.\n",
        "\n",
        "**Potential Challenges and Considerations:**\n",
        "\n",
        "- **Graph Structure Quality:** The effectiveness of the GAT layer heavily relies on the quality of the graph structure you create from the input text. Dependency parsing is a good starting point, but you might want to explore other ways to construct the graph to better capture the relationships between words and phrases.\n",
        "\n",
        "- **Overfitting with LoRA:** Even with LoRA, overfitting can be a concern, especially with a small dataset. Monitor your validation loss and consider techniques like early stopping or adding more training data (if possible).\n",
        "\n",
        "- **Memory Usage:**  Graph models can be memory intensive, especially with large input sequences or large batch sizes. You might need to experiment with techniques like gradient checkpointing or gradient accumulation to manage memory usage.\n",
        "\n",
        "- **Training Stability:** Training large language models can sometimes be unstable. Experiment with different learning rates, optimizers, and warm-up strategies to find the best settings for your model.\n",
        "\n",
        "- **Evaluation Metrics:** Choose appropriate evaluation metrics that reflect the quality of the generated SQL queries. Consider using both automated metrics (e.g., BLEU score, accuracy) and human evaluation to assess the model's performance.\n",
        "\n",
        "\n",
        "Let me know if you have any specific questions or concerns!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjgBpi5eCiJI"
      },
      "source": [
        "* Original Code - TRAINING ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m9ufONvpW2G"
      },
      "source": [
        "\n",
        "TO BE TEST IT WORK FOR trainer = SFTTrainer( ; from trl import SFTTrainer;\n",
        "\n",
        "max_seq_length = 2048 --- max sequence length for model and packing of the dataset\n",
        "\n",
        " https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Mistral_7B_Instruct_v0_1_for_text_to_SQL_EVALDATA.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9xVfjpHjI9l"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFOcr3UYpRH_"
      },
      "outputs": [],
      "source": [
        "# 8. Evaluation Metric (Semantic Similarity)\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
        "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute embeddings for predictions and references\n",
        "    prediction_embeddings = sentence_transformer_model.encode(predictions, convert_to_tensor=True)\n",
        "    reference_embeddings = sentence_transformer_model.encode(references, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = util.cos_sim(prediction_embeddings, reference_embeddings)\n",
        "    similarities = torch.diag(cosine_similarities).cpu().numpy()  # Extract similarities for corresponding pairs\n",
        "\n",
        "    # Return average similarity as the metric\n",
        "    em = metric.compute(predictions=predictions, references=references)[\"exact_match\"]\n",
        "    return {\"semantic_similarity\": np.mean(similarities), \"exact_match\": em}\n",
        "\n",
        "# 9. Training Arguments and Trainer\n",
        "training_args = TrainingArguments(\n",
        "    \"graph-T2SQL\",\n",
        "    logging_dir=\"graph-T2SQL\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    push_to_hub=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_semantic_similarity\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8  # Ensure tensors divisible by 8 for optimized performance\n",
        ")\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "# 10. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "\n",
        "# 11. Train the model\n",
        "print('\\n\\n')\n",
        "trainer.train()\n",
        "print('\\n\\n')\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n",
        "print(f'Test Exact Match: {test_results[\"eval_exact_match\"]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPLqrRtAS9r2"
      },
      "source": [
        "## Enhance - 1: Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQckSalcwOv9"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXlOUvZNSWlz"
      },
      "outputs": [],
      "source": [
        "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
        "from nlpaug.augmenter.sentence import RandomInsertionAug\n",
        "\n",
        "def augment_data(dataset):\n",
        "    augmented_data = []\n",
        "\n",
        "    synonym_aug = SynonymAug(aug_src='wordnet')\n",
        "    insert_aug = RandomInsertionAug(aug_p=0.1)  # Insert with 10% probability\n",
        "    delete_aug = RandomWordAug(action=\"delete\", aug_p=0.05)  # Delete with 5% probability\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "\n",
        "        # Create augmented examples\n",
        "        augmented_questions = [\n",
        "            synonym_aug.augment(question),\n",
        "            insert_aug.augment(question),\n",
        "            delete_aug.augment(question)\n",
        "        ]\n",
        "\n",
        "        # Add original and augmented examples to the dataset\n",
        "        for aug_question in augmented_questions:\n",
        "            augmented_data.append({'question': aug_question, 'answer': answer})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "# Augment the training dataset\n",
        "train_dataset_augmented = augment_data(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdh2zS5wTKh0"
      },
      "source": [
        "## Enhance - 2: Alternative Architecture: Graph Transformer Network (GTN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCCpH_DmwusS"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwON48w8ScUs"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GTConv\n",
        "\n",
        "class GTNModel(torch.nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(GTNModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.gtn = GTConv(4096, 4096, num_layers=2)  # 2 GTN layers\n",
        "        self.pool = lambda x, batch: torch.mean(x, dim=0, keepdim=True)\n",
        "        self.lm_head = torch.nn.Linear(4096, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        x = inputs['input_ids'].to(mistral_model.device)\n",
        "        edge_index = inputs['edge_index'].to(mistral_model.device)\n",
        "        embeddings = self.encoder(x).last_hidden_state.cpu()\n",
        "        gtn_out = self.gtn(embeddings, edge_index.cpu())\n",
        "        pooled = self.pool(gtn_out, inputs.get('batch', torch.zeros(gtn_out.size(0)).long()))\n",
        "        out = self.lm_head(pooled.to(mistral_model.device))\n",
        "        return {\"logits\": out}\n",
        "\n",
        "# Replace the model with the GTNModel\n",
        "model = GTNModel(mistral_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyjnyL2RTMpX"
      },
      "source": [
        "## Enhance - 3: Integration Enhance - 1 and Enhance - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMNXPR2XS1RT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import numpy as np\n",
        "from torch_geometric.nn import GTConv  # For GTN\n",
        "from trl import setup_chat_format\n",
        "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
        "from nlpaug.augmenter.sentence import RandomInsertionAug\n",
        "\n",
        "# ... (rest of the imports and model/tokenizer loading from the original code)\n",
        "\n",
        "# Data Augmentation Function\n",
        "def augment_data(dataset):\n",
        "    augmented_data = []\n",
        "\n",
        "    synonym_aug = SynonymAug(aug_src='wordnet')\n",
        "    insert_aug = RandomInsertionAug(aug_p=0.1)  # Insert with 10% probability\n",
        "    delete_aug = RandomWordAug(action=\"delete\", aug_p=0.05)  # Delete with 5% probability\n",
        "\n",
        "    for item in dataset:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "\n",
        "        # Create augmented examples\n",
        "        augmented_questions = [\n",
        "            synonym_aug.augment(question),\n",
        "            insert_aug.augment(question),\n",
        "            delete_aug.augment(question)\n",
        "        ]\n",
        "\n",
        "        # Add original and augmented examples to the dataset\n",
        "        for aug_question in augmented_questions:\n",
        "            augmented_data.append({'question': aug_question, 'answer': answer})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "# TextToSQLDataset (unchanged from the original code)\n",
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        #text = item['question'] + \" \" + item['context']\n",
        "        text = item['question']\n",
        "        target_text = item['answer']\n",
        "\n",
        "        tokenized_input = self.tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "        tokenized_target = self.tokenizer(target_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Dependency Parsing for Edge Index\n",
        "        doc = nlp(text)\n",
        "        edges = []\n",
        "        for token in doc:\n",
        "            if token.i < len(doc) - 1:\n",
        "                edges.append([token.i, token.head.i])\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokenized_input['input_ids'].flatten(),\n",
        "            'attention_mask': tokenized_input['attention_mask'].flatten(),\n",
        "            'labels': tokenized_target['input_ids'].flatten(),\n",
        "            'edge_index': edge_index,\n",
        "        }\n",
        "\n",
        "# GTN Model WITH GTConv\n",
        "from torch_geometric.nn import GTConv\n",
        "\n",
        "class GTNModel(torch.nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(GTNModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.gtn = GTConv(4096, 4096, num_layers=2)  # 2 GTN layers\n",
        "        self.pool = lambda x, batch: torch.mean(x, dim=0, keepdim=True)\n",
        "        self.lm_head = torch.nn.Linear(4096, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        x = inputs['input_ids'].to(mistral_model.device)\n",
        "        edge_index = inputs['edge_index'].to(mistral_model.device)\n",
        "        embeddings = self.encoder(x).last_hidden_state.cpu()\n",
        "\n",
        "        gtn_out = self.gtn(embeddings, edge_index.cpu())\n",
        "        pooled = self.pool(gtn_out, inputs.get('batch', torch.zeros(gtn_out.size(0)).long()))\n",
        "        out = self.lm_head(pooled.to(mistral_model.device))\n",
        "        return {\"logits\": out}\n",
        "\n",
        "# Replace the model with the GTNModel\n",
        "model = GTNModel(mistral_model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "\n",
        "# 7. Evaluation Metric (Semantic Similarity)\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
        "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Compute embeddings for predictions and references\n",
        "    prediction_embeddings = sentence_transformer_model.encode(predictions, convert_to_tensor=True)\n",
        "    reference_embeddings = sentence_transformer_model.encode(references, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = util.cos_sim(prediction_embeddings, reference_embeddings)\n",
        "    similarities = torch.diag(cosine_similarities).cpu().numpy()  # Extract similarities for corresponding pairs\n",
        "\n",
        "    # Return average similarity as the metric\n",
        "    return {\"semantic_similarity\": np.mean(similarities)}\n",
        "\n",
        "\n",
        "# Augment the training dataset\n",
        "train_dataset_augmented = augment_data(train_dataset)\n",
        "\n",
        "# Create datasets (using augmented training data)\n",
        "train_dataset = TextToSQLDataset(train_dataset_augmented, tokenizer)\n",
        "val_dataset = TextToSQLDataset(val_dataset, tokenizer)\n",
        "test_dataset = TextToSQLDataset(test_dataset, tokenizer)\n",
        "\n",
        "# Use the GTNModel\n",
        "model = GTNModel(mistral_model)\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 10. Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    data_collator=lambda x: x,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCYvI4AeUaJu"
      },
      "source": [
        "## Enhance - 4: Regularization Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8neBYzeNUJD_"
      },
      "outputs": [],
      "source": [
        "# ... (other imports)\n",
        "from torch_geometric.nn import GTConv\n",
        "\n",
        "# ... (other classes and functions)\n",
        "\n",
        "# GTN Model with Dropout\n",
        "class GTNModel(torch.nn.Module):\n",
        "    def __init__(self, encoder, dropout_prob=0.1):  # Add dropout probability\n",
        "        super(GTNModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.gtn = GTConv(4096, 4096, num_layers=2)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
        "        self.pool = lambda x, batch: torch.mean(x, dim=0, keepdim=True)\n",
        "        self.lm_head = torch.nn.Linear(4096, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        # ... (rest of the forward method)\n",
        "        x = inputs['input_ids'].to(mistral_model.device)\n",
        "        edge_index = inputs['edge_index'].to(mistral_model.device)\n",
        "        embeddings = self.encoder(x).last_hidden_state.cpu()\n",
        "\n",
        "\n",
        "        gtn_out = self.gtn(embeddings, edge_index.cpu())\n",
        "        gtn_out = self.dropout(gtn_out)  # Apply dropout after GTN\n",
        "\n",
        "        pooled = self.pool(gtn_out, inputs.get('batch', torch.zeros(gtn_out.size(0)).long()))\n",
        "\n",
        "        out = self.lm_head(pooled.to(mistral_model.device))\n",
        "        return {\"logits\": out}\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "# Create the model with dropout\n",
        "model = GTNModel(mistral_model, dropout_prob=0.1)  # Set dropout probability\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "\n",
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 10. Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    data_collator=lambda x: x,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1syJYIaXvil"
      },
      "source": [
        "## Enhance - 5:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GN68X1NHaED"
      },
      "source": [
        "* MultiTaskDataset: This class combines multiple datasets and assigns task IDs to each sample.\n",
        "\n",
        "* MultiTaskModel: This class extends the model to include a separate head for the classification task.\n",
        "\n",
        "* Task-Specific Forward Pass: The forward method now takes a task_id input and uses the appropriate head based on the task.\n",
        "\n",
        "* Training Loop: The training loop needs to be modified to handle the multi-task dataset and compute losses for both tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoHDHFZGXuyG"
      },
      "outputs": [],
      "source": [
        "class TextToSQLDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, task_id=0):  # Add task_id argument\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.task_id = task_id  # Store task ID\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        question = item['question'] if self.task_id == 0 else item['text']  # Use appropriate field\n",
        "        answer = item['answer'] if self.task_id == 0 else item['label']  # Use appropriate field\n",
        "        # ... (rest of the __getitem__ method for tokenization and graph construction)\n",
        "\n",
        "\n",
        "        item = {'input_ids': input_ids, 'attention_mask': attention_mask,\n",
        "                'edge_index': edge_index, 'task_id': self.task_id}  # Add task_id\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNy48iw6YmA7"
      },
      "outputs": [],
      "source": [
        "# Create the multi-task model with dropout\n",
        "model = MultiTaskModel(mistral_model, dropout_prob=0.1, num_classes=num_classification_classes)\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfHkTCn0Yto7"
      },
      "outputs": [],
      "source": [
        "# ... (other parts of the code)\n",
        "\n",
        "train_dataset_sql = TextToSQLDataset(train_dataset_augmented, tokenizer, task_id=0)\n",
        "train_dataset_classification = TextToSQLDataset(dataset_classification, tokenizer, task_id=1)\n",
        "\n",
        "# Combine datasets (consider balancing them if needed)\n",
        "train_dataset = torch.utils.data.ConcatDataset([train_dataset_sql, train_dataset_classification])\n",
        "\n",
        "# ... (rest of the training and evaluation code, modified for multi-task)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IZVtS_5Y7wC"
      },
      "source": [
        "In the provided code, the new train_dataset brings several enhancements compared to the original version:\n",
        "\n",
        "Data Augmentation: The most significant change is the incorporation of data augmentation. The original train_dataset is passed through the augment_data function, which creates additional training examples by applying techniques like synonym replacement, random word insertion, and random word deletion. This augmented dataset is then used to create the new train_dataset.\n",
        "\n",
        "Benefit: Data augmentation helps the model generalize better to different phrasing and vocabulary variations in natural language input, leading to improved performance on unseen examples.\n",
        "Multi-Task Learning: The new train_dataset is designed to support multi-task learning. It combines examples from both the original text-to-SQL task and an additional classification task. Each example in the dataset is associated with a task_id to indicate which task it belongs to.\n",
        "\n",
        "Benefit: Multi-task learning allows the model to learn shared representations between related tasks, potentially improving performance and generalization on both tasks.\n",
        "Dynamic Padding: While not explicitly mentioned, the code likely uses dynamic padding when creating the train_dataset. This means that each example is padded to the length of the longest sequence in its batch, rather than using a fixed maximum length.\n",
        "\n",
        "Benefit: Dynamic padding reduces unnecessary padding tokens, which can speed up training and potentially improve model performance.\n",
        "\n",
        "Sources and related content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbw0TlBKlWI0"
      },
      "source": [
        "## Enhance - 6: Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AP4hNkmlSyy"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "\n",
        "# Custom Loss Function (integrated)\n",
        "def compute_loss(model, inputs, return_outputs=False):\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    task_ids = inputs.pop(\"task_id\")\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    sql_logits = outputs.logits[task_ids == 0]  # SQL task logits\n",
        "    sql_labels = labels[task_ids == 0]          # SQL task labels\n",
        "    sql_loss = CrossEntropyLoss()(sql_logits, sql_labels)\n",
        "\n",
        "    classification_logits = outputs.logits[task_ids == 1]  # Classification task logits\n",
        "    classification_labels = labels[task_ids == 1]          # Classification task labels\n",
        "    classification_loss = MSELoss()(classification_logits, classification_labels)\n",
        "\n",
        "    # Combine Losses (adjust weights as needed)\n",
        "    loss = 0.5 * sql_loss + 0.5 * classification_loss\n",
        "\n",
        "    return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# 10. Create Trainer instance (with the custom loss)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=lambda x: x,\n",
        "    compute_metrics=compute_metrics,\n",
        "    compute_loss=compute_loss,  # Pass the custom loss function here\n",
        "    optimizers=(optimizer, None)\n",
        ")\n",
        "\n",
        "# 11. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 12. Evaluate on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Semantic Similarity: {test_results[\"eval_semantic_similarity\"]:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fsx_NaeSTP2j",
        "Reg6DPZJP2Mn",
        "nDzroTjfY9EO",
        "fhEZZJMdzZuK",
        "GoAUvQWiz4ZJ",
        "2hAPAwznG6Gl",
        "9QB1kXab0rHT",
        "AgAXg074JWdR",
        "h2dZ6KwetpRM",
        "K5VetLhm5Bzk",
        "6ndDhOiL5iTN",
        "WPLqrRtAS9r2",
        "Sdh2zS5wTKh0",
        "ZyjnyL2RTMpX",
        "wCYvI4AeUaJu",
        "h1syJYIaXvil",
        "Qbw0TlBKlWI0"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO9a7lxydNWeQO0HgkQJND+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0521e1c76c1542c282524521d9eb1582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c87d072c978a48b8b3da725eabfaf47d",
              "IPY_MODEL_d44c2b84ef674be0877058a3f54220c4",
              "IPY_MODEL_8018fb76448542ea89acda6f39017e63"
            ],
            "layout": "IPY_MODEL_86062eaaca1a479eb2eda1adadf99968"
          }
        },
        "c87d072c978a48b8b3da725eabfaf47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18fb71f9578643f094178a07751de8c3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9e7fca599fb244738111ea2aa48e565c",
            "value": "Generatingâ€‡trainâ€‡split:â€‡"
          }
        },
        "d44c2b84ef674be0877058a3f54220c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca0232ec91334eb5868fd586c3ebf297",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f567d59910c147d0a3bb0fce3ede2760",
            "value": 1
          }
        },
        "8018fb76448542ea89acda6f39017e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d65278d8fad40f8b14ca8776cad73e4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c96ba40d07ab4927b74712a65a1160fb",
            "value": "â€‡10000/0â€‡[00:00&lt;00:00,â€‡13653.82â€‡examples/s]"
          }
        },
        "86062eaaca1a479eb2eda1adadf99968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18fb71f9578643f094178a07751de8c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e7fca599fb244738111ea2aa48e565c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca0232ec91334eb5868fd586c3ebf297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f567d59910c147d0a3bb0fce3ede2760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d65278d8fad40f8b14ca8776cad73e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c96ba40d07ab4927b74712a65a1160fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55c6b2755d59441b8250f7b74aa5a5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b89201e08758407eb55e14082c582afe",
              "IPY_MODEL_e3ec263bb96e457c8a6cf4a56eed81d7",
              "IPY_MODEL_fe689d8161614141af7a9e6999a72098"
            ],
            "layout": "IPY_MODEL_f16922a7f7724901a6022d022b87a9a2"
          }
        },
        "b89201e08758407eb55e14082c582afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9090acd6a4bc43ea9194c3c568dcfee6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ba16a0dce4b34fe6ac5a8c0f67e7a1ea",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "e3ec263bb96e457c8a6cf4a56eed81d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5a5b4c41bff4463ab0a90a2a2f3e98e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daafcc04d255402599863bd6a9b73dcc",
            "value": 3
          }
        },
        "fe689d8161614141af7a9e6999a72098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c42c3e07acb4d1d85d01aa9f514cfcd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7c34ccc429a44ff989bc9bca5070c941",
            "value": "â€‡3/3â€‡[01:16&lt;00:00,â€‡25.18s/it]"
          }
        },
        "f16922a7f7724901a6022d022b87a9a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9090acd6a4bc43ea9194c3c568dcfee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba16a0dce4b34fe6ac5a8c0f67e7a1ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5a5b4c41bff4463ab0a90a2a2f3e98e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daafcc04d255402599863bd6a9b73dcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c42c3e07acb4d1d85d01aa9f514cfcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c34ccc429a44ff989bc9bca5070c941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a2045cbccea4534a8c1cf4d6b6067cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_926cf10dfa35406c86470b28d60b2194",
              "IPY_MODEL_e839ce131c0d4003a05dd226d1a759ab",
              "IPY_MODEL_12c19c485ac44c6a84f85ca59ee542b0"
            ],
            "layout": "IPY_MODEL_86a32fa3c71d4ac3a87276ac5f82620f"
          }
        },
        "926cf10dfa35406c86470b28d60b2194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6afc1acd104348c18b6cc817db818d78",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f2804d4db57941c19aa1d4d5ae31950c",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "e839ce131c0d4003a05dd226d1a759ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e3313496754b59b24608088c941ebc",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20c25722b3da462cbb83d02d3c430f2e",
            "value": 3
          }
        },
        "12c19c485ac44c6a84f85ca59ee542b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b375f93729f74e8382bcc99397d3a7a0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_edcdd0e394f5471d98c6781b0f7036e4",
            "value": "â€‡3/3â€‡[00:22&lt;00:00,â€‡â€‡5.68s/it]"
          }
        },
        "86a32fa3c71d4ac3a87276ac5f82620f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6afc1acd104348c18b6cc817db818d78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2804d4db57941c19aa1d4d5ae31950c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27e3313496754b59b24608088c941ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20c25722b3da462cbb83d02d3c430f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b375f93729f74e8382bcc99397d3a7a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edcdd0e394f5471d98c6781b0f7036e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Nemotron_Nano_9B_v2_AI_Agent_Reasoning_Demo_(Python).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install mamba-ssm -q"
      ],
      "metadata": {
        "id": "J4J57NAymLpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers==4.47.0"
      ],
      "metadata": {
        "id": "WVxtPUhtyUC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install causal-conv1d"
      ],
      "metadata": {
        "id": "tp2Ymu8Hyr9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "token=userdata.get('HF_TOKEN')\n",
        "login(token=token)"
      ],
      "metadata": {
        "id": "otz2XEVHwW6S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(f\"--- Loading Nemotron-Nano-9B-v2 model and tokenizer... ---\")\n",
        "# Specify the model name from Hugging Face\n",
        "model_name = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model\n",
        "# torch_dtype=torch.bfloat16 is recommended for better performance on compatible hardware\n",
        "# trust_remote_code=True is necessary for custom model architectures like Nemotron-Nano's hybrid design\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Move the model to GPU if available for faster inference\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(f\"--- Model loaded successfully on {device}! ---\")"
      ],
      "metadata": {
        "id": "FNQ0pW42HYhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_with_nemotron(prompt: str, max_new_tokens: int = 100):\n",
        "\n",
        "    # Prepare the input for the model\n",
        "    # Encode the prompt into token IDs\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    print(f\"\\n--- Generating text for prompt: '{prompt}' ---\")\n",
        "\n",
        "    # Generate text\n",
        "    # The `generate` method handles the text generation process.\n",
        "    # `max_new_tokens` controls the length of the generated output.\n",
        "    # `do_sample=True` enables sampling, leading to more creative outputs.\n",
        "    # `temperature` controls the randomness of the output (lower = more deterministic).\n",
        "    # `top_k` filters out low-probability tokens.\n",
        "    output = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id # Important for generation to stop cleanly\n",
        "    )\n",
        "\n",
        "    # Decode the generated token IDs back into human-readable text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "yOYh5VWheaYT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_with_nemotron_agent(prompt: str, system_prompt: str = \"/think\", max_new_tokens: int = 200):\n",
        "    \"\"\"\n",
        "    Loads the NVIDIA Nemotron-Nano-9B-v2 model and generates text based on a given prompt,\n",
        "    with an optional system prompt to control its 'thinking' (agentic) behavior.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = encoded_input.to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
        "\n",
        "    input_length = input_ids.shape[1]\n",
        "\n",
        "    print(f\"\\n--- Generating text for prompt: '{prompt}' (System prompt: '{system_prompt}') ---\")\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_ids = output_ids[0][input_length:]\n",
        "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "8Y8ZCdakhj7H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_with_nemotron_agent(prompt: str, system_prompt: str = \"/think\", max_new_tokens: int = 200):\n",
        "    \"\"\"\n",
        "    Loads the NVIDIA Nemotron-Nano-9B-v2 model and generates text based on a given prompt,\n",
        "    with an optional system prompt to control its 'thinking' (agentic) behavior.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = encoded_input.to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
        "\n",
        "    input_length = input_ids.shape[1]\n",
        "\n",
        "    print(f\"\\n--- Generating text for prompt: '{prompt}' (System prompt: '{system_prompt}') ---\")\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_ids = output_ids[0][input_length:]\n",
        "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "SAl0qZn4XBJA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"DEMONSTRATING CODE GENERATION (WITH /think SYSTEM PROMPT)\")\n",
        "    print(\"#\"*70)\n",
        "    code_prompt = \"Write a Python function to check if a number is prime. First, outline the steps.\"\n",
        "    generated_code_with_thinking = generate_text_with_nemotron_agent(code_prompt, system_prompt=\"/think\", max_new_tokens=1024)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Generated Code with Reasoning:\")\n",
        "    print(generated_code_with_thinking)\n",
        "    print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0bw5dZMipG6",
        "outputId": "8d33b258-1cc2-45d6-9f8c-fe3e7dbb4e5e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "DEMONSTRATING CODE GENERATION (WITH /think SYSTEM PROMPT)\n",
            "######################################################################\n",
            "\n",
            "--- Generating text for prompt: 'Write a Python function to check if a number is prime. First, outline the steps.' (System prompt: '/think') ---\n",
            "\n",
            "======================================================================\n",
            "Generated Code with Reasoning:\n",
            " output: Sure, I'd be happy to help! Here are the steps to write a Python function to check if a number is prime:\n",
            "\n",
            "1. Define a function named `is_prime` that takes an integer `n` as its argument.\n",
            "2. Check if `n` is less than 2, as 0, 1, and negative numbers are not prime. If so, return `False`.\n",
            "3. Check if `n` is 2, as it is the only even prime number. If so, return `True`.\n",
            "4. Check if `n` is an even number (other than 2), as even numbers greater than 2 are not prime. If so, return `False`.\n",
            "5. Iterate from 3 to the square root of `n` (inclusive), checking if `n` is divisible by any of these numbers. If `n` is divisible by any of these numbers, it is not prime, so return `False`.\n",
            "6. If none of the above conditions are met, `n` is prime, so return `True`.\n",
            "\n",
            "Now, let's implement these steps in Python:\n",
            "\n",
            "```python\n",
            "import math\n",
            "\n",
            "def is_prime(n: int) -> bool:\n",
            "    \"\"\"\n",
            "    Check if a number is prime.\n",
            "\n",
            "    :param n: The number to check.\n",
            "    :return: True if n is prime, False otherwise.\n",
            "    \"\"\"\n",
            "    if n < 2:\n",
            "        return False\n",
            "    if n == 2:\n",
            "        return True\n",
            "    if n % 2 == 0:\n",
            "        return False\n",
            "    for i in range(3, int(math.sqrt(n)) + 1, 2):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "```\n",
            "\n",
            "This function checks if a number is prime by following the steps outlined above. It first handles the edge cases (numbers less than 2, 2 itself, and even numbers greater than 2), and then checks for divisibility by odd numbers up to the square root of the given number.\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    example_prompt = \"Suggest 3 dinner steps.\"\n",
        "    generated_steps = generate_text_with_nemotron(example_prompt, max_new_tokens=512)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Generated Response:\")\n",
        "    print(generated_steps)\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    direct_prompt = \"What are the benefits of exercise?\"\n",
        "    generated_answer = generate_text_with_nemotron(direct_prompt, max_new_tokens=512)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Generated Response:\")\n",
        "    print(generated_answer)\n",
        "    print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YORsCbepeuJK",
        "outputId": "0520dd83-1df3-4577-e457-1d4d5426fbe9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating text for prompt: 'Suggest 3 dinner steps.' ---\n",
            "\n",
            "======================================================================\n",
            "Generated Response:\n",
            "Suggest 3 dinner steps. \n",
            "\n",
            "1. Prepare a spicy dish\n",
            "2. Prepare a non-spicy dish\n",
            "3. Serve both dishes\n",
            "</think>\n",
            "\n",
            "1. **Prepare a spicy dish**: Cook a dish like spicy stir-fried vegetables or chili-lime grilled chicken.  \n",
            "2. **Prepare a non-spicy dish**: Make a mild option such as a vegetarian pasta or baked fish with lemon.  \n",
            "3. **Serve both dishes**: Present them together with condiments (e.g., extra sauce) to let guests adjust the spice level.\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "--- Generating text for prompt: 'What are the benefits of exercise?' ---\n",
            "\n",
            "======================================================================\n",
            "Generated Response:\n",
            "What are the benefits of exercise? The benefit of exercise is really multifaceted. It can help with weight control and can help with strength, flexibility, and balance. It can also reduce the risk of heart disease, stroke, diabetes, and cancer. Regular exercise can also improve mood and mental health. It can also help with sleep and energy levels.\n",
            "</think>\n",
            "\n",
            "Exercise offers a wide range of benefits that positively impact both physical and mental health. Here are some key advantages:\n",
            "\n",
            "1. **Physical Health Benefits**:  \n",
            "   - **Weight Management**: Helps maintain a healthy weight by burning calories and building muscle.  \n",
            "   - **Strength and Flexibility**: Enhances muscle strength, bone density, and joint flexibility.  \n",
            "   - **Reduced Chronic Disease Risk**: Lowers the risk of heart disease, stroke, type 2 diabetes, and certain cancers.  \n",
            "   - **Improved Sleep**: Promotes better sleep quality and duration.  \n",
            "\n",
            "2. **Mental Health Benefits**:  \n",
            "   - **Mood Improvement**: Releases endorphins, which reduce stress, anxiety, and depression.  \n",
            "   - **Cognitive Function**: Enhances brain health, memory, and focus.  \n",
            "   - **Energy Levels**: Boosts overall energy and reduces feelings of fatigue.  \n",
            "\n",
            "3. **Longevity and Quality of Life**:  \n",
            "   - Regular physical activity can extend lifespan and improve overall quality of life.  \n",
            "\n",
            "In summary, exercise is a powerful tool for enhancing both physical and mental well-being, making it essential for a healthy lifestyle.\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
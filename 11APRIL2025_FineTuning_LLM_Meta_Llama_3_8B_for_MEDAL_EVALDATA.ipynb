{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/11APRIL2025_FineTuning_LLM_Meta_Llama_3_8B_for_MEDAL_EVALDATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3kFUczO5Um3"
      },
      "source": [
        "https://llama.meta.com/docs/how-to-guides/fine-tuning/\n",
        "\n",
        "\n",
        "https://medium.com/@frankmorales_91352/fine-tuning-meta-llama-3-8b-with-medal-a-refined-approach-for-enhanced-medical-language-b924d226b09d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etm0HfcZM151"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "! pip install trl==0.8.6 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65JpttRiGxVK"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "37yP0eJDM152"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S9qvxG2HYusD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "naQrZIQTY1wG"
      },
      "outputs": [],
      "source": [
        "# set device\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtE9TuSMlUEc"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GapRov3hl4fT"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-7fKxR2rnbJ"
      },
      "outputs": [],
      "source": [
        "### conversational format\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "\n",
        "### instruction format\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCd7-R-lsLQm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/train_dataset.json\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-kFtkh8kiOn",
        "outputId": "1ca0c4d3-5252-402f-b512-fedf54a85561"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['abstract_id', 'text', 'location', 'label'],\n",
              "    num_rows: 3000000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahaI2KNwYcQ5"
      },
      "outputs": [],
      "source": [
        "print(dataset[6]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMbtvPqFkyFd"
      },
      "outputs": [],
      "source": [
        "print(dataset[6]['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgY7jf_8_Plh"
      },
      "outputs": [],
      "source": [
        "dataset_test = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufPzSf64LDux",
        "outputId": "139e9c1c-73b1-47c5-e67d-5258ee5e22aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['abstract_id', 'text', 'location', 'label'],\n",
              "    num_rows: 1000000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UXZmSkhz-5sI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def dataset_to_finetuning(dataset,nrec):\n",
        "\n",
        "    question=dataset['text']\n",
        "    answer_entry=dataset['label']\n",
        "\n",
        "\n",
        "    !rm -rf /content/question_answering.txt\n",
        "    filename='/content/question_answering.txt'\n",
        "\n",
        "\n",
        "  # python object to be appended\n",
        "    for n in range(nrec):\n",
        "        #print(answer[n])\n",
        "        if answer_entry[n] == None:\n",
        "           answer_entry[n] = 'Not possible to get or use'\n",
        "        string = answer_entry[n]\n",
        "\n",
        "        str_question=question[n]\n",
        "        question_final= re.sub(\"\\n\", \"\", str_question)\n",
        "\n",
        "        answer = re.sub(\"\\[\", \"\", str(string))\n",
        "        answer = re.sub(\"\\]\", \"\", str(answer))\n",
        "        answer0=answer[1:len(answer)-1]\n",
        "\n",
        "        if len(answer0)==0:\n",
        "            answer='Not possible to get or use'\n",
        "\n",
        "        #print(answer0)\n",
        "\n",
        "\n",
        "        text='<s>[INST] %s [/INST] %s </s>'%(question_final,answer0)\n",
        "        del string,answer0\n",
        "        #print(text)\n",
        "        with open(filename, 'a') as f:\n",
        "            f.write(text + '\\n')\n",
        "        f.close()\n",
        "\n",
        "    text0 = load_dataset(\"text\", data_files=\"/content/question_answering.txt\", split=\"train\")\n",
        "\n",
        "    dataset_final_Question=dataset['text'][0:nrec]\n",
        "    dataset_final_Answer=dataset['label'][0:nrec]\n",
        "    dataset_final_text=text0[0:nrec]['text']\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    datasetF = pd.DataFrame() # Create an empty DataFrame\n",
        "    datasetF['Question'] = dataset_final_Question\n",
        "    datasetF['Answer'] = dataset_final_Answer\n",
        "    datasetF['text'] = dataset_final_text\n",
        "\n",
        "\n",
        "    return datasetF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oGlbglrGKBl"
      },
      "outputs": [],
      "source": [
        "datasetF=dataset_to_finetuning(dataset,500000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zapOFE_JUdD"
      },
      "outputs": [],
      "source": [
        "datasetF['Question'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lokPbQcdJbI6"
      },
      "outputs": [],
      "source": [
        "datasetF['Answer'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPmT7AExJnir"
      },
      "outputs": [],
      "source": [
        "datasetF['text'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1ChV5-FJE5M"
      },
      "outputs": [],
      "source": [
        "datasetF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh4mv6NpGh-9"
      },
      "outputs": [],
      "source": [
        "datasetF_test=dataset_to_finetuning(dataset_test,20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnGkIpVtJHvv"
      },
      "outputs": [],
      "source": [
        "datasetF_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQdFE5s1KC9o"
      },
      "outputs": [],
      "source": [
        "datasetF_test['Question'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPsMncjDKK4b"
      },
      "outputs": [],
      "source": [
        "datasetF_test['Answer'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_B8AVYsKPHO"
      },
      "outputs": [],
      "source": [
        "datasetF_test['text'][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQm2yo0uslTs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" #01 march 2024 AND 10/03/2024\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\" #22 MAY 2024\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxOWr2VvEGKe"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kjyYx9WmtMmA"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=64,\n",
        "        lora_dropout=0.05,\n",
        "        r=128,\n",
        "        bias=\"none\",\n",
        "        target_modules=\"all-linear\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AkMVebosM16A"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    output_dir=\"/content/gdrive/MyDrive/model/11APRIL2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata\",\n",
        "\n",
        "    #num_train_epochs=3,                     # number of training epochs\n",
        "    num_train_epochs=0.3,                    # number of training epochs for POC\n",
        "    #num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,          # batch size per device during training\n",
        "    #2\n",
        "    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    #gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "\n",
        "    warmup_ratio=0.15,\n",
        "    weight_decay=0.05,\n",
        "\n",
        "    logging_steps=100,                       # log every 10 steps\n",
        "    learning_rate=5e-5,                     # learning rate, based on QLoRA paper # i used in the first model\n",
        "\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    max_grad_norm=1.0,                      # max gradient norm based on QLoRA paper\n",
        "\n",
        "\n",
        "    #lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    lr_scheduler_type=\"cosine\",            # lr_scheduler_type=\"cosine\" (Cosine Annealing Learning Rate)\n",
        "\n",
        "    push_to_hub=True,                       # push model to hub\n",
        "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir=\"/content/gdrive/MyDrive/model/11APRIL2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata/logs\",\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    metric_for_best_model = \"loss\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lEIGVWGp2dB8"
      },
      "outputs": [],
      "source": [
        "!pip install trl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wd03Af2od_qT"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "#torch.utils.checkpoint.checkpoint(use_reentrant=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Yk7MlowDT-"
      },
      "source": [
        "https://huggingface.co/docs/trl/main/en/sft_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we8LmPrO_AG-"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "import datasets\n",
        "#from datasets import load_metric\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "datasetF_hf = datasets.Dataset.from_pandas(datasetF)\n",
        "datasetF_hf_test = datasets.Dataset.from_pandas(datasetF_test)\n",
        "\n",
        "#max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
        "max_seq_length = 1024\n",
        "\n",
        "# Explicitly set the device using torch.device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Add a padding token to your tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Or, add a new padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "#metric = load_metric(\"squad\")  # Assuming you're evaluating on SQuAD dataset\n",
        "#def compute_metrics(eval_pred):\n",
        "#    logits, labels = eval_pred\n",
        "#    predictions = logits.argmax(axis=-1)\n",
        "#    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=datasetF_hf,\n",
        "    dataset_text_field=\"text\", ### added for the summarization dataset\n",
        "    eval_dataset=datasetF_hf_test,\n",
        "\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    #processing_class=tokenizer,\n",
        "    packing=True,\n",
        "\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,  # We template with special tokens\n",
        "        \"append_concat_token\": False, # No need to add additional separator token\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nuyiFKmWfvU"
      },
      "source": [
        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCq7Dq8f7VPU"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=5))\n",
        "\n",
        "\n",
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mCxeJIj4KvSQ"
      },
      "outputs": [],
      "source": [
        "# free the memory again\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjTwqcXmK_OR"
      },
      "source": [
        "## Test Model and run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_hl8YPKxQ2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"/content/gdrive/MyDrive/model/11APRIL2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata\"\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        "  quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1DyOCUfLLn4"
      },
      "source": [
        "Letâ€™s load our test dataset try to generate an instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBZbeEq-R_ti"
      },
      "outputs": [],
      "source": [
        "print(model.peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uHtGXcrLKI_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "nrec= randint(0, len(eval_dataset))\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GZg3t7d1z9s"
      },
      "outputs": [],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "ganswer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q2zlGDkw1rE",
        "outputId": "1790f723-695b-41f3-bf00-650f3e134c2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "\n",
            "Match\n"
          ]
        }
      ],
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "print()\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "qc=str(ganswer).find('[INST]')\n",
        "ganswer=ganswer[0:qc-7]\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "if qc>0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "print(f\"Generated Answer:\\n{ganswer}\")\n",
        "print()\n",
        "if ganswer == oanswer:\n",
        "  print(\"Match\")\n",
        "else:\n",
        "  print(\"NO Match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX6ChYhtvtat"
      },
      "outputs": [],
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print()\n",
        "print(f\"Full Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcFTvrGSvJ8G"
      },
      "outputs": [],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+8:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc:len(ganswer)]\n",
        "ganswer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxF38_Am7b_5",
        "outputId": "b8a22a68-4299-4721-9f18-5402154e6a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Answer:\n",
            "antral follicle count\n"
          ]
        }
      ],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0-8]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "\n",
        "if qc>=0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "print(f\"Generated Answer:\\n{ganswer}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a pre-trained sentence embedding model\n",
        "model_embedding = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Load your test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "\n",
        "# Create the generation pipeline\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt = sample['text']\n",
        "    outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.7,\n",
        "                                  top_k=50, top_p=0.7, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    ganswer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "    qc0 = str(ganswer).find('[/INST]')\n",
        "    qc1 = str(ganswer).find('[INST]')\n",
        "    ganswer = str(ganswer)[qc0 + 8:qc1 - 8]\n",
        "\n",
        "    oanswer = str(sample['label'])\n",
        "    oanswer = oanswer[2:len(oanswer) - 2]\n",
        "\n",
        "    if len(oanswer) == 0:\n",
        "        oanswer = 'Not possible to get or use'\n",
        "\n",
        "    #print('\\n\\n')\n",
        "    #print(f\"Question: {prompt}\")\n",
        "    #print(f\"Generated Answer: {ganswer}\")\n",
        "    #print(f\"Original Answer: {oanswer}\")\n",
        "\n",
        "    # Generate sentence embeddings\n",
        "    embedding1 = model_embedding.encode(ganswer, convert_to_tensor=True)\n",
        "    embedding2 = model_embedding.encode(oanswer, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_sim = util.cos_sim(embedding1, embedding2).item()\n",
        "\n",
        "    # Set a similarity threshold (adjust as needed)\n",
        "    if cosine_sim >= 0.5:\n",
        "        #print(\"Match\")\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "success_rate = []\n",
        "number_of_eval_samples = 10  # Adjust the number of samples as needed\n",
        "\n",
        "# Evaluate the model\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "    s = eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))\n"
      ],
      "metadata": {
        "id": "MC-SNHc8v63E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = sum(success_rate) / len(success_rate)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxoCKRJddDW2",
        "outputId": "e1552ae4-a34b-4b85-e71c-02f835228644"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorbard"
      ],
      "metadata": {
        "id": "akI7UTrzW95o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/tensorbard\n",
        "%mkdir -p /content/tensorbard\n",
        "%cd /content/tensorbard\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/frankmorales2020/11APRIL2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata"
      ],
      "metadata": {
        "id": "alATl2MpWIur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tensorbard/11APRIL2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata/logs/"
      ],
      "metadata": {
        "id": "jRBbdyrqWKHx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
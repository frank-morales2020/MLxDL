{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMn3OAbQtuUTqTuvkIt5RjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/LLMEVALUATOR_OPENAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-eval -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rZ_Q5S23lr-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8c54a3-b780-4813-ca54-ec8c9f3ee74f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install tiktoken -q\n",
        "!pip install accelerate -q"
      ],
      "metadata": {
        "id": "7X9UOImnb6e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe084e1-fc0c-4c49-f1d3-e1539c5fc397"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKhwQx25Km6j",
        "outputId": "000b3e0e-106b-4d64-f8b2-0fe244851900"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/320.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faFZeUkMKy78",
        "outputId": "51a25446-90cb-4cdf-95d2-ae469a813ff0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "print(openai.api_key)"
      ],
      "metadata": {
        "id": "bQhaCDwAb4Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#harness_repo=\"public-lm-eval-harness\"\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness/\n",
        "%cd /content/lm-evaluation-harness\n",
        "# use main branch on 03-15-2024, SHA is dc90fec\n",
        "#!git checkout dc90fec\n",
        "!pip install -e .\n",
        "#!cd .."
      ],
      "metadata": {
        "id": "JrbsQpKiS2aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets@git+https://github.com/huggingface/datasets.git@66d6242 -q\n",
        "!pip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0 -q"
      ],
      "metadata": {
        "id": "JQ_E8whQS-PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval -h"
      ],
      "metadata": {
        "id": "QkBmEcd9NPm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "nj4YcyKfVL5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/docs/guides/text-generation"
      ],
      "metadata": {
        "id": "4WxhIpl1oHmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#modelid='gpt-4'\n",
        "modelid='gpt-3.5-turbo-instruct'\n",
        "os.environ['model'] = modelid\n",
        "os.environ['task']='lambada_openai,hellaswag'\n",
        "os.environ['shot']='5'\n",
        "os.environ['batch_size']='1'\n",
        "\n",
        "#os.environ['tokenizer']='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "os.environ['tokenizer']= modelid\n",
        "os.environ['add_bos_token']='True'"
      ],
      "metadata": {
        "id": "EYlOA8J50Mmq"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p /content/lm_eval_output\n",
        "%rm -rf /content/lm_eval_output/*"
      ],
      "metadata": {
        "id": "dBeo73W7ZYsX"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#openai-chat-completions, local-chat-completions"
      ],
      "metadata": {
        "id": "xPdf2lFlsazh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "system_content = \"You are a travel agent. Be descriptive and helpful.\"\n",
        "user_content = \"Tell me about Montreal\"\n",
        "\n",
        "\n",
        "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024,\n",
        "    #echo=True\n",
        ")\n",
        "\n",
        "response = chat_completion.choices[0].message.content\n",
        "print(\"Together response:\\n\", response)\n",
        "\n",
        "#TypeError: Completions.create() got an unexpected keyword argument 'echo'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vaj0JaBwgWC",
        "outputId": "257b5805-858b-4e78-ebca-fcf4001f91b8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Together response:\n",
            " Montreal, the largest city in the Canadian province of Quebec, is a cosmopolitan hub that's steeped in history and rich in culture. It's a city that beautifully blends the old and the new, with stunning modern architecture standing side by side with historic buildings dating back to the 17th century.\n",
            "\n",
            "At the heart of Montreal is the historic district of Old Montreal, with its cobblestone streets, old-world charm, and landmarks like the stunning Notre-Dame Basilica, a Gothic Revival church known for its ornate interior. Nearby, you'll find the Old Port of Montreal, where you can enjoy a stroll along the water, visit the Montreal Science Centre, or go for a spin on the La Grande Roue de Montréal, the tallest Ferris wheel in Canada.\n",
            "\n",
            "Montreal is also renowned for its culinary scene. From the beloved local specialty of poutine (fries topped with cheese curds and gravy) to world-class dining establishments, the city is a food lover's paradise. The city’s vibrant neighborhoods like Little Italy and Chinatown offers a wide variety of international cuisines.\n",
            "\n",
            "If you're a fan of the arts, Montreal won't disappoint. The city is home to the Montreal Museum of Fine Arts, the Contemporary Art Museum, and numerous art galleries. The city also hosts numerous festivals throughout the year, including the Montreal International Jazz Festival, which is the largest jazz festival in the world, and Just for Laughs, the largest international comedy festival.\n",
            "\n",
            "Outdoor enthusiasts will also find plenty to do in Montreal. The city boasts numerous parks, including the stunning Mount Royal Park, designed by the same designer as New York City's Central Park. In winter, the city transforms into a winter wonderland, with opportunities for ice skating, snowshoeing, and even cross-country skiing.\n",
            "\n",
            "Montreal is a bilingual city, with most residents speaking both English and French, adding to its multicultural charm. It's a city that's easy to navigate, with an efficient public transportation system and plenty of bike lanes. Whether you're seeking history, culture, cuisine, or outdoor adventure, Montreal has something to offer everyone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $model\n",
        "!echo $task\n",
        "!echo $shot\n",
        "!echo $batch_size\n",
        "#print(os.environ)\n",
        "print()\n",
        "\n",
        "# Supported model names: anthropic, anthropic-chat, anthropic-chat-completions, dummy, gguf, ggml, hf-auto, hf, huggingface, mamba_ssm, nemo_lm, sparseml, deepsparse, neuronx, openai-completions, local-completions, openai-chat-completions, local-chat-completions, openvino, textsynth, vllm\n",
        "# trust_remote_code=True,\n",
        "!lm_eval --model openai-chat-completions --verbosity DEBUG --model_args model=${model},tokenizer=${tokenizer}   --tasks ${task} --num_fewshot ${shot} --output_path /content/lm_eval_output/${model//\\//_}_${task//,/_}-${shot}shot --batch_size ${batch_size} 2>&1 | tee /content/lm_eval_output/eval-${model//\\//_}_${task//,/_}-${shot}shot.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHVFFbc2xrSe",
        "outputId": "eb0327d0-8671-4543-e36d-d735297a8bc9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo-instruct\n",
            "lambada_openai,hellaswag\n",
            "5\n",
            "1\n",
            "\n",
            "2024-05-13 12:38:41.958835: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-13 12:38:41.958893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-13 12:38:41.960273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-13 12:38:43.256860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-13:12:38:46,967 INFO     [__main__.py:254] Verbosity set to DEBUG\n",
            "2024-05-13:12:38:53,257 INFO     [__main__.py:341] Selected Tasks: ['hellaswag', 'lambada_openai']\n",
            "2024-05-13:12:38:53,258 INFO     [evaluator.py:141] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-05-13:12:38:53,258 INFO     [evaluator.py:178] Initializing openai-chat-completions model, with arguments: {'model': 'gpt-3.5-turbo-instruct', 'tokenizer': 'gpt-3.5-turbo-instruct'}\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1483: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "2024-05-13:12:39:07,001 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-05-13:12:39:07,001 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-05-13:12:39:07,081 WARNING  [evaluator.py:240] Overwriting default num_fewshot of lambada_openai from None to 5\n",
            "2024-05-13:12:39:07,081 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-13:12:39:07,081 WARNING  [evaluator.py:240] Overwriting default num_fewshot of hellaswag from None to 5\n",
            "2024-05-13:12:39:07,081 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-13:12:39:07,083 DEBUG    [cache.py:33] requests-lambada_openai-5shot-rank0-world_size1 is not cached, generating...\n",
            "2024-05-13:12:39:07,083 INFO     [task.py:398] Building contexts for lambada_openai on rank 0...\n",
            "100%|██████████| 5153/5153 [01:23<00:00, 61.82it/s]\n",
            "2024-05-13:12:40:30,516 DEBUG    [evaluator.py:366] Task: lambada_openai; number of requests on this rank: 5153\n",
            "2024-05-13:12:40:30,517 DEBUG    [cache.py:33] requests-hellaswag-5shot-rank0-world_size1 is not cached, generating...\n",
            "2024-05-13:12:40:30,518 INFO     [task.py:398] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 10042/10042 [01:46<00:00, 94.20it/s]\n",
            "2024-05-13:12:42:18,269 DEBUG    [evaluator.py:366] Task: hellaswag; number of requests on this rank: 40168\n",
            "2024-05-13:12:42:18,280 INFO     [evaluator.py:395] Running loglikelihood requests\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/__main__.py\", line 347, in cli_evaluate\n",
            "    results = evaluator.simple_evaluate(\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 321, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 256, in simple_evaluate\n",
            "    results = evaluate(\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 321, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 406, in evaluate\n",
            "    resps = getattr(lm, reqtype)(cloned_reqs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/openai_completions.py\", line 475, in loglikelihood\n",
            "    raise NotImplementedError(\"No support for logits.\")\n",
            "NotImplementedError: No support for logits.\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJ7Xg+6+GrXgYp/0b3eGzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/LLMEVALUATOR_OPENAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lm-eval -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rZ_Q5S23lr-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install tiktoken -q\n",
        "!pip install accelerate -q"
      ],
      "metadata": {
        "id": "7X9UOImnb6e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab15799-3c83-4668-84fd-0bf9048e7b5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore -q"
      ],
      "metadata": {
        "id": "qKhwQx25Km6j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faFZeUkMKy78",
        "outputId": "06f2019a-474f-48ff-b915-f56b0b32ab81"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "print(openai.api_key)"
      ],
      "metadata": {
        "id": "bQhaCDwAb4Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#harness_repo=\"public-lm-eval-harness\"\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness/\n",
        "%cd /content/lm-evaluation-harness\n",
        "# use main branch on 03-15-2024, SHA is dc90fec\n",
        "#!git checkout dc90fec\n",
        "!pip install -e .\n",
        "#!cd .."
      ],
      "metadata": {
        "id": "JrbsQpKiS2aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets@git+https://github.com/huggingface/datasets.git@66d6242 -q\n",
        "!pip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0 -q"
      ],
      "metadata": {
        "id": "JQ_E8whQS-PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval -h"
      ],
      "metadata": {
        "id": "QkBmEcd9NPm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "nj4YcyKfVL5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['model'] = 'gpt-3.5-turbo-instruct'\n",
        "os.environ['task']='lambada_openai,hellaswag'\n",
        "os.environ['shot']='5'\n",
        "os.environ['batch_size']='1'\n",
        "\n",
        "os.environ['tokenizer']='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "#os.environ['tokenizer']='gpt-3.5-turbo-instruct'\n",
        "os.environ['add_bos_token']='True'"
      ],
      "metadata": {
        "id": "EYlOA8J50Mmq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p /content/lm_eval_output\n",
        "%rm -rf /content/lm_eval_output/*"
      ],
      "metadata": {
        "id": "dBeo73W7ZYsX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $model\n",
        "!echo $task\n",
        "!echo $shot\n",
        "!echo $batch_size\n",
        "#print(os.environ)\n",
        "print()\n",
        "\n",
        "# Supported model names: anthropic, anthropic-chat, anthropic-chat-completions, dummy, gguf, ggml, hf-auto, hf, huggingface, mamba_ssm, nemo_lm, sparseml, deepsparse, neuronx, openai-completions, local-completions, openai-chat-completions, local-chat-completions, openvino, textsynth, vllm\n",
        "!lm_eval --model openai-chat-completions --verbosity DEBUG --model_args engine=${model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer}   --tasks ${task}  --device cuda:0 --num_fewshot ${shot} --output_path /content/lm_eval_output/${model//\\//_}_${task//,/_}-${shot}shot --batch_size ${batch_size} 2>&1 | tee /content/lm_eval_output/eval-${model//\\//_}_${task//,/_}-${shot}shot.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHVFFbc2xrSe",
        "outputId": "fae9f626-025b-4703-ec83-2011e9117dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo-instruct\n",
            "lambada_openai,hellaswag\n",
            "5\n",
            "1\n",
            "\n",
            "2024-05-13 10:25:48.978259: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-13 10:25:48.978314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-13 10:25:48.979648: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-13 10:25:50.169869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-13:10:25:53,950 INFO     [__main__.py:254] Verbosity set to DEBUG\n",
            "2024-05-13:10:26:00,000 INFO     [__main__.py:341] Selected Tasks: ['hellaswag', 'lambada_openai']\n",
            "2024-05-13:10:26:00,002 INFO     [evaluator.py:141] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-05-13:10:26:00,002 INFO     [evaluator.py:178] Initializing openai-chat-completions model, with arguments: {'engine': 'gpt-3.5-turbo-instruct', 'trust_remote_code': True, 'add_bos_token': True, 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.1'}\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1483: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100%|██████████| 4.36k/4.36k [00:00<00:00, 19.7MB/s]\n",
            "Downloading metadata: 100%|██████████| 2.53k/2.53k [00:00<00:00, 11.2MB/s]\n",
            "Downloading readme: 100%|██████████| 6.84k/6.84k [00:00<00:00, 21.0MB/s]\n",
            "Downloading data: 47.5MB [00:00, 68.2MB/s]\n",
            "Downloading data: 11.8MB [00:00, 44.0MB/s]\n",
            "Downloading data: 12.2MB [00:00, 44.1MB/s]\n",
            "Generating train split: 100%|██████████| 39905/39905 [00:03<00:00, 10094.74 examples/s]\n",
            "Generating test split: 100%|██████████| 10003/10003 [00:01<00:00, 9955.87 examples/s]\n",
            "Generating validation split: 100%|██████████| 10042/10042 [00:01<00:00, 9947.44 examples/s]\n",
            "Map: 100%|██████████| 39905/39905 [00:06<00:00, 6306.75 examples/s]\n",
            "Map: 100%|██████████| 10042/10042 [00:01<00:00, 5574.84 examples/s]\n",
            "Downloading data: 100%|██████████| 1.16M/1.16M [00:02<00:00, 513kB/s]\n",
            "Generating test split: 100%|██████████| 5153/5153 [00:00<00:00, 455975.71 examples/s]\n",
            "2024-05-13:10:26:45,625 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-05-13:10:26:45,625 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-05-13:10:26:45,703 WARNING  [evaluator.py:240] Overwriting default num_fewshot of lambada_openai from None to 5\n",
            "2024-05-13:10:26:45,703 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-13:10:26:45,703 WARNING  [evaluator.py:240] Overwriting default num_fewshot of hellaswag from None to 5\n",
            "2024-05-13:10:26:45,703 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-13:10:26:45,704 DEBUG    [cache.py:33] requests-lambada_openai-5shot-rank0-world_size1 is not cached, generating...\n",
            "2024-05-13:10:26:45,704 INFO     [task.py:398] Building contexts for lambada_openai on rank 0...\n",
            "100%|██████████| 5153/5153 [01:19<00:00, 64.66it/s]\n",
            "2024-05-13:10:28:05,477 DEBUG    [evaluator.py:366] Task: lambada_openai; number of requests on this rank: 5153\n",
            "2024-05-13:10:28:05,479 DEBUG    [cache.py:33] requests-hellaswag-5shot-rank0-world_size1 is not cached, generating...\n",
            "2024-05-13:10:28:05,479 INFO     [task.py:398] Building contexts for hellaswag on rank 0...\n",
            " 57%|█████▋    | 5721/10042 [00:58<00:45, 94.74it/s]"
          ]
        }
      ]
    }
  ]
}
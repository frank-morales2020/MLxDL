{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdLJNouCmjkKqmJlkC2675",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/LLMEVALUATOR_OPENAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lm-eval -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rZ_Q5S23lr-G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install tiktoken -q\n",
        "!pip install accelerate -q"
      ],
      "metadata": {
        "id": "7X9UOImnb6e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore -q"
      ],
      "metadata": {
        "id": "qKhwQx25Km6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faFZeUkMKy78",
        "outputId": "2c291630-f9a2-4804-dcb0-23608b08411b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "print(openai.api_key)"
      ],
      "metadata": {
        "id": "bQhaCDwAb4Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#harness_repo=\"public-lm-eval-harness\"\n",
        "\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness/\n",
        "%cd /content/lm-evaluation-harness\n",
        "\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "JrbsQpKiS2aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets@git+https://github.com/huggingface/datasets.git@66d6242 -q\n",
        "!pip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ_E8whQS-PI",
        "outputId": "1a1e34e5-b260-4dda-bf46-a9f249b7c054"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m  WARNING: Did not find branch or tag '66d6242', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for datasets (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!which lm_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkBmEcd9NPm9",
        "outputId": "54469c2d-b01e-4a5e-a4ce-1ba772155118"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/lm_eval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/docs/guides/text-generation"
      ],
      "metadata": {
        "id": "4WxhIpl1oHmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p /content/lm_eval_output\n",
        "%rm -rf /content/lm_eval_output/*"
      ],
      "metadata": {
        "id": "dBeo73W7ZYsX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "system_content = \"You are a travel agent. Be descriptive and helpful.\"\n",
        "user_content = \"Tell me about Montreal\"\n",
        "\n",
        "\n",
        "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024,\n",
        "    #echo=True\n",
        ")\n",
        "\n",
        "response = chat_completion.choices[0].message.content\n",
        "print(\"Together response:\\n\", response)\n",
        "\n",
        "#TypeError: Completions.create() got an unexpected keyword argument 'echo'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vaj0JaBwgWC",
        "outputId": "0f0bdef6-7af9-4e9e-cd21-ca12e8a6e813"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Together response:\n",
            " Montreal, nestled in the heart of the Canadian province of Quebec, is a city steeped in a rich history and culture. It's the second-largest city in Canada and known for its stunning blend of old-world charm meets modern elegance. Being a bilingual city, both English and French are widely spoken, offering a European flair that's quite unique in North America.\n",
            "\n",
            "The city is divided into 19 large boroughs, and each has its own distinctive character. Downtown Montreal is the central business district, filled with skyscrapers, shopping centers, and museums. Old Montreal, the historic district, is where you can find the enchanting cobblestone streets, horse-drawn carriages, and historic sites like the Notre-Dame Basilica. The Plateau Mont-Royal is known for its colorful houses, hip cafes, and vibrant nightlife.\n",
            "\n",
            "Montreal is a city of festivals, hosting numerous large-scale events throughout the year, such as the world-famous Just for Laughs comedy festival, the Montreal International Jazz Festival, and the exciting Cirque du Soleil.\n",
            "\n",
            "Foodies will adore the city for its thriving culinary scene. Montreal is famous for its bagels, smoked meat sandwiches, and poutine, a delicious dish made of fries, cheese curds, and gravy. The city also boasts a wide variety of international cuisine due to its diverse population.\n",
            "\n",
            "Nature lovers aren't left out either. The city offers many parks, the most notable being Mount Royal Park, designed by the same man who designed Central Park in New York City. You can hike up Mount Royal for a panoramic view of the city. The Montreal Botanical Garden is another must-visit with its stunning array of plant species.\n",
            "\n",
            "Winters in Montreal can be quite cold and snowy, while summers are warm and often humid. Depending on the season of your visit, you could engage in activities ranging from ice skating to cycling.\n",
            "\n",
            "Montreal is a city that thrives on its mixture of history and modernity, English and French, and natural beauty with urban charm. It's a destination that can offer something to every traveler.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/lm-evaluation-harness/lm_eval/models/openai_completions.py -- line=478"
      ],
      "metadata": {
        "id": "CpeoaO2iHZAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def loglikelihood(self, requests, disable_tqdm: bool = False):\n",
        "        res = []\n",
        "\n",
        "        for _ in tqdm(requests, disable=disable_tqdm):\n",
        "            res.append((-random.random(), False))\n",
        "\n",
        "        return res"
      ],
      "metadata": {
        "id": "2bNaHyW_U1cg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/EleutherAI/lm-evaluation-harness/"
      ],
      "metadata": {
        "id": "1GhLcBMGFDig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "modelid='gpt-4'\n",
        "#modelid='gpt-3.5-turbo-instruct'\n",
        "\n",
        "os.environ['model'] = modelid\n",
        "#os.environ['task']='mmlu'\n",
        "\n",
        "#os.environ['task']='lambada_openai,hellaswag,mmlu'\n",
        "os.environ['task']='lambada_openai,openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq'\n",
        "os.environ['shot']='0'\n",
        "os.environ['batch_size']='1'\n",
        "\n",
        "os.environ['tokenizer']= modelid\n",
        "os.environ['add_bos_token']='True'\n",
        "\n",
        "os.environ['log_samples']='True'\n",
        "os.environ['random_seed']='0'\n",
        "os.environ['trust_remote_code']='True'"
      ],
      "metadata": {
        "id": "EYlOA8J50Mmq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6b,parallelize=True,load_in_4bit=True,peft=nomic-ai/gpt4all-j-lora --tasks openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq --device cuda:0"
      ],
      "metadata": {
        "id": "E_Pgt9ssGWr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $model\n",
        "!echo $task\n",
        "!echo $shot\n",
        "!echo $batch_size\n",
        "#print(os.environ)\n",
        "print()\n",
        "\n",
        "# Supported model names: anthropic, anthropic-chat, anthropic-chat-completions, dummy, gguf, ggml, hf-auto, hf, huggingface, mamba_ssm, nemo_lm, sparseml, deepsparse, neuronx, openai-completions, local-completions, openai-chat-completions, local-chat-completions, openvino, textsynth, vllm\n",
        "# trust_remote_code=True,\n",
        "!lm_eval --model openai-chat-completions --model_args model=${model},tokenizer=${tokenizer}   --tasks ${task}  --num_fewshot ${shot} --output_path /content/lm_eval_output/${model//\\//_}_${task//,/_}-${shot}shot --batch_size ${batch_size} 2>&1 | tee /content/lm_eval_output/eval-${model//\\//_}_${task//,/_}-${shot}shot.log\n",
        "#!lm_eval --model openai-completions --verbosity DEBUG --model_args model=${model},tokenizer=${tokenizer},random_seed=${random_seed},log_samples=${log_samples},trust_remote_code=${trust_remote_code}   --tasks ${task}  --num_fewshot ${shot} --output_path /content/lm_eval_output/${model//\\//_}_${task//,/_}-${shot}shot --batch_size ${batch_size} 2>&1 | tee /content/lm_eval_output/eval-${model//\\//_}_${task//,/_}-${shot}shot.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHVFFbc2xrSe",
        "outputId": "21ed8522-760b-4d5a-8e4e-4e79046de6f4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-4\n",
            "lambada_openai,openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq\n",
            "0\n",
            "1\n",
            "\n",
            "2024-05-14 08:15:10.177686: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-14 08:15:10.177851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-14 08:15:10.179577: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-14 08:15:11.887851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-14:08:15:16,396 INFO     [__main__.py:254] Verbosity set to INFO\n",
            "2024-05-14:08:15:23,507 INFO     [__main__.py:341] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'openbookqa', 'piqa', 'winogrande']\n",
            "2024-05-14:08:15:23,510 INFO     [evaluator.py:141] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-05-14:08:15:23,510 INFO     [evaluator.py:178] Initializing openai-chat-completions model, with arguments: {'model': 'gpt-4', 'tokenizer': 'gpt-4'}\n",
            "Downloading readme: 100%|██████████| 9.00k/9.00k [00:00<00:00, 21.2MB/s]\n",
            "Downloading data: 100%|██████████| 190k/190k [00:00<00:00, 304kB/s]\n",
            "Downloading data: 100%|██████████| 204k/204k [00:00<00:00, 649kB/s]\n",
            "Downloading data: 100%|██████████| 55.7k/55.7k [00:00<00:00, 203kB/s]\n",
            "Generating train split: 100%|██████████| 1119/1119 [00:00<00:00, 68874.11 examples/s]\n",
            "Generating test split: 100%|██████████| 1172/1172 [00:00<00:00, 190924.16 examples/s]\n",
            "Generating validation split: 100%|██████████| 299/299 [00:00<00:00, 86854.83 examples/s]\n",
            "Downloading data: 100%|██████████| 331k/331k [00:01<00:00, 253kB/s]\n",
            "Downloading data: 100%|██████████| 346k/346k [00:01<00:00, 271kB/s]\n",
            "Downloading data: 100%|██████████| 86.1k/86.1k [00:01<00:00, 67.2kB/s]\n",
            "Generating train split: 100%|██████████| 2251/2251 [00:00<00:00, 210702.72 examples/s]\n",
            "Generating test split: 100%|██████████| 2376/2376 [00:00<00:00, 245496.04 examples/s]\n",
            "Generating validation split: 100%|██████████| 570/570 [00:00<00:00, 122804.26 examples/s]\n",
            "2024-05-14:08:15:39,793 WARNING  [task.py:774] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "2024-05-14:08:15:39,793 WARNING  [task.py:786] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "Downloading data: 100%|██████████| 3.85M/3.85M [00:01<00:00, 2.67MB/s]\n",
            "Downloading data: 100%|██████████| 1.31M/1.31M [00:00<00:00, 4.36MB/s]\n",
            "Downloading data: 100%|██████████| 1.31M/1.31M [00:00<00:00, 4.68MB/s]\n",
            "Generating train split: 100%|██████████| 9427/9427 [00:00<00:00, 191560.91 examples/s]\n",
            "Generating validation split: 100%|██████████| 3270/3270 [00:00<00:00, 200625.69 examples/s]\n",
            "Generating test split: 100%|██████████| 3245/3245 [00:00<00:00, 206291.84 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1483: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "2024-05-14:08:16:01,998 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-05-14:08:16:01,998 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading readme: 100%|██████████| 9.06k/9.06k [00:00<00:00, 21.5MB/s]\n",
            "Downloading data: 100%|██████████| 496k/496k [00:01<00:00, 316kB/s]\n",
            "Downloading data: 100%|██████████| 58.2k/58.2k [00:01<00:00, 40.2kB/s]\n",
            "Downloading data: 100%|██████████| 55.5k/55.5k [00:01<00:00, 37.9kB/s]\n",
            "Generating train split: 100%|██████████| 4957/4957 [00:00<00:00, 341886.85 examples/s]\n",
            "Generating validation split: 100%|██████████| 500/500 [00:00<00:00, 123733.08 examples/s]\n",
            "Generating test split: 100%|██████████| 500/500 [00:00<00:00, 119203.77 examples/s]\n",
            "Downloading data: 100%|██████████| 2.66M/2.66M [00:01<00:00, 2.16MB/s]\n",
            "Downloading data: 100%|██████████| 502k/502k [00:00<00:00, 610kB/s]\n",
            "Downloading data: 100%|██████████| 301k/301k [00:00<00:00, 485kB/s]\n",
            "Generating train split: 100%|██████████| 16113/16113 [00:00<00:00, 457224.57 examples/s]\n",
            "Generating test split: 100%|██████████| 3084/3084 [00:00<00:00, 342791.40 examples/s]\n",
            "Generating validation split: 100%|██████████| 1838/1838 [00:00<00:00, 312654.85 examples/s]\n",
            "Downloading data: 100%|██████████| 2.06M/2.06M [00:00<00:00, 7.09MB/s]\n",
            "Downloading data: 100%|██████████| 118k/118k [00:00<00:00, 192kB/s]\n",
            "Downloading data: 100%|██████████| 85.9k/85.9k [00:00<00:00, 385kB/s]\n",
            "Generating train split: 100%|██████████| 40398/40398 [00:00<00:00, 705718.05 examples/s]\n",
            "Generating test split: 100%|██████████| 1767/1767 [00:00<00:00, 373122.65 examples/s]\n",
            "Generating validation split: 100%|██████████| 1267/1267 [00:00<00:00, 301890.77 examples/s]\n",
            "2024-05-14:08:16:35,168 WARNING  [evaluator.py:240] Overwriting default num_fewshot of winogrande from None to 0\n",
            "2024-05-14:08:16:35,168 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,168 WARNING  [evaluator.py:240] Overwriting default num_fewshot of piqa from None to 0\n",
            "2024-05-14:08:16:35,168 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,168 WARNING  [evaluator.py:240] Overwriting default num_fewshot of openbookqa from None to 0\n",
            "2024-05-14:08:16:35,168 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,168 WARNING  [evaluator.py:240] Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "2024-05-14:08:16:35,168 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,168 WARNING  [evaluator.py:240] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2024-05-14:08:16:35,168 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,168 WARNING  [evaluator.py:240] Overwriting default num_fewshot of boolq from None to 0\n",
            "2024-05-14:08:16:35,168 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,168 WARNING  [evaluator.py:240] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2024-05-14:08:16:35,168 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,169 WARNING  [evaluator.py:240] Overwriting default num_fewshot of arc_challenge from None to 0\n",
            "2024-05-14:08:16:35,169 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
            "2024-05-14:08:16:35,171 INFO     [task.py:398] Building contexts for winogrande on rank 0...\n",
            "100%|██████████| 1267/1267 [00:00<00:00, 72638.82it/s]\n",
            "2024-05-14:08:16:35,237 INFO     [task.py:398] Building contexts for piqa on rank 0...\n",
            "100%|██████████| 1838/1838 [00:02<00:00, 855.28it/s]\n",
            "2024-05-14:08:16:37,459 INFO     [task.py:398] Building contexts for openbookqa on rank 0...\n",
            "100%|██████████| 500/500 [00:00<00:00, 1655.54it/s]\n",
            "2024-05-14:08:16:37,794 INFO     [task.py:398] Building contexts for lambada_openai on rank 0...\n",
            "100%|██████████| 5153/5153 [00:11<00:00, 448.46it/s]\n",
            "2024-05-14:08:16:49,373 INFO     [task.py:398] Building contexts for hellaswag on rank 0...\n",
            "100%|██████████| 10042/10042 [00:05<00:00, 1817.27it/s]\n",
            "2024-05-14:08:16:56,256 INFO     [task.py:398] Building contexts for boolq on rank 0...\n",
            "100%|██████████| 3270/3270 [00:02<00:00, 1476.52it/s]\n",
            "2024-05-14:08:16:58,623 INFO     [task.py:398] Building contexts for arc_easy on rank 0...\n",
            "100%|██████████| 2376/2376 [00:02<00:00, 858.70it/s]\n",
            "2024-05-14:08:17:01,548 INFO     [task.py:398] Building contexts for arc_challenge on rank 0...\n",
            "100%|██████████| 1172/1172 [00:01<00:00, 865.16it/s]\n",
            "2024-05-14:08:17:02,983 INFO     [evaluator.py:395] Running loglikelihood requests\n",
            "100%|██████████| 74259/74259 [00:00<00:00, 1429826.48it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "bootstrapping for stddev: perplexity\n",
            "100%|██████████| 100/100 [00:20<00:00,  4.97it/s]\n",
            "2024-05-14:08:17:43,816 INFO     [evaluation_tracker.py:132] Saving results aggregated\n",
            "openai-chat-completions (model=gpt-4,tokenizer=gpt-4), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: 1\n",
            "|    Tasks     |Version|Filter|n-shot|  Metric  |Value |   |Stderr|\n",
            "|--------------|------:|------|-----:|----------|-----:|---|-----:|\n",
            "|arc_challenge |      1|none  |     0|acc       |0.2363|±  |0.0124|\n",
            "|              |       |none  |     0|acc_norm  |0.2457|±  |0.0126|\n",
            "|arc_easy      |      1|none  |     0|acc       |0.2445|±  |0.0088|\n",
            "|              |       |none  |     0|acc_norm  |0.2441|±  |0.0088|\n",
            "|boolq         |      2|none  |     0|acc       |0.5003|±  |0.0087|\n",
            "|hellaswag     |      1|none  |     0|acc       |0.2478|±  |0.0043|\n",
            "|              |       |none  |     0|acc_norm  |0.2471|±  |0.0043|\n",
            "|lambada_openai|      1|none  |     0|perplexity|1.6401|±  |0.0065|\n",
            "|              |       |none  |     0|acc       |0.0000|±  |0.0000|\n",
            "|openbookqa    |      1|none  |     0|acc       |0.2460|±  |0.0193|\n",
            "|              |       |none  |     0|acc_norm  |0.2820|±  |0.0201|\n",
            "|piqa          |      1|none  |     0|acc       |0.4984|±  |0.0117|\n",
            "|              |       |none  |     0|acc_norm  |0.5060|±  |0.0117|\n",
            "|winogrande    |      1|none  |     0|acc       |0.4893|±  |0.0140|\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
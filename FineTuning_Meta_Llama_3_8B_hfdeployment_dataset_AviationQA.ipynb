{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "SPAPb3JdCIan",
        "sPAxhdxMAdBS",
        "of-Z3B28BNBr",
        "NcaH5WrdBW8e",
        "sNtNfyP6BuOI"
      ],
      "authorship_tag": "ABX9TyO3L8HPyDwLUdoH0go71gJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FineTuning_Meta_Llama_3_8B_hfdeployment_dataset_AviationQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/mistral-7b-tutorial"
      ],
      "metadata": {
        "id": "Kw_Oj0C8H2tJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPENDENCIES"
      ],
      "metadata": {
        "id": "SPAPb3JdCIan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorboard"
      ],
      "metadata": {
        "id": "LsJqORyIZjmp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etm0HfcZM151"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "#!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# versions: 0.0.1, 0.0.2, 0.0.3, 0.1.0, 0.2.0, 0.2.1, 0.3.0, 0.3.1, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.5.0, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5, 0.8.6, 0.9.2, 0.9.3, 0.9.4)\n",
        "#ERROR: No matching distribution found for trl==0.0.99\n",
        "! pip install trl==0.8.6 -q"
      ],
      "metadata": {
        "id": "cI7mHbyTQWT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "65JpttRiGxVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "6usdVgDF50fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3cb285-7e81-44cb-b734-af176a492b19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "S9qvxG2HYusD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "naQrZIQTY1wG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KtE9TuSMlUEc",
        "outputId": "ad5e7aa1-3c3c-4bb0-bebc-660a370b226b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get update && apt-get install -y cuda-11-0"
      ],
      "metadata": {
        "id": "LY6iadKNl1Af"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GapRov3hl4fT",
        "outputId": "91d7be90-9960-4b63-c4f5-51e597cd2702"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Thu Jun 20 00:10:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0              40W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### conversational format\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "\n",
        "### instruction format\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
      ],
      "metadata": {
        "id": "p-7fKxR2rnbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "sPAxhdxMAdBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://huggingface.co/datasets/sakharamg/AviationQA"
      ],
      "metadata": {
        "id": "ue5Leyc90T6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "print(\"Preprocessing dataset AviationQA\")\n",
        "dataset = load_dataset(\"sakharamg/AviationQA\")\n",
        "\n",
        "# save datasets to disk\n",
        "dataset[\"train\"].to_json(\"train_dataset_AviationQA.json\", orient=\"records\")\n",
        "dataset[\"validation\"].to_json(\"validation_dataset_AviationQA.json\", orient=\"records\")\n",
        "dataset[\"test\"].to_json(\"test_dataset_AviationQA.json\", orient=\"records\")"
      ],
      "metadata": {
        "id": "WY55Lb9ixyCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP4XSSClx2IX",
        "outputId": "2472a278-7c16-4aa0-f212-fc725567f07f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 1057986\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 11888\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 10807\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/train_dataset_AviationQA.json\", split=\"train\")"
      ],
      "metadata": {
        "id": "SInSDc3Xx6w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9c4RuZCx_gn",
        "outputId": "5e201205-dd64-4ce8-e43d-c6bbc6610bc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'Question', 'Answer'],\n",
            "    num_rows: 1057986\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_question=dataset['Question']\n",
        "train_answer=dataset['Answer']"
      ],
      "metadata": {
        "id": "t3C7lGBlFXog"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename='/content/aviation-qa.txt'\n",
        "# python object to be appended\n",
        "for n in range(10000):\n",
        "    if train_answer[n] == None:\n",
        "       train_answer[n] = 'Not possible to get or use'\n",
        "    text='<s>[INST] %s [/INST] %s </s>'%(train_question[n],train_answer[n])\n",
        "    #print(text)\n",
        "    with open(filename, 'a') as f:\n",
        "        f.write(text + '\\n')"
      ],
      "metadata": {
        "id": "WLgn-AwEDuKD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text0 = load_dataset(\"text\", data_files=\"/content/aviation-qa.txt\", split=\"train\")"
      ],
      "metadata": {
        "id": "j4AtoIrsD0k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text0[2]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MNdRQ4UTD5e4",
        "outputId": "c68e7055-2e00-4495-e0d2-ccd7beeb51d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] Who determines the probable cause(s) of this accident no. ERA22LA104? [/INST] The National Transportation Safety Board </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrec=10000\n",
        "dataset_final_id=dataset['id'][0:nrec]\n",
        "dataset_final_Question=dataset['Question'][0:nrec]\n",
        "dataset_final_Answer=dataset['Answer'][0:nrec]\n",
        "dataset_final_text=text0[0:nrec]['text']"
      ],
      "metadata": {
        "id": "50KFalQfEDkI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35LRbDnvyGAP",
        "outputId": "a4b6ec15-0c7f-497f-a724-4ae478990d18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'a529971724',\n",
              " 'Question': 'Who determines the probable cause(s) of this accident no. ERA22LA104?',\n",
              " 'Answer': 'The National Transportation Safety Board'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "datasetF = pd.DataFrame() # Create an empty DataFrame\n",
        "datasetF['id'] = dataset_final_id\n",
        "datasetF['Question'] = dataset_final_Question\n",
        "datasetF['Answer'] = dataset_final_Answer\n",
        "datasetF['text'] = dataset_final_text"
      ],
      "metadata": {
        "id": "07nkZ2Pu3va9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "B24h4n-E3fkg",
        "outputId": "058e83f5-6299-46f3-92eb-2b96001fe149"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               id                                           Question  \\\n",
              "0     a9567233027  Where was the Destination of the accident no. ...   \n",
              "1     a4671579659  What is the name of the Engine Manufacturer of...   \n",
              "2      a529971724  Who determines the probable cause(s) of this a...   \n",
              "3     a1197873694  What are the Probable findings of the accident...   \n",
              "4     a7803769704  In the accident with no. DCA16CA009, is the ai...   \n",
              "...           ...                                                ...   \n",
              "9995  a4688878518  Was there any Operating Certificate(s) held of...   \n",
              "9996  a8331736975  What was the VFR Approach/Landing of the accid...   \n",
              "9997  a4342209241  Was there fire on aircraft of the accident no....   \n",
              "9998  a2435551586  Who determines the probable cause(s) of this a...   \n",
              "9999  a5620706103  What is the Airplane Rating(s) of the accident...   \n",
              "\n",
              "                                                 Answer  \\\n",
              "0                                       Los Angeles, CA   \n",
              "1                                       Pratt & Whitney   \n",
              "2              The National Transportation Safety Board   \n",
              "3     Environmental issues Terrain induced turbulenc...   \n",
              "4                                                  None   \n",
              "...                                                 ...   \n",
              "9995                                               None   \n",
              "9996                                          Go around   \n",
              "9997                                               None   \n",
              "9998           The National Transportation Safety Board   \n",
              "9999  Single-engine land; Single-engine sea; Multi-e...   \n",
              "\n",
              "                                                   text  \n",
              "0     <s>[INST] Where was the Destination of the acc...  \n",
              "1     <s>[INST] What is the name of the Engine Manuf...  \n",
              "2     <s>[INST] Who determines the probable cause(s)...  \n",
              "3     <s>[INST] What are the Probable findings of th...  \n",
              "4     <s>[INST] In the accident with no. DCA16CA009,...  \n",
              "...                                                 ...  \n",
              "9995  <s>[INST] Was there any Operating Certificate(...  \n",
              "9996  <s>[INST] What was the VFR Approach/Landing of...  \n",
              "9997  <s>[INST] Was there fire on aircraft of the ac...  \n",
              "9998  <s>[INST] Who determines the probable cause(s)...  \n",
              "9999  <s>[INST] What is the Airplane Rating(s) of th...  \n",
              "\n",
              "[10000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8ee9997-cf36-4a3a-be51-6ab3b8e8ccf3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a9567233027</td>\n",
              "      <td>Where was the Destination of the accident no. ...</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>&lt;s&gt;[INST] Where was the Destination of the acc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a4671579659</td>\n",
              "      <td>What is the name of the Engine Manufacturer of...</td>\n",
              "      <td>Pratt &amp; Whitney</td>\n",
              "      <td>&lt;s&gt;[INST] What is the name of the Engine Manuf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a529971724</td>\n",
              "      <td>Who determines the probable cause(s) of this a...</td>\n",
              "      <td>The National Transportation Safety Board</td>\n",
              "      <td>&lt;s&gt;[INST] Who determines the probable cause(s)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a1197873694</td>\n",
              "      <td>What are the Probable findings of the accident...</td>\n",
              "      <td>Environmental issues Terrain induced turbulenc...</td>\n",
              "      <td>&lt;s&gt;[INST] What are the Probable findings of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a7803769704</td>\n",
              "      <td>In the accident with no. DCA16CA009, is the ai...</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;s&gt;[INST] In the accident with no. DCA16CA009,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>a4688878518</td>\n",
              "      <td>Was there any Operating Certificate(s) held of...</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;s&gt;[INST] Was there any Operating Certificate(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>a8331736975</td>\n",
              "      <td>What was the VFR Approach/Landing of the accid...</td>\n",
              "      <td>Go around</td>\n",
              "      <td>&lt;s&gt;[INST] What was the VFR Approach/Landing of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>a4342209241</td>\n",
              "      <td>Was there fire on aircraft of the accident no....</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;s&gt;[INST] Was there fire on aircraft of the ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>a2435551586</td>\n",
              "      <td>Who determines the probable cause(s) of this a...</td>\n",
              "      <td>The National Transportation Safety Board</td>\n",
              "      <td>&lt;s&gt;[INST] Who determines the probable cause(s)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>a5620706103</td>\n",
              "      <td>What is the Airplane Rating(s) of the accident...</td>\n",
              "      <td>Single-engine land; Single-engine sea; Multi-e...</td>\n",
              "      <td>&lt;s&gt;[INST] What is the Airplane Rating(s) of th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8ee9997-cf36-4a3a-be51-6ab3b8e8ccf3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8ee9997-cf36-4a3a-be51-6ab3b8e8ccf3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8ee9997-cf36-4a3a-be51-6ab3b8e8ccf3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-333c6911-490c-48df-983d-db47c4d80943\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-333c6911-490c-48df-983d-db47c4d80943')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-333c6911-490c-48df-983d-db47c4d80943 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b6463385-607a-412a-9d06-4c5d7f206713\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('datasetF')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b6463385-607a-412a-9d06-4c5d7f206713 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('datasetF');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "datasetF",
              "summary": "{\n  \"name\": \"datasetF\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"a8544539478\",\n          \"a171502243\",\n          \"a7944339308\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"Which runway was used by the aircraft having accident no. ERA17CA047? \",\n          \"What was the Altimeter Setting of the accident no. CEN15LA343?\",\n          \"How many engines were there in the aircraft of the accident no. GAA17CA109?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4107,\n        \"samples\": [\n          \"Scattered / 1500 ft AGL\",\n          \"18-4426\",\n          \"N26136\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"<s>[INST] Which runway was used by the aircraft having accident no. ERA17CA047?  [/INST] 03L </s>\",\n          \"<s>[INST] What was the Altimeter Setting of the accident no. CEN15LA343? [/INST] 29.84 inches Hg </s>\",\n          \"<s>[INST] How many engines were there in the aircraft of the accident no. GAA17CA109? [/INST] 1 Reciprocating </s>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF['Question'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bJIEeujGvocn",
        "outputId": "a6369a1b-c010-4c0e-dc2c-31586f2abe58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Who determines the probable cause(s) of this accident no. ERA22LA104?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF['text'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "CNGf8BmXEZnF",
        "outputId": "a0d01b10-8af1-4048-b5ec-32240f26c86b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] Who determines the probable cause(s) of this accident no. ERA22LA104? [/INST] The National Transportation Safety Board </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "id": "npGef0PAwBkJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "id": "G756UDF1k3hp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9a6a6a-e922-482e-8792-fd52ea95abda"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            83Gi       2.5Gi       3.0Gi       6.0Mi        77Gi        80Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Meta-Llama-3-8B model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vuHodNjdAol6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference -q"
      ],
      "metadata": {
        "id": "VTaNFDF9TedV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face model id\n",
        "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" #01 march 2024, 08 MARCH 2024 and 27 MAY 2024\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\" #22 MAY 2024\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    #is_decoder=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
        "#model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "id": "lQm2yo0uslTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "EqToGYwPagxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c847af79-e3ea-47c3-f831-66e086fb8831"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaFlashAttention2(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference\n",
        "\n",
        "Build an inference pipeline with tokenizer and the model."
      ],
      "metadata": {
        "id": "pfFTlpqJFOdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "CHUtHvtzMlYB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt=\"What was the first album Beyoncé released as a solo artist?\"\n",
        "prompt0=\"What is the capital of russia?\"\n",
        "#prompt=\"What are some interesting sites to visit in the Bay Area?\"\n",
        "\n",
        "prompt = f\"Instruct: Find THE answer for the following  question.\\n{prompt0}\\nOutput:\\n\"\n",
        "#prompt = f\"Instruct: Find only the answer for the following  question.\\n{prompt0}\"\n",
        "\n",
        "#prompt=\"What are some interesting sites to visit in the Bay Area?\"\n",
        "#prompt=\"What is the capital of russia?\"\n",
        "outputs = pipe(prompt, max_new_tokens=128, temperature=0.9,do_sample=True,top_k=30, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "\n",
        "print('Question: %s'%prompt0)\n",
        "print()\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt0):].strip()}\")\n"
      ],
      "metadata": {
        "id": "VZ5nnw6tMsVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a659a17-3c78-4eba-f1ac-d7d27e193a84"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of russia?\n",
            "\n",
            "Generated Answer:\n",
            "the following  question.\n",
            "What is the capital of russia?\n",
            "Output:\n",
            "The capital of russia is Moscow.\n",
            "\\end{blockquote}\n",
            "\n",
            "I've done some research on the web, but haven't found anything useful yet. If you know something about it or have a good idea how to approach the problem, please let me know.\n",
            "\n",
            "Thanks in advance!\n",
            "\n",
            "Comment: I think you need to do a web search on \"nlg\" for \"natural language generation\". I believe it is a very active area of research, and you'll have to do some reading to understand how it works.\n",
            "\n",
            "Comment: Thank you for your comment, I've been doing some research on \"Natural Language Generation\" and \"Natural Language Processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "mJOKD2w5QSWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42b6039-755b-4bd9-d877-e91d72d7afaf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruct: Find THE answer for the following  question.\n",
            "What is the capital of russia?\n",
            "Output:\n",
            "The capital of russia is Moscow.\n",
            "\\end{blockquote}\n",
            "\n",
            "I've done some research on the web, but haven't found anything useful yet. If you know something about it or have a good idea how to approach the problem, please let me know.\n",
            "\n",
            "Thanks in advance!\n",
            "\n",
            "Comment: I think you need to do a web search on \"nlg\" for \"natural language generation\". I believe it is a very active area of research, and you'll have to do some reading to understand how it works.\n",
            "\n",
            "Comment: Thank you for your comment, I've been doing some research on \"Natural Language Generation\" and \"Natural Language Processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
        "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
        "\n",
        "def gen(model,p, maxlen=1024, sample=True):\n",
        "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
        "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.9,num_beams=1,top_p=0.95,).to('cuda')\n",
        "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "CUFk-I1pugxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index=321458\n",
        "#index=10000\n",
        "#features: ['id', 'Question', 'Answer'],\n",
        "index=2\n",
        "prompt = datasetF['id'][index]\n",
        "print(prompt)\n",
        "\n",
        "prompt = datasetF['Question'][index]\n",
        "print(prompt)\n",
        "\n",
        "prompt = datasetF['Answer'][index]\n",
        "print(prompt)\n",
        "\n",
        "prompt = datasetF['text'][index]\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_TvVz3B2eso",
        "outputId": "fa403db1-9ea9-4857-cdf5-96c1997f6938"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a529971724\n",
            "Who determines the probable cause(s) of this accident no. ERA22LA104?\n",
            "The National Transportation Safety Board\n",
            "<s>[INST] Who determines the probable cause(s) of this accident no. ERA22LA104? [/INST] The National Transportation Safety Board </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL GENERATION - ZERO SHOT"
      ],
      "metadata": {
        "id": "KYysLZxRz9_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=2\n",
        "\n",
        "#features: ['id', 'Question', 'Answer'],\n",
        "#prompt = dataset[index]['Question']\n",
        "#summary = dataset[index]['Answer']\n",
        "\n",
        "prompt = datasetF['Question'][index]\n",
        "summary = datasetF['Answer'][index]\n",
        "\n",
        "original_model = model\n",
        "\n",
        "formatted_prompt = f\"Instruct: provide an answer for the following dialog.\\n{prompt}\"\n",
        "\n",
        "res = original_model.generate(tokenizer.encode(formatted_prompt, return_tensors=\"pt\"), max_length=128, do_sample=False)\n",
        "output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
        "\n",
        "#input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to('cuda') # Assuming you want to run on GPU\n",
        "#input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(torch.bfloat16)\n",
        "#res = model.generate(input_ids, max_length=512, do_sample=False)\n",
        "#output = tokenizer.decode(res[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "1GN8Vns5m7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "\n",
        "print(f'Dialogue :\\n{prompt}')\n",
        "print(dash_line)\n",
        "\n",
        "print(f'ACTION BY THE AGENT-HUMAN:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckW4p9r9N9J8",
        "outputId": "db2b7ccc-3075-4d62-c559-81f1a2587ec2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue :\n",
            "Who determines the probable cause(s) of this accident no. ERA22LA104?\n",
            "---------------------------------------------------------------------------------------------------\n",
            "ACTION BY THE AGENT-HUMAN:\n",
            "The National Transportation Safety Board\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Instruct: provide an answer for the following dialog.\n",
            "Who determines the probable cause(s) of this accident no. ERA22LA104?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(datasetF))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjQlF7kZXJUN",
        "outputId": "c173286d-c84b-48f2-add0-5f924c85de06"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "kxOWr2VvEGKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacb39cc-2918-4496-cc1a-ac02e3edfb1c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaFlashAttention2(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding the adopter to the layer"
      ],
      "metadata": {
        "id": "of-Z3B28BNBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=128,\n",
        "        lora_dropout=0.05,\n",
        "        r=256,\n",
        "        bias=\"none\",\n",
        "        target_modules=\"all-linear\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "kjyYx9WmtMmA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "NcaH5WrdBW8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "AkMVebosM16A"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    output_dir=\"/content/gdrive/MyDrive/model/Meta-Llama-3-8B_AviationQA-cosine\", # 06/06/2024 #dataset_AviationQA\n",
        "\n",
        "    #num_train_epochs=3,                    # number of training epochs\n",
        "    num_train_epochs=30,                   # number of training epochs for POC\n",
        "    per_device_train_batch_size=3,          # batch size per device during training\n",
        "    per_device_eval_batch_size = 6,         #batch size for evaluations\n",
        "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "    logging_steps=10,                       # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "    learning_rate=1e-4,                     # learning rate, based on QLoRA paper\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"cosine\",           # use constant learning rate scheduler\n",
        "    push_to_hub=True,                       # push model to hub\n",
        "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/trl/en/sft_trainer\n",
        "\n",
        "https://github.com/frank-morales2020/MLxDL/blob/main/FineTuning_LLM_Meta_Llama_3_8B_for_text_to_SQL.ipynb"
      ],
      "metadata": {
        "id": "5BMkpAwk6uhW"
      }
    },
    {
      "source": [
        "#print(dataset.column_names)\n",
        "print(datasetF.columns)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVqCgdDqB7Aq",
        "outputId": "14a877db-4fdd-436a-f62b-f4082ffd89a1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'Question', 'Answer', 'text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasetF['Answer'][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6-OvnJGd7o3",
        "outputId": "18476341-490b-4117-873a-16d71125212b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The National Transportation Safety Board\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised fine-tuning (SFT)  parameters"
      ],
      "metadata": {
        "id": "0G2D93SDBjgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl -q"
      ],
      "metadata": {
        "id": "6gIkAfvBHh8n"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint"
      ],
      "metadata": {
        "id": "YEa0T6DjGlJC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "import datasets\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "datasetF_hf = datasets.Dataset.from_pandas(datasetF)\n",
        "\n",
        "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
        "#max_seq_length = 2048\n",
        "\n",
        "# Add a padding token to your tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=datasetF_hf,\n",
        "    dataset_text_field=\"text\", ### added for the summarization dataset\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    #formatting_func=formatting_func,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,  # We template with special tokens\n",
        "        \"append_concat_token\": False, # No need to add additional separator token\n",
        "        ##\"per_device_train_batch_size\": 8,\n",
        "        #\"per_device_eval_batch_size\": 8,\n",
        "        #\"num_train_epochs\": 3.0},\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ZklRuRAatoZ8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training AND Saving the fine-tuned model"
      ],
      "metadata": {
        "id": "sNtNfyP6BuOI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCq7Dq8f7VPU"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ]
    },
    {
      "source": [
        "#from accelerate import Accelerator\n",
        "\n",
        "# Reinitialize the Accelerator\n",
        "#accelerator = Accelerator()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "V0IIAcgzbGYM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free the memory again\n",
        "del model\n",
        "del original_model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mCxeJIj4KvSQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "e0U_CeRoB9kM"
      }
    },
    {
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "P-sM4GnQxrjM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "\n",
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM"
      ],
      "metadata": {
        "id": "DNgzXJgEnEB9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/gdrive/MyDrive/model/Meta-Llama-3-8B_AviationQA-cosine/"
      ],
      "metadata": {
        "id": "0_kGS5wGPGfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1768aeca-9163-41df-86a5-88eaf0d5963f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.3G\n",
            "-rw------- 1 root root 1.3K Jun 20 02:01 README.md\n",
            "-rw------- 1 root root  335 Jun 20 02:01 special_tokens_map.json\n",
            "-rw------- 1 root root  50K Jun 20 02:01 tokenizer_config.json\n",
            "-rw------- 1 root root 8.7M Jun 20 02:01 tokenizer.json\n",
            "-rw------- 1 root root 5.2K Jun 20 02:01 training_args.bin\n",
            "-rw------- 1 root root  731 Jun 20 02:01 adapter_config.json\n",
            "-rw------- 1 root root 1.3G Jun 20 02:01 adapter_model.safetensors\n",
            "drwx------ 2 root root 4.0K Jun 20 01:59 checkpoint-600\n",
            "drwx------ 2 root root 4.0K Jun 20 01:56 checkpoint-580\n",
            "drwx------ 2 root root 4.0K Jun 20 01:52 checkpoint-560\n",
            "drwx------ 2 root root 4.0K Jun 20 01:49 checkpoint-540\n",
            "drwx------ 2 root root 4.0K Jun 20 01:45 checkpoint-520\n",
            "drwx------ 2 root root 4.0K Jun 20 01:42 checkpoint-500\n",
            "drwx------ 2 root root 4.0K Jun 20 01:39 checkpoint-480\n",
            "drwx------ 2 root root 4.0K Jun 20 01:35 checkpoint-460\n",
            "drwx------ 2 root root 4.0K Jun 20 01:32 checkpoint-440\n",
            "drwx------ 2 root root 4.0K Jun 20 01:29 checkpoint-420\n",
            "drwx------ 2 root root 4.0K Jun 20 01:25 checkpoint-400\n",
            "drwx------ 2 root root 4.0K Jun 20 01:22 checkpoint-380\n",
            "drwx------ 2 root root 4.0K Jun 20 01:18 checkpoint-360\n",
            "drwx------ 2 root root 4.0K Jun 20 01:15 checkpoint-340\n",
            "drwx------ 2 root root 4.0K Jun 20 01:12 checkpoint-320\n",
            "drwx------ 2 root root 4.0K Jun 20 01:08 checkpoint-300\n",
            "drwx------ 2 root root 4.0K Jun 20 01:05 checkpoint-280\n",
            "drwx------ 2 root root 4.0K Jun 20 01:01 checkpoint-260\n",
            "drwx------ 2 root root 4.0K Jun 20 00:58 checkpoint-240\n",
            "drwx------ 2 root root 4.0K Jun 20 00:55 checkpoint-220\n",
            "drwx------ 2 root root 4.0K Jun 20 00:51 checkpoint-200\n",
            "drwx------ 2 root root 4.0K Jun 20 00:47 checkpoint-180\n",
            "drwx------ 2 root root 4.0K Jun 20 00:43 checkpoint-160\n",
            "drwx------ 2 root root 4.0K Jun 20 00:39 checkpoint-140\n",
            "drwx------ 2 root root 4.0K Jun 20 00:35 checkpoint-120\n",
            "drwx------ 2 root root 4.0K Jun 20 00:31 checkpoint-100\n",
            "drwx------ 2 root root 4.0K Jun 20 00:27 checkpoint-80\n",
            "drwx------ 2 root root 4.0K Jun 20 00:23 checkpoint-60\n",
            "drwx------ 2 root root 4.0K Jun 20 00:19 checkpoint-40\n",
            "drwx------ 2 root root 4.0K Jun 20 00:15 checkpoint-20\n",
            "drwx------ 4 root root 4.0K Jun 20 00:11 runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "%cd /content/\n",
        "peft_model_id = \"/content/gdrive/MyDrive/model/Meta-Llama-3-8B_AviationQA-cosine\"\n",
        "\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "modelnew = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        "  torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "\n",
        "# load into pipeline\n",
        "generation_pipeline = pipeline(\"text-generation\", model=modelnew, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "kW_hl8YPKxQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(ganswer):\n",
        "    qc0=str(ganswer).find('[INST]')\n",
        "\n",
        "    ganswer=str(ganswer)[0:qc0-8]\n",
        "    qc=str(ganswer).find('[/INST]')\n",
        "    if qc>=0:\n",
        "      ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "    return ganswer"
      ],
      "metadata": {
        "id": "nHO4cwZo9Yr8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index=2\n",
        "prompt = datasetF['Question'][index]\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=512, do_sample=True, temperature=0.1,\n",
        "                              top_k=50, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                              pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "sIWQzNCi8NpX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oanswer = datasetF['Answer'][index]\n",
        "print()\n",
        "print('ORIGINAL ANSWER: %s'%oanswer)\n",
        "\n",
        "answer=outputs[0]['generated_text']\n",
        "ganswer=generate_answer(answer)\n",
        "print()\n",
        "print('PREDICTED ANSWER: %s'%ganswer)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llM7ewOX8ivK",
        "outputId": "2f433f2f-9dec-4825-e30b-d1cdfa43102c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ORIGINAL ANSWER: The National Transportation Safety Board\n",
            "\n",
            "PREDICTED ANSWER: The National Transportation Safety Board\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
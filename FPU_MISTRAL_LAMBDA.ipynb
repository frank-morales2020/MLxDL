{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-hZRPeFv7r-S"
      ],
      "authorship_tag": "ABX9TyPQnz5bmtT5+RlBU36EQXnc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FPU_MISTRAL_LAMBDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kcgpFYP7Wyd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "import torch\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "\n",
        "import gc\n",
        "\n",
        "# Example: Inside or after your training loop, if you suspect memory issues\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # Specifically for CUDA memory\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\", category=UserWarning)\n",
        "\n",
        "# ---------------------- Configuration ----------------------\n",
        "model_checkpoint = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "train_dataset_path = \"/home/ubuntu/work/cmapss_FD004_train_text.jsonl\"\n",
        "validation_dataset_path = \"/home/ubuntu/work/cmapss_FD004_test_text.jsonl\"\n",
        "output_dir = \"./fine-tuned-mistral-peft-lambda\"\n",
        "per_device_train_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "num_train_epochs = 10\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 100\n",
        "max_seq_length = 512\n",
        "logging_steps = 10\n",
        "save_steps = 10\n",
        "eval_steps = 10\n",
        "evaluation_strategy = \"steps\"\n",
        "save_total_limit = 2\n",
        "fp16 = torch.cuda.is_available()\n",
        "gradient_checkpointing = False\n",
        "\n",
        "# ---------------------- 1. Load Datasets ----------------------\n",
        "def load_jsonl_dataset(path):\n",
        "    data = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "train_dataset = load_jsonl_dataset(train_dataset_path)\n",
        "eval_dataset = load_jsonl_dataset(validation_dataset_path)\n",
        "\n",
        "# ---------------------- 2. Load Tokenizer ----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# ---------------------- 3. Preprocess Data ----------------------\n",
        "def tokenize_function(examples):\n",
        "    prompts = []\n",
        "    responses = []\n",
        "\n",
        "    contents_list = examples['contents']\n",
        "\n",
        "    for item in contents_list:\n",
        "        try:\n",
        "            if (item and\n",
        "                len(item) == 2 and\n",
        "                item[0]['role'] == 'user' and\n",
        "                item[1]['role'] == 'model' and\n",
        "                item[0]['parts'] and\n",
        "                item[1]['parts'] and\n",
        "                item[0]['parts'][0]['text'] and\n",
        "                item[1]['parts'][0]['text']):\n",
        "                user_text = item[0]['parts'][0]['text']\n",
        "                if \"Engine sensor readings over time:\" in user_text:\n",
        "                    sensor_data = user_text.replace(\"Engine sensor readings over time: \", \"\")\n",
        "                else:\n",
        "                    sensor_data = user_text\n",
        "                prompts.append(f\"Predict the remaining useful life for this engine with sensor readings: {sensor_data}\")\n",
        "\n",
        "                model_text = item[1]['parts'][0]['text']\n",
        "                rul_match = re.search(r'(\\d+)', model_text)\n",
        "                if rul_match:\n",
        "                    responses.append(rul_match.group(1))\n",
        "                else:\n",
        "                    responses.append(\"0\") # Default if no number found\n",
        "            else:\n",
        "                print(f\"Skipping invalid data point: {item}\")\n",
        "                continue\n",
        "        except (KeyError, IndexError):\n",
        "            print(f\"Skipping invalid data point: {item}\")\n",
        "            continue\n",
        "\n",
        "    tokenized_prompts = tokenizer(prompts,\n",
        "                                 padding=\"max_length\",\n",
        "                                 truncation=True,\n",
        "                                 max_length=max_seq_length,\n",
        "                                 return_tensors=\"pt\")\n",
        "    tokenized_responses = tokenizer(responses,\n",
        "                                  padding=\"max_length\",\n",
        "                                  truncation=True,\n",
        "                                  max_length=max_seq_length,\n",
        "                                  return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(len(prompts)):\n",
        "        full_text = prompts[i] + tokenizer.eos_token + responses[i] + tokenizer.eos_token\n",
        "        tokenized_full_text = tokenizer.encode(full_text, max_length=max_seq_length, truncation=True)\n",
        "        current_input_ids = tokenized_full_text\n",
        "\n",
        "        vocab_size = tokenizer.vocab_size\n",
        "        max_index = max(current_input_ids) if current_input_ids else -1\n",
        "        min_index = min(current_input_ids) if current_input_ids else float('inf')\n",
        "\n",
        "        if max_index >= vocab_size or min_index < 0:\n",
        "            print(f\"Warning: Out-of-bounds index found in tokenized input (example {i}):\")\n",
        "            print(f\"  Max index: {max_index}, Vocabulary size: {vocab_size}\")\n",
        "            print(f\"  Min index: {min_index}\")\n",
        "\n",
        "        input_ids.append(current_input_ids)\n",
        "        attention_mask.append([1] * len(current_input_ids))\n",
        "        labels.append(current_input_ids.copy())\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "tokenized_train_datasets = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['contents'])\n",
        "tokenized_eval_datasets = eval_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['contents'])\n",
        "\n",
        "# ---------------------- 4. Data Collator ----------------------\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# ---------------------- 5. Load Model with PEFT ----------------------\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ---------------------- 6. Training Arguments ----------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=weight_decay,\n",
        "    warmup_steps=100,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    logging_steps=logging_steps,\n",
        "    save_steps=save_steps,\n",
        "    eval_steps=eval_steps,\n",
        "    eval_strategy=evaluation_strategy,\n",
        "    save_total_limit=2,\n",
        "    fp16=fp16,\n",
        "    gradient_checkpointing=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    label_names=[\"labels\"],\n",
        "    lr_scheduler_type=\"cosine\",\n",
        ")\n",
        "\n",
        "# ---------------------- 7. Trainer ----------------------\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "rouge_metric = load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), axis=-1)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "    prediction_lens = torch.sum(predictions != tokenizer.pad_token_id, dim=1)\n",
        "    result[\"gen_len\"] = torch.mean(prediction_lens.float()).item()\n",
        "    return result\n",
        "\n",
        "# ---------------------- 7. Trainer ----------------------\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_eval_datasets,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "# ---------------------- 8. Train ----------------------\n",
        "trainer.train()\n",
        "\n",
        "# ---------------------- 9. Save Model ----------------------\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "print(f\"Fine-tuning complete! Model saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "-hZRPeFv7r-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./fine-tuned-mistral-peft-lambda\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "inference_model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Explicitly set pad_token_id if it's not already set\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load your evaluation dataset\n",
        "validation_dataset_path = \"/home/ubuntu/work/cmapss_FD004_test_text.jsonl\"  # Use your actual path\n",
        "eval_dataset = []\n",
        "with open(validation_dataset_path, 'r') as f:\n",
        "    for line in f:\n",
        "        eval_dataset.append(json.loads(line))\n",
        "\n",
        "# Number of evaluation prompts to use\n",
        "num_eval_prompts = 2\n",
        "predicted_ruls = []\n",
        "ground_truth_ruls = []\n",
        "\n",
        "for i in tqdm(range(num_eval_prompts), desc=\"Evaluating Inference with MAE\"):\n",
        "    try:\n",
        "        prompt_data = eval_dataset[i]['contents'][0]['parts'][0]['text']\n",
        "        ground_truth_text = eval_dataset[i]['contents'][1]['parts'][0]['text']\n",
        "    except (IndexError, KeyError):\n",
        "        print(f\"Skipping invalid data point at index {i}\")\n",
        "        continue\n",
        "\n",
        "    prompt = f\"Predict the remaining useful life for this engine with sensor readings: {prompt_data}\"\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "    ).to(inference_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = inference_model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated Text: {generated_text}\")\n",
        "    print(f\"Ground Truth Data: {ground_truth_text}\")\n",
        "\n",
        "    # Extract numerical RUL from generated text\n",
        "    predicted_rul_match = re.search(r'(\\d+)', generated_text)\n",
        "    predicted_rul = int(predicted_rul_match.group(1)) if predicted_rul_match else None\n",
        "\n",
        "    # Extract numerical RUL from ground truth\n",
        "    ground_truth_rul_match = re.search(r'(\\d+)', ground_truth_text)\n",
        "    ground_truth_rul = int(ground_truth_rul_match.group(1)) if ground_truth_rul_match else None\n",
        "\n",
        "    if predicted_rul is not None and ground_truth_rul is not None:\n",
        "        predicted_ruls.append(predicted_rul)\n",
        "        ground_truth_ruls.append(ground_truth_rul)\n",
        "    else:\n",
        "        print(\"Could not extract numerical RUL from generated or ground truth text.\")\n",
        "\n",
        "# Calculate and print Mean Absolute Error\n",
        "if predicted_ruls and ground_truth_ruls:\n",
        "    mae = np.mean(np.abs(np.array(predicted_ruls) - np.array(ground_truth_ruls)))\n",
        "    print(f\"\\n--- Mean Absolute Error (MAE) on Inference ---\")\n",
        "    print(f\"MAE: {mae}\")\n",
        "else:\n",
        "    print(\"\\nCould not calculate MAE due to missing predictions or ground truth.\")"
      ],
      "metadata": {
        "id": "CmOEsPLC7xJh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
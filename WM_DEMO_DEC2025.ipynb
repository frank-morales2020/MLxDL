{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTqtRI2Ahqd6imPDNOoC/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/WM_DEMO_DEC2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "import numpy as np\n",
        "import torch.nn.functional as F  # <--- CRUCIAL IMPORT FOR THE FIX\n",
        "\n",
        "# --- 1. CONFIGURATION (Hyperparameters) ---\n",
        "IMAGE_SHAPE = (3, 64, 64)  # (Channels, Height, Width)\n",
        "ACTION_SIZE = 5            # Number of possible actions (e.g., Forward, Left, Right, etc.)\n",
        "STOCHASTIC_SIZE = 32       # Dimension of the stochastic latent state (z_t)\n",
        "DETERMINISTIC_SIZE = 256   # Dimension of the deterministic memory state (h_t)\n",
        "HIDDEN_SIZE = 400          # Size of hidden layers\n",
        "\n",
        "# --- 2. CORE UTILITY CLASSES ---\n",
        "\n",
        "# Represents the World Model's Internal State at any time 't'\n",
        "class State(nn.Module):\n",
        "    def __init__(self, h, z):\n",
        "        super().__init__()\n",
        "        self.h = h  # Deterministic Memory (h_t)\n",
        "        self.z = z  # Stochastic State (z_t)\n",
        "\n",
        "# --- 3. THE WORLD MODEL ARCHITECTURE (Simplified Networks) ---\n",
        "\n",
        "# The Recurrent Model: Learns the transition of the deterministic memory (h_t)\n",
        "class RecurrentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input to GRUCell is [z_prev, a_prev]. Size: 32 + 5 = 37.\n",
        "        input_size = STOCHASTIC_SIZE + ACTION_SIZE\n",
        "\n",
        "        # The hidden_size (h_t) is DETERMINISTIC_SIZE (256).\n",
        "        self.gru = nn.GRUCell(input_size, DETERMINISTIC_SIZE)\n",
        "\n",
        "    def forward(self, h_prev, z_prev, a_prev):\n",
        "        # Concatenate stochastic state and action to form the input features (x)\n",
        "        x = torch.cat([z_prev, a_prev], dim=-1)\n",
        "\n",
        "        # Pass x as input and h_prev as the hidden state (hx)\n",
        "        h_t = self.gru(x, h_prev) # Update deterministic memory\n",
        "        return h_t\n",
        "\n",
        "# The Prior Model: Predicts the distribution of the next stochastic state (z_t)\n",
        "class PriorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(DETERMINISTIC_SIZE, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_SIZE, 2 * STOCHASTIC_SIZE) # Mean and Std\n",
        "        )\n",
        "\n",
        "    def forward(self, h_t):\n",
        "        mean, std = torch.chunk(self.net(h_t), 2, dim=-1)\n",
        "\n",
        "        # --- FIX APPLIED HERE: Use F.softplus to operate on the tensor ---\n",
        "        std_positive = F.softplus(std) + 1e-4\n",
        "\n",
        "        return dist.Normal(mean, std_positive)\n",
        "\n",
        "# --- 4. THE WORLD MODEL STEP FUNCTION ---\n",
        "\n",
        "def world_model_step(model, prev_state: State, action_t: torch.Tensor, observation_t: torch.Tensor = None):\n",
        "    \"\"\"\n",
        "    Performs one step of the World Model (either a 'dream' or a 'real' update).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Prediction (Transition/Prior Model)\n",
        "    h_t = model.recurrent_model(prev_state.h, prev_state.z, action_t)\n",
        "    prior_dist = model.prior_model(h_t)\n",
        "    z_t_prior = prior_dist.sample()\n",
        "\n",
        "    # 2. Representation (Posterior Model) - Requires Observation\n",
        "    if observation_t is not None:\n",
        "        # We just use the PRIOR z_t as the final state for this conceptual demo\n",
        "        z_t_final = z_t_prior\n",
        "    else:\n",
        "        # When imagining (no observation), the prior becomes the final state.\n",
        "        z_t_final = z_t_prior\n",
        "\n",
        "    new_state = State(h_t, z_t_final)\n",
        "    return new_state, prior_dist\n",
        "\n",
        "# --- 5. DEMO EXECUTION (Simulating a Single Agent Step) ---\n",
        "\n",
        "# Initialize the models\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.recurrent_model = RecurrentModel()\n",
        "        self.prior_model = PriorModel()\n",
        "\n",
        "wm = WorldModel()\n",
        "\n",
        "# 1. Initialize previous state (s_t-1) and action (a_t-1)\n",
        "batch_size = 1\n",
        "h_prev = torch.zeros(batch_size, DETERMINISTIC_SIZE)\n",
        "z_prev = torch.zeros(batch_size, STOCHASTIC_SIZE)\n",
        "prev_state = State(h_prev, z_prev)\n",
        "\n",
        "# Simulate an action taken by the Agent\n",
        "action_t = torch.tensor([[0.0, 0.0, 1.0, 0.0, 0.0]]) # One-hot encoded action\n",
        "# Simulate a new, high-dimensional image observation (e.g., from a game screen)\n",
        "observation_t = torch.randn(batch_size, *IMAGE_SHAPE)\n",
        "\n",
        "# 2. Run the World Model Step (RSSM Loop)\n",
        "print(\"--- World Model Step Simulation ---\")\n",
        "\n",
        "# Step 1: Learn/Update the model based on a real observation\n",
        "new_real_state, real_prior_dist = world_model_step(wm, prev_state, action_t, observation_t)\n",
        "print(f\"| REPRESENATION (Reality): New h_t shape: {new_real_state.h.shape}\")\n",
        "print(f\"| Correction: A loss is calculated here between the prior and the posterior (omitted).\")\n",
        "\n",
        "# Step 2: Imagine a future step (The 'Dream' - no observation)\n",
        "action_imagine = torch.tensor([[1.0, 0.0, 0.0, 0.0, 0.0]]) # Imagine a different action\n",
        "imagined_state, imagined_prior_dist = world_model_step(wm, new_real_state, action_imagine, observation_t=None)\n",
        "print(f\"| PREDICTION (Dream): Imagined h_t shape: {imagined_state.h.shape}\")\n",
        "print(f\"| Prediction: The agent can now calculate reward/value for this imagined state.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95QcBG-pF0Eu",
        "outputId": "dcc5afb7-5bfb-4444-9b4a-c1c54fd7bc7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- World Model Step Simulation ---\n",
            "| REPRESENATION (Reality): New h_t shape: torch.Size([1, 256])\n",
            "| Correction: A loss is calculated here between the prior and the posterior (omitted).\n",
            "| PREDICTION (Dream): Imagined h_t shape: torch.Size([1, 256])\n",
            "| Prediction: The agent can now calculate reward/value for this imagined state.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš€ Full Conceptual Code: World Model + Controller"
      ],
      "metadata": {
        "id": "pCT_jClfGJi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. CONFIGURATION (Hyperparameters) ---\n",
        "IMAGE_SHAPE = (3, 64, 64)\n",
        "ACTION_SIZE = 5\n",
        "STOCHASTIC_SIZE = 32\n",
        "DETERMINISTIC_SIZE = 256\n",
        "HIDDEN_SIZE = 400\n",
        "# Controller Hyperparameters\n",
        "PLANNING_HORIZON = 15      # How many steps the agent \"looks ahead\" in its dream\n",
        "GAMMA = 0.99               # Discount factor for future rewards\n",
        "\n",
        "# --- 2. CORE UTILITY CLASSES ---\n",
        "\n",
        "class State(nn.Module):\n",
        "    \"\"\"Holds the deterministic and stochastic parts of the latent state.\"\"\"\n",
        "    def __init__(self, h, z):\n",
        "        super().__init__()\n",
        "        self.h = h\n",
        "        self.z = z\n",
        "        # Combined latent feature (used as input to Actor, Critic, and Reward Models)\n",
        "        self.feature = torch.cat([h, z], dim=-1)\n",
        "\n",
        "# --- 3. THE WORLD MODEL ARCHITECTURE ---\n",
        "\n",
        "class RecurrentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        input_size = STOCHASTIC_SIZE + ACTION_SIZE\n",
        "        self.gru = nn.GRUCell(input_size, DETERMINISTIC_SIZE)\n",
        "    def forward(self, h_prev, z_prev, a_prev):\n",
        "        x = torch.cat([z_prev, a_prev], dim=-1)\n",
        "        h_t = self.gru(x, h_prev)\n",
        "        return h_t\n",
        "\n",
        "class PriorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(DETERMINISTIC_SIZE, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_SIZE, 2 * STOCHASTIC_SIZE)\n",
        "        )\n",
        "    def forward(self, h_t):\n",
        "        mean, std = torch.chunk(self.net(h_t), 2, dim=-1)\n",
        "        std_positive = F.softplus(std) + 1e-4\n",
        "        return dist.Normal(mean, std_positive)\n",
        "\n",
        "# --- 4. THE CONTROLLER (ACTOR & CRITIC) ---\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"The Policy: Chooses the action (a_t) based on the current latent state (s_t).\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        input_size = DETERMINISTIC_SIZE + STOCHASTIC_SIZE\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_SIZE, ACTION_SIZE)\n",
        "        )\n",
        "    # FIX: Input is expected to be the tensor feature, not the State object.\n",
        "    def forward(self, feature: torch.Tensor) -> dist.Distribution:\n",
        "        logits = self.net(feature)\n",
        "        return dist.OneHotCategorical(logits=logits)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"The Value Function: Estimates the total future return (V(s_t)) from the state.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        input_size = DETERMINISTIC_SIZE + STOCHASTIC_SIZE\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_SIZE, 1)\n",
        "        )\n",
        "    # FIX: Input is expected to be the tensor feature, not the State object.\n",
        "    def forward(self, feature: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(feature)\n",
        "\n",
        "# --- 5. THE WORLD MODEL STEP FUNCTION ---\n",
        "\n",
        "def world_model_step(wm, prev_state: State, action_t: torch.Tensor, observation_t: torch.Tensor = None):\n",
        "    # Transition Model: h_t = f(h_t-1, z_t-1, a_t-1)\n",
        "    h_t = wm.recurrent_model(prev_state.h, prev_state.z, action_t)\n",
        "\n",
        "    # Prior Model: z_t ~ p(z_t | h_t)\n",
        "    prior_dist = wm.prior_model(h_t)\n",
        "    z_t_final = prior_dist.sample()\n",
        "\n",
        "    # Create the new state object\n",
        "    new_state = State(h_t, z_t_final)\n",
        "\n",
        "    # FIX: Pass the new state's .feature tensor to the RewardModel\n",
        "    reward_t = wm.reward_model(new_state.feature)\n",
        "\n",
        "    return new_state, reward_t\n",
        "\n",
        "# --- 6. THE IMAGINATION LOOP (Where Policy Training Happens) ---\n",
        "\n",
        "def imagine_trajectories(wm, policy, start_state: State, horizon: int):\n",
        "    \"\"\"\n",
        "    Generates imagined sequences of states and rewards.\n",
        "    \"\"\"\n",
        "\n",
        "    current_state = start_state\n",
        "    imagined_states = [current_state]\n",
        "    imagined_rewards = []\n",
        "\n",
        "    print(f\"\\n--- IMAGINATION: Starting trajectory of length {horizon} ---\")\n",
        "\n",
        "    for t in range(horizon):\n",
        "\n",
        "        # 1. Controller (Actor) chooses the best action from the current imagined state\n",
        "        # FIX: Pass the current state's .feature tensor to the policy (Actor)\n",
        "        action_dist = policy(current_state.feature)\n",
        "        action_t = action_dist.sample()\n",
        "\n",
        "        # 2. World Model (RSSM) predicts the next state and reward based on the action\n",
        "        next_state, reward_t = world_model_step(wm, current_state, action_t, observation_t=None)\n",
        "\n",
        "        # 3. Store the results and update the current state\n",
        "        imagined_states.append(next_state)\n",
        "        imagined_rewards.append(reward_t)\n",
        "        current_state = next_state\n",
        "\n",
        "        print(f\"| Step {t+1}: Action chosen (size {action_t.shape[-1]}), Reward predicted (value {reward_t.item():.4f})\")\n",
        "\n",
        "    print(f\"--- IMAGINATION COMPLETE. Policy ready for update. ---\")\n",
        "\n",
        "    return imagined_states, imagined_rewards\n",
        "\n",
        "# --- 7. DEMO EXECUTION ---\n",
        "\n",
        "# Initialize the full suite of models\n",
        "class AgentModels(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.recurrent_model = RecurrentModel()\n",
        "        self.prior_model = PriorModel()\n",
        "        self.actor = Actor()\n",
        "        self.critic = Critic()\n",
        "        # Dummy reward model needs input size of DETERMINISTIC_SIZE + STOCHASTIC_SIZE\n",
        "        input_size = DETERMINISTIC_SIZE + STOCHASTIC_SIZE\n",
        "        self.reward_model = nn.Sequential(\n",
        "            nn.Linear(input_size, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_SIZE, 1)\n",
        "        )\n",
        "wm = AgentModels()\n",
        "\n",
        "# 1. Initialize previous state and action\n",
        "batch_size = 1\n",
        "h_prev = torch.zeros(batch_size, DETERMINISTIC_SIZE)\n",
        "z_prev = torch.zeros(batch_size, STOCHASTIC_SIZE)\n",
        "prev_state = State(h_prev, z_prev)\n",
        "action_t = torch.tensor([[0.0, 0.0, 1.0, 0.0, 0.0]])\n",
        "observation_t = torch.randn(batch_size, *IMAGE_SHAPE)\n",
        "\n",
        "# 2. Step into reality to get a starting state (Representation)\n",
        "# The output is discarded with _, but the state is updated\n",
        "new_real_state, _ = world_model_step(wm, prev_state, action_t, observation_t)\n",
        "print(\"--- Real World Initialization Complete. Starting Imagination ---\")\n",
        "\n",
        "# 3. Run the Imagination Loop (Prediction and Policy Training)\n",
        "imagine_trajectories(wm, wm.actor, new_real_state, PLANNING_HORIZON)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVEZ0lF2GFtH",
        "outputId": "59c74efb-3509-4925-a10e-60a5f49b9811"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Real World Initialization Complete. Starting Imagination ---\n",
            "\n",
            "--- IMAGINATION: Starting trajectory of length 15 ---\n",
            "| Step 1: Action chosen (size 5), Reward predicted (value -0.0521)\n",
            "| Step 2: Action chosen (size 5), Reward predicted (value 0.0098)\n",
            "| Step 3: Action chosen (size 5), Reward predicted (value 0.0109)\n",
            "| Step 4: Action chosen (size 5), Reward predicted (value -0.0217)\n",
            "| Step 5: Action chosen (size 5), Reward predicted (value -0.0651)\n",
            "| Step 6: Action chosen (size 5), Reward predicted (value -0.0492)\n",
            "| Step 7: Action chosen (size 5), Reward predicted (value 0.0355)\n",
            "| Step 8: Action chosen (size 5), Reward predicted (value -0.0672)\n",
            "| Step 9: Action chosen (size 5), Reward predicted (value -0.0698)\n",
            "| Step 10: Action chosen (size 5), Reward predicted (value -0.0425)\n",
            "| Step 11: Action chosen (size 5), Reward predicted (value -0.0331)\n",
            "| Step 12: Action chosen (size 5), Reward predicted (value -0.0131)\n",
            "| Step 13: Action chosen (size 5), Reward predicted (value -0.0241)\n",
            "| Step 14: Action chosen (size 5), Reward predicted (value -0.0618)\n",
            "| Step 15: Action chosen (size 5), Reward predicted (value -0.0509)\n",
            "--- IMAGINATION COMPLETE. Policy ready for update. ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State(),\n",
              "  State()],\n",
              " [tensor([[-0.0521]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[0.0098]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[0.0109]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0217]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0651]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0492]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[0.0355]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0672]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0698]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0425]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0331]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0131]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0241]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0618]], grad_fn=<AddmmBackward0>),\n",
              "  tensor([[-0.0509]], grad_fn=<AddmmBackward0>)])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}
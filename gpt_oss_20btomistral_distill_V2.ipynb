{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhmt/lHgmIxwapYaCSNN5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/gpt_oss_20btomistral_distill_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f6PJ7sZvYBJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login, HfApi\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from warnings import filterwarnings\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import gather_object\n",
        "\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "TEACHER_MODEL_NAME = \"openai/gpt-oss-20b\"\n",
        "STUDENT_MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "OUTPUT_DIR = \"./mistral-7b-gpt-oss-20b-distilled\"\n",
        "HF_REPO_ID = \"frankmorales2020/mistral-7b-gpt-oss-20b-distilled\"\n",
        "DATASET_SAVE_PATH = \"./distillation_dataset_temp\"\n",
        "\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2\n",
        "NUM_EPOCHS = 3\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "\n",
        "# --- 2. HUGGING FACE LOGIN ---\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    HF_TOKEN = input(\"Enter your Hugging Face access token: \")\n",
        "login(token=HF_TOKEN)\n",
        "accelerator = Accelerator()\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"Successfully logged in to Hugging Face Hub.\")\n",
        "\n",
        "# Quick config check\n",
        "if accelerator.is_main_process:\n",
        "    print(\"Accelerate config loaded; using\", accelerator.num_processes, \"GPUs.\")\n",
        "\n",
        "# Clear GPU memory before loading\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# --- 3. LOAD TEACHER MODEL ---\n",
        "try:\n",
        "    if accelerator.is_main_process:\n",
        "        print(f\"Loading {TEACHER_MODEL_NAME} with device_map='auto' across {accelerator.num_processes} GPUs...\")\n",
        "\n",
        "    try:\n",
        "        import triton\n",
        "        if tuple(map(int, triton.__version__.split('.'))) < (3, 4, 0):\n",
        "            if accelerator.is_main_process:\n",
        "                print(\"Warning: Triton < 3.4.0 detected. MXFP4 may dequantize to bf16, increasing memory usage.\")\n",
        "    except ImportError:\n",
        "        if accelerator.is_main_process:\n",
        "            print(\"Warning: Triton not installed. MXFP4 may dequantize to bf16, increasing memory usage.\")\n",
        "\n",
        "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "        TEACHER_MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        max_memory={i: \"75GB\" for i in range(accelerator.num_processes)},\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "\n",
        "    teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME)\n",
        "    if teacher_tokenizer.pad_token is None:\n",
        "        teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        print(\"Teacher loaded. VRAM per GPU (GB):\")\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"  GPU {i}: Allocated {torch.cuda.memory_allocated(i)/1e9:.1f}GB / Reserved {torch.cuda.memory_reserved(i)/1e9:.1f}GB\")\n",
        "\n",
        "except Exception as e:\n",
        "    error_msg = f\"Load failed: {e}\\n\\nFALLBACK: Use TEACHER_MODEL_NAME='openai/gpt-oss-20b' and run 'python {{__file__}}' (single GPU).\"\n",
        "    if accelerator.is_main_process:\n",
        "        raise RuntimeError(error_msg)\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "# --- 4. DATASET CREATION ---\n",
        "distillation_dataset = None\n",
        "if accelerator.is_main_process:\n",
        "    print(\"Generating dataset...\")\n",
        "    base_prompts = [\n",
        "        \"Explain the concept of quantum computing in one sentence.\",\n",
        "        \"Write a Python function to compute Fibonacci numbers efficiently.\",\n",
        "        \"Describe the process of photosynthesis.\",\n",
        "        \"What are the key principles of object-oriented programming?\",\n",
        "    ]\n",
        "    prompts = (base_prompts * (20 // len(base_prompts) + 1))[:20]\n",
        "\n",
        "    local_samples = 20 // accelerator.num_processes\n",
        "    local_start = accelerator.process_index * local_samples\n",
        "    local_prompts = prompts[local_start:local_start + local_samples]\n",
        "\n",
        "    local_data = []\n",
        "    teacher_model.eval()\n",
        "    gen_batch_size = 1\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(local_prompts), gen_batch_size), desc=\"Generating\", disable=not accelerator.is_main_process):\n",
        "            batch_prompts = local_prompts[i:i + gen_batch_size]\n",
        "            batch_conversations = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": p}] for p in batch_prompts]\n",
        "\n",
        "            inputs = teacher_tokenizer.apply_chat_template(\n",
        "                batch_conversations, return_tensors=\"pt\", padding=True, truncation=True, max_length=256, add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "            if not isinstance(inputs, dict):\n",
        "                inputs = {'input_ids': inputs, 'attention_mask': torch.ones_like(inputs)}\n",
        "\n",
        "            inputs = {k: v.to(teacher_model.device) if torch.is_tensor(v) else v for k, v in inputs.items()}\n",
        "\n",
        "            try:\n",
        "                outputs = teacher_model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=128,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=teacher_tokenizer.eos_token_id\n",
        "                )\n",
        "                for j, output in enumerate(outputs):\n",
        "                    prompt_len = inputs['input_ids'][j].shape[0]\n",
        "                    response = teacher_tokenizer.decode(output[prompt_len:], skip_special_tokens=True).strip()\n",
        "                    local_data.append({\"text\": f\"### Instruction:\\n{batch_prompts[j]}\\n\\n### Response:\\n{response}\\n\"})\n",
        "            except Exception as e:\n",
        "                print(f\"Gen error at {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    all_data = gather_object(local_data)\n",
        "    dataset_data = all_data\n",
        "    distillation_dataset = Dataset.from_list(dataset_data)\n",
        "    print(f\"Dataset created ({len(distillation_dataset)} samples).\")\n",
        "    distillation_dataset.save_to_disk(DATASET_SAVE_PATH)\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "# Load the dataset from the saved file on all processes\n",
        "distillation_dataset = Dataset.load_from_disk(DATASET_SAVE_PATH)\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "# Cleanup teacher on all processes\n",
        "if accelerator.is_main_process:\n",
        "    print(\"Cleaning teacher...\")\n",
        "del teacher_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "# --- 5. FINE-TUNING ---\n",
        "if accelerator.is_main_process:\n",
        "    print(f\"Loading student model: {STUDENT_MODEL_NAME}...\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    STUDENT_MODEL_NAME,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Use the loaded dataset, now identical on all processes.\n",
        "distillation_dataset_for_trainer = distillation_dataset\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=5,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_steps=1,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(\"Setting up the SFTTrainer...\")\n",
        "\n",
        "try:\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=distillation_dataset_for_trainer,\n",
        "        peft_config=lora_config,\n",
        "        args=training_args,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=1024,\n",
        "        dataset_text_field=\"text\",\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "    # Prepare the trainer, model, and optimizer for distributed training\n",
        "    trainer.model, trainer.args = accelerator.prepare(trainer.model, trainer.args)\n",
        "\n",
        "    # --- 6. EXECUTE TRAINING ---\n",
        "    if accelerator.is_main_process:\n",
        "        print(f\"Executing fine-tuning for {NUM_EPOCHS} epochs...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # --- 7. SAVE AND UPLOAD MODEL ---\n",
        "    if accelerator.is_main_process:\n",
        "        print(f\"Saving the fine-tuned model locally to {OUTPUT_DIR}...\")\n",
        "        trainer.save_model(OUTPUT_DIR)\n",
        "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "        print(f\"Uploading model to Hugging Face Hub: {HF_REPO_ID}...\")\n",
        "        trainer.model.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n",
        "        tokenizer.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n",
        "        api = HfApi()\n",
        "        model_card_content = \"\"\"\n",
        "# Mistral-7B-GPT-OSS-20B-Distilled\n",
        "\n",
        "This model is a fine-tuned version of {STUDENT_MODEL_NAME}, distilled from {TEACHER_MODEL_NAME} using LoRA and SFTTrainer.\n",
        "\n",
        "## Training Details\n",
        "- **Teacher Model**: {TEACHER_MODEL_NAME}\n",
        "- **Dataset**: Synthetic dataset of {len} samples generated by the teacher.\n",
        "- **LoRA Config**: r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\n",
        "- **Training Hyperparams**: {NUM_EPOCHS} epochs, learning rate={LEARNING_RATE}, batch size={BATCH_SIZE}\n",
        "\n",
        "## Usage\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{STUDENT_MODEL_NAME}\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{STUDENT_MODEL_NAME}\")\n",
        "model = PeftModel.from_pretrained(model, \"{HF_REPO_ID}\")\n",
        "```\n",
        "\"\"\".format(\n",
        "            STUDENT_MODEL_NAME=STUDENT_MODEL_NAME,\n",
        "            TEACHER_MODEL_NAME=TEACHER_MODEL_NAME,\n",
        "            LORA_R=LORA_R,\n",
        "            LORA_ALPHA=LORA_ALPHA,\n",
        "            LORA_DROPOUT=LORA_DROPOUT,\n",
        "            NUM_EPOCHS=NUM_EPOCHS,\n",
        "            LEARNING_RATE=LEARNING_RATE,\n",
        "            BATCH_SIZE=BATCH_SIZE,\n",
        "            HF_REPO_ID=HF_REPO_ID,\n",
        "            len=len(distillation_dataset)\n",
        "        )\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=model_card_content.encode(),\n",
        "            path_in_repo=\"README.md\",\n",
        "            repo_id=HF_REPO_ID,\n",
        "            repo_type=\"model\",\n",
        "            token=HF_TOKEN\n",
        "        )\n",
        "        print(f\"Model and model card successfully uploaded to {HF_REPO_ID}!\")\n",
        "\n",
        "except Exception as e:\n",
        "    if accelerator.is_main_process:\n",
        "        raise RuntimeError(f\"Fine-tuning failed: {e}\")\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "# --- 8. CLEANUP ---\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(\"Distillation, fine-tuning, and upload complete!\")"
      ]
    }
  ]
}
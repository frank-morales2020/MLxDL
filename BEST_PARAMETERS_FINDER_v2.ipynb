{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzSv9nn1xe5iljBKQq4mlz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/BEST_PARAMETERS_FINDER_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b020989"
      },
      "source": [
        "!pip install keras-tuner -q\n",
        "!pip install ta -q\n",
        "!pip install tensorflow -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "PVyVgRGNpQtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOL_SQLITE_DB_PATH = '/content/gdrive/MyDrive/TradingBotLogs/ohlcv_data_SOL.db'\n",
        "SOL_SQLITE_TABLE_NAME = 'solusd_1h_data'\n",
        "SOL_MODEL_PATH = '/content/gdrive/MyDrive/TradingBotLogs/crypto_model_retrained_500epochs_v2_SOL.keras'\n",
        "\n",
        "\n",
        "\n",
        "LDO_SQLITE_DB_PATH = '/content/gdrive/MyDrive/TradingBotLogs/ohlcv_data_LDO.db'\n",
        "LDO_SQLITE_TABLE_NAME = 'ldousd_1h_data'\n",
        "LDO_MODEL_PATH = '/content/gdrive/MyDrive/TradingBotLogs/crypto_model_retrained_500epochs_v2_LDO.keras'\n",
        "\n",
        "\n",
        "TAO_SQLITE_DB_PATH = '/content/gdrive/MyDrive/TradingBotLogs/ohlcv_data_TAO.db'\n",
        "TAO_SQLITE_TABLE_NAME = 'taousd_1h_data'\n",
        "TAO_MODEL_PATH = '/content/gdrive/MyDrive/TradingBotLogs/crypto_model_retrained_500epochs_v2_TAO.keras'\n",
        "\n",
        "\n",
        "ETH_SQLITE_DB_PATH = '/content/gdrive/MyDrive/TradingBotLogs/ohlcv_data.db'\n",
        "ETH_SQLITE_TABLE_NAME = 'ethusd_1h_data'\n",
        "ETH_MODEL_PATH = '/content/gdrive/MyDrive/TradingBotLogs/crypto_model_retrained_500epochs_v2.keras'\n",
        "\n",
        "\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "db_configs = [\n",
        "    {'db_path': SOL_SQLITE_DB_PATH, 'table_name': SOL_SQLITE_TABLE_NAME, 'symbol': 'SOL/USD'},\n",
        "    {'db_path': LDO_SQLITE_DB_PATH, 'table_name': LDO_SQLITE_TABLE_NAME, 'symbol': 'LDO/USD'},\n",
        "    {'db_path': TAO_SQLITE_DB_PATH, 'table_name': TAO_SQLITE_TABLE_NAME, 'symbol': 'TAO/USD'},\n",
        "    {'db_path': ETH_SQLITE_DB_PATH, 'table_name': ETH_SQLITE_TABLE_NAME, 'symbol': 'ETH/USD'},\n",
        "]\n",
        "\n",
        "for config in db_configs:\n",
        "    db_path = config['db_path']\n",
        "    table_name = config['table_name']\n",
        "    symbol = config['symbol']\n",
        "\n",
        "    print(f\"\\n--- Data for {symbol} (Table: {table_name}, DB: {db_path}) ---\")\n",
        "\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "\n",
        "        # Get the total number of rows to handle cases with less than 10 rows\n",
        "        count_query = f\"SELECT COUNT(*) FROM {table_name}\"\n",
        "        total_rows = pd.read_sql_query(count_query, conn).iloc[0, 0]\n",
        "        print(f\"Total rows: {total_rows}\")\n",
        "\n",
        "        if total_rows == 0:\n",
        "            print(\"Table is empty.\")\n",
        "            conn.close()\n",
        "            continue\n",
        "\n",
        "        # Fetch first 5 rows\n",
        "        head_query = f\"SELECT * FROM {table_name} ORDER BY timestamp ASC LIMIT 5\"\n",
        "        df_head = pd.read_sql_query(head_query, conn)\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        display(df_head)\n",
        "\n",
        "        # Fetch last 5 rows, ensuring we don't overlap with the head if total rows are small\n",
        "        tail_limit = min(5, total_rows)\n",
        "        offset = max(0, total_rows - tail_limit)\n",
        "        tail_query = f\"SELECT * FROM {table_name} ORDER BY timestamp ASC LIMIT {tail_limit} OFFSET {offset}\"\n",
        "        df_tail = pd.read_sql_query(tail_query, conn)\n",
        "        print(\"\\nLast 5 rows:\")\n",
        "        display(df_tail)\n",
        "\n",
        "        conn.close()\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Error accessing database {db_path} or table {table_name}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred for {symbol}: {e}\")"
      ],
      "metadata": {
        "id": "5PbDzviTVVlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPF"
      ],
      "metadata": {
        "id": "z0BtEyVLpI4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/gdrive/MyDrive/TradingBotLogs/keras_tuner_dir/trading_param_tuning_LDO_USD"
      ],
      "metadata": {
        "id": "2nqrVwxtQDkQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dc52150d",
        "outputId": "880128a7-3b18-4db8-8b14-66997834fc96"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from ta.trend import MACD, CCIIndicator\n",
        "from ta.volatility import BollingerBands, AverageTrueRange\n",
        "from ta.volume import on_balance_volume\n",
        "from ta.momentum import RSIIndicator, StochasticOscillator, WilliamsRIndicator\n",
        "import logging\n",
        "import keras_tuner as kt\n",
        "import shutil\n",
        "\n",
        "# Set up logger\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def load_sqlite_data(symbol_config):\n",
        "    \"\"\"Load OHLCV data from SQLite database and preprocess it.\"\"\"\n",
        "    try:\n",
        "        symbol = symbol_config['symbol']\n",
        "        db_path = symbol_config['db_path']\n",
        "        table_name = symbol_config['table_name']\n",
        "        if not os.path.exists(db_path):\n",
        "            raise FileNotFoundError(f\"Database not found at {db_path}\")\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        query = f\"SELECT * FROM {table_name}\"\n",
        "        df = pd.read_sql_query(query, conn, parse_dates=['timestamp'])\n",
        "        conn.close()\n",
        "        if df.empty:\n",
        "            raise ValueError(f\"No data in {db_path}/{table_name}\")\n",
        "        required_columns = ['open', 'high', 'low', 'close', 'volume', 'timestamp']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            raise ValueError(f\"Missing required columns in {table_name}: {required_columns}\")\n",
        "        if df['timestamp'].dt.tz is None:\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize('UTC').dt.tz_convert('America/New_York')\n",
        "        else:\n",
        "            df['timestamp'] = df['timestamp'].dt.tz_convert('America/New_York')\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        df = df[~df.index.duplicated(keep='last')]\n",
        "        logging.info(f\"Loaded {len(df)} rows from {db_path}/{table_name}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load data: {e}\")\n",
        "        raise\n",
        "\n",
        "def calculate_technical_indicators(df):\n",
        "    \"\"\"Calculate technical indicators for the dataframe.\"\"\"\n",
        "    df = df.copy()\n",
        "    df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].ffill()\n",
        "    df['RSI'] = RSIIndicator(df['close'], window=14).rsi()\n",
        "    macd = MACD(df['close'], window_slow=26, window_fast=12, window_sign=9)\n",
        "    df['MACD'] = macd.macd()\n",
        "    df['MACD_Signal'] = macd.macd_signal()\n",
        "    bb = BollingerBands(df['close'], window=20, window_dev=2)\n",
        "    df['BB_Upper'] = bb.bollinger_hband()\n",
        "    df['BB_Lower'] = bb.bollinger_lband()\n",
        "    df['OBV'] = on_balance_volume(df['close'], df['volume'])\n",
        "    df['ATR'] = AverageTrueRange(df['high'], df['low'], df['close'], window=14).average_true_range()\n",
        "    df['Williams_R'] = WilliamsRIndicator(high=df['high'], low=df['low'], close=df['close'], lbp=14).williams_r()\n",
        "    stoch = StochasticOscillator(high=df['high'], low=df['low'], close=df['close'], window=14, smooth_window=3)\n",
        "    df['Stoch_K'] = stoch.stoch()\n",
        "    df['Stoch_D'] = stoch.stoch_signal()\n",
        "    df['CCI'] = CCIIndicator(high=df['high'], low=df['low'], close=df['close'], window=14).cci()\n",
        "    df = df.dropna()\n",
        "    logging.info(f\"After indicators, data has {len(df)} rows\")\n",
        "    return df\n",
        "\n",
        "def prepare_backtest_data(df, look_back):\n",
        "    \"\"\"Prepare data for backtesting, caching calculated indicators.\"\"\"\n",
        "    if not hasattr(prepare_backtest_data, 'cached_df'):\n",
        "        prepare_backtest_data.cached_df = calculate_technical_indicators(df)\n",
        "    df = prepare_backtest_data.cached_df\n",
        "    if len(df) < look_back:\n",
        "        raise ValueError(f\"Insufficient rows after indicators ({len(df)}, need {look_back})\")\n",
        "    return df, df['close'].copy(), df['ATR'].copy()\n",
        "\n",
        "class PredictionAgent:\n",
        "    def __init__(self, model_path, look_back=72, default_features=None):\n",
        "        \"\"\"Initialize prediction agent with a pre-trained model.\"\"\"\n",
        "        self.look_back = look_back\n",
        "        self.default_features = default_features or [\n",
        "            'open', 'high', 'low', 'close', 'volume', 'RSI', 'MACD', 'MACD_Signal',\n",
        "            'BB_Upper', 'BB_Lower', 'OBV', 'ATR', 'Williams_R', 'Stoch_K', 'Stoch_D', 'CCI'\n",
        "        ]\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        self.model = load_model(model_path)\n",
        "        self.expected_features = self.model.input_shape[-1]\n",
        "        logging.info(f\"Model input shape: {self.model.input_shape}\")\n",
        "        self.features = self.default_features[:self.expected_features]\n",
        "        if len(self.features) != self.expected_features:\n",
        "            logging.warning(f\"Feature mismatch: Model expects {self.expected_features}, using {len(self.features)}. Features: {self.features}\")\n",
        "        logging.info(f\"Using features: {self.features}\")\n",
        "        self.predict_fn = tf.function(self.model, input_signature=[tf.TensorSpec(shape=(None, self.look_back, self.expected_features), dtype=tf.float32)])\n",
        "\n",
        "    def preprocess(self, df):\n",
        "        \"\"\"Preprocess data for model prediction.\"\"\"\n",
        "        try:\n",
        "            feature_data = []\n",
        "            for f in self.features:\n",
        "                if f in df.columns:\n",
        "                    feature_data.append(df[f].values)\n",
        "                else:\n",
        "                    logging.warning(f\"Feature {f} not in data, using zeros.\")\n",
        "                    feature_data.append(np.zeros(len(df)))\n",
        "            data = np.column_stack(feature_data).astype(np.float32)\n",
        "            scaled = self.scaler.fit_transform(data)\n",
        "            X = scaled[-self.look_back:].reshape(1, self.look_back, self.expected_features)\n",
        "            X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "            return X\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Preprocessing error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def execute(self, df):\n",
        "        \"\"\"Execute model prediction.\"\"\"\n",
        "        try:\n",
        "            X = self.preprocess(df)\n",
        "            pred_probs = self.predict_fn(X)\n",
        "            pred_probs = np.asarray(pred_probs, dtype=np.float32)\n",
        "            prob_sum = np.sum(pred_probs)\n",
        "            if not np.isclose(prob_sum, 1.0, atol=0.01):\n",
        "                logging.warning(f\"Prediction probabilities sum to {prob_sum:.4f}, expected ~1.0\")\n",
        "            return pred_probs\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Prediction error: {e}\")\n",
        "            raise\n",
        "\n",
        "def run_backtest_for_tuner(trade_params, prediction_agent, df_test, original_prices_test, original_atr_test, symbol=\"Generic\", symbol_config=None, test_mode=False, trial_id=None):\n",
        "    \"\"\"Run backtest with trading parameters, storing trade details without per-trade logs.\"\"\"\n",
        "    original_log_level = logging.getLogger().level\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    initial_capital = 2500.0\n",
        "    capital = initial_capital\n",
        "    qty = 0.0\n",
        "    num_trades = 0\n",
        "    in_position = False\n",
        "    entry_price = 0.0\n",
        "    equity_curve = [initial_capital]\n",
        "    pred_classes = []\n",
        "    trades = []\n",
        "\n",
        "    look_back = prediction_agent.look_back\n",
        "    atr_threshold = original_atr_test.rolling(window=1000, min_periods=1).quantile(0.25)\n",
        "\n",
        "    logging.info(f\"Starting backtest for {symbol} (Test Mode: {test_mode})\")\n",
        "    logging.info(f\"Initial Capital: {initial_capital}, Look Back: {look_back}, Parameters: {trade_params}\")\n",
        "\n",
        "    for t in range(look_back, len(df_test)):\n",
        "        current_timestamp = df_test.index[t]\n",
        "        current_price = original_prices_test.iloc[t]\n",
        "        atr = original_atr_test.iloc[t]\n",
        "\n",
        "        window_df = df_test.iloc[max(0, t - look_back):t]\n",
        "        if len(window_df) < look_back:\n",
        "            equity_curve.append(capital)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pred_probs = prediction_agent.execute(window_df)\n",
        "            pred_class = np.argmax(pred_probs, axis=1)[0]\n",
        "            confidence = pred_probs[0, pred_class]\n",
        "            pred_classes.append(pred_class)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Prediction failed at step {t}: {e}\")\n",
        "            equity_curve.append(capital)\n",
        "            continue\n",
        "\n",
        "        is_atr_valid = atr >= atr_threshold.iloc[t] * symbol_config['atr_relax_factor']\n",
        "\n",
        "        if not is_atr_valid:\n",
        "            equity_curve.append(capital)\n",
        "            continue\n",
        "\n",
        "        if not in_position and pred_class == 1 and confidence >= trade_params['CONFIDENCE_THRESHOLD']:\n",
        "            position_size_usd = capital * trade_params['MAX_POSITION_SIZE']\n",
        "            if current_price > 0:\n",
        "                qty = position_size_usd / current_price\n",
        "            else:\n",
        "                logging.warning(f\"Current price is zero or negative at step {t}. Skipping trade.\")\n",
        "                equity_curve.append(capital)\n",
        "                continue\n",
        "            min_volume = symbol_config.get('min_volume', 0.001)\n",
        "            if qty < min_volume:\n",
        "                equity_curve.append(capital)\n",
        "                continue\n",
        "            capital -= position_size_usd\n",
        "            entry_price = current_price\n",
        "            in_position = True\n",
        "            num_trades += 1\n",
        "            trades.append({\n",
        "                'trade_id': num_trades,\n",
        "                'entry_time': str(current_timestamp),\n",
        "                'entry_price': entry_price,\n",
        "                'confidence': float(confidence),\n",
        "                'quantity': qty\n",
        "            })\n",
        "\n",
        "        elif in_position:\n",
        "            exit_reason = None\n",
        "            exit_price = current_price\n",
        "            tp_price = entry_price + atr * trade_params['ATR_MULTIPLIER_TP']\n",
        "            sl_price = entry_price - atr * trade_params['ATR_MULTIPLIER_SL']\n",
        "\n",
        "            if current_price >= tp_price:\n",
        "                exit_reason = 'TP'\n",
        "                exit_price = tp_price\n",
        "            elif current_price <= sl_price:\n",
        "                exit_reason = 'SL'\n",
        "                exit_price = sl_price\n",
        "            elif pred_class == 2 and confidence >= trade_params['CONFIDENCE_THRESHOLD']:\n",
        "                exit_reason = 'Sell_Signal'\n",
        "\n",
        "            if exit_reason:\n",
        "                profit_or_loss = (exit_price - entry_price) * qty\n",
        "                capital += (entry_price * qty) + profit_or_loss\n",
        "                if trades and 'exit_time' not in trades[-1]:\n",
        "                    trades[-1].update({\n",
        "                        'exit_time': str(current_timestamp),\n",
        "                        'exit_reason': exit_reason,\n",
        "                        'exit_price': exit_price,\n",
        "                        'profit_loss': profit_or_loss,\n",
        "                        'capital': capital\n",
        "                    })\n",
        "                in_position = False\n",
        "                qty = 0.0\n",
        "\n",
        "        equity_curve.append(capital)\n",
        "\n",
        "    if in_position:\n",
        "        profit_or_loss = (original_prices_test.iloc[-1] - entry_price) * qty\n",
        "        capital += original_prices_test.iloc[-1] * qty\n",
        "        if trades and 'exit_time' not in trades[-1]:\n",
        "            trades[-1].update({\n",
        "                'exit_time': str(df_test.index[-1]),\n",
        "                'exit_reason': 'Final_Close',\n",
        "                'exit_price': original_prices_test.iloc[-1],\n",
        "                'profit_loss': profit_or_loss,\n",
        "                'capital': capital\n",
        "            })\n",
        "\n",
        "    total_return = ((capital - initial_capital) / initial_capital) * 100\n",
        "    class_counts = np.bincount(pred_classes, minlength=3)\n",
        "    tp_count = sum(1 for t in trades if t.get('exit_reason') == 'TP')\n",
        "    sl_count = sum(1 for t in trades if t.get('exit_reason') == 'SL')\n",
        "    sell_count = sum(1 for t in trades if t.get('exit_reason') == 'Sell_Signal')\n",
        "    total_pl = sum(t.get('profit_loss', 0) for t in trades)\n",
        "    logging.info(f\"Backtest {'test' if test_mode else ''} Finished for {symbol}: Return {total_return:.2f}%, Trades {num_trades}, TP={tp_count}, SL={sl_count}, Sell={sell_count}, Total P/L {total_pl:.2f}, Classes: Buy={class_counts[1]}, Hold={class_counts[0]}, Sell={class_counts[2]}\")\n",
        "\n",
        "    trade_log_path = os.path.join(symbol_config['db_path'].rsplit('/', 1)[0], f\"backtest_trades_{'test' if test_mode else f'trial_{trial_id}' if trial_id else 'final'}.json\")\n",
        "    try:\n",
        "        with open(trade_log_path, 'w') as f:\n",
        "            json.dump(trades, f, indent=4)\n",
        "        logging.info(f\"Saved trade log to {trade_log_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving trade log: {e}\")\n",
        "\n",
        "    logging.getLogger().setLevel(original_log_level)\n",
        "    return total_return, num_trades, equity_curve\n",
        "\n",
        "# Corrected build_model function to work with kt.Hyperband\n",
        "def build_model(hp, **kwargs):\n",
        "    \"\"\"\n",
        "    Builds a dummy Keras model for Hyperband.\n",
        "    This model's \"training\" is actually a backtest, and its \"loss\" is the negative total return.\n",
        "    \"\"\"\n",
        "    trade_params = {\n",
        "        'CONFIDENCE_THRESHOLD': hp.Float('confidence_threshold', min_value=0.001, max_value=0.05, step=0.001),\n",
        "        'ATR_MULTIPLIER_TP': hp.Float('atr_multiplier_tp', min_value=1.0, max_value=5.0, step=0.5),\n",
        "        'ATR_MULTIPLIER_SL': hp.Float('atr_multiplier_sl', min_value=0.5, max_value=3.0, step=0.5),\n",
        "        'MAX_POSITION_SIZE': hp.Float('max_position_size', min_value=0.01, max_value=0.1, step=0.01)\n",
        "    }\n",
        "\n",
        "    # Use kwargs to pass necessary data for backtesting\n",
        "    prediction_agent = kwargs['prediction_agent']\n",
        "    df_test = kwargs['df_test']\n",
        "    original_prices_test = kwargs['original_prices_test']\n",
        "    original_atr_test = kwargs['original_atr_test']\n",
        "    symbol_config = kwargs['symbol_config']\n",
        "\n",
        "    total_return, num_trades, _ = run_backtest_for_tuner(\n",
        "        trade_params, prediction_agent, df_test, original_prices_test, original_atr_test, symbol_config['symbol'], symbol_config\n",
        "    )\n",
        "\n",
        "    # We define a custom metric that Keras Tuner can track\n",
        "    class CustomReturnMetric(tf.keras.metrics.Metric):\n",
        "        def __init__(self, name='custom_return_metric', **kwargs):\n",
        "            super().__init__(name=name, **kwargs)\n",
        "            self.total_return_value = self.add_weight(name='total_return', initializer='zeros')\n",
        "\n",
        "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "            # We don't use y_true or y_pred, we just set the value from the backtest\n",
        "            self.total_return_value.assign(tf.constant(total_return, dtype=tf.float32))\n",
        "\n",
        "        def result(self):\n",
        "            return self.total_return_value\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,)),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # We must use a loss that is independent of model weights and provides a gradient of 0.\n",
        "    # We use a custom lambda layer that does nothing, ensuring no gradients are calculated.\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse', # The loss doesn't matter since the model is not trained.\n",
        "        metrics=[CustomReturnMetric()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/gdrive', force_remount=True)\n",
        "        logging.info(\"Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to mount Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    symbol_config = {\n",
        "        \"symbol\": \"LDO/USD\",\n",
        "        \"model_path\": '/content/gdrive/MyDrive/TradingBotLogs/crypto_model_retrained_500epochs_v2_LDO.keras',\n",
        "        \"params\": {\"CONFIDENCE_THRESHOLD\": 0.001, \"ATR_MULTIPLIER_TP\": 2.0, \"ATR_MULTIPLIER_SL\": 1.0, \"MAX_POSITION_SIZE\": 0.03},\n",
        "        \"db_path\": '/content/gdrive/MyDrive/TradingBotLogs/ohlcv_data_LDO.db',\n",
        "        \"table_name\": 'ldousd_1h_data',\n",
        "        \"min_volume\": 0.001,\n",
        "        \"atr_relax_factor\": 0.5\n",
        "    }\n",
        "\n",
        "    tuner_dir = '/content/gdrive/MyDrive/TradingBotLogs/keras_tuner_dir'\n",
        "    project_name = f'trading_param_tuning_{symbol_config[\"symbol\"].replace(\"/\", \"_\")}'\n",
        "    full_tuner_path = os.path.join(tuner_dir, project_name)\n",
        "\n",
        "    # Clean up tuner directory\n",
        "    if os.path.exists(full_tuner_path):\n",
        "        try:\n",
        "            shutil.rmtree(full_tuner_path)\n",
        "            logging.info(f\"Deleted tuner directory {full_tuner_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error deleting tuner directory {full_tuner_path}: {e}\")\n",
        "\n",
        "    logging.info(f\"Starting parameter optimization for {symbol_config['symbol']}\")\n",
        "\n",
        "    try:\n",
        "        df = load_sqlite_data(symbol_config)\n",
        "        df_test, original_prices_test, original_atr_test = prepare_backtest_data(df, 72)\n",
        "        prediction_agent = PredictionAgent(symbol_config['model_path'])\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"Setup failed: {e}\")\n",
        "        exit()\n",
        "\n",
        "    logging.info(\"Running standalone test backtest\")\n",
        "    default_trade_params = symbol_config['params']\n",
        "    test_return, test_trades, test_equity_curve = run_backtest_for_tuner(\n",
        "        default_trade_params, prediction_agent, df_test, original_prices_test, original_atr_test, symbol_config['symbol'], symbol_config, test_mode=True\n",
        "    )\n",
        "    logging.info(f\"Test Backtest: Return {test_return:.2f}%, Trades {test_trades}\")\n",
        "    if test_trades == 0:\n",
        "        logging.warning(\"No trades in test. Check model predictions or lower CONFIDENCE_THRESHOLD.\")\n",
        "\n",
        "    # Redefine build_model to pass the necessary backtest data\n",
        "    def build_model_with_data(hp):\n",
        "        return build_model(hp, prediction_agent=prediction_agent, df_test=df_test, original_prices_test=original_prices_test, original_atr_test=original_atr_test, symbol_config=symbol_config)\n",
        "\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model_with_data,\n",
        "        objective=kt.Objective('custom_return_metric', direction='max'),\n",
        "        max_epochs=10,\n",
        "        factor=3,\n",
        "        directory=tuner_dir,\n",
        "        project_name=project_name,\n",
        "        overwrite=True\n",
        "    )\n",
        "\n",
        "    logging.info(\"Starting tuner search\")\n",
        "    try:\n",
        "        # Dummy data to satisfy the Keras model.fit() call\n",
        "        tuner.search(np.zeros((1, 1)), np.zeros((1, 1)), epochs=1)\n",
        "        logging.info(\"Tuner search completed successfully\")\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"Tuner search failed: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # The rest of the code for retrieving and logging results remains the same\n",
        "    logging.info(\"Listing trials from tuner\")\n",
        "    saved_trial_results = []\n",
        "    try:\n",
        "        trials = tuner.oracle.get_best_trials(num_trials=1000)\n",
        "        for i, trial in enumerate(trials):\n",
        "            trial_id = trial.trial_id\n",
        "            score = trial.score if trial.score is not None else -1000\n",
        "            logging.info(f\"Trial {i+1}: ID={trial_id}, Score={score:.2f}, Parameters={trial.hyperparameters.values}\")\n",
        "\n",
        "            trade_params = {\n",
        "                'CONFIDENCE_THRESHOLD': trial.hyperparameters.get('confidence_threshold'),\n",
        "                'ATR_MULTIPLIER_TP': trial.hyperparameters.get('atr_multiplier_tp'),\n",
        "                'ATR_MULTIPLIER_SL': trial.hyperparameters.get('atr_multiplier_sl'),\n",
        "                'MAX_POSITION_SIZE': trial.hyperparameters.get('max_position_size')\n",
        "            }\n",
        "            total_return, num_trades, _ = run_backtest_for_tuner(\n",
        "                trade_params, prediction_agent, df_test, original_prices_test, original_atr_test, symbol_config['symbol'], symbol_config, trial_id=trial_id\n",
        "            )\n",
        "            trial_info = {\n",
        "                'trial_id': trial_id,\n",
        "                'parameters': trial.hyperparameters.values,\n",
        "                'score': score,\n",
        "                'total_return': total_return,\n",
        "                'num_trades': num_trades,\n",
        "                'status': 'completed'\n",
        "            }\n",
        "            trial_log_path = os.path.join(tuner_dir, project_name, trial_id, 'trial_log.json')\n",
        "            os.makedirs(os.path.dirname(trial_log_path), exist_ok=True)\n",
        "            with open(trial_log_path, 'w') as f:\n",
        "                json.dump(trial_info, f, indent=4)\n",
        "            saved_trial_results.append(trial_info)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error listing trials: {e}\")\n",
        "\n",
        "    all_trials_output_path = '/content/gdrive/MyDrive/TradingBotLogs/all_trials.json'\n",
        "    try:\n",
        "        with open(all_trials_output_path, 'w') as f:\n",
        "            json.dump(saved_trial_results, f, indent=4)\n",
        "        logging.info(f\"Saved trial results to {all_trials_output_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving trial results: {e}\")\n",
        "\n",
        "    logging.info(\"Retrieving best hyperparameters\")\n",
        "    try:\n",
        "        best_trials = tuner.oracle.get_best_trials(num_trials=1)\n",
        "        if not best_trials:\n",
        "            logging.error(\"No successful trials found.\")\n",
        "            exit()\n",
        "        best_hps = best_trials[0].hyperparameters\n",
        "        best_trade_params = {\n",
        "            'CONFIDENCE_THRESHOLD': best_hps.get('confidence_threshold'),\n",
        "            'ATR_MULTIPLIER_TP': best_hps.get('atr_multiplier_tp'),\n",
        "            'ATR_MULTIPLIER_SL': best_hps.get('atr_multiplier_sl'),\n",
        "            'MAX_POSITION_SIZE': best_hps.get('max_position_size')\n",
        "        }\n",
        "\n",
        "        logging.info(f\"Best Parameters for {symbol_config['symbol']}:\")\n",
        "        logging.info(f\"  CONFIDENCE_THRESHOLD: {best_hps.get('confidence_threshold'):.4f}\")\n",
        "        logging.info(f\"  ATR_MULTIPLIER_TP: {best_hps.get('atr_multiplier_tp'):.2f}\")\n",
        "        logging.info(f\"  ATR_MULTIPLIER_SL: {best_hps.get('atr_multiplier_sl'):.2f}\")\n",
        "        logging.info(f\"  MAX_POSITION_SIZE: {best_hps.get('max_position_size'):.4f}\")\n",
        "\n",
        "        final_return, final_trades, equity_curve = run_backtest_for_tuner(\n",
        "            best_trade_params, prediction_agent, df_test, original_prices_test, original_atr_test, symbol_config['symbol'], symbol_config\n",
        "        )\n",
        "        logging.info(f\"Final Total Return: {final_return:.2f}%\")\n",
        "        logging.info(f\"Number of Trades: {final_trades}\")\n",
        "\n",
        "        best_params = {**best_trade_params, 'total_return': final_return, 'num_trades': final_trades}\n",
        "        best_params_output_path = '/content/gdrive/MyDrive/TradingBotLogs/best_params_LDO.json'\n",
        "        with open(best_params_output_path, 'w') as f:\n",
        "            json.dump(best_params, f, indent=4)\n",
        "        logging.info(f\"Best parameters saved to {best_params_output_path}\")\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"Error retrieving best hyperparameters: {e}\")\n",
        "        exit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6 Complete [00h 04m 02s]\n",
            "custom_return_metric: 6.4890546798706055\n",
            "\n",
            "Best custom_return_metric So Far: 10.590460777282715\n",
            "Total elapsed time: 00h 28m 19s\n",
            "\n",
            "Search: Running Trial #7\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "0.021             |0.021             |confidence_threshold\n",
            "3                 |3.5               |atr_multiplier_tp\n",
            "2.5               |2                 |atr_multiplier_sl\n",
            "0.09              |0.07              |max_position_size\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Starting backtest for LDO/USD (Test Mode: False)\n",
            "INFO:root:Initial Capital: 2500.0, Look Back: 72, Parameters: {'CONFIDENCE_THRESHOLD': 0.021, 'ATR_MULTIPLIER_TP': 3.0, 'ATR_MULTIPLIER_SL': 2.5, 'MAX_POSITION_SIZE': 0.09}\n"
          ]
        }
      ]
    }
  ]
}
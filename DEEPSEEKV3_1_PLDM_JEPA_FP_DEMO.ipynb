{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNrQN7SyHYgCZPj+umgSWjx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/DEEPSEEKV3_1_PLDM_JEPA_FP_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av -q\n",
        "!pip install colab-env -q\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "ph5nY4qpcBZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOhqW-c9i24V",
        "outputId": "6788eef3-3075-4de9-8e9a-96e87fb3f61a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 23 06:41:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   51C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEEPSEEK V3.1"
      ],
      "metadata": {
        "id": "q5ga6JW6eyEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please install OpenAI SDK first: `pip3 install openai`\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key=userdata.get(\"DEEPSEEK_API_KEY\")\n",
        "\n",
        "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")"
      ],
      "metadata": {
        "id": "CN6cdK_wexrc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_reasoner = client.chat.completions.create(\n",
        "    model=\"deepseek-reasoner\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that can solve logic puzzles.\"},\n",
        "        {\"role\": \"user\", \"content\": \"There are three boxes. One contains only apples, one contains only oranges, and one contains both apples and oranges. Each box is incorrectly labeled. You can only open one box and take out one fruit. By looking at the fruit, how can you label all the boxes correctly?\"},\n",
        "    ],\n",
        "    stream=False\n",
        ")\n",
        "\n",
        "print(response_reasoner.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8334DrHIe_d-",
        "outputId": "13958712-2ce8-4366-c493-0211ee857e30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve this puzzle, open the box labeled \"Both\" and take out one fruit. Since all boxes are incorrectly labeled, the box labeled \"Both\" cannot contain both apples and oranges. Therefore, based on the fruit you see, you can deduce the contents of all boxes and correct the labels.\n",
            "\n",
            "- If you take out an **apple** from the box labeled \"Both\":\n",
            "  - This box must contain only apples (because it cannot contain both). Relabel it as \"Apples\".\n",
            "  - The box labeled \"Apples\" cannot contain only apples (since labels are wrong), and it cannot be the apples-only box (which you just found), so it must contain only oranges. Relabel it as \"Oranges\".\n",
            "  - The box labeled \"Oranges\" must then contain both apples and oranges (since it cannot contain only oranges). Relabel it as \"Both\".\n",
            "\n",
            "- If you take out an **orange** from the box labeled \"Both\":\n",
            "  - This box must contain only oranges (because it cannot contain both). Relabel it as \"Oranges\".\n",
            "  - The box labeled \"Oranges\" cannot contain only oranges (since labels are wrong), and it cannot be the oranges-only box (which you just found), so it must contain both apples and oranges. Relabel it as \"Both\".\n",
            "  - The box labeled \"Apples\" must then contain only apples (since it cannot contain only apples originally). Relabel it as \"Apples\".\n",
            "\n",
            "By following this process, you can correctly label all boxes after opening only one box and examining one fruit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual Modifications to Cell 1: All Setup, Definitions, and Model Instantiations"
      ],
      "metadata": {
        "id": "PgrvDYWdbzTG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X4xHb8tbmya"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Conceptual Modifications - Aviation Data Definitions\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import av\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "import datetime\n",
        "import pytz\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s')\n",
        "\n",
        "\n",
        "\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"deepseek-reasoner\"\n",
        "    CLASS_LABELS = [\n",
        "        \"airplane landing\",\n",
        "        \"airplane takeoff\",\n",
        "        \"airport ground operations\",\n",
        "        \"in-flight cruise\",\n",
        "        \"emergency landing\",\n",
        "        \"pre-flight check/maintenance\",\n",
        "        \"en-route cruise\",\n",
        "        \"climb phase\",\n",
        "        \"descent phase\",\n",
        "        \"holding pattern\"\n",
        "    ]\n",
        "\n",
        "# Define num_classes globally\n",
        "num_classes = len(AgentConfig.CLASS_LABELS)\n",
        "\n",
        "# --- FIX: CLASSIFIER_SAVE_PATH moved to global scope ---\n",
        "CLASSIFIER_SAVE_PATH = \"classifier_head_trained_on_tartan_aviation_sample.pth\"\n",
        "\n",
        "AIRPORTS = {\n",
        "    \"CYUL\": {\"name\": \"Montreal-Trudeau International\", \"lat\": 45.4706, \"lon\": -73.7408, \"elevation_ft\": 118},\n",
        "    \"LFPG\": {\"name\": \"Paris-Charles de Gaulle\", \"lat\": 49.0097, \"lon\": 2.5479, \"elevation_ft\": 392},\n",
        "}\n",
        "\n",
        "AIRCRAFT_PERFORMANCE = {\n",
        "    \"Boeing777_300ER\": {\n",
        "        \"cruise_speed_knots\": 490,\n",
        "        \"fuel_burn_kg_per_hour\": 7000,\n",
        "        \"max_range_nm\": 7900,\n",
        "        \"climb_rate_fpm\": 2500,\n",
        "        \"descent_rate_fpm\": 2000,\n",
        "        \"typical_cruise_altitude_ft\": 37000,\n",
        "        \"fuel_capacity_kg\": 145000\n",
        "    }\n",
        "}\n",
        "\n",
        "hf_repo = \"facebook/vjepa2-vitg-fpc64-256\"\n",
        "EXTRACTED_FEATURES_DIR = \"/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/\"\n",
        "\n",
        "TOTAL_FLATTENED_VJEPA_DIM = 2048 * 1408\n",
        "\n",
        "CONCEPTUAL_PLDM_LATENT_DIM = 1024\n",
        "\n",
        "latent_dim_pldm = CONCEPTUAL_PLDM_LATENT_DIM\n",
        "action_dim = 8\n",
        "\n",
        "def load_and_process_video(video_path, processor_instance, model_instance, device_instance, num_frames_to_sample=16):\n",
        "    \"\"\"\n",
        "    Loads a video, samples frames, and extracts V-JEPA features.\n",
        "    Returns extracted features (torch.Tensor, shape like [1, 2048, 1408]) and the frames.\n",
        "    Does NOT flatten the V-JEPA output here, keeping it as model's raw output.\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    if not os.path.exists(video_path):\n",
        "        logging.error(f\"ERROR: Video file '{video_path}' not found.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        container = av.open(video_path)\n",
        "        total_frames_in_video = container.streams.video[0].frames\n",
        "        sampling_interval = max(1, total_frames_in_video // num_frames_to_sample)\n",
        "        logging.info(f\"Total frames in video: {total_frames_in_video}\")\n",
        "        logging.info(f\"Sampling interval: {sampling_interval} frames\")\n",
        "\n",
        "        for i, frame in enumerate(container.decode(video=0)):\n",
        "            if len(frames) >= num_frames_to_sample:\n",
        "                break\n",
        "            if i % sampling_interval == 0:\n",
        "                img = frame.to_rgb().to_ndarray()\n",
        "                frames.append(img)\n",
        "\n",
        "        if not frames:\n",
        "            logging.error(f\"ERROR: No frames could be loaded from '{video_path}'.\")\n",
        "            return None, None\n",
        "        elif len(frames) < num_frames_to_sample:\n",
        "            logging.warning(f\"WARNING: Only {len(frames)} frames loaded. Requested: {num_frames_to_sample}.\")\n",
        "\n",
        "        inputs = processor_instance(videos=list(frames), return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device_instance) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model_instance(**inputs).last_hidden_state\n",
        "\n",
        "        logging.info(f\"Successfully extracted V-JEPA features with raw shape: {features.shape}\")\n",
        "        return features, frames\n",
        "\n",
        "    except av.FFmpegError as e:\n",
        "        logging.error(f\"Error loading video with PyAV: {e}\")\n",
        "        logging.error(\"This might indicate an issue with the video file itself or PyAV installation.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred: {e}\")\n",
        "        logging.error(\"Ensure 'av' library is installed (`pip install av`) and video file is not corrupt.\")\n",
        "        return None, None\n",
        "\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(self.relu(self.fc1(x))))\n",
        "\n",
        "class LatentDynamicsPredictor(torch.nn.Module):\n",
        "    def __init__(self, latent_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_state, action):\n",
        "        combined_input = torch.cat([latent_state, action], dim=-1)\n",
        "        predicted_next_latent_state = self.layers(combined_input)\n",
        "        return predicted_next_latent_state\n",
        "\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.projector = nn.Linear(input_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.projector(x))\n",
        "\n",
        "print(\"\\n--- Instantiating Models and Optimizers ---\")\n",
        "model = AutoModel.from_pretrained(hf_repo)\n",
        "processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "latent_projector = LatentProjector(TOTAL_FLATTENED_VJEPA_DIM, CONCEPTUAL_PLDM_LATENT_DIM)\n",
        "latent_projector.to(device)\n",
        "\n",
        "predictor = LatentDynamicsPredictor(latent_dim_pldm, action_dim)\n",
        "predictor.to(device)\n",
        "optimizer_pldm = torch.optim.Adam(list(predictor.parameters()) + list(latent_projector.parameters()), lr=0.001)\n",
        "\n",
        "classifier = ClassifierHead(input_dim=1408, num_classes=num_classes)\n",
        "classifier.to(device)\n",
        "\n",
        "print(f\"Models instantiated and moved to {device}.\")\n",
        "print(\"\\nCell 1 setup complete for conceptual flight planning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Core Execution - Feature Extraction, Classifier Training & Inference, LLM Interaction"
      ],
      "metadata": {
        "id": "tiopWke5dXcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2: Core Execution Feature Extraction, Classifier Training & Inference, LLM Interaction, and PLDM Training/Planning\n",
        "# This cell assumes Cell 1 has been successfully executed in the current session.\n",
        "# All objects (model, processor, classifier, predictor, device, optimizer_pldm)\n",
        "# and all function definitions (load_and_process_video, ClassifierHead, LatentDynamicsPredictor)\n",
        "# are expected to be available from Cell 1's execution.\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import json\n",
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import datetime\n",
        "import pytz\n",
        "\n",
        "#Mounting Google Drive\n",
        "print(\"\\n--- Cell 2: Mounting Google Drive for dataset access ---\")\n",
        "drive.mount('/content/gdrive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "print(f\"Checking for extracted features directory: {EXTRACTED_FEATURES_DIR}\")\n",
        "if not os.path.exists(EXTRACTED_FEATURES_DIR):\n",
        "    logging.error(f\"ERROR: Extracted features directory '{EXTRACTED_FEATURES_DIR}' not found. Please create it and upload V-JEPA features.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"Extracted features directory found at {EXTRACTED_FEATURES_DIR}\")\n",
        "\n",
        "# Part 1: Load and process airplane-landing.mp4 for initial observation\n",
        "print(f\"\\n--- Cell 2: Part 1 - Loading actual video '/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4' for feature extraction ---\")\n",
        "flight_video_path = '/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4'\n",
        "# Use the defined load_and_process_video helper function. It now returns features and frames.\n",
        "video_features_for_inference_raw, frames_for_pldm_planning = load_and_process_video(flight_video_path, processor, model, device_instance=device)\n",
        "\n",
        "# -- CRITICAL: Process raw V-JEPA features to match ClassifierHead's expected input_dim --\n",
        "if video_features_for_inference_raw is not None:\n",
        "    # V-JEPA output shape is typically [1, 2048, 1408] (Batch, Channels, Height * Width if 1D)\n",
        "    # Your old code pooled it as squeeze(0).mean(dim=0).unsqueeze(0), which resulted in [1, 1408] for classifier.\n",
        "    # So, extracted_embedding_dim should be 1408 for the classifier.\n",
        "    pooled_features_for_classifier = video_features_for_inference_raw.squeeze(0).mean(dim=0).unsqueeze(0)\n",
        "    extracted_embedding_dim_for_classifier = pooled_features_for_classifier.shape[-1]\n",
        "    logging.info(f\"Dynamically determined extracted_embedding_dim for ClassifierHead: {extracted_embedding_dim_for_classifier}\")\n",
        "else:\n",
        "    pooled_features_for_classifier = None\n",
        "    extracted_embedding_dim_for_classifier = -1\n",
        "    logging.error(\"Failed to extract video features for classifier. Exiting Cell 2.\")\n",
        "    exit()\n",
        "\n",
        "# Part 2: Classifier Training\n",
        "print(f\"\\n--- Cell 2: Part 2 - Starting Classifier Training ---\")\n",
        "print(f\"Attempting to load real V-JEPA features for classifier training or generate synthetic data.\")\n",
        "print(f\"Using device for classifier training: {device}\")\n",
        "\n",
        "try:\n",
        "    # Re-initialize classifier with the correct, dynamically determined input_dim\n",
        "    classifier = ClassifierHead(input_dim=extracted_embedding_dim_for_classifier, num_classes=num_classes)\n",
        "    classifier.to(device)\n",
        "\n",
        "    train_features_list = []\n",
        "    train_labels_list = []\n",
        "\n",
        "    map_file_path = os.path.join(EXTRACTED_FEATURES_DIR, \"feature_label_map.json\")\n",
        "    if not os.path.exists(map_file_path):\n",
        "        logging.warning(f\"Feature-label map file '{map_file_path}' not found. Generating synthetic data.\")\n",
        "        feature_label_map = {}\n",
        "    else:\n",
        "        with open(map_file_path, 'r') as f:\n",
        "            feature_label_map = json.load(f)\n",
        "\n",
        "    if not feature_label_map:\n",
        "        logging.warning(f\"Feature-label map at {map_file_path} is empty. Generating synthetic data.\")\n",
        "        num_training_samples = 2_000_000\n",
        "        # Synthetic data generation uses the dynamically determined input_dim\n",
        "        train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier)\n",
        "        train_labels = torch.randint(0, num_classes, (num_training_samples,))\n",
        "        train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=32, shuffle=True)\n",
        "        val_loader = None\n",
        "        print(f\"Loaded {num_training_samples} SYNTHETIC features for training.\")\n",
        "    else:\n",
        "        for item in tqdm(feature_label_map, desc=\"Loading real V-JEPA features\"):\n",
        "            feature_path = item['feature_path']\n",
        "            label_idx = item['label_idx']\n",
        "            try:\n",
        "                if not os.path.isabs(feature_path):\n",
        "                    feature_path = os.path.join(EXTRACTED_FEATURES_DIR, feature_path)\n",
        "\n",
        "                if not os.path.exists(feature_path):\n",
        "                    logging.warning(f\"Feature file not found at {feature_path}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                feature = torch.load(feature_path, map_location=device)\n",
        "\n",
        "                # Match your working code's pooling/squashing logic to get [1408] dim\n",
        "                if feature.ndim == 3:\n",
        "                    feature = feature.squeeze(0).mean(dim=0)\n",
        "                elif feature.ndim == 2:\n",
        "                    if feature.shape[0] == 1 and feature.shape[1] == 1408:\n",
        "                        feature = feature.squeeze(0)\n",
        "                    elif feature.shape[1] == 1408:\n",
        "                        feature = feature.mean(dim=0)\n",
        "                    else:\n",
        "                        feature = feature.flatten()\n",
        "                elif feature.ndim == 1:\n",
        "                    pass\n",
        "                else:\n",
        "                    logging.warning(f\"Skipping malformed feature from {feature_path}. Unexpected dimensions: {feature.ndim}\")\n",
        "                    continue\n",
        "\n",
        "                # Final check after processing. Should be 1D with 1408 elements.\n",
        "                if feature.shape[0] != extracted_embedding_dim_for_classifier:\n",
        "                    logging.warning(f\"Skipping feature at {feature_path}. Dimension mismatch: expected {extracted_embedding_dim_for_classifier}, got {feature.shape[0]}.\")\n",
        "                    continue\n",
        "\n",
        "                train_features_list.append(feature)\n",
        "                train_labels_list.append(label_idx)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error loading feature from {feature_path}: {e}. Skipping.\")\n",
        "\n",
        "        if train_features_list:\n",
        "            train_features = torch.stack(train_features_list).to(device)\n",
        "            train_labels = torch.tensor(train_labels_list).to(device)\n",
        "            num_training_samples = len(train_features)\n",
        "            print(f\"Loaded {num_training_samples} REAL V-JEPA features for training.\")\n",
        "\n",
        "            if num_training_samples < 2:\n",
        "                print(\"WARNING: Only 1 real V-JEPA feature loaded. Training may be unstable. Consider more data.\")\n",
        "                train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=1, shuffle=True)\n",
        "                val_loader = None\n",
        "            else:\n",
        "                dataset_size = len(train_features)\n",
        "                train_size = int(0.8 * dataset_size)\n",
        "                val_size = dataset_size - train_size\n",
        "\n",
        "                if val_size == 0 and train_size > 0:\n",
        "                    train_size = dataset_size\n",
        "                    train_dataset_real = TensorDataset(train_features, train_labels)\n",
        "                    val_dataset_real = None\n",
        "                else:\n",
        "                    train_dataset_real, val_dataset_real = torch.utils.data.random_split(\n",
        "                        TensorDataset(train_features, train_labels), [train_size, val_size]\n",
        "                    )\n",
        "                train_loader = DataLoader(train_dataset_real, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_dataset_real, batch_size=32, shuffle=False) if val_dataset_real else None\n",
        "                print(f\"Training on {len(train_dataset_real)} samples, Validation on {len(val_dataset_real) if val_dataset_real else 0} samples.\")\n",
        "        else:\n",
        "            logging.error(\"No real V-JEPA features could be loaded from map file. Generating synthetic data as fallback.\")\n",
        "            num_training_samples = 2_000_000\n",
        "            train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier)\n",
        "            train_labels = torch.randint(0, num_classes, (num_training_samples,))\n",
        "            train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=32, shuffle=True)\n",
        "            val_loader = None\n",
        "            print(f\"Loaded {num_training_samples} SYNTHETIC features for training as fallback.\")\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 20\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer_classifier.zero_grad()\n",
        "            outputs = classifier(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_classifier.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        if val_loader and len(val_loader.dataset) > 0:\n",
        "            classifier.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = classifier(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
        "            print(\"No validation data available or validation dataset is empty.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Classifier Training Complete ---\")\n",
        "    torch.save(classifier.state_dict(), CLASSIFIER_SAVE_PATH)\n",
        "    print(f\"Classifier saved to: {CLASSIFIER_SAVE_PATH}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error during classifier training: {e}\")\n"
      ],
      "metadata": {
        "id": "OvOjHsvdden7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3: Classification Inference and Gemini LLM Interaction\n",
        "print(\"\\n--- Cell 2: Part 3 - Starting V-JEPA Feature-Driven Classification Inference and DEEPSEEK LLM Interaction ---\")\n",
        "if pooled_features_for_classifier is None:\n",
        "    logging.error(\"ERROR: Cannot perform classifier inference as 'pooled_features_for_classifier' is None.\")\n",
        "else:\n",
        "    try:\n",
        "        pooled_features_for_inference_on_device = pooled_features_for_classifier.to(device)\n",
        "\n",
        "        classifier.load_state_dict(torch.load(CLASSIFIER_SAVE_PATH, map_location=device))\n",
        "        logging.info(f\"Classifier weights loaded from: {CLASSIFIER_SAVE_PATH}\")\n",
        "\n",
        "        classifier.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = classifier(pooled_features_for_inference_on_device)\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "        predicted_confidence = probabilities[0, predicted_class_idx].item()\n",
        "\n",
        "\n",
        "        # --- FIX: Use AgentConfig.CLASS_LABELS ---\n",
        "        predicted_label = AgentConfig.CLASS_LABELS[predicted_class_idx]\n",
        "\n",
        "        llm_input_description = \"\"\n",
        "        if predicted_label == \"airplane landing\":\n",
        "            llm_input_description = \"The visual system detected an airplane landing.\"\n",
        "        elif predicted_label == \"airplane takeoff\":\n",
        "            llm_input_description = \"The visual system detected an airplane takeoff.\"\n",
        "        elif predicted_label == \"airport ground operations\":\n",
        "            llm_input_description = \"The visual system detected airport ground operations.\"\n",
        "        elif predicted_label == \"in-flight cruise\":\n",
        "            llm_input_description = \"The visual system detected an airplane in-flight cruise.\"\n",
        "        elif predicted_label == \"emergency landing\":\n",
        "            llm_input_description = \"The visual system detected a possible emergency landing.\"\n",
        "        elif predicted_label == \"pre-flight check/maintenance\":\n",
        "            llm_input_description = \"The visual system detected pre-flight check or maintenance activity.\"\n",
        "        else:\n",
        "            llm_input_description = \"The visual system detected an unrecognised flight activity.\"\n",
        "\n",
        "        llm_input_description += f\" (Confidence: {predicted_confidence:.2f})\"\n",
        "\n",
        "        print(f\"\\n--- AI Agent's Understanding from Classifier ---\")\n",
        "        print(f\"**Primary Classification (Predicted by AI):** '{predicted_label}' {llm_input_description.split('Confidence:')[1].strip()}\")\n",
        "        print(f\"**Description for LLM:** {llm_input_description}\")\n",
        "        print(f\"Note: This classification's accuracy depends heavily on the quality and size of the real dataset used for classifier training.\")\n",
        "\n",
        "        print(\"\\n--- Engaging DEEPSEEK LLM for Further Reasoning ---\")\n",
        "        try:\n",
        "            llm_model = AgentConfig.LLM_MODEL_NAME\n",
        "\n",
        "            prompt_for_deepseek = f\"\"\"\n",
        "                  You are an AI assistant for flight planning operations.\n",
        "                  Current visual observation: {llm_input_description}\n",
        "                  Current time (EST): {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\n",
        "\n",
        "                  Based on this visual observation, provide a concise operational assessment relevant to flight planning.\n",
        "                  If the observation seems random or uncertain, state that. Do not add any conversational filler.\n",
        "                  \"\"\"\n",
        "\n",
        "\n",
        "            deepseek_response = client.chat.completions.create(\n",
        "            model=llm_model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "                {\"role\": \"user\", \"content\": prompt_for_deepseek},\n",
        "            ],\n",
        "            stream=False\n",
        "            )\n",
        "\n",
        "\n",
        "            #print('DS RESPONSE')\n",
        "            #print(deepseek_response.choices[0].message.content)\n",
        "            #print('\\n')\n",
        "\n",
        "\n",
        "            # Remove the lines that cause the error by trying to access unavailable attributes\n",
        "            # print('DS metadata')\n",
        "            # print(deepseek_response.metadata)\n",
        "            # print('\\n')\n",
        "            # print('DS candidates')\n",
        "            # print(deepseek_response.candidates)\n",
        "            # print('DS prompt_feedback')\n",
        "            # print(deepseek_response.prompt_feedback)\n",
        "            # print('DS error')\n",
        "            # print(deepseek_response.error)\n",
        "            # print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "            print(\"\\n--- DEEPSEEK LLM Response ---\")\n",
        "            if deepseek_response.choices and deepseek_response.choices[0].message.content:\n",
        "                print(deepseek_response.choices[0].message.content)\n",
        "                print(\"--- DEEPSEEK LLM Response - END ---\")\n",
        "                print('\\n')\n",
        "            else:\n",
        "\n",
        "                print(\"DEEPSEEK LLM did not provide a text response or cannot provide one.\")\n",
        "                # Check if there's an error attribute before trying to print it\n",
        "                if hasattr(deepseek_response, 'error') and deepseek_response.error:\n",
        "                     print(f\"LLM Error: {deepseek_response.error}\")\n",
        "                # Check if there's a prompt_feedback attribute before trying to print it\n",
        "                if hasattr(deepseek_response, 'prompt_feedback') and deepseek_response.prompt_feedback:\n",
        "                    print(f\"Prompt Feedback: {deepseek_response.prompt_feedback}\")\n",
        "\n",
        "\n",
        "        except Exception as llm_e:\n",
        "            logging.error(f\"Error interacting with DEEPSEEK LLM: {llm_e}\")\n",
        "            logging.error(\"Ensure your DEEPSEEK_API_KEY is correctly set in Colab Secrets.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during classification inference or overall Cell 2 execution: {e}\")\n",
        "\n",
        "print(f\"The V-JEPA features (shape: {pooled_features_for_classifier.shape}) are the core input that a trained classifier would learn from.\")\n",
        "print(f\"Current time in EST: {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\")\n",
        "\n",
        "print(\"\\nCell 2 execution complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyglK1NaUtPq",
        "outputId": "3ac18c3a-1979-4115-9805-d2c0203e6d1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cell 2: Part 3 - Starting V-JEPA Feature-Driven Classification Inference and DEEPSEEK LLM Interaction ---\n",
            "\n",
            "--- AI Agent's Understanding from Classifier ---\n",
            "**Primary Classification (Predicted by AI):** 'airplane landing' 1.00)\n",
            "**Description for LLM:** The visual system detected an airplane landing. (Confidence: 1.00)\n",
            "Note: This classification's accuracy depends heavily on the quality and size of the real dataset used for classifier training.\n",
            "\n",
            "--- Engaging DEEPSEEK LLM for Further Reasoning ---\n",
            "\n",
            "--- DEEPSEEK LLM Response ---\n",
            "Runway occupancy likely in progress; expect temporary unavailability for takeoff or landing.\n",
            "--- DEEPSEEK LLM Response - END ---\n",
            "\n",
            "\n",
            "The V-JEPA features (shape: torch.Size([1, 1408])) are the core input that a trained classifier would learn from.\n",
            "Current time in EST: 2025-08-23 02:03:03 EST\n",
            "\n",
            "Cell 2 execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prediction comes from a classifier that was trained on the provided V-JEPA features.\n",
        "For *truly accurate and high-confidence predictions on real videos,\n",
        "the classifier needs to be trained on a large, diverse dataset of *real V-JEPA features and their corresponding labels*."
      ],
      "metadata": {
        "id": "tOjda_Iku5m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual Modifications to Cell 3: Conceptual PLDM Latent Dynamics Training"
      ],
      "metadata": {
        "id": "mCEsxMRec2rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Conceptual Modifications - PLDM Latent Dynamics Training for Real Flights\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "EXTRACTED_FEATURES_DIR = \"/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/\"\n",
        "TOTAL_FLATTENED_VJEPA_DIM = 2048 * 1408\n",
        "# Use the new conceptual latent dimension for dummy data generation\n",
        "latent_dim = CONCEPTUAL_PLDM_LATENT_DIM\n",
        "action_dim = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_flight_dynamics_training_data_real_conceptual(device, num_simulated_real_trajectories=100):\n",
        "    \"\"\"\n",
        "    CONCEPTUAL FUNCTION: Loads reward-free offline flight data for Latent Dynamics training.\n",
        "    For this conceptual demo, it simulates loading 'real-like' sequences.\n",
        "    \"\"\"\n",
        "    print(f\"Loading conceptual real flight dynamics data for training from Montreal-Paris context.\")\n",
        "\n",
        "    dynamics_training_data = []\n",
        "\n",
        "    latent_vec_dim = latent_dim # This will now be CONCEPTUAL_PLDM_LATENT_DIM\n",
        "    action_vec_dim = action_dim\n",
        "\n",
        "    for _ in range(num_simulated_real_trajectories):\n",
        "        current_latent = torch.rand(1, latent_vec_dim).to(device) * 0.1\n",
        "        trajectory_length = random.randint(5, 20)\n",
        "\n",
        "        for _ in range(trajectory_length):\n",
        "            action = torch.randn(1, action_vec_dim).to(device) * 0.1\n",
        "\n",
        "            # Project action's influence to match latent_vec_dim for dummy data generation\n",
        "            action_influence_on_latent = action.mean() * torch.ones(1, latent_vec_dim).to(device) * 0.5\n",
        "\n",
        "            next_latent = current_latent + action_influence_on_latent + (torch.randn(1, latent_vec_dim) * 0.005).to(device)\n",
        "\n",
        "            dynamics_training_data.append((current_latent, action, next_latent))\n",
        "            current_latent = next_latent.clone()\n",
        "\n",
        "    print(f\"Loaded {len(dynamics_training_data)} conceptual 'real-like' dynamics training samples.\")\n",
        "    return dynamics_training_data\n",
        "\n",
        "def train_latent_dynamics_model(predictor_model, optimizer, training_data, epochs=10):\n",
        "    predictor_model.train()\n",
        "    print(\"\\n-- Training Latent Dynamics Predictor for Conceptual Real Flights ---\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (latent_s_t, action_t, latent_s_t_plus_1) in tqdm(\n",
        "            enumerate(training_data),\n",
        "            total=len(training_data),\n",
        "            desc=f\"Epoch {epoch+1}/{epochs}\"\n",
        "        ):\n",
        "            latent_s_t, action_t, latent_s_t_plus_1 = latent_s_t.to(device), action_t.to(device), latent_s_t_plus_1.to(device)\n",
        "\n",
        "            predicted_z_t_plus_1 = predictor_model(latent_s_t, action_t)\n",
        "            loss = torch.nn.functional.mse_loss(predicted_z_t_plus_1, latent_s_t_plus_1)\n",
        "\n",
        "            optimizer_pldm.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss / len(training_data)}\")\n",
        "\n",
        "\n",
        "print(\"\\n--Cell 3: Starting Conceptual PLDM Latent Dynamics Training for Montreal-Paris context ---\")\n",
        "\n",
        "dynamics_training_data = load_flight_dynamics_training_data_real_conceptual(device)\n",
        "\n",
        "if dynamics_training_data:\n",
        "    train_latent_dynamics_model(predictor, optimizer_pldm, dynamics_training_data)\n",
        "else:\n",
        "    print(\"Skipping Latent Dynamics Training as no conceptual data was loaded.\")\n",
        "\n",
        "print(\"\\nCell 3 execution complete.\")"
      ],
      "metadata": {
        "id": "ddZ3X3KXc8BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual Modifications to Cell 4: Conceptual PLDM Planning"
      ],
      "metadata": {
        "id": "WTG-g7dHeWlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Conceptual Modifications - PLDM Planning for Montreal to Paris\n",
        "\n",
        "import logging\n",
        "import torch\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pytz\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "TOTAL_FLATTENED_VJEPA_DIM = 2048 * 1408\n",
        "# Use the new conceptual latent dimension for planning\n",
        "latent_dim = CONCEPTUAL_PLDM_LATENT_DIM\n",
        "action_dim = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def plan_montreal_to_paris_flight(start_airport_data, target_airport_data, aircraft_model_data,\n",
        "                                  encoder_model, processor_instance, predictor_model, latent_projector_instance,\n",
        "                                  planning_horizon=50):\n",
        "    \"\"\"\n",
        "    CONCEPTUAL FUNCTION: Plans a flight path from Montreal to Paris using the learned latent dynamics.\n",
        "    This is a conceptual MPPI implementation demonstrating the flow for a real flight scenario.\n",
        "    \"\"\"\n",
        "    encoder_model.eval()\n",
        "    predictor_model.eval()\n",
        "    latent_projector_instance.eval() # Ensure projector is in eval mode\n",
        "\n",
        "    # --- FIX: Project V-JEPA output to the smaller latent_dim for planning ---\n",
        "    # First, get a dummy raw V-JEPA feature (as if from a video of Montreal airport environment)\n",
        "    # In a real scenario, this would be an actual V-JEPA encoding of the initial state.\n",
        "    dummy_initial_vjepa_feature_raw = torch.rand(1, 2048, 1408).to(device) # Matches V-JEPA's typical output shape\n",
        "    dummy_target_vjepa_feature_raw = torch.rand(1, 2048, 1408).to(device) # Matches V-JEPA's typical output shape + slight difference\n",
        "\n",
        "    # Flatten the raw V-JEPA output to match TOTAL_FLATTENED_VJEPA_DIM\n",
        "    flattened_initial_vjepa_feature = dummy_initial_vjepa_feature_raw.flatten(start_dim=1)\n",
        "    flattened_target_vjepa_feature = dummy_target_vjepa_feature_raw.flatten(start_dim=1)\n",
        "\n",
        "    # Now project to the smaller CONCEPTUAL_PLDM_LATENT_DIM\n",
        "    with torch.no_grad():\n",
        "        current_latent_state = latent_projector_instance(flattened_initial_vjepa_feature)\n",
        "        target_latent_state = latent_projector_instance(flattened_target_vjepa_feature)\n",
        "    # --- END FIX ---\n",
        "\n",
        "    print(\"\\n--- Starting Conceptual Flight Plan: Montreal (CYUL) to Paris (LFPG) ---\")\n",
        "    print('\\n')\n",
        "    print(f\"Conceptual Current Latent State Shape: {current_latent_state.shape}\")\n",
        "    print(f\"Conceptual Target Latent State Shape: {target_latent_state.shape}\")\n",
        "    print('\\n')\n",
        "\n",
        "    best_action_sequence = []\n",
        "\n",
        "    num_action_samples = 100\n",
        "\n",
        "    for step in range(planning_horizon):\n",
        "        candidate_actions = torch.rand(num_action_samples, action_dim).to(device)\n",
        "\n",
        "        simulated_trajectories_cost = []\n",
        "\n",
        "        for i in range(num_action_samples):\n",
        "            simulated_next_latent = predictor_model(current_latent_state, candidate_actions[i].unsqueeze(0))\n",
        "\n",
        "            # 1. Goal Proximity Cost (Primary)\n",
        "            goal_proximity_cost = torch.norm(target_latent_state - simulated_next_latent)\n",
        "\n",
        "            # 2. Conceptual Fuel Cost\n",
        "            conceptual_fuel_cost = torch.rand(1).to(device) * 0.01\n",
        "\n",
        "            # 3. Conceptual Environmental Cost\n",
        "            conceptual_weather_cost = torch.rand(1).to(device) * 0.02\n",
        "\n",
        "            # 4. Conceptual Regulatory/Efficiency Cost\n",
        "            conceptual_efficiency_cost = torch.rand(1).to(device) * 0.005\n",
        "\n",
        "            total_cost = goal_proximity_cost + conceptual_fuel_cost + conceptual_weather_cost + conceptual_efficiency_cost\n",
        "            simulated_trajectories_cost.append(total_cost)\n",
        "\n",
        "        best_candidate_idx = torch.argmin(torch.tensor(simulated_trajectories_cost).squeeze())\n",
        "        optimal_action_for_step = candidate_actions[best_candidate_idx]\n",
        "\n",
        "        best_action_sequence.append(optimal_action_for_step.squeeze().cpu().numpy())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            current_latent_state = predictor_model(current_latent_state, optimal_action_for_step.unsqueeze(0))\n",
        "\n",
        "    print(f\"Conceptual Plan for {planning_horizon} steps (first 5 actions shown):\")\n",
        "    for i, action in enumerate(best_action_sequence[:5]):\n",
        "        print(f\"Step {i+1}: {action}\")\n",
        "    print(\"-- Conceptual Planning Complete ---\")\n",
        "\n",
        "    return best_action_sequence\n",
        "\n",
        "print(\"\\n--Cell 4: Starting Conceptual PLDM Planning for Montreal to Paris ---\")\n",
        "\n",
        "start_airport_data = AIRPORTS[\"CYUL\"]\n",
        "target_airport_data = AIRPORTS[\"LFPG\"]\n",
        "aircraft_model_data = AIRCRAFT_PERFORMANCE[\"Boeing777_300ER\"]\n",
        "\n",
        "# --- FIX: Pass the new latent_projector to the planning function ---\n",
        "conceptual_flight_plan_actions = plan_montreal_to_paris_flight(\n",
        "    start_airport_data,\n",
        "    target_airport_data,\n",
        "    aircraft_model_data,\n",
        "    model,\n",
        "    processor,\n",
        "    predictor,\n",
        "    latent_projector # Pass the projector instance\n",
        ")\n",
        "\n",
        "print(f\"Current time in EST: {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\")\n",
        "print(\"\\nCell 4 execution complete.\")"
      ],
      "metadata": {
        "id": "_l3KJeT_eg7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ec03d1-5960-41a0-c1f2-e325d8894e6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--Cell 4: Starting Conceptual PLDM Planning for Montreal to Paris ---\n",
            "\n",
            "--- Starting Conceptual Flight Plan: Montreal (CYUL) to Paris (LFPG) ---\n",
            "\n",
            "\n",
            "Conceptual Current Latent State Shape: torch.Size([1, 1024])\n",
            "Conceptual Target Latent State Shape: torch.Size([1, 1024])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1205309873.py:76: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  best_candidate_idx = torch.argmin(torch.tensor(simulated_trajectories_cost).squeeze())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conceptual Plan for 50 steps (first 5 actions shown):\n",
            "Step 1: [0.3079524  0.42773765 0.09694833 0.38194418 0.16068906 0.2680508\n",
            " 0.19140571 0.26403862]\n",
            "Step 2: [0.06593829 0.01272601 0.28270882 0.07377458 0.1413129  0.47454768\n",
            " 0.4236831  0.613305  ]\n",
            "Step 3: [0.30838513 0.08085066 0.19037002 0.04160297 0.20280921 0.8317725\n",
            " 0.21357375 0.45853674]\n",
            "Step 4: [0.18256778 0.27216673 0.04214883 0.04314739 0.2727937  0.19067621\n",
            " 0.4011432  0.04063672]\n",
            "Step 5: [0.06874019 0.2800737  0.18193549 0.4362843  0.2810461  0.13977498\n",
            " 0.32510954 0.21687269]\n",
            "-- Conceptual Planning Complete ---\n",
            "Current time in EST: 2025-08-23 01:48:26 EST\n",
            "\n",
            "Cell 4 execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24c830ee",
        "outputId": "94d46f6e-f87c-41f8-e9ba-cbfca30c0704"
      },
      "source": [
        "# Use the DeepSeek LLM to summarize the findings\n",
        "\n",
        "summary_prompt = \"\"\"\n",
        "Based on the provided outputs from a Google Colab notebook execution, please summarize the key findings and steps performed. The notebook involved:\n",
        "1. Setup and model instantiation (V-JEPA, Classifier, Latent Projector, Latent Dynamics Predictor).\n",
        "2. Classification of a video using a trained classifier.\n",
        "3. Interaction with the DeepSeek LLM based on the classification result.\n",
        "4. Conceptual training of a Latent Dynamics Predictor.\n",
        "5. Conceptual flight planning using the Latent Dynamics Predictor.\n",
        "\n",
        "Provide a concise summary of what was achieved.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    llm_model = AgentConfig.LLM_MODEL_NAME\n",
        "\n",
        "    summary_response = client.chat.completions.create(\n",
        "        model=llm_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes technical findings.\"},\n",
        "            {\"role\": \"user\", \"content\": summary_prompt},\n",
        "        ],\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- DEEPSEEK LLM Summary of Findings ---\")\n",
        "    if summary_response.choices and summary_response.choices[0].message.content:\n",
        "        print(summary_response.choices[0].message.content)\n",
        "    else:\n",
        "        print(\"DEEPSEEK LLM did not provide a text response for the summary.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error requesting summary from DEEPSEEK LLM: {e}\")\n",
        "    print(\"Ensure your DEEPSEEK_API_KEY is correctly set in Colab Secrets and the client is initialized.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DEEPSEEK LLM Summary of Findings ---\n",
            "Based on the provided outputs, the notebook successfully demonstrated an integrated pipeline combining vision, language, and planning components. Key achievements include:\n",
            "\n",
            "1. **Model Setup**: Instantiated a V-JEPA model for video representation learning, along with a classifier, latent projector, and latent dynamics predictor.\n",
            "\n",
            "2. **Video Classification**: Processed an input video through the trained classifier, which correctly identified the content (e.g., \"a person doing a backflip on a grass field\").\n",
            "\n",
            "3. **LLM Interaction**: Used DeepSeek LLM to generate a high-level plan based on the classification (e.g., steps to perform a backflip), bridging perception and action.\n",
            "\n",
            "4. **Conceptual Training**: Simulated training a latent dynamics predictor to model transitions in latent space, though actual training was not performed (conceptual).\n",
            "\n",
            "5. **Conceptual Flight Planning**: Illustrated how the dynamics predictor could be used for planning (e.g., optimizing a trajectory for a drone to replicate the action), emphasizing its potential for real-world applications.\n",
            "\n",
            "The workflow highlighted the synergy between visual understanding, language reasoning, and latent-space planning, showcasing a proof-of-concept for autonomous decision-making systems.\n"
          ]
        }
      ]
    }
  ]
}
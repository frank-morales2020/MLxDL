{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "438a937b",
        "1de77dae"
      ],
      "authorship_tag": "ABX9TyO5ew054czO1OK/xXc/osBo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/ARAG_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install PSQL and DEV Libraries locally\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "\n",
        "\n",
        "!apt-get install postgresql-server-dev-14 -q"
      ],
      "metadata": {
        "id": "70LprfxjBbHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pgvector/pgvector.git\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install\n",
        "#print('END: PG VECTOR COMPILATION')"
      ],
      "metadata": {
        "id": "325AxKk5CGGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pgvector -q\n",
        "!pip install openai -q\n",
        "!pip install colab-env -q"
      ],
      "metadata": {
        "id": "3P1epmDsCogJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import psycopg2\n",
        "from psycopg2.extras import RealDictCursor\n",
        "import numpy as np\n",
        "from pgvector.psycopg2 import register_vector # Import pgvector's psycopg2 integration\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "8Q6_GiRrCz8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUo_mv10BVID",
        "outputId": "867ce76e-f857-4c3c-b9d0-3ffe50f33253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALTER ROLE\n",
            "NOTICE:  extension \"vector\" already exists, skipping\n",
            "CREATE EXTENSION\n",
            "ERROR:  table \"embeddings\" does not exist\n",
            "Ensuring pgvector extension is created...\n",
            "pgvector extension checked/created.\n",
            "Creating 'embeddings' table if it doesn't exist...\n",
            "Table 'embeddings' checked/created.\n",
            "Database setup complete.\n"
          ]
        }
      ],
      "source": [
        "# PostGRES SQL Settings\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "connection_string = 'postgresl://postgres:postgres@localhost:5432/postgres'\n",
        "\n",
        "#CREATE EXTENSION IF NOT EXISTS btree_gist\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION IF NOT EXISTS vector\"\n",
        "\n",
        "import psycopg2 as ps\n",
        "\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "\n",
        "conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "cur = conn.cursor() # creating a cursor\n",
        "\n",
        "# Connect to PostgreSQL database in Timescale using connection string\n",
        "#conn = psycopg2.connect(connection_string)\n",
        "\n",
        "cur = conn.cursor()\n",
        "\n",
        "#install pgvector\n",
        "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\");\n",
        "conn.commit()\n",
        "\n",
        "from pgvector.psycopg2 import register_vector\n",
        "\n",
        "# Register the vector type with psycopg2\n",
        "register_vector(conn)\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE embeddings\"\n",
        "\n",
        "# Create table to store embeddings and metadata\n",
        "table_create_command = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS embeddings (\n",
        "            id bigserial primary key,\n",
        "            title text,\n",
        "            url text,\n",
        "            content text,\n",
        "            tokens integer,\n",
        "            embedding vector(1536)\n",
        "            );\n",
        "            \"\"\"\n",
        "\n",
        "cur.execute(table_create_command)\n",
        "cur.close()\n",
        "conn.commit()\n",
        "\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import psycopg2\n",
        "from psycopg2.extras import RealDictCursor\n",
        "import numpy as np\n",
        "from pgvector.psycopg2 import register_vector # Import pgvector's psycopg2 integration\n",
        "import colab_env\n",
        "\n",
        "# --- Configuration & Initialization ---\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Using the database credentials provided in your reference\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "\n",
        "# Construct the DATABASE_URL from individual components for consistency\n",
        "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
        "\n",
        "# Validate environment variables\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please set it.\")\n",
        "\n",
        "# --- Database Connection and Setup (using the provided table schema) ---\n",
        "conn = None\n",
        "try:\n",
        "    conn = psycopg2.connect(\n",
        "        database=DB_NAME,\n",
        "        user=DB_USER,\n",
        "        password=DB_PASS,\n",
        "        host=DB_HOST,\n",
        "        port=DB_PORT\n",
        "    )\n",
        "    conn.autocommit = True # For CREATE EXTENSION\n",
        "\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create the vector extension if not already present\n",
        "    # This command is also run via shell, but idempotent so safe to run here too\n",
        "    print(\"Ensuring pgvector extension is created...\")\n",
        "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "    print(\"pgvector extension checked/created.\")\n",
        "\n",
        "    # Drop table if it exists (for fresh runs, as in your reference)\n",
        "    # Be cautious with this in production! Uncomment only if you want to reset the table on each run.\n",
        "    # print(\"Attempting to drop existing 'embeddings' table (if any)...\")\n",
        "    # cur.execute(\"DROP TABLE IF EXISTS embeddings;\")\n",
        "    # print(\"'embeddings' table dropped (if it existed).\")\n",
        "\n",
        "    # Create table to store embeddings and metadata, using your provided schema\n",
        "    print(\"Creating 'embeddings' table if it doesn't exist...\")\n",
        "    table_create_command = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS embeddings (\n",
        "                id BIGSERIAL PRIMARY KEY,\n",
        "                title TEXT,\n",
        "                url TEXT,\n",
        "                content TEXT NOT NULL,\n",
        "                tokens INTEGER,\n",
        "                embedding VECTOR(1536) NOT NULL\n",
        "                );\n",
        "    \"\"\"\n",
        "    cur.execute(table_create_command)\n",
        "    print(\"Table 'embeddings' checked/created.\")\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    print(\"Database setup complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during database initial setup: {e}\")\n",
        "    if conn:\n",
        "        conn.close()\n",
        "    # It's critical to ensure the database is set up, so we exit if it fails\n",
        "    # In a full application, you'd handle this more gracefully\n",
        "    exit()\n",
        "\n",
        "# --- OpenAI Client Initialization ---\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# --- OpenAI Embedding Function ---\n",
        "def get_embedding(text: str, model: str = \"text-embedding-3-small\"):\n",
        "    \"\"\"\n",
        "    Generates an embedding for the given text using OpenAI's embedding model.\n",
        "    Converts the list embedding to a numpy array, which pgvector's psycopg2\n",
        "    integration expects.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    try:\n",
        "        response = openai_client.embeddings.create(input=[text], model=model)\n",
        "        return np.array(response.data[0].embedding)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embedding for text: '{text[:50]}...'. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- PostgreSQL Interaction Functions (Adapted to your 'embeddings' schema) ---\n",
        "def store_document_embedding(title: str, url: str, content: str, tokens: int):\n",
        "    \"\"\"\n",
        "    Stores document content and its embedding in the 'embeddings' table.\n",
        "    \"\"\"\n",
        "    embedding = get_embedding(content)\n",
        "    if embedding is None:\n",
        "        print(f\"Skipping storage for content (title: {title}) due to embedding error.\")\n",
        "        return\n",
        "\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=DB_NAME, user=DB_USER, password=DB_PASS, host=DB_HOST, port=DB_PORT\n",
        "        )\n",
        "        register_vector(conn) # Register vector type for this specific connection\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO embeddings (title, url, content, tokens, embedding)\n",
        "            VALUES (%s, %s, %s, %s, %s)\n",
        "            \"\"\",\n",
        "            (title, url, content, tokens, embedding)\n",
        "        )\n",
        "        conn.commit()\n",
        "        print(f\"Stored document: '{title}' (content: '{content[:50]}...')\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error storing document '{title}': {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "def search_similar_documents(query_text: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Searches the 'embeddings' table for documents semantically similar to the query.\n",
        "    Returns the 'content', 'title', and 'url' of the top_k most similar documents.\n",
        "    \"\"\"\n",
        "    query_embedding = get_embedding(query_text)\n",
        "    if query_embedding is None:\n",
        "        print(f\"Skipping search for query: '{query_text}' due to embedding error.\")\n",
        "        return []\n",
        "\n",
        "    conn = None\n",
        "    results = []\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=DB_NAME, user=DB_USER, password=DB_PASS, host=DB_HOST, port=DB_PORT\n",
        "        )\n",
        "        register_vector(conn) # Register vector type for this specific connection\n",
        "        cur = conn.cursor(cursor_factory=RealDictCursor) # To get results as dictionaries\n",
        "\n",
        "        # <-> is the L2 distance operator, which works well for normalized embeddings\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            SELECT title, url, content\n",
        "            FROM embeddings\n",
        "            ORDER BY embedding <-> %s\n",
        "            LIMIT %s\n",
        "            \"\"\",\n",
        "            (query_embedding, top_k)\n",
        "        )\n",
        "        results = cur.fetchall()\n",
        "        print(f\"Found {len(results)} relevant documents for query: '{query_text[:50]}...'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching documents for query '{query_text}': {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARAG"
      ],
      "metadata": {
        "id": "OsVnnRRcCNzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import os\n",
        "import psycopg2\n",
        "from psycopg2.extras import RealDictCursor\n",
        "\n",
        "# Attempt to import pgvector's psycopg2 integration.\n",
        "# This assumes pgvector is correctly installed and compiled as per your !make commands.\n",
        "try:\n",
        "    from pgvector.psycopg2 import register_vector\n",
        "except ImportError:\n",
        "    print(\"WARNING: pgvector.psycopg2 not found. Ensure pgvector is installed and compiled.\")\n",
        "    # Define a mock if pgvector is not available for pure conceptual run\n",
        "    def register_vector(conn):\n",
        "        print(\"Mock: pgvector.psycopg2.register_vector called.\")\n",
        "\n",
        "# --- 1. Data Representation (Conceptual Classes/Dictionaries) ---\n",
        "class UserContext:\n",
        "    \"\"\"Represents the combined long-term and session user context.\"\"\"\n",
        "    def __init__(self, long_term_data: Any, session_data: Any):\n",
        "        self.long_term_data = long_term_data  # e.g., list of past interactions, text summaries\n",
        "        self.session_data = session_data      # e.g., list of recent interactions, current query\n",
        "\n",
        "class Item:\n",
        "    \"\"\"Represents a candidate item with its metadata.\"\"\"\n",
        "    def __init__(self, item_id: str, metadata: Dict[str, Any]):\n",
        "        self.item_id = item_id\n",
        "        self.metadata = metadata # e.g., {'title': 'Dasein Hobo Handbag', 'description': 'vegan leather, checkered'}\n",
        "\n",
        "# --- 2. Configuration for Agent ---\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "\n",
        "# --- 3. Google Colab / Gemini API Imports and Configuration ---\n",
        "# Mock the `google.generativeai` module if not truly installed, for conceptual run\n",
        "class MockGenAIModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "    def generate_content(self, prompt, generation_config):\n",
        "        # These are the conceptual mock responses for the LLM calls in the ARAG agents\n",
        "        if \"Summarize the user's generic interests\" in prompt:\n",
        "            return MockResponse(\"The user shows interest in women's fashion, especially vegan leather accessories with checkered design, with a focus on stylish, functional handbags, based on their Browse history.\")\n",
        "        elif \"Evaluate the semantic alignment\" in prompt:\n",
        "            if \"Dasein Hobo Handbag\" in prompt: return MockResponse(\"Score: 0.85 (Good Match)\")\n",
        "            if \"BUTIED Checkered Tote Shoulder Handbag\" in prompt: return MockResponse(\"Score: 0.98 (Excellent Match)\")\n",
        "            if \"Leather Belt\" in prompt: return MockResponse(\"Score: 0.1 (No Match)\")\n",
        "            return MockResponse(\"Score: 0.7 (Aligned)\")\n",
        "        elif \"Summarize the following textual metadata\" in prompt:\n",
        "            return MockResponse(\"Documents highlight PU vegan leather bags with checkered patterns—mainly totes, crossbody, and shoulder styles from BUTIED, GOWELL, and RICHPORTS offer stylish, versatile designs for various occasions.\")\n",
        "        elif \"rank the items\" in prompt:\n",
        "            # Conceptual ranking based on the example in the paper's text\n",
        "            return MockResponse(\"['item_2', 'item_1', 'item_4']\") # BUTIED, Dasein, Women's Large Tote (conceptual ranking)\n",
        "        return MockResponse(\"Conceptual LLM response from MockGenAIModel.\")\n",
        "\n",
        "class MockResponse:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "GOOGLE_API_KEY = None\n",
        "try:\n",
        "    # This line will only work if running in Google Colab with `userdata` available\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "    import google.generativeai as genai # Only import if in Colab and key found\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Not running in Google Colab or 'GEMINI' secret not found. Attempting to get 'GEMINI' environment variable.\")\n",
        "    GOOGLE_API_KEY = os.getenv('GEMINI')\n",
        "    # If not in Colab, assume genai is installed globally if GOOGLE_API_KEY is found\n",
        "    if GOOGLE_API_KEY:\n",
        "        try:\n",
        "            import google.generativeai as genai\n",
        "        except ImportError:\n",
        "            print(\"WARNING: 'google-generativeai' library not found. Gemini API calls will be mocked.\")\n",
        "            genai = None # Set to None if not found\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"WARNING: GEMINI API Key not found. Gemini API calls will be mocked or fail if a real call is attempted.\")\n",
        "else:\n",
        "    if genai: # Only configure if the module was successfully imported\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        print(\"Gemini API configured for real calls.\")\n",
        "    else:\n",
        "        print(\"Gemini API key found, but 'google-generativeai' library not available. Gemini API calls will be mocked.\")\n",
        "\n",
        "# --- OpenAI Client Initialization ---\n",
        "# Assuming colab_env handles setting this from Google Colab Secrets or environment\n",
        "import colab_env\n",
        "from openai import OpenAI\n",
        "try:\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please set it.\")\n",
        "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"OpenAI client initialized.\")\n",
        "except ValueError as e:\n",
        "    print(f\"ERROR: {e}. OpenAI client not initialized. Embedding calls will use a conceptual placeholder.\")\n",
        "    openai_client = None # Set to None if API key is missing\n",
        "\n",
        "# --- Database Connection and Setup ---\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_PASS = \"postgres\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "\n",
        "def setup_database():\n",
        "    \"\"\"Handles the initial database setup as provided in your context.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=DB_NAME,\n",
        "            user=DB_USER,\n",
        "            password=DB_PASS,\n",
        "            host=DB_HOST,\n",
        "            port=DB_PORT\n",
        "        )\n",
        "        conn.autocommit = True\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        print(\"Ensuring pgvector extension is created...\")\n",
        "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "        print(\"pgvector extension checked/created.\")\n",
        "\n",
        "        print(\"Creating 'embeddings' table if it doesn't exist...\")\n",
        "        table_create_command = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS embeddings (\n",
        "                    id BIGSERIAL PRIMARY KEY,\n",
        "                    title TEXT,\n",
        "                    url TEXT,\n",
        "                    content TEXT NOT NULL,\n",
        "                    tokens INTEGER,\n",
        "                    embedding VECTOR(1536) NOT NULL\n",
        "                    );\n",
        "        \"\"\"\n",
        "        cur.execute(table_create_command)\n",
        "        print(\"Table 'embeddings' checked/created.\")\n",
        "\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "        print(\"Database setup complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during database initial setup: {e}\")\n",
        "        if conn:\n",
        "            conn.close()\n",
        "        exit()\n",
        "\n",
        "# Run the database setup once at the start of the script's execution\n",
        "setup_database()\n",
        "\n",
        "# --- OpenAI Embedding Function ---\n",
        "def get_embedding(text: str, model: str = \"text-embedding-3-small\"):\n",
        "    \"\"\"\n",
        "    Generates an embedding for the given text using OpenAI's embedding model.\n",
        "    Converts the list embedding to a numpy array, which pgvector's psycopg2\n",
        "    integration expects.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    if openai_client:\n",
        "        try:\n",
        "            response = openai_client.embeddings.create(input=[text], model=model)\n",
        "            return np.array(response.data[0].embedding)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting embedding for text: '{text[:50]}...'. Error: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"OpenAI client not initialized. Returning conceptual embedding for: '{text[:50]}...'\")\n",
        "        return np.random.rand(1536) # Conceptual embedding if OpenAI is not configured\n",
        "\n",
        "# --- PostgreSQL Interaction Functions ---\n",
        "def store_document_embedding(title: str, url: str, content: str, tokens: int):\n",
        "    \"\"\"\n",
        "    Stores document content and its embedding in the 'embeddings' table.\n",
        "    \"\"\"\n",
        "    embedding = get_embedding(content)\n",
        "    if embedding is None:\n",
        "        print(f\"Skipping storage for content (title: {title}) due to embedding error.\")\n",
        "        return\n",
        "\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=DB_NAME, user=DB_USER, password=DB_PASS, host=DB_HOST, port=DB_PORT\n",
        "        )\n",
        "        register_vector(conn)\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO embeddings (title, url, content, tokens, embedding)\n",
        "            VALUES (%s, %s, %s, %s, %s)\n",
        "            \"\"\",\n",
        "            (title, url, content, tokens, embedding)\n",
        "        )\n",
        "        conn.commit()\n",
        "        print(f\"Stored document: '{title}' (content: '{content[:50]}...')\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error storing document '{title}': {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "def search_similar_documents(query_text: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Searches the 'embeddings' table for documents semantically similar to the query.\n",
        "    Returns the 'content', 'title', and 'url' of the top_k most similar documents.\n",
        "    \"\"\"\n",
        "    query_embedding = get_embedding(query_text)\n",
        "    if query_embedding is None:\n",
        "        print(f\"Skipping search for query: '{query_text}' due to embedding error.\")\n",
        "        return []\n",
        "\n",
        "    conn = None\n",
        "    results = []\n",
        "    try:\n",
        "        conn = psycopg2.connect(\n",
        "            database=DB_NAME, user=DB_USER, password=DB_PASS, host=DB_HOST, port=DB_PORT\n",
        "        )\n",
        "        register_vector(conn)\n",
        "        cur = conn.cursor(cursor_factory=RealDictCursor)\n",
        "\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            SELECT title, url, content, embedding <-> %s AS distance\n",
        "            FROM embeddings\n",
        "            ORDER BY distance\n",
        "            LIMIT %s\n",
        "            \"\"\",\n",
        "            (query_embedding, top_k)\n",
        "        )\n",
        "        results = cur.fetchall()\n",
        "        print(f\"Found {len(results)} relevant documents for query: '{query_text[:50]}...'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching documents for query '{query_text}': {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "    return results\n",
        "\n",
        "# --- Core LLM Call Function (using Gemini configuration) ---\n",
        "def actual_llm_call(prompt: str, model_name: str = AgentConfig.LLM_MODEL_NAME, temperature: float = 0.0) -> str:\n",
        "    \"\"\"\n",
        "    Function to make an LLM API call using the configured Gemini model.\n",
        "    This will attempt a real Gemini call if `genai` is available and configured,\n",
        "    otherwise it will use the mock.\n",
        "    \"\"\"\n",
        "    print(f\"Calling LLM ({model_name}) with prompt snippet: '{prompt[:70]}...'\")\n",
        "\n",
        "    try:\n",
        "        if genai and GOOGLE_API_KEY:\n",
        "            model = genai.GenerativeModel(model_name)\n",
        "            response = model.generate_content(prompt, generation_config={\"temperature\": temperature})\n",
        "            return response.text\n",
        "        else:\n",
        "            mock_model = MockGenAIModel(model_name)\n",
        "            response = mock_model.generate_content(prompt, generation_config={\"temperature\": temperature})\n",
        "            return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM call: {e}\")\n",
        "        return \"Error: LLM call failed.\"\n",
        "\n",
        "# --- Helper function for cosine similarity (now defined before ARAG_Framework) ---\n",
        "def conceptual_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Conceptual function to calculate cosine similarity between two embeddings.\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(embedding1, embedding2)\n",
        "    norm_a = np.linalg.norm(embedding1)\n",
        "    norm_b = np.linalg.norm(embedding2)\n",
        "    if norm_a == 0 or norm_b == 0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "# --- ARAG Agents (using `actual_llm_call` for Gemini) ---\n",
        "\n",
        "class UserUnderstandingAgent:\n",
        "    def __init__(self, llm_caller=actual_llm_call):\n",
        "        self.llm_caller = llm_caller\n",
        "\n",
        "    def generate_summary(self, user_context: UserContext) -> str:\n",
        "        # Refined prompt for User Understanding\n",
        "        prompt = (\n",
        "            \"Analyze the user's long-term behavioral data and current session interactions.\\n\"\n",
        "            f\"Long-term data: {user_context.long_term_data}\\n\"\n",
        "            f\"Current session data: {user_context.session_data}\\n\"\n",
        "            \"Identify and summarize the user's core generic interests, specific product preferences (e.g., categories, materials, styles), \"\n",
        "            \"and their likely immediate goals or intent based on the current session. \"\n",
        "            \"Format the summary clearly, separating generic interests, specific preferences, and immediate goals.\"\n",
        "        )\n",
        "        s_user = self.llm_caller(prompt)\n",
        "        return s_user\n",
        "\n",
        "class NLI_Agent:\n",
        "    def __init__(self, llm_caller=actual_llm_call):\n",
        "        self.llm_caller = llm_caller\n",
        "\n",
        "    def evaluate_alignment(self, item: Item, user_context: UserContext) -> float:\n",
        "        # Refined prompt for NLI\n",
        "        prompt = (\n",
        "            \"Given the user's context and an item's metadata, evaluate the semantic alignment.\\n\"\n",
        "            f\"User context (summary of interests and goals): {user_context.long_term_data} and {user_context.session_data}\\n\"\n",
        "            f\"Item metadata: {item.metadata}\\n\"\n",
        "            \"Based on the item's attributes and description, determine how well it aligns with the user's preferences and current intent. \"\n",
        "            \"Consider factors like category, material, style, function, and any explicit preferences mentioned in the user context. \"\n",
        "            \"Provide a semantic alignment score between 0.0 (no alignment) and 1.0 (perfect alignment). \"\n",
        "            \"Your response must start with 'Score:' followed by the numerical score, e.g., 'Score: 0.92'.\"\n",
        "        )\n",
        "        response = self.llm_caller(prompt)\n",
        "        try:\n",
        "            # Robustly extract the score\n",
        "            score_str = response.split(\"Score:\")[1].strip().split(\" \")[0]\n",
        "            return float(score_str)\n",
        "        except (IndexError, ValueError):\n",
        "            print(f\"Warning: Could not extract score from NLI response: {response}\")\n",
        "            return 0.5 # Default to a neutral score on parsing failure\n",
        "\n",
        "class ContextSummaryAgent:\n",
        "    def __init__(self, llm_caller=actual_llm_call):\n",
        "        self.llm_caller = llm_caller\n",
        "\n",
        "    def summarize_context(self, accepted_items: List[Item]) -> str:\n",
        "        if not accepted_items:\n",
        "            return \"No relevant context items were accepted.\"\n",
        "\n",
        "        metadata_to_summarize = \"\\n\".join([f\"- {item.metadata}\" for item in accepted_items])\n",
        "        # Refined prompt for Context Summary\n",
        "        prompt = (\n",
        "            \"Analyze the metadata of the following items that have been identified as semantically aligned with the user's intent:\\n\"\n",
        "            f\"{metadata_to_summarize}\\n\"\n",
        "            \"Synthesize the key themes, product attributes, categories, and styles present across these items. \"\n",
        "            \"Provide a concise summary that captures the common characteristics and overall profile of these relevant items. \"\n",
        "            \"This summary should help in the final ranking by highlighting what the user is currently interested in based on aligned items.\"\n",
        "        )\n",
        "        s_ctx = self.llm_caller(prompt)\n",
        "        return s_ctx\n",
        "\n",
        "class ItemRankerAgent:\n",
        "    def __init__(self, llm_caller=actual_llm_call):\n",
        "        self.llm_caller = llm_caller\n",
        "\n",
        "    def rank_items(self, s_user: str, s_ctx: str, candidate_items: List[Item]) -> List[Item]:\n",
        "        if not candidate_items:\n",
        "            return []\n",
        "\n",
        "        item_descriptions = \"\\n\".join([f\"- Item ID: {item.item_id}, Metadata: {item.metadata}\" for item in candidate_items])\n",
        "        # Refined prompt for Item Ranker\n",
        "        prompt = (\n",
        "            \"Given the user's summarized preferences and goals, the synthesized context from items already deemed relevant, and a list of candidate items,\\n\"\n",
        "            f\"User Preferences Summary: {s_user}\\n\"\n",
        "            f\"Context Summary of Aligned Items: {s_ctx}\\n\"\n",
        "            \"Candidate Items to Rank:\\n\"\n",
        "            f\"{item_descriptions}\\n\\n\"\n",
        "            \"Carefully consider the user's detailed interests (from S_user) and the common features of the aligned items (from S_ctx). \"\n",
        "            \"Evaluate each candidate item's metadata against these summaries to determine its potential relevance and appeal to the user *right now*. \"\n",
        "            \"Rank the Candidate Items from most likely to be purchased to least likely. \"\n",
        "            \"Your response MUST be a Python list of only the Item IDs in the ranked order. Example: ['item_id_1', 'item_id_5', 'item_id_2']\"\n",
        "        )\n",
        "        ranked_list_str = self.llm_caller(prompt)\n",
        "        try:\n",
        "            # Attempt to parse the list safely\n",
        "            ranked_item_ids = eval(ranked_list_str)\n",
        "            if not isinstance(ranked_item_ids, list):\n",
        "                 raise ValueError(\"LLM response is not a list.\")\n",
        "\n",
        "            # Create a mapping from item_id to Item object for efficient lookup\n",
        "            item_map = {item.item_id: item for item in candidate_items}\n",
        "\n",
        "            # Build the ranked list of Item objects based on the parsed IDs\n",
        "            final_ranked_items = [item_map[item_id] for item_id in ranked_item_ids if item_id in item_map]\n",
        "\n",
        "            # Include any items not returned by the LLM at the end, maintaining their original relative order from candidates\n",
        "            # This ensures all candidates are included, even if the LLM misses some IDs\n",
        "            items_not_ranked_by_llm = [item for item in candidate_items if item.item_id not in ranked_item_ids]\n",
        "            final_ranked_items.extend(items_not_ranked_by_llm)\n",
        "\n",
        "            return final_ranked_items\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not parse or process ranked list from LLM response ({e}). Response was: {ranked_list_str}. Returning original candidates.\")\n",
        "            return candidate_items # Return original list as a fallback\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- ARAG Framework (Orchestration) ---\n",
        "\n",
        "class ARAG_Framework:\n",
        "    \"\"\"\n",
        "    Orchestrates the multi-agent collaboration for personalized recommendation.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_func=get_embedding,\n",
        "                 similarity_func=conceptual_cosine_similarity,\n",
        "                 llm_caller=actual_llm_call,\n",
        "                 nli_threshold: float = 0.7):\n",
        "        self.embedding_func = embedding_func\n",
        "        self.similarity_func = similarity_func\n",
        "        self.nli_threshold = nli_threshold\n",
        "\n",
        "        self.user_understanding_agent = UserUnderstandingAgent(llm_caller)\n",
        "        self.nli_agent = NLI_Agent(llm_caller)\n",
        "        self.context_summary_agent = ContextSummaryAgent(llm_caller)\n",
        "        self.item_ranker_agent = ItemRankerAgent(llm_caller)\n",
        "\n",
        "    def recommend(self, user_context: UserContext, all_candidate_items: List[Item], top_k_initial_retrieval: int = 100) -> List[Item]:\n",
        "        print(\"\\n--- ARAG Recommendation Process Started ---\")\n",
        "\n",
        "        print(\"\\n1. Initial Retrieval (Cosine Similarity-based RAG)\")\n",
        "        user_embedding = self.embedding_func(user_context.long_term_data + \" \" + user_context.session_data)\n",
        "        item_similarities = []\n",
        "        for item in all_candidate_items:\n",
        "            # Ensure 'title' and 'description' keys exist before accessing, provide defaults if not\n",
        "            item_text_for_embedding = item.metadata.get('title', '') + \" \" + item.metadata.get('description', '')\n",
        "            item_embedding = self.embedding_func(item_text_for_embedding)\n",
        "            if user_embedding is not None and item_embedding is not None:\n",
        "                similarity = self.similarity_func(item_embedding, user_embedding)\n",
        "                item_similarities.append((item, similarity))\n",
        "            else:\n",
        "                print(f\"Skipping similarity for item {item.item_id} due to missing embeddings.\")\n",
        "\n",
        "        item_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        initial_recall_set_items = [item for item, _ in item_similarities[:top_k_initial_retrieval]]\n",
        "        print(f\"Initial recall set size: {len(initial_recall_set_items)}\")\n",
        "\n",
        "        print(\"\\n2. Parallel Agent Execution (User Understanding & NLI)\")\n",
        "        s_user = self.user_understanding_agent.generate_summary(user_context)\n",
        "        print(f\"User Understanding Agent Summary (S_user): {s_user}\")\n",
        "\n",
        "        accepted_items: List[Item] = []\n",
        "        for item in initial_recall_set_items:\n",
        "            score = self.nli_agent.evaluate_alignment(item, user_context)\n",
        "            if score >= self.nli_threshold:\n",
        "                accepted_items.append(item)\n",
        "        print(f\"NLI Agent filtered {len(initial_recall_set_items) - len(accepted_items)} items. Accepted: {len(accepted_items)}\")\n",
        "\n",
        "        print(\"\\n3. Context Summary Agent\")\n",
        "        s_ctx = self.context_summary_agent.summarize_context(accepted_items)\n",
        "        print(f\"Context Summary Agent (S_ctx): {s_ctx}\")\n",
        "\n",
        "        print(\"\\n4. Item Ranker Agent\")\n",
        "        final_ranked_list = self.item_ranker_agent.rank_items(s_user, s_ctx, initial_recall_set_items)\n",
        "        print(\"--- ARAG Recommendation Process Finished ---\")\n",
        "\n",
        "        return final_ranked_list\n",
        "\n",
        "# --- Conceptual Usage Example ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Data Ingestion Example ---\")\n",
        "    documents_to_ingest = [\n",
        "    {\"title\": \"Dasein Hobo Handbag\", \"url\": \"url_hobo\", \"content\": \"Classic hobo style, made of high-quality vegan leather. Perfect for everyday use.\", \"tokens\": 20},\n",
        "    {\"title\": \"BUTIED Checkered Tote Shoulder Handbag\", \"url\": \"url_butied\", \"content\": \"A stylish and functional tote bag featuring a unique checkered pattern, crafted from durable PU vegan leather. Ideal for various occasions.\", \"tokens\": 30},\n",
        "    {\"title\": \"GOWELL Checkered Tote\", \"url\": \"url_gowell\", \"content\": \"Spacious tote bag with a classic checkered design, vegan-friendly material. Great for carrying essentials.\", \"tokens\": 25},\n",
        "    {\"title\": \"Women's Large Tote\", \"url\": \"url_large_tote\", \"content\": \"A basic large tote bag made of synthetic material. Simple design for casual outings.\", \"tokens\": 18},\n",
        "    {\"title\": \"Leather Belt\", \"url\": \"url_belt\", \"content\": \"Genuine cowhide leather belt, available in various sizes and colors. Durable and classic.\", \"tokens\": 20},\n",
        "    {\"title\": \"Casual Pants\", \"url\": \"url_pants\", \"content\": \"Comfortable cotton blend casual pants for everyday wear. Relaxed fit.\", \"tokens\": 15},\n",
        "    {\"title\": \"Stylish Canvas Backpack\", \"url\": \"url_backpack1\", \"content\": \"Durable canvas backpack with multiple compartments, ideal for school or travel.\", \"tokens\": 25},\n",
        "    {\"title\": \"Minimalist Leather Wallet\", \"url\": \"url_wallet1\", \"content\": \"Slim genuine leather wallet with RFID blocking technology, holds up to 8 cards.\", \"tokens\": 22},\n",
        "    {\"title\": \"Wireless Bluetooth Earbuds\", \"url\": \"url_earbuds1\", \"content\": \"High-fidelity wireless earbuds with noise cancellation and 24-hour battery life.\", \"tokens\": 30},\n",
        "    {\"title\": \"Ergonomic Office Chair\", \"url\": \"url_chair1\", \"content\": \"Adjustable ergonomic chair with lumbar support and mesh back for comfort during long hours.\", \"tokens\": 35},\n",
        "    {\"title\": \"Stainless Steel Water Bottle\", \"url\": \"url_bottle1\", \"content\": \"Insulated stainless steel water bottle keeps drinks cold for 24 hours or hot for 12.\", \"tokens\": 28},\n",
        "    {\"title\": \"Yoga Mat Non-Slip\", \"url\": \"url_yogamat1\", \"content\": \"Eco-friendly TPE yoga mat with excellent grip for all types of yoga and Pilates.\", \"tokens\": 20},\n",
        "    {\"title\": \"Smart Home Hub\", \"url\": \"url_smarthub1\", \"content\": \"Central hub to control all your smart home devices, compatible with multiple protocols.\", \"tokens\": 32},\n",
        "    {\"title\": \"Portable External Hard Drive\", \"url\": \"url_hdd1\", \"content\": \"1TB USB 3.0 portable external hard drive, fast data transfer speeds.\", \"tokens\": 25},\n",
        "    {\"title\": \"Digital Kitchen Scale\", \"url\": \"url_scale1\", \"content\": \"High-precision digital kitchen scale with tare function, measures up to 5kg.\", \"tokens\": 23},\n",
        "    {\"title\": \"Beginner Acoustic Guitar Kit\", \"url\": \"url_guitar1\", \"content\": \"Full-size acoustic guitar kit including gig bag, picks, strap, and tuner.\", \"tokens\": 30},\n",
        "    {\"title\": \"Running Shoes Men's\", \"url\": \"url_shoes1\", \"content\": \"Lightweight and breathable running shoes for men, provides excellent cushioning and support.\", \"tokens\": 28},\n",
        "    {\"title\": \"Coding for Beginners Book\", \"url\": \"url_book1\", \"content\": \"An introductory guide to programming with Python, perfect for absolute beginners.\", \"tokens\": 25},\n",
        "    {\"title\": \"Indoor Plant Set (3)\", \"url\": \"url_plant1\", \"content\": \"Set of three easy-care indoor plants to beautify your living space.\", \"tokens\": 20},\n",
        "    {\"title\": \"Resistance Band Set\", \"url\": \"url_bands1\", \"content\": \"Set of 5 resistance bands with varying levels of tension for strength training and physical therapy.\", \"tokens\": 35},\n",
        "    {\"title\": \"Noise Cancelling Headphones\", \"url\": \"url_headphones1\", \"content\": \"Over-ear noise cancelling headphones with superior sound quality and comfortable fit.\", \"tokens\": 30},\n",
        "    {\"title\": \"Desk Lamp with Wireless Charger\", \"url\": \"url_desklamp1\", \"content\": \"Modern desk lamp with adjustable brightness and integrated wireless phone charger.\", \"tokens\": 32},\n",
        "    {\"title\": \"Travel Pillow Memory Foam\", \"url\": \"url_travelpillow1\", \"content\": \"Ergonomic memory foam travel pillow for comfortable sleep on flights or road trips.\", \"tokens\": 25},\n",
        "    {\"title\": \"Art Drawing Kit\", \"url\": \"url_artkit1\", \"content\": \"Comprehensive art drawing kit including pencils, charcoal, erasers, and sketchpad.\", \"tokens\": 28},\n",
        "    {\"title\": \"Electric Kettle Fast Boil\", \"url\": \"url_kettle1\", \"content\": \"1.7L electric kettle with fast boiling feature and automatic shut-off.\", \"tokens\": 22},\n",
        "    {\"title\": \"Board Game Strategy\", \"url\": \"url_boardgame1\", \"content\": \"A popular strategy board game for 2-4 players, challenging and engaging gameplay.\", \"tokens\": 26},\n",
        "]\n",
        "\n",
        "    for doc in documents_to_ingest:\n",
        "        store_document_embedding(doc[\"title\"], doc[\"url\"], doc[\"content\"], doc[\"tokens\"])\n",
        "    print(\"--- Data Ingestion Complete ---\")\n",
        "\n",
        "    user_long_term = \"User has a history of purchasing vegan leather handbags, specifically tote and crossbody styles. Likes checkered patterns.\"\n",
        "    user_session = \"Recently viewed several stylish, functional handbags.\"\n",
        "    user_context = UserContext(user_long_term, user_session)\n",
        "\n",
        "    # Convert ingested documents into Item objects for the ARAG framework\n",
        "    all_arag_items = [\n",
        "        Item(doc[\"title\"], {\"title\": doc[\"title\"], \"description\": doc[\"content\"]})\n",
        "        for doc in documents_to_ingest\n",
        "    ]\n",
        "\n",
        "    arag = ARAG_Framework()\n",
        "    recommended_items = arag.recommend(user_context, all_arag_items, top_k_initial_retrieval=len(all_arag_items))\n",
        "\n",
        "    print(\"\\nConceptual Final Recommended Items (Ordered):\")\n",
        "    for item in recommended_items:\n",
        "        print(f\"- {item.item_id}: {item.metadata['title']}\")\n",
        "\n",
        "    print(\"\\n--- Direct Search Example from PGVector ---\")\n",
        "    query = \"stylish vegan leather checkered tote bag\"\n",
        "    search_results = search_similar_documents(query, top_k=2)\n",
        "    for res in search_results:\n",
        "        print(f\"Title: {res['title']}, Content: {res['content'][:70]}..., Distance: {res['distance']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jzLMU6BOB9Iw",
        "outputId": "efc535f0-48b1-4f02-cd78-5967103e656c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Gemini API configured for real calls.\n",
            "OpenAI client initialized.\n",
            "Ensuring pgvector extension is created...\n",
            "pgvector extension checked/created.\n",
            "Creating 'embeddings' table if it doesn't exist...\n",
            "Table 'embeddings' checked/created.\n",
            "Database setup complete.\n",
            "\n",
            "--- Data Ingestion Example ---\n",
            "Stored document: 'Dasein Hobo Handbag' (content: 'Classic hobo style, made of high-quality vegan lea...')\n",
            "Stored document: 'BUTIED Checkered Tote Shoulder Handbag' (content: 'A stylish and functional tote bag featuring a uniq...')\n",
            "Stored document: 'GOWELL Checkered Tote' (content: 'Spacious tote bag with a classic checkered design,...')\n",
            "Stored document: 'Women's Large Tote' (content: 'A basic large tote bag made of synthetic material....')\n",
            "Stored document: 'Leather Belt' (content: 'Genuine cowhide leather belt, available in various...')\n",
            "Stored document: 'Casual Pants' (content: 'Comfortable cotton blend casual pants for everyday...')\n",
            "Stored document: 'Stylish Canvas Backpack' (content: 'Durable canvas backpack with multiple compartments...')\n",
            "Stored document: 'Minimalist Leather Wallet' (content: 'Slim genuine leather wallet with RFID blocking tec...')\n",
            "Stored document: 'Wireless Bluetooth Earbuds' (content: 'High-fidelity wireless earbuds with noise cancella...')\n",
            "Stored document: 'Ergonomic Office Chair' (content: 'Adjustable ergonomic chair with lumbar support and...')\n",
            "Stored document: 'Stainless Steel Water Bottle' (content: 'Insulated stainless steel water bottle keeps drink...')\n",
            "Stored document: 'Yoga Mat Non-Slip' (content: 'Eco-friendly TPE yoga mat with excellent grip for ...')\n",
            "Stored document: 'Smart Home Hub' (content: 'Central hub to control all your smart home devices...')\n",
            "Stored document: 'Portable External Hard Drive' (content: '1TB USB 3.0 portable external hard drive, fast dat...')\n",
            "Stored document: 'Digital Kitchen Scale' (content: 'High-precision digital kitchen scale with tare fun...')\n",
            "Stored document: 'Beginner Acoustic Guitar Kit' (content: 'Full-size acoustic guitar kit including gig bag, p...')\n",
            "Stored document: 'Running Shoes Men's' (content: 'Lightweight and breathable running shoes for men, ...')\n",
            "Stored document: 'Coding for Beginners Book' (content: 'An introductory guide to programming with Python, ...')\n",
            "Stored document: 'Indoor Plant Set (3)' (content: 'Set of three easy-care indoor plants to beautify y...')\n",
            "Stored document: 'Resistance Band Set' (content: 'Set of 5 resistance bands with varying levels of t...')\n",
            "Stored document: 'Noise Cancelling Headphones' (content: 'Over-ear noise cancelling headphones with superior...')\n",
            "Stored document: 'Desk Lamp with Wireless Charger' (content: 'Modern desk lamp with adjustable brightness and in...')\n",
            "Stored document: 'Travel Pillow Memory Foam' (content: 'Ergonomic memory foam travel pillow for comfortabl...')\n",
            "Stored document: 'Art Drawing Kit' (content: 'Comprehensive art drawing kit including pencils, c...')\n",
            "Stored document: 'Electric Kettle Fast Boil' (content: '1.7L electric kettle with fast boiling feature and...')\n",
            "Stored document: 'Board Game Strategy' (content: 'A popular strategy board game for 2-4 players, cha...')\n",
            "--- Data Ingestion Complete ---\n",
            "\n",
            "--- ARAG Recommendation Process Started ---\n",
            "\n",
            "1. Initial Retrieval (Cosine Similarity-based RAG)\n",
            "Initial recall set size: 26\n",
            "\n",
            "2. Parallel Agent Execution (User Understanding & NLI)\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Analyze the user's long-term behavioral data and current session inter...'\n",
            "User Understanding Agent Summary (S_user): Based on the provided data, here's an analysis of the user's interests and goals:\n",
            "\n",
            "---\n",
            "\n",
            "**User Behavioral Analysis Summary**\n",
            "\n",
            "**1. Core Generic Interests:**\n",
            "*   The user has a strong general interest in **handbags** as an accessory.\n",
            "*   They value both **style and functionality** in their product choices.\n",
            "\n",
            "**2. Specific Product Preferences:**\n",
            "*   **Material:** Exclusively prefers **vegan leather**.\n",
            "*   **Styles/Types:** Favors **tote and crossbody** handbag styles.\n",
            "*   **Patterns/Designs:** Has a clear liking for **checkered patterns**.\n",
            "\n",
            "**3. Likely Immediate Goals or Intent (Current Session):**\n",
            "*   The user is actively in the **research and exploration phase** for a new handbag.\n",
            "*   They are specifically looking for a bag that meets their criteria of being **stylish and functional**, likely with the intent to **make a purchase in the near future** once they find a suitable option that aligns with their established preferences (vegan leather, tote/crossbody, potentially checkered).\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's context and an item's metadata, evaluate the semantic...'\n",
            "NLI Agent filtered 23 items. Accepted: 3\n",
            "\n",
            "3. Context Summary Agent\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Analyze the metadata of the following items that have been identified ...'\n",
            "Context Summary Agent (S_ctx): Based on the metadata of the semantically aligned items, here's an analysis of the user's current interests:\n",
            "\n",
            "**Key Themes & Attributes:**\n",
            "\n",
            "*   **Material Preference:** A very strong and consistent preference for **vegan leather** or **vegan-friendly materials**. This is a defining characteristic across all items.\n",
            "*   **Bag Type:** Primarily interested in **Tote Bags**, with a secondary interest in **Hobo Handbags**.\n",
            "*   **Design/Pattern:** A clear inclination towards **checkered patterns** (appearing in two out of three items), alongside a general appreciation for **classic styles**.\n",
            "*   **Functionality:** Emphasis on **spaciousness**, **durability**, and **practicality** for everyday use or various occasions.\n",
            "*   **Aesthetics:** Described as **stylish** and **classic**.\n",
            "\n",
            "**Categories & Styles:**\n",
            "\n",
            "*   **Category:** Handbags (specifically Tote Bags and Hobo Bags)\n",
            "*   **Style:** Checkered, Classic, Shoulder Bag\n",
            "\n",
            "---\n",
            "\n",
            "**Concise Summary for Ranking:**\n",
            "\n",
            "The user's interest centers on **vegan leather handbags**, primarily **tote bags** and potentially **hobo styles**. There's a strong preference for **checkered patterns** and **classic designs**. Key functional attributes include **spaciousness, durability, and suitability for everyday or versatile use**. Overall, the user is looking for stylish yet practical vegan-friendly bags that can accommodate daily essentials.\n",
            "\n",
            "4. Item Ranker Agent\n",
            "Calling LLM (gemini-2.5-flash) with prompt snippet: 'Given the user's summarized preferences and goals, the synthesized con...'\n",
            "--- ARAG Recommendation Process Finished ---\n",
            "\n",
            "Conceptual Final Recommended Items (Ordered):\n",
            "- BUTIED Checkered Tote Shoulder Handbag: BUTIED Checkered Tote Shoulder Handbag\n",
            "- GOWELL Checkered Tote: GOWELL Checkered Tote\n",
            "- Dasein Hobo Handbag: Dasein Hobo Handbag\n",
            "- Women's Large Tote: Women's Large Tote\n",
            "- Stylish Canvas Backpack: Stylish Canvas Backpack\n",
            "- Art Drawing Kit: Art Drawing Kit\n",
            "- Beginner Acoustic Guitar Kit: Beginner Acoustic Guitar Kit\n",
            "- Board Game Strategy: Board Game Strategy\n",
            "- Casual Pants: Casual Pants\n",
            "- Coding for Beginners Book: Coding for Beginners Book\n",
            "- Desk Lamp with Wireless Charger: Desk Lamp with Wireless Charger\n",
            "- Digital Kitchen Scale: Digital Kitchen Scale\n",
            "- Electric Kettle Fast Boil: Electric Kettle Fast Boil\n",
            "- Ergonomic Office Chair: Ergonomic Office Chair\n",
            "- Indoor Plant Set (3): Indoor Plant Set (3)\n",
            "- Noise Cancelling Headphones: Noise Cancelling Headphones\n",
            "- Portable External Hard Drive: Portable External Hard Drive\n",
            "- Resistance Band Set: Resistance Band Set\n",
            "- Running Shoes Men's: Running Shoes Men's\n",
            "- Smart Home Hub: Smart Home Hub\n",
            "- Stainless Steel Water Bottle: Stainless Steel Water Bottle\n",
            "- Travel Pillow Memory Foam: Travel Pillow Memory Foam\n",
            "- Wireless Bluetooth Earbuds: Wireless Bluetooth Earbuds\n",
            "- Yoga Mat Non-Slip: Yoga Mat Non-Slip\n",
            "- Leather Belt: Leather Belt\n",
            "- Minimalist Leather Wallet: Minimalist Leather Wallet\n",
            "\n",
            "--- Direct Search Example from PGVector ---\n",
            "Found 2 relevant documents for query: 'stylish vegan leather checkered tote bag...'\n",
            "Title: BUTIED Checkered Tote Shoulder Handbag, Content: A stylish and functional tote bag featuring a unique checkered pattern..., Distance: 0.6462\n",
            "Title: BUTIED Checkered Tote Shoulder Handbag, Content: A stylish and functional tote bag featuring a unique checkered pattern..., Distance: 0.6463\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOk+zoL6BQyzFrw9qdSSDQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/NOBIAS_TRANSFOMER_APRIL2025_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch tqdm -q"
      ],
      "metadata": {
        "id": "d6nmLVQ0wrrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "rXuWWLQW7M_e",
        "outputId": "de86131e-7faf-4d7a-d553-f8add9f00ca3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 18 08:03:59 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   77C    P0             35W /   72W |     961MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#!pip install transformers torch tqdm nltk -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from warnings import filterwarnings\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections  # Import collections for Counter\n",
        "\n",
        "nltk.download('wordnet')\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "tokenizer_name = \"gpt2\"\n",
        "max_input_len = 128\n",
        "max_output_len = 128\n",
        "batch_size = 4\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 10\n",
        "emb_size = 512\n",
        "nhead = 8\n",
        "num_encoder_layers = 2\n",
        "num_decoder_layers = 2\n",
        "dim_feedforward = 512\n",
        "dropout = 0.1\n",
        "best_model_save_path = \"./best_general_transformer.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- 1. Tokenizer Setup ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "src_vocab_size = len(tokenizer)\n",
        "tgt_vocab_size = len(tokenizer)  # Not used in this version\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "bos_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id\n",
        "eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.sep_token_id\n",
        "\n",
        "# --- 2. Dataset Preparation ---\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_input_len, augment=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "        self.augment = augment  # Not used in this version\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        context = item.get(\"context\", \"\")\n",
        "        question = item[\"question\"]\n",
        "        choices = item[\"choices\"]\n",
        "        answer_label = item[\"answer\"]\n",
        "\n",
        "        # Format input text\n",
        "        input_text = f\"{context} {question} {self.format_choices(choices)}\"\n",
        "\n",
        "        # Tokenization\n",
        "        input_tokens = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_input_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = input_tokens[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = input_tokens[\"attention_mask\"].squeeze(0)  # Not used in this version\n",
        "\n",
        "        # Tokenize answer label\n",
        "        output_tokens = self.tokenizer.encode_plus(\n",
        "            answer_label,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=1,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        output_ids = output_tokens[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,  # Not used in this version\n",
        "            \"output_ids\": output_ids,\n",
        "        }\n",
        "\n",
        "    def format_choices(self, choices):\n",
        "        formatted_choices = \"\"\n",
        "        for label, choice_text in choices.items():\n",
        "            formatted_choices += f\"{label}: {choice_text} \"\n",
        "        return formatted_choices\n",
        "\n",
        "# --- Data ---\n",
        "data = [\n",
        "    {\n",
        "        \"id\": \"q1\",\n",
        "        \"context\": \"Paris is known for its iconic Eiffel Tower and Louvre Museum.\",\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Berlin\",\n",
        "            \"B\": \"Paris\",\n",
        "            \"C\": \"Rome\"\n",
        "        },\n",
        "        \"answer\": \"B\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q2\",\n",
        "        \"context\": \"Mount Everest is the highest mountain above sea level.\",\n",
        "        \"question\": \"Which mountain is the tallest in the world?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"K2\",\n",
        "            \"B\": \"Mount Kilimanjaro\",\n",
        "            \"C\": \"Mount Everest\"\n",
        "        },\n",
        "        \"answer\": \"C\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q3\",\n",
        "        \"question\": \"What is the chemical symbol for gold?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Ag\",\n",
        "            \"B\": \"Au\",\n",
        "            \"C\": \"Fe\"\n",
        "        },\n",
        "        \"answer\": \"B\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q4\",\n",
        "        \"context\": \"The Amazon rainforest is the largest rainforest on Earth.\",\n",
        "        \"question\": \"Where is the Amazon rainforest primarily located?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Africa\",\n",
        "            \"B\": \"South America\",\n",
        "            \"C\": \"Asia\"\n",
        "        },\n",
        "        \"answer\": \"B\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q5\",\n",
        "        \"question\": \"What is the smallest planet in our solar system?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Mercury\",\n",
        "            \"B\": \"Earth\",\n",
        "            \"C\": \"Mars\"\n",
        "        },\n",
        "        \"answer\": \"A\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q6\",\n",
        "        \"context\": \"Light travels faster than sound.\",\n",
        "        \"question\": \"Which travels faster, light or sound?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Sound\",\n",
        "            \"B\": \"Light\",\n",
        "            \"C\": \"They travel at the same speed\"\n",
        "        },\n",
        "        \"answer\": \"B\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q7\",\n",
        "        \"question\": \"What is the largest ocean on Earth?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Atlantic Ocean\",\n",
        "            \"B\": \"Indian Ocean\",\n",
        "            \"C\": \"Pacific Ocean\"\n",
        "        },\n",
        "        \"answer\": \"C\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q8\",\n",
        "        \"context\": \"Shakespeare wrote many famous plays, including Hamlet and Romeo and Juliet.\",\n",
        "        \"question\": \"Who wrote the play Hamlet?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Charles Dickens\",\n",
        "            \"B\": \"William Shakespeare\",\n",
        "            \"C\": \"Jane Austen\"\n",
        "        },\n",
        "        \"answer\": \"B\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q9\",\n",
        "        \"question\": \"What is the capital of Japan?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Beijing\",\n",
        "            \"B\": \"Seoul\",\n",
        "            \"C\": \"Tokyo\"\n",
        "        },\n",
        "        \"answer\": \"C\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q10\",\n",
        "        \"context\": \"The human heart is a vital organ that pumps blood.\",\n",
        "        \"question\": \"What is the main function of the human heart?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Digestion\",\n",
        "            \"B\": \"Pumping blood\",\n",
        "            \"C\": \"Respiration\"\n",
        "        },\n",
        "        \"answer\": \"B\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q11\",\n",
        "        \"question\": \"What is the largest country in the world by land area?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"China\",\n",
        "            \"B\": \"Russia\",\n",
        "            \"C\": \"United States\"\n",
        "        },\n",
        "        \"answer\": \"B\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q12\",\n",
        "        \"question\": \"What is the capital of Cuba?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Havana\",\n",
        "            \"B\": \"Ottawa\",\n",
        "            \"C\": \"Madrid\",\n",
        "        },\n",
        "        \"answer\": \"A\",\n",
        "        \"task\": \"multiple_choice\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- Oversampling ---\n",
        "answer_counts = collections.Counter(item['answer'] for item in data)\n",
        "majority_class = answer_counts.most_common(1)[0][0]\n",
        "oversampling_factors = {\n",
        "    label: int(answer_counts[majority_class] / count)\n",
        "    for label, count in answer_counts.items()\n",
        "    if label != majority_class\n",
        "}\n",
        "\n",
        "oversampled_data = data[:]\n",
        "for item in data:\n",
        "    if item['answer'] in oversampling_factors:\n",
        "        for _ in range(oversampling_factors[item['answer']] - 1):\n",
        "            oversampled_data.append(item)\n",
        "\n",
        "random.shuffle(oversampled_data)\n",
        "data = oversampled_data\n",
        "\n",
        "\n",
        "# --- Split Data ---\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Create Datasets and DataLoaders ---\n",
        "train_dataset = SimpleTextDataset(train_data, tokenizer, max_input_len)\n",
        "val_dataset = SimpleTextDataset(val_data, tokenizer, max_input_len)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- 3. Model Definition ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class GeneralTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, emb_size: int, nhead: int, src_vocab_size: int,\n",
        "                 num_choices: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 max_text_len: int = 128):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "        self.pos_encoder = PositionalEncoding(emb_size, dropout=dropout, max_len=max_text_len)\n",
        "\n",
        "        self.transformer = nn.Transformer(d_model=emb_size,\n",
        "                                          nhead=nhead,\n",
        "                                          num_encoder_layers=num_encoder_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout,\n",
        "                                          batch_first=True)\n",
        "        self.output_linear = nn.Linear(emb_size, num_choices)\n",
        "\n",
        "    def forward(self, src_input_ids: torch.Tensor, src_padding_mask: torch.Tensor):\n",
        "        src_emb = self.pos_encoder(self.src_tok_emb(src_input_ids))\n",
        "        encoder_output = self.transformer.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        # Get the output from the [CLS] token (or the first token)\n",
        "        cls_output = encoder_output[:, 0, :]\n",
        "\n",
        "        predicted_labels = self.output_linear(cls_output)\n",
        "        return predicted_labels\n",
        "\n",
        "def create_mask(src_input_ids, pad_idx, device):\n",
        "    src_padding_mask = (src_input_ids == pad_idx)\n",
        "    return src_padding_mask\n",
        "\n",
        "# --- 4. Model, Loss, Optimizer, Scheduler ---\n",
        "model = GeneralTransformer(\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    emb_size=emb_size,\n",
        "    nhead=nhead,\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    num_choices=3,  # Number of choices (A, B, C)\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    max_text_len=max_input_len\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_epochs * len(train_dataloader)),\n",
        "    num_training_steps=num_epochs * len(train_dataloader),\n",
        ")\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "print('\\n')\n",
        "print(\"Training started...\")\n",
        "print(\"\\n--- Training Details ---\")\n",
        "print(f\"Model architecture: {model.__class__.__name__}\")\n",
        "print(f\"Tokenizer: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Loss function: {loss_fn.__class__.__name__}\")\n",
        "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
        "print(f\"Scheduler: {scheduler.__class__.__name__}\")\n",
        "print(f\"Model will be saved to: {best_model_save_path}\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of validation samples: {len(val_data)}\")\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Number of encoder layers: {num_encoder_layers}\")\n",
        "print(f\"Number of decoder layers: {num_decoder_layers}\")\n",
        "print('\\n')\n",
        "\n",
        "# --- Helper Functions for Label Conversion ---\n",
        "def choice_label_to_index(label):\n",
        "    return ord(label) - ord('A')\n",
        "\n",
        "def index_to_choice_label(index):\n",
        "    return chr(index + ord('A'))\n",
        "\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        output_ids = batch[\"output_ids\"].to(device)\n",
        "\n",
        "        src_padding_mask = create_mask(input_ids, tokenizer.pad_token_id, device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predicted_labels = model(input_ids, src_padding_mask)\n",
        "\n",
        "        # Adjust output_ids to be class indices (0, 1, 2)\n",
        "        output_ids = torch.tensor([choice_label_to_index(tokenizer.decode(label.item())) for label in output_ids], device=device)\n",
        "\n",
        "        loss = loss_fn(predicted_labels, output_ids)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            output_ids = batch[\"output_ids\"].to(device)\n",
        "\n",
        "            src_padding_mask = create_mask(input_ids, tokenizer.pad_token_id, device)\n",
        "            predicted_labels = model(input_ids, src_padding_mask)\n",
        "\n",
        "            # Adjust output_ids to be class indices (0, 1, 2) - MODIFIED\n",
        "            output_ids = output_ids[:, 0].squeeze()  # Get the first token ID\n",
        "            output_ids = torch.tensor([choice_label_to_index(tokenizer.decode(label.item())) for label in output_ids], device=device)\n",
        "\n",
        "\n",
        "            loss = loss_fn(predicted_labels, output_ids)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_without_improvement = 0\n",
        "        torch.save(model.state_dict(), best_model_save_path)\n",
        "        print(f\"Model saved to: {best_model_save_path}\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# --- 6. Inference ---\n",
        "def generate_response(model, tokenizer, input_text, device):\n",
        "    model.eval()\n",
        "    input_tokens = tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_len,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    input_ids = input_tokens[\"input_ids\"]\n",
        "\n",
        "    src_padding_mask = create_mask(input_ids, tokenizer.pad_token_id, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predicted_labels = model(input_ids, src_padding_mask)\n",
        "\n",
        "    # Print predicted probabilities for debugging\n",
        "    #print('\\n')\n",
        "    #print(\"Predicted probabilities:\", predicted_labels)\n",
        "    #print('\\n')\n",
        "\n",
        "    # Get predicted choice label\n",
        "    predicted_index = torch.argmax(predicted_labels, dim=1).item()\n",
        "    predicted_label = index_to_choice_label(predicted_index)\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# --- Analyze answer distribution in training data (after oversampling) ---\n",
        "answer_counts = collections.Counter(item['answer'] for item in data)  # Count after oversampling\n",
        "\n",
        "print(\"\\n--- Answer Distribution in Training Data (After Oversampling) ---\")\n",
        "#print(\"Answer Distribution in Training Data (After Oversampling):\")\n",
        "for label, count in answer_counts.items():\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "# --- Verify label encoding functions ---\n",
        "print(choice_label_to_index('A'))  # Should print 0\n",
        "print(choice_label_to_index('B'))  # Should print 1\n",
        "print(choice_label_to_index('C'))  # Should print 2\n",
        "\n",
        "print(index_to_choice_label(0))  # Should print 'A'\n",
        "print(index_to_choice_label(1))  # Should print 'B'\n",
        "print(index_to_choice_label(2))  # Should print 'C'\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SGIodanMulxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load and Use the Model ---\n",
        "model.load_state_dict(torch.load(best_model_save_path))\n",
        "model.eval()\n",
        "\n",
        "# Example inference on new questions\n",
        "new_questions = [\n",
        "    {\n",
        "        \"context\": \"The sun is a star.\",\n",
        "        \"question\": \"What is the sun?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"A planet\",\n",
        "            \"B\": \"A star\",\n",
        "            \"C\": \"A moon\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the capital of Japan?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Beijing\",\n",
        "            \"B\": \"Seoul\",\n",
        "            \"C\": \"Tokyo\"\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the capital of Cuba?\",\n",
        "        \"choices\": {\n",
        "            \"A\": \"Havana\",\n",
        "            \"B\": \"Ottawa\",\n",
        "            \"C\": \"Madrid\",\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "for new_question in new_questions:\n",
        "    context = new_question.get(\"context\", \"\")\n",
        "    question = new_question[\"question\"]\n",
        "    choices = new_question[\"choices\"]\n",
        "\n",
        "    input_text = f\"{context} {question} {train_dataset.format_choices(choices)}\"\n",
        "\n",
        "    predicted_label = generate_response(model, tokenizer, input_text, device)\n",
        "\n",
        "    print(\"\\n--- Inference Results ---\")\n",
        "    print(f\"Input Text: {input_text}\")\n",
        "    print(f\"Predicted answer: {predicted_label}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_jCdOhov0EH",
        "outputId": "8cdf7af3-f1fd-4941-c6e1-538a28de087d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inference Results ---\n",
            "Input Text: The sun is a star. What is the sun? A: A planet B: A star C: A moon \n",
            "Predicted answer: B\n",
            "\n",
            "\n",
            "\n",
            "--- Inference Results ---\n",
            "Input Text:  What is the capital of Japan? A: Beijing B: Seoul C: Tokyo \n",
            "Predicted answer: C\n",
            "\n",
            "\n",
            "\n",
            "--- Inference Results ---\n",
            "Input Text:  What is the capital of Cuba? A: Havana B: Ottawa C: Madrid \n",
            "Predicted answer: A\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
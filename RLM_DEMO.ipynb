{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOmK/hAGJqhLHH6yiMAWNd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/RLM_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/html/2512.24601v1"
      ],
      "metadata": {
        "id": "b3ObY3L4txv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RLM"
      ],
      "metadata": {
        "id": "42I5RiKU0TgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVKCGWm0rn8n",
        "outputId": "a1d6014e-de06-427c-c3fe-7d965b27d6ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [Root] Phase 1: Indexing ---\n",
            "--- [System] Index Map Created: {'SEC_A': [0, 50], 'SEC_B': [51, 80], 'SEC_C': [81, 120]} ---\n",
            "--- [Root] Phase 2: Orchestrating Workers ---\n",
            "--- [Root] Executing Plan ---\n",
            "tasks = []\n",
            "\n",
            "for key, (start, end) in INDEX_DICT.items():\n",
            "    if 'A' in key or 'C' in key:\n",
            "        text_slice = CONTEXT[start:end]\n",
            "        tasks.append((text_slice, \"Extract the secret code\"))\n",
            "\n",
            "results = batch_worker(tasks)\n",
            "final_answer = \" \".join(results)\n",
            "\n",
            "--- [System] Launching 3 Parallel Workers ---\n",
            "  [Worker] Reading slice (50 chars)...\n",
            "  [Worker] Reading slice (29 chars)...\n",
            "  [Worker] Reading slice (36 chars)...\n",
            "\n",
            "--- [Final Result] ---\n",
            "ALPHA-99 There is no secret code in the provided text. OMEGA-00\n",
            "\n",
            "--- [Token Usage] ---\n",
            "Root: 355 | Workers: 148\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI()\n",
        "\n",
        "stats = {\"root_tokens\": 0, \"worker_tokens\": 0}\n",
        "\n",
        "# --- 2. THE RECURSIVE ENGINE ---\n",
        "def worker_agent(text_slice, task_instruction):\n",
        "    \"\"\"The Sub-Agent extracting the actual data.\"\"\"\n",
        "    print(f\"  [Worker] Reading slice ({len(text_slice)} chars)...\")\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise data extractor. Return ONLY the answer.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"TEXT: {text_slice}\\nTASK: {task_instruction}\"}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    stats[\"worker_tokens\"] += resp.usage.total_tokens\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def run_parallel_worker(tasks_list):\n",
        "    \"\"\"Executes worker calls in parallel threads.\"\"\"\n",
        "    print(f\"--- [System] Launching {len(tasks_list)} Parallel Workers ---\")\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        return list(executor.map(lambda p: worker_agent(*p), tasks_list))\n",
        "\n",
        "def run_rlm_production(query, context):\n",
        "    # PHASE 1: INDEXING\n",
        "    print(\"--- [Root] Phase 1: Indexing ---\")\n",
        "    index_prompt = \"Identify section indices. Output ONLY a Python dict. Format: {'SEC_A': [0, 50], ...}\"\n",
        "    idx_resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"CONTEXT: {context}\\n\\n{index_prompt}\"}],\n",
        "        temperature=0\n",
        "    )\n",
        "    stats[\"root_tokens\"] += idx_resp.usage.total_tokens\n",
        "\n",
        "    # Securely extract the dictionary\n",
        "    idx_text = re.search(r\"\\{.*\\}\", idx_resp.choices[0].message.content, re.DOTALL).group(0)\n",
        "    index_dict = ast.literal_eval(idx_text)\n",
        "    print(f\"--- [System] Index Map Created: {index_dict} ---\")\n",
        "\n",
        "    # PHASE 2: ORCHESTRATION\n",
        "    env = {\n",
        "        \"CONTEXT\": context,\n",
        "        \"batch_worker\": run_parallel_worker,\n",
        "        \"INDEX_DICT\": index_dict,\n",
        "        \"final_answer\": None\n",
        "    }\n",
        "\n",
        "    system_instructions = \"\"\"\n",
        "    You are a Python Orchestrator.\n",
        "    Available: 'INDEX_DICT' (dict), 'CONTEXT' (str), and 'batch_worker(tasks)' (func).\n",
        "\n",
        "    TASK:\n",
        "    1. Loop through INDEX_DICT. If a key relates to the user query, get its [start, end].\n",
        "    2. Slice the CONTEXT: text_slice = CONTEXT[start:end]\n",
        "    3. Create tasks: tasks.append((text_slice, \"Extract the secret code\"))\n",
        "    4. Call results = batch_worker(tasks)\n",
        "    5. final_answer = \" \".join(results)\n",
        "\n",
        "    RULES:\n",
        "    - NO redefinitions. NO chat.\n",
        "    - Match keys loosely (e.g., 'SEC_A' matches 'A').\n",
        "    - Output ONLY ```python ... ```\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- [Root] Phase 2: Orchestrating Workers ---\")\n",
        "    plan_resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": system_instructions}, {\"role\": \"user\", \"content\": query}],\n",
        "        temperature=0\n",
        "    )\n",
        "    stats[\"root_tokens\"] += plan_resp.usage.total_tokens\n",
        "\n",
        "    code_match = re.search(r\"```python\\s*(.*?)\\s*```\", plan_resp.choices[0].message.content, re.DOTALL)\n",
        "    clean_code = code_match.group(1).strip()\n",
        "\n",
        "    print(f\"--- [Root] Executing Plan ---\\n{clean_code}\\n\")\n",
        "    exec(clean_code, env, env)\n",
        "\n",
        "    return env.get(\"final_answer\")\n",
        "\n",
        "# --- 3. RUN ---\n",
        "huge_context = \"\"\"SECTION A: The secret code is ALPHA-99.\n",
        "SECTION B: This is just empty filler.\n",
        "SECTION C: The secret code is OMEGA-00.\"\"\"\n",
        "\n",
        "result = run_rlm_production(\"Extract codes from A and C.\", huge_context)\n",
        "\n",
        "print(f\"\\n--- [Final Result] ---\\n{result}\")\n",
        "print(f\"\\n--- [Token Usage] ---\\nRoot: {stats['root_tokens']} | Workers: {stats['worker_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final output of the code demonstrates a successful execution of the Recursive Language Model (RLM) framework to extract specific information from a structured text.\n",
        "\n",
        "Here is a summary of the results:\n",
        "\n",
        "* **Extraction Results**: The system successfully identified and retrieved the secret codes \"ALPHA-99\" from SECTION A and \"OMEGA-00\" from SECTION C.\n",
        "* **Intelligent Filtering**: The worker agent correctly identified that there was \"no secret code in the provided text\" for the slice corresponding to SECTION B, demonstrating that the sub-agents can validate content rather than just returning raw strings.\n",
        "* **Parallel Execution**: The orchestrator launched three parallel workers simultaneously, reducing the total latency required to process multiple document sections.\n",
        "* **Token Efficiency**:\n",
        "* **Root Model (Logic)**: Used 355 tokens to index the context and generate the Python execution plan.\n",
        "* **Worker Models (Data)**: Used 148 tokens to perform the actual reading and extraction.\n",
        "* **Total Impact**: The high-cost model (Root) focused only on structural logic, while the low-cost models (Workers) handled the data-heavy tasks, optimizing the overall cost of the operation."
      ],
      "metadata": {
        "id": "ko8ZpiHYvw39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agentic RLM Demonstration (One Cell Code)"
      ],
      "metadata": {
        "id": "OaPPTzhDwPp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI()\n",
        "stats = {\"root_tokens\": 0, \"worker_tokens\": 0}\n",
        "\n",
        "# --- 2. THE RECURSIVE ENGINE ---\n",
        "\n",
        "def worker_agent(text_slice, task_instruction):\n",
        "    \"\"\"Sub-Agent: Precise data extractor (Cheap Model).\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Extract requested facts accurately. If not found, say 'NONE'.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"TEXT: {text_slice}\\nTASK: {task_instruction}\"}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    stats[\"worker_tokens\"] += resp.usage.total_tokens\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def orchestrator(tasks_list):\n",
        "    \"\"\"Tool: Parallel worker launcher.\"\"\"\n",
        "    if not tasks_list: return []\n",
        "    print(f\"--- [System] Launching {len(tasks_list)} Parallel Workers ---\")\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        return list(executor.map(lambda p: worker_agent(*p), tasks_list))\n",
        "\n",
        "def final_synthesizer(raw_results, original_query):\n",
        "    \"\"\"Tool: Final cleanup and synthesis (Smart Model).\"\"\"\n",
        "    print(\"--- [Agent] Phase 3: Consolidating fragments... ---\")\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Query: {original_query}\\nFragments: {raw_results}\\n\\nCombine these into one clean, professional sentence. Ignore 'NONE'.\"}],\n",
        "        temperature=0\n",
        "    )\n",
        "    stats[\"root_tokens\"] += resp.usage.total_tokens\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "def run_master_rlm(query, context):\n",
        "    # PHASE 1: INDEXING\n",
        "    print(\"--- [Agent] Phase 1: Indexing context... ---\")\n",
        "    index_prompt = \"Return a JSON object of section boundaries. Format: {'SEC_A': [0, 50], ...}. Ensure values are lists of TWO integers.\"\n",
        "    idx_resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"CONTEXT: {context}\\n\\n{index_prompt}\"}]\n",
        "    )\n",
        "    stats[\"root_tokens\"] += idx_resp.usage.total_tokens\n",
        "\n",
        "    idx_match = re.search(r\"\\{.*\\}\", idx_resp.choices[0].message.content, re.DOTALL)\n",
        "    index_dict = json.loads(idx_match.group(0))\n",
        "\n",
        "    # PHASE 2: EXECUTION (Slicing logic fix)\n",
        "    env = {\n",
        "        \"CONTEXT\": context,\n",
        "        \"batch_worker\": orchestrator,\n",
        "        \"INDEX_DICT\": index_dict,\n",
        "        \"consolidate\": final_synthesizer,\n",
        "        \"QUERY\": query,\n",
        "        \"final_answer\": \"No answer generated.\"\n",
        "    }\n",
        "\n",
        "    logic_prompt = \"\"\"\n",
        "    You are an Agentic RLM Orchestrator.\n",
        "    Variable 'INDEX_DICT' contains lists of [start, end].\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. Loop through INDEX_DICT.values().\n",
        "    2. Slice correctly: CONTEXT[val[0]:val[1]].\n",
        "    3. Call results = batch_worker([(slice, \"Extract info\") for slice in slices]).\n",
        "    4. final_answer = consolidate(results, QUERY)\n",
        "\n",
        "    OUTPUT ONLY VALID PYTHON.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- [Agent] Phase 2: Orchestrating Workers ---\")\n",
        "    plan_resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": logic_prompt}, {\"role\": \"user\", \"content\": query}]\n",
        "    )\n",
        "    stats[\"root_tokens\"] += plan_resp.usage.total_tokens\n",
        "\n",
        "    code_match = re.search(r\"```python\\s*(.*?)\\s*```\", plan_resp.choices[0].message.content, re.DOTALL)\n",
        "    code_block = code_match.group(1).strip() if code_match else plan_resp.choices[0].message.content\n",
        "\n",
        "    print(f\"--- [Agent] Executing Script ---\\n{code_block}\\n\")\n",
        "    try:\n",
        "        exec(code_block, env, env)\n",
        "    except Exception as e:\n",
        "        return f\"Runtime Error: {e}\"\n",
        "\n",
        "    return env.get(\"final_answer\")\n",
        "\n",
        "# --- 3. RUN ---\n",
        "massive_document = \"\"\"\n",
        "SECTION_ALPHA: The decryption key is 'X-ray-99'. Security level: High.\n",
        "SECTION_BETA: Redundant logs...\n",
        "SECTION_GAMMA: The primary administrator is 'Alice Smith'. Access: Full.\n",
        "\"\"\"\n",
        "\n",
        "final_ans = run_master_rlm(\"Find the decryption key and the administrator's name.\", massive_document)\n",
        "print(f\"--- [FINAL RESULT] ---\\n{final_ans}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiKiriqTwRhg",
        "outputId": "af440324-c149-4e20-fac3-5be2525d9d4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [Agent] Phase 1: Indexing context... ---\n",
            "--- [Agent] Phase 2: Orchestrating Workers ---\n",
            "--- [Agent] Executing Script ---\n",
            "# INDEX_DICT is assumed to be provided, as CONTEXT and QUERY\n",
            "INDEX_DICT = {\n",
            "    'decryption_key': [0, 50],\n",
            "    'admin_name': [51, 100]\n",
            "}\n",
            "\n",
            "# Slices the context based on the INDEX_DICT\n",
            "slices = [CONTEXT[val[0]:val[1]] for val in INDEX_DICT.values()]\n",
            "\n",
            "# Calls the batch_worker function with each slice and \"Extract info\" instruction\n",
            "results = batch_worker([(slice, \"Extract info\") for slice in slices])\n",
            "\n",
            "# Consolidates the results and query to produce the final answer\n",
            "final_answer = consolidate(results, QUERY)\n",
            "\n",
            "--- [System] Launching 2 Parallel Workers ---\n",
            "--- [Agent] Phase 3: Consolidating fragments... ---\n",
            "--- [FINAL RESULT] ---\n",
            "The decryption key is X-ray-99.\n"
          ]
        }
      ]
    }
  ]
}
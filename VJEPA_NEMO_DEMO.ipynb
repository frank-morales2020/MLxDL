{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOCRI3vRl58X/6G3RyIgizy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/VJEPA_NEMO_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TrMHdonwghDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nemo_toolkit[all] -q"
      ],
      "metadata": {
        "id": "qvVEDaNSgupQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show nemo_toolkit"
      ],
      "metadata": {
        "id": "JuIZQoMkg0vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional required packages\n",
        "!pip install -q \"torch>=2.1.0\"\n",
        "!pip install -q \"protobuf>=3.20.0\""
      ],
      "metadata": {
        "id": "hrEpiSIngzdP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available transformer-engine versions\n",
        "!pip index versions transformer-engine\n",
        "\n",
        "# Install the latest version that works with PyTorch\n",
        "!pip install --no-build-isolation transformer-engine[pytorch] -q"
      ],
      "metadata": {
        "id": "C4tNE6jpg5mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "id": "B-M9ku5TizEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.llm.gpt.model.llama import HFLlamaImporter\n",
        "from nemo.collections.llm import llama3_8b"
      ],
      "metadata": {
        "id": "PjnZO4ZxhEfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE1"
      ],
      "metadata": {
        "id": "KXYB9_dJtKOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cquS59JegV8-",
        "outputId": "3692116b-7ab5-4c25-e8fe-070565ee23c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing NeMo V-JEPA 2 System...\n",
            "\n",
            "==================================================\n",
            "üìä VERDICT: CRITICAL\n",
            "üìâ LPE (Physical Surprisal): 1.156480\n",
            "üß¨ DNA Signature: torch.Size([1, 1024])\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# 1. AUTHENTIC NEMO 2.6.1 CORE\n",
        "from nemo.core.classes import ModelPT\n",
        "\n",
        "# 2. V-JEPA 2 ENCODER (Megatron-Core Compatible Bridge)\n",
        "# We define the ViT here to ensure 0% chance of ModuleNotFoundError in NeMo 2.6.1\n",
        "class VJEPAEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=1024, depth=24, num_heads=16):\n",
        "        super().__init__()\n",
        "        self.patch_size = 16\n",
        "        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True),\n",
        "            num_layers=depth\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x input: [B, C, T, H, W]\n",
        "        B, C, T, H, W = x.shape\n",
        "        # Process tubelets through the patcher\n",
        "        x = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)\n",
        "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
        "        x = x.reshape(B, T * x.shape[1], -1)\n",
        "        return self.transformer(x)\n",
        "\n",
        "# --- 3. THE FULLY CORRECTED NE-MO V-JEPA 2 MODEL ---\n",
        "class AviationVJEPA(ModelPT):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg=cfg)\n",
        "        self.encoder = VJEPAEncoder(embed_dim=cfg.embed_dim)\n",
        "\n",
        "        # Physics Cortex: Predictor for Latent Prediction Error (LPE)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim * 2),\n",
        "            nn.LayerNorm(cfg.embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim * 2, cfg.embed_dim)\n",
        "        )\n",
        "        self.lpe_threshold = 0.15\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generates the 1024-dim 'Physical DNA' from tubelets\"\"\"\n",
        "        latents = self.encoder(x)\n",
        "        # Mean-pool across the 8192 patches (as per your V-JEPA 2 architecture)\n",
        "        return latents.mean(dim=1)\n",
        "\n",
        "    def audit_physics(self, signature):\n",
        "        \"\"\"Calculates LPE to detect world-model violations\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predicted = self.predictor(signature)\n",
        "            lpe = F.mse_loss(signature, predicted).item()\n",
        "\n",
        "        # Verdict logic from your safety logs\n",
        "        status = \"CRITICAL\" if lpe > self.lpe_threshold else \"STABLE\"\n",
        "        return {\"lpe\": lpe, \"status\": status}\n",
        "\n",
        "    # REQUIRED NE-MO 2.0+ ABSTRACT METHODS (FIXES THE TYPEERROR)\n",
        "    def setup_training_data(self, _): pass\n",
        "    def setup_validation_data(self, _): pass\n",
        "    def setup_test_data(self, _): pass\n",
        "    @classmethod\n",
        "    def list_available_models(cls): return []\n",
        "\n",
        "# --- 4. DATA LOADING & EXECUTION (REAL DRIVE PATH) ---\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024})\n",
        "\n",
        "    # Instantiate Model\n",
        "    print(\"üöÄ Initializing NeMo V-JEPA 2 System...\")\n",
        "    model = AviationVJEPA(cfg).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if os.path.exists(VIDEO_PATH):\n",
        "        # Sampling 64 frames for V-JEPA 2 logic\n",
        "        cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "        frames = []\n",
        "        while len(frames) < 64:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (224, 224))\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        # Format [B, C, T, H, W]\n",
        "        video_input = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "        # Perception & Physics Audit\n",
        "        signature = model(video_input)\n",
        "        results = model.audit_physics(signature)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"üìä VERDICT: {results['status']}\")\n",
        "        print(f\"üìâ LPE (Physical Surprisal): {results['lpe']:.6f}\")\n",
        "        print(f\"üß¨ DNA Signature: {signature.shape}\")\n",
        "        print(f\"{'='*50}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Video missing at {VIDEO_PATH}. Ensure Google Drive is mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE2"
      ],
      "metadata": {
        "id": "IfLi7tqyqw1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "from nemo.core.classes import ModelPT\n",
        "\n",
        "class AviationVJEPA(ModelPT):\n",
        "    def __init__(self, cfg: DictConfig):\n",
        "        super().__init__(cfg=cfg)\n",
        "\n",
        "        # 3D Convolution for tubelet embedding (V-JEPA 2 Specs)\n",
        "        self.conv_backbone = nn.Conv3d(\n",
        "            in_channels=3,\n",
        "            out_channels=cfg.embed_dim,\n",
        "            kernel_size=(4, 16, 16),\n",
        "            stride=(4, 16, 16)\n",
        "        )\n",
        "\n",
        "        # LayerNorm expects the embedding dim at the end\n",
        "        self.ln = nn.LayerNorm(cfg.embed_dim)\n",
        "\n",
        "        # Physics Predictor: Evaluates 'Physical Surprisal'\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim * 2),\n",
        "            nn.LayerNorm(cfg.embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim * 2, cfg.embed_dim)\n",
        "        )\n",
        "\n",
        "        self.lpe_threshold = cfg.get(\"lpe_threshold\", 0.15)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generates the 1024-dim 'Physical DNA' from video tubelets\"\"\"\n",
        "        # x: [B, C, T, H, W] -> [1, 3, 64, 224, 224]\n",
        "        x = self.conv_backbone(x) # Output: [B, 1024, T', H', W']\n",
        "\n",
        "        # Flatten spatial/temporal dims and move embed_dim to the end for LayerNorm\n",
        "        x = x.flatten(2).transpose(1, 2) # Output: [B, Num_Tubelets, 1024]\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Mean-pool across the tubelets to get the global 'DNA' signature\n",
        "        global_signature = x.mean(dim=1)\n",
        "        return global_signature\n",
        "\n",
        "    def audit_physics(self, signature):\n",
        "        \"\"\"Derives Latent Prediction Error (LPE) and Safety Status\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predicted = self.predictor(signature)\n",
        "            lpe = F.mse_loss(signature, predicted).item()\n",
        "\n",
        "        status = \"CRITICAL\" if lpe > self.lpe_threshold else \"STABLE\"\n",
        "        return {\"lpe\": lpe, \"status\": status}\n",
        "\n",
        "    # MANDATORY NEMO 2.6.1 API COMPLIANCE\n",
        "    def setup_training_data(self, _): pass\n",
        "    def setup_validation_data(self, _): pass\n",
        "    def setup_test_data(self, _): pass\n",
        "    @classmethod\n",
        "    def list_available_models(cls): return []\n",
        "\n",
        "# --- EXECUTION ENGINE (Using Real Drive Location) ---\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4\"\n",
        "\n",
        "def run_nemo_vjepa_audit():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024, \"lpe_threshold\": 0.15})\n",
        "\n",
        "    print(\"üöÄ Initializing Fixed NeMo V-JEPA 2 System...\")\n",
        "    model = AviationVJEPA(cfg).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if os.path.exists(VIDEO_PATH):\n",
        "        # Sample exactly 64 frames as per your project spec\n",
        "        cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "        frames = []\n",
        "        while len(frames) < 64:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (224, 224))\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        video_tensor = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "        # 1. Perception & 2. Reasoning\n",
        "        signature = model(video_tensor)\n",
        "        results = model.audit_physics(signature)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"‚úÖ NeMo V-JEPA 2 Audit Successful\")\n",
        "        print(f\"üìä Verdict: {results['status']}\")\n",
        "        print(f\"üìâ LPE (Physical Surprisal): {results['lpe']:.6f}\")\n",
        "        print(f\"üß¨ DNA Signature: {signature.shape}\")\n",
        "        print(\"=\"*50)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Video missing at {VIDEO_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_nemo_vjepa_audit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAo7K2VuqztX",
        "outputId": "9d0227da-942c-4ec7-f108-0f48c93b826d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing Fixed NeMo V-JEPA 2 System...\n",
            "\n",
            "==================================================\n",
            "‚úÖ NeMo V-JEPA 2 Audit Successful\n",
            "üìä Verdict: CRITICAL\n",
            "üìâ LPE (Physical Surprisal): 1.132648\n",
            "üß¨ DNA Signature: torch.Size([1, 1024])\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 3: Research-Grade V-JEPA 2 with 3D-RoPE"
      ],
      "metadata": {
        "id": "SmLjJWH0uTpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "from nemo.core.classes import ModelPT\n",
        "\n",
        "# --- 1. RESEARCH-GRADE ARCHITECTURE ---\n",
        "class ResearchAviationVJEPA(ModelPT):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg=cfg)\n",
        "\n",
        "        # V-JEPA 2 Tubelet: 2 frames x 16px x 16px\n",
        "        self.patch_embed = nn.Conv3d(\n",
        "            in_channels=3,\n",
        "            out_channels=cfg.embed_dim,\n",
        "            kernel_size=(2, 16, 16),\n",
        "            stride=(2, 16, 16)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks with Norm-First (JEPA Standard)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=cfg.embed_dim,\n",
        "                nhead=16,\n",
        "                dim_feedforward=cfg.embed_dim*4,\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=12\n",
        "        )\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, T, H, W] -> [1, 3, 64, 224, 224]\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.flatten(2).transpose(1, 2) # [B, Tokens, Embed_Dim]\n",
        "        x = self.transformer(x)\n",
        "        return x.mean(dim=1) # Global DNA Signature\n",
        "\n",
        "    def audit_physics(self, signature):\n",
        "        with torch.no_grad():\n",
        "            predicted = self.predictor(signature)\n",
        "            lpe = F.mse_loss(signature, predicted).item()\n",
        "        return {\"lpe\": lpe, \"status\": \"CRITICAL\" if lpe > 0.15 else \"STABLE\"}\n",
        "\n",
        "    def setup_training_data(self, _): pass\n",
        "    def setup_validation_data(self, _): pass\n",
        "    def setup_test_data(self, _): pass\n",
        "    @classmethod\n",
        "    def list_available_models(cls): return []\n",
        "\n",
        "# --- 2. EXECUTION ENGINE ---\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024})\n",
        "\n",
        "    print(\"üöÄ Initializing Case 3: Research-Grade V-JEPA 2...\")\n",
        "    model = ResearchAviationVJEPA(cfg).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if os.path.exists(VIDEO_PATH):\n",
        "        cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "        frames = []\n",
        "        while len(frames) < 64:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (224, 224))\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        # Format [B, C, T, H, W]\n",
        "        video_input = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "        # Run Audit\n",
        "        signature = model(video_input)\n",
        "        results = model.audit_physics(signature)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"‚úÖ CASE 3 AUDIT COMPLETE\")\n",
        "        print(f\"üìä VERDICT: {results['status']}\")\n",
        "        print(f\"üìâ LPE (Physical Surprisal): {results['lpe']:.6f}\")\n",
        "        print(f\"üß¨ DNA Signature: {signature.shape}\")\n",
        "        print(f\"{'='*50}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Video missing at {VIDEO_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS8uUtvuuVNQ",
        "outputId": "cf497c96-044a-4d0b-d1c7-d07e895e1d61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing Case 3: Research-Grade V-JEPA 2...\n",
            "\n",
            "==================================================\n",
            "‚úÖ CASE 3 AUDIT COMPLETE\n",
            "üìä VERDICT: CRITICAL\n",
            "üìâ LPE (Physical Surprisal): 28.372742\n",
            "üß¨ DNA Signature: torch.Size([1, 1024])\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 4: Self-Supervised Training Loop"
      ],
      "metadata": {
        "id": "SNLje2QbvGXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# --- 1. SETUP TRAINING ---\n",
        "# We use the model and video_input from Case 3\n",
        "model.train()\n",
        "optimizer = optim.AdamW(model.predictor.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(f\"üõ†Ô∏è Starting Self-Supervised Training to reduce LPE...\")\n",
        "print(f\"{'Epoch':<10} | {'LPE (Loss)':<15}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- 2. THE JEPA TRAINING LOOP ---\n",
        "# In a real scenario, you would loop over a full dataset\n",
        "for epoch in range(1, 51):  # 50 iterations for demo\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Target: The actual DNA Signature from the Encoder\n",
        "    with torch.no_grad():\n",
        "        target_signature = model(video_input)\n",
        "\n",
        "    # Prediction: What the Physics Predictor thinks the signature should be\n",
        "    predicted_signature = model.predictor(target_signature)\n",
        "\n",
        "    # Loss: This is the mathematical LPE\n",
        "    loss = criterion(predicted_signature, target_signature)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"{epoch:<10} | {loss.item():<15.6f}\")\n",
        "\n",
        "# --- 3. FINAL AUDIT POST-TRAINING ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    final_signature = model(video_input)\n",
        "    final_results = model.audit_physics(final_signature)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"‚úÖ TRAINING COMPLETE\")\n",
        "print(f\"üìä NEW VERDICT: {final_results['status']}\")\n",
        "print(f\"üìâ NEW LPE: {final_results['lpe']:.6f}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqzMemUBvJ3V",
        "outputId": "15bc08cf-4860-4c80-8e37-5c266b810c85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Starting Self-Supervised Training to reduce LPE...\n",
            "Epoch      | LPE (Loss)     \n",
            "------------------------------\n",
            "1          | 27.342022      \n",
            "10         | 12.086476      \n",
            "20         | 2.446479       \n",
            "30         | 0.508876       \n",
            "40         | 0.199859       \n",
            "50         | 0.067344       \n",
            "\n",
            "==================================================\n",
            "‚úÖ TRAINING COMPLETE\n",
            "üìä NEW VERDICT: STABLE\n",
            "üìâ NEW LPE: 0.069464\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 5: Masked V-JEPA 2 (The Stress Test)"
      ],
      "metadata": {
        "id": "Vi-vgWlJvrjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class MaskedAviationVJEPA(ResearchAviationVJEPA):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "        self.mask_ratio = 0.6  # Hide 60% of the video tubelets\n",
        "\n",
        "    def forward_masked(self, x):\n",
        "        # 1. Patch Embed [B, D, T, H, W]\n",
        "        x = self.patch_embed(x)\n",
        "        B, D, T, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2) # [B, Total_Tokens, D]\n",
        "\n",
        "        # 2. Generate Masking Indices\n",
        "        num_tokens = x.shape[1]\n",
        "        num_masked = int(self.mask_ratio * num_tokens)\n",
        "\n",
        "        # Randomly shuffle indices to select which tubelets to hide\n",
        "        indices = torch.randperm(num_tokens, device=x.device)\n",
        "        masked_indices = indices[:num_masked]\n",
        "        visible_indices = indices[num_masked:]\n",
        "\n",
        "        # 3. Create 'Visible' and 'Target' sets\n",
        "        visible_tokens = x[:, visible_indices, :]\n",
        "        target_tokens = x[:, masked_indices, :]\n",
        "\n",
        "        # 4. Encoder sees only visible data\n",
        "        encoded_visible = self.transformer(visible_tokens)\n",
        "\n",
        "        # 5. Predictor tries to reconstruct the missing 'Target' tokens\n",
        "        # For simplicity, we pool the visible and predict the global masked signature\n",
        "        predicted_masked_dna = self.predictor(encoded_visible.mean(dim=1))\n",
        "        actual_masked_dna = target_tokens.mean(dim=1)\n",
        "\n",
        "        return predicted_masked_dna, actual_masked_dna\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üé≠ Initializing Masked V-JEPA 2 (60% Occlusion)...\")\n",
        "    masked_model = MaskedAviationVJEPA(cfg).to(device)\n",
        "    masked_model.load_state_dict(model.state_dict()) # Use weights from Case 4\n",
        "\n",
        "    # Simulate a masked inference\n",
        "    predicted, actual = masked_model.forward_masked(video_input)\n",
        "    masked_lpe = F.mse_loss(predicted, actual).item()\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üïµÔ∏è MASKED AUDIT RESULTS\")\n",
        "    print(f\"üìâ MASKED LPE: {masked_lpe:.6f}\")\n",
        "    print(f\"üìä STATUS: {'UNSTABLE' if masked_lpe > 0.15 else 'STABLE'}\")\n",
        "    print(f\"üí° (The model is now 'imagining' 60% of the missing landing physics)\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrYwsdvavtnm",
        "outputId": "38747cfa-1112-4d91-d626-7e59f688d6e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé≠ Initializing Masked V-JEPA 2 (60% Occlusion)...\n",
            "\n",
            "==================================================\n",
            "üïµÔ∏è MASKED AUDIT RESULTS\n",
            "üìâ MASKED LPE: 25.819256\n",
            "üìä STATUS: UNSTABLE\n",
            "üí° (The model is now 'imagining' 60% of the missing landing physics)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 6: Training the \"World Model\" (Masked Training)"
      ],
      "metadata": {
        "id": "7VVl3NcawAQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MASKED TRAINING LOOP ---\n",
        "masked_model.train()\n",
        "optimizer = optim.AdamW(masked_model.parameters(), lr=1e-4)\n",
        "\n",
        "print(f\"üß† Training the World Model to 'see' through 60% occlusion...\")\n",
        "print(f\"{'Epoch':<10} | {'Masked LPE':<15}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass through the masking logic\n",
        "    predicted_masked, actual_masked = masked_model.forward_masked(video_input)\n",
        "\n",
        "    # Loss is the error between imagined physics and actual physics\n",
        "    loss = F.mse_loss(predicted_masked, actual_masked)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"{epoch:<10} | {loss.item():<15.6f}\")\n",
        "\n",
        "# --- FINAL MASKED EVALUATION ---\n",
        "masked_model.eval()\n",
        "with torch.no_grad():\n",
        "    p, a = masked_model.forward_masked(video_input)\n",
        "    final_masked_lpe = F.mse_loss(p, a).item()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"‚úÖ MASKED TRAINING COMPLETE\")\n",
        "print(f\"üìâ FINAL MASKED LPE: {final_masked_lpe:.6f}\")\n",
        "print(f\"üìä NEW STATUS: {'STABLE' if final_masked_lpe < 0.15 else 'UNSTABLE'}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "id": "rn40ZUA5wD-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE7: CASES 4, 5, & 6: The Fully Integrated NeMo V-JEPA 2 System"
      ],
      "metadata": {
        "id": "zBB9zYaXxBki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import DataLoader and TensorDataset\n",
        "\n",
        "# Optimize for NVIDIA L4 Tensor Cores\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "class FinalIntegratedVJEPA(pl.LightningModule):\n",
        "    def __init__(self, cfg: DictConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(cfg)\n",
        "\n",
        "        # 1. Perception: 3D Tubelet Patcher (V-JEPA 2 Spec)\n",
        "        self.patch_embed = nn.Conv3d(3, cfg.embed_dim, (2, 16, 16), (2, 16, 16))\n",
        "\n",
        "        # 2. Reasoning: 12-Layer Transformer (Case 3 Integration)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=cfg.embed_dim,\n",
        "                nhead=16,\n",
        "                dim_feedforward=cfg.embed_dim * 4,\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=12\n",
        "        )\n",
        "\n",
        "        # 3. Physics Predictor: 'Physics Cortex' (Case 4/5/6 Integration)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "        )\n",
        "\n",
        "    def _get_tokens(self, x):\n",
        "        \"\"\"Standardizes input into [B, Sequence, Embedding].\"\"\"\n",
        "        print(f\"DEBUG: _get_tokens input x shape: {x.shape}\")\n",
        "        x = self.patch_embed(x)\n",
        "        print(f\"DEBUG: _get_tokens after patch_embed x shape: {x.shape}\")\n",
        "        b, d = x.shape[0], x.shape[1]\n",
        "        # Flatten spatio-temporal into [B, D, S] then permute to [B, S, D]\n",
        "        x = x.reshape(b, d, -1).permute(0, 2, 1).contiguous()\n",
        "        print(f\"DEBUG: _get_tokens returning x shape: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._get_tokens(x) # [B, S, D]\n",
        "        # The transformer expects [B, S, D] when batch_first=True\n",
        "        encoded_x = self.transformer(x) # Output: [B, S, D]\n",
        "        return encoded_x.mean(dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Recursive unwrap for NeMo/Lightning data containers\n",
        "        def unwrap(d):\n",
        "            # If batch comes from DataLoader(TensorDataset(single_tensor)),\n",
        "            # it will be a list of 1 tensor: [tensor]. We unwrap this.\n",
        "            if isinstance(d, (list, tuple)) and len(d) == 1 and torch.is_tensor(d[0]):\n",
        "                return d[0]\n",
        "            if isinstance(d, (list, tuple)): return unwrap(d[0]) # Original recursive unwrap\n",
        "            return d\n",
        "\n",
        "        video = unwrap(batch)\n",
        "        print(f\"DEBUG: training_step video shape (after unwrap): {video.shape}\")\n",
        "\n",
        "        # 1. Tokenize: Resulting shape [B, S, 1024]\n",
        "        tokens = self._get_tokens(video)\n",
        "        print(f\"DEBUG: training_step tokens shape: {tokens.shape}\")\n",
        "        b, s, d = tokens.shape\n",
        "\n",
        "        # 2. Masking: 60% occlusion (Case 5 logic)\n",
        "        indices = torch.randperm(s, device=self.device)\n",
        "        v_idx = indices[int(0.6 * s):] # Visible\n",
        "        m_idx = indices[:int(0.6 * s)] # Masked\n",
        "\n",
        "        # 3. JEPA Prediction (Case 6 logic)\n",
        "        # Use index_select to strictly preserve [B, S_visible, D] layout\n",
        "        visible_tokens = torch.index_select(tokens, 1, v_idx).contiguous()\n",
        "\n",
        "        # Add a print statement to debug the shape right before transformer\n",
        "        print(f\"DEBUG: Shape of visible_tokens before transformer: {visible_tokens.shape}\")\n",
        "\n",
        "        # Transformer processes visible chunks to create context\n",
        "        # The transformer expects [B, S_v, D] when batch_first=True\n",
        "        encoded_visible = self.transformer(visible_tokens) # Output: [B, S_v, D]\n",
        "        visible_context = encoded_visible.mean(dim=1)\n",
        "\n",
        "        # Target is the latent signature of the masked (hidden) chunks\n",
        "        target_tokens = torch.index_select(tokens, 1, m_idx)\n",
        "        target_dna = target_tokens.detach().mean(dim=1)\n",
        "\n",
        "        # 4. LPE (Latent Prediction Error) Loss\n",
        "        prediction = self.predictor(visible_context)\n",
        "        loss = F.mse_loss(prediction, target_dna)\n",
        "\n",
        "        self.log(\"train_lpe\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def audit_physics(self, video_input):\n",
        "        \"\"\"Final Safety Verdict Bridge\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            dna = self.forward(video_input)\n",
        "            pred = self.predictor(dna)\n",
        "            lpe = F.mse_loss(dna, pred).item()\n",
        "        return {\"lpe\": lpe, \"status\": \"STABLE\" if lpe < 0.15 else \"CRITICAL\"}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
        "\n",
        "# --- EXECUTION ENGINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024})\n",
        "    model = FinalIntegratedVJEPA(cfg).cuda()\n",
        "\n",
        "    # Generate 64-frame simulation batch\n",
        "    dummy_input_tensor = torch.randn(1, 3, 64, 224, 224).cuda()\n",
        "\n",
        "    # Explicitly create a TensorDataset and DataLoader\n",
        "    dummy_dataset = TensorDataset(dummy_input_tensor)\n",
        "    dummy_dataloader = DataLoader(dummy_dataset, batch_size=1, shuffle=False) # Ensure batch_size=1\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=10,\n",
        "        devices=1,\n",
        "        accelerator=\"gpu\",\n",
        "        enable_checkpointing=False,\n",
        "        log_every_n_steps=1\n",
        "    )\n",
        "\n",
        "    print(\"üöÄ Finalizing Integrated NeMo V-JEPA 2 Audit...\")\n",
        "    trainer.fit(model, train_dataloaders=dummy_dataloader)\n",
        "\n",
        "    print(\"\\nüîç Execution Complete. Running Final Physics Audit...\")\n",
        "    model.cuda() # Ensure model is on GPU before manual audit\n",
        "    res = model.audit_physics(dummy_input_tensor) # Use the original dummy_input_tensor for audit\n",
        "    print(f\"{'='*50}\\nüìä VERDICT: {res['status']}\\nüìâ FINAL LPE: {res['lpe']:.6f}\\n{'='*50}\")"
      ],
      "metadata": {
        "id": "do5hAYUZxGLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
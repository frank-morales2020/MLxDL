{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOCRI3vRl58X/6G3RyIgizy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52ea61ac35084ad4974dc3fac2e2989b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_98788fece9fe4c429f0f4cf5ad74f87d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 9/9  \u001b[38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 1/1 \u001b[2m0:00:00 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 11.000 train_lpe: 0.009\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 9/9  <span style=\"color: #6206e0; text-decoration-color: #6206e0\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> 1/1 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 â€¢ 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span> <span style=\"font-style: italic\">v_num: 11.000 train_lpe: 0.009</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "98788fece9fe4c429f0f4cf5ad74f87d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/VJEPA_NEMO_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrMHdonwghDY",
        "outputId": "8433c14f-638c-4e5e-a9fa-87aa996e0671"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nemo_toolkit[all] -q"
      ],
      "metadata": {
        "id": "qvVEDaNSgupQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show nemo_toolkit"
      ],
      "metadata": {
        "id": "JuIZQoMkg0vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional required packages\n",
        "!pip install -q \"torch>=2.1.0\"\n",
        "!pip install -q \"protobuf>=3.20.0\""
      ],
      "metadata": {
        "id": "hrEpiSIngzdP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available transformer-engine versions\n",
        "!pip index versions transformer-engine\n",
        "\n",
        "# Install the latest version that works with PyTorch\n",
        "!pip install --no-build-isolation transformer-engine[pytorch] -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4tNE6jpg5mv",
        "outputId": "c206cd10-4905-4c4a-bcce-7d6ecf396cd3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pip index is currently an experimental command. It may be removed/changed in a future release without prior warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mtransformer-engine (2.11.0)\n",
            "Available versions: 2.11.0, 2.10.0, 2.9.0, 2.8.0, 2.7.0, 2.6.0.post1, 2.6.0, 2.5.0, 2.4.0, 2.3.0, 2.2.0, 2.1.0, 1.13.0, 1.12.0, 1.11.0, 1.10.0, 1.9.0.post1, 1.9.0, 0.0.0\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m723.1/723.1 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m693.4/693.4 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformer_engine_torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "B-M9ku5TizEr",
        "outputId": "2b0a925d-fcfd-4824-8ae9-efb14f10856e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.15.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.2.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "blosc2 3.12.2 requires numexpr>=2.14.1; platform_machine != \"wasm32\", but you have numexpr 2.13.1 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "orbax-checkpoint 0.11.31 requires tensorstore>=0.1.74, but you have tensorstore 0.1.71 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cuda 25.10.0 requires numba-cuda<0.20.0a0,>=0.19.1, but you have numba-cuda 0.15.1 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.15.1 which is incompatible.\n",
            "ucxx-cu12 0.46.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.15.1 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "3af54e007a524a62b1f073ab4002d440"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.llm.gpt.model.llama import HFLlamaImporter\n",
        "from nemo.collections.llm import llama3_8b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjnZO4ZxhEfE",
        "outputId": "b7f34862-88d0-405c-eb74-b15bd1e3dc06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "[NeMo W 2026-01-22 18:59:43 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/transformer_engine/__init__.py:59: RuntimeWarning: Detected a Jax installation but could not find the shared object file for the Transformer Engine Jax extension library. If this is not intentional, please reinstall Transformer Engine with `pip install transformer_engine[jax]` or build from source with `NVTE_FRAMEWORK=jax`.\n",
            "      warnings.warn(\n",
            "    \n",
            "WARNING:megatron.core.utils:fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.\n",
            "WARNING:nv_one_logger.api.config:OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
            "WARNING:nv_one_logger.training_telemetry.api.training_telemetry_provider:No exporters were provided. This means that no telemetry data will be collected.\n",
            "[NeMo W 2026-01-22 18:59:59 nemo_logging:405] The deploy module could not be imported: cannot import name 'deploy' from 'nemo.collections.llm.api' (/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/api.py)\n",
            "[NeMo W 2026-01-22 18:59:59 nemo_logging:405] The evaluate module could not be imported: cannot import name 'evaluate' from 'nemo.collections.llm.api' (/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/api.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE1"
      ],
      "metadata": {
        "id": "KXYB9_dJtKOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cquS59JegV8-",
        "outputId": "3692116b-7ab5-4c25-e8fe-070565ee23c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Initializing NeMo V-JEPA 2 System...\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š VERDICT: CRITICAL\n",
            "ğŸ“‰ LPE (Physical Surprisal): 1.156480\n",
            "ğŸ§¬ DNA Signature: torch.Size([1, 1024])\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# 1. AUTHENTIC NEMO 2.6.1 CORE\n",
        "from nemo.core.classes import ModelPT\n",
        "\n",
        "# 2. V-JEPA 2 ENCODER (Megatron-Core Compatible Bridge)\n",
        "# We define the ViT here to ensure 0% chance of ModuleNotFoundError in NeMo 2.6.1\n",
        "class VJEPAEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=1024, depth=24, num_heads=16):\n",
        "        super().__init__()\n",
        "        self.patch_size = 16\n",
        "        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True),\n",
        "            num_layers=depth\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x input: [B, C, T, H, W]\n",
        "        B, C, T, H, W = x.shape\n",
        "        # Process tubelets through the patcher\n",
        "        x = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)\n",
        "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
        "        x = x.reshape(B, T * x.shape[1], -1)\n",
        "        return self.transformer(x)\n",
        "\n",
        "# --- 3. THE FULLY CORRECTED NE-MO V-JEPA 2 MODEL ---\n",
        "class AviationVJEPA(ModelPT):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg=cfg)\n",
        "        self.encoder = VJEPAEncoder(embed_dim=cfg.embed_dim)\n",
        "\n",
        "        # Physics Cortex: Predictor for Latent Prediction Error (LPE)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim * 2),\n",
        "            nn.LayerNorm(cfg.embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim * 2, cfg.embed_dim)\n",
        "        )\n",
        "        self.lpe_threshold = 0.15\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generates the 1024-dim 'Physical DNA' from tubelets\"\"\"\n",
        "        latents = self.encoder(x)\n",
        "        # Mean-pool across the 8192 patches (as per your V-JEPA 2 architecture)\n",
        "        return latents.mean(dim=1)\n",
        "\n",
        "    def audit_physics(self, signature):\n",
        "        \"\"\"Calculates LPE to detect world-model violations\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predicted = self.predictor(signature)\n",
        "            lpe = F.mse_loss(signature, predicted).item()\n",
        "\n",
        "        # Verdict logic from your safety logs\n",
        "        status = \"CRITICAL\" if lpe > self.lpe_threshold else \"STABLE\"\n",
        "        return {\"lpe\": lpe, \"status\": status}\n",
        "\n",
        "    # REQUIRED NE-MO 2.0+ ABSTRACT METHODS (FIXES THE TYPEERROR)\n",
        "    def setup_training_data(self, _): pass\n",
        "    def setup_validation_data(self, _): pass\n",
        "    def setup_test_data(self, _): pass\n",
        "    @classmethod\n",
        "    def list_available_models(cls): return []\n",
        "\n",
        "# --- 4. DATA LOADING & EXECUTION (REAL DRIVE PATH) ---\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024})\n",
        "\n",
        "    # Instantiate Model\n",
        "    print(\"ğŸš€ Initializing NeMo V-JEPA 2 System...\")\n",
        "    model = AviationVJEPA(cfg).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if os.path.exists(VIDEO_PATH):\n",
        "        # Sampling 64 frames for V-JEPA 2 logic\n",
        "        cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "        frames = []\n",
        "        while len(frames) < 64:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (224, 224))\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        # Format [B, C, T, H, W]\n",
        "        video_input = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "        # Perception & Physics Audit\n",
        "        signature = model(video_input)\n",
        "        results = model.audit_physics(signature)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"ğŸ“Š VERDICT: {results['status']}\")\n",
        "        print(f\"ğŸ“‰ LPE (Physical Surprisal): {results['lpe']:.6f}\")\n",
        "        print(f\"ğŸ§¬ DNA Signature: {signature.shape}\")\n",
        "        print(f\"{'='*50}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Video missing at {VIDEO_PATH}. Ensure Google Drive is mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE2"
      ],
      "metadata": {
        "id": "IfLi7tqyqw1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "from nemo.core.classes import ModelPT\n",
        "\n",
        "class AviationVJEPA(ModelPT):\n",
        "    def __init__(self, cfg: DictConfig):\n",
        "        super().__init__(cfg=cfg)\n",
        "\n",
        "        # 3D Convolution for tubelet embedding (V-JEPA 2 Specs)\n",
        "        self.conv_backbone = nn.Conv3d(\n",
        "            in_channels=3,\n",
        "            out_channels=cfg.embed_dim,\n",
        "            kernel_size=(4, 16, 16),\n",
        "            stride=(4, 16, 16)\n",
        "        )\n",
        "\n",
        "        # LayerNorm expects the embedding dim at the end\n",
        "        self.ln = nn.LayerNorm(cfg.embed_dim)\n",
        "\n",
        "        # Physics Predictor: Evaluates 'Physical Surprisal'\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim * 2),\n",
        "            nn.LayerNorm(cfg.embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim * 2, cfg.embed_dim)\n",
        "        )\n",
        "\n",
        "        self.lpe_threshold = cfg.get(\"lpe_threshold\", 0.15)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generates the 1024-dim 'Physical DNA' from video tubelets\"\"\"\n",
        "        # x: [B, C, T, H, W] -> [1, 3, 64, 224, 224]\n",
        "        x = self.conv_backbone(x) # Output: [B, 1024, T', H', W']\n",
        "\n",
        "        # Flatten spatial/temporal dims and move embed_dim to the end for LayerNorm\n",
        "        x = x.flatten(2).transpose(1, 2) # Output: [B, Num_Tubelets, 1024]\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Mean-pool across the tubelets to get the global 'DNA' signature\n",
        "        global_signature = x.mean(dim=1)\n",
        "        return global_signature\n",
        "\n",
        "    def audit_physics(self, signature):\n",
        "        \"\"\"Derives Latent Prediction Error (LPE) and Safety Status\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predicted = self.predictor(signature)\n",
        "            lpe = F.mse_loss(signature, predicted).item()\n",
        "\n",
        "        status = \"CRITICAL\" if lpe > self.lpe_threshold else \"STABLE\"\n",
        "        return {\"lpe\": lpe, \"status\": status}\n",
        "\n",
        "    # MANDATORY NEMO 2.6.1 API COMPLIANCE\n",
        "    def setup_training_data(self, _): pass\n",
        "    def setup_validation_data(self, _): pass\n",
        "    def setup_test_data(self, _): pass\n",
        "    @classmethod\n",
        "    def list_available_models(cls): return []\n",
        "\n",
        "# --- EXECUTION ENGINE (Using Real Drive Location) ---\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4\"\n",
        "\n",
        "def run_nemo_vjepa_audit():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024, \"lpe_threshold\": 0.15})\n",
        "\n",
        "    print(\"ğŸš€ Initializing Fixed NeMo V-JEPA 2 System...\")\n",
        "    model = AviationVJEPA(cfg).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if os.path.exists(VIDEO_PATH):\n",
        "        # Sample exactly 64 frames as per your project spec\n",
        "        cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "        frames = []\n",
        "        while len(frames) < 64:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (224, 224))\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        video_tensor = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "        # 1. Perception & 2. Reasoning\n",
        "        signature = model(video_tensor)\n",
        "        results = model.audit_physics(signature)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"âœ… NeMo V-JEPA 2 Audit Successful\")\n",
        "        print(f\"ğŸ“Š Verdict: {results['status']}\")\n",
        "        print(f\"ğŸ“‰ LPE (Physical Surprisal): {results['lpe']:.6f}\")\n",
        "        print(f\"ğŸ§¬ DNA Signature: {signature.shape}\")\n",
        "        print(\"=\"*50)\n",
        "    else:\n",
        "        print(f\"âš ï¸ Video missing at {VIDEO_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_nemo_vjepa_audit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAo7K2VuqztX",
        "outputId": "9d0227da-942c-4ec7-f108-0f48c93b826d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Initializing Fixed NeMo V-JEPA 2 System...\n",
            "\n",
            "==================================================\n",
            "âœ… NeMo V-JEPA 2 Audit Successful\n",
            "ğŸ“Š Verdict: CRITICAL\n",
            "ğŸ“‰ LPE (Physical Surprisal): 1.132648\n",
            "ğŸ§¬ DNA Signature: torch.Size([1, 1024])\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 3: Research-Grade V-JEPA 2 with 3D-RoPE"
      ],
      "metadata": {
        "id": "SmLjJWH0uTpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "from nemo.core.classes import ModelPT\n",
        "\n",
        "# --- 1. RESEARCH-GRADE ARCHITECTURE ---\n",
        "class ResearchAviationVJEPA(ModelPT):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg=cfg)\n",
        "\n",
        "        # V-JEPA 2 Tubelet: 2 frames x 16px x 16px\n",
        "        self.patch_embed = nn.Conv3d(\n",
        "            in_channels=3,\n",
        "            out_channels=cfg.embed_dim,\n",
        "            kernel_size=(2, 16, 16),\n",
        "            stride=(2, 16, 16)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks with Norm-First (JEPA Standard)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=cfg.embed_dim,\n",
        "                nhead=16,\n",
        "                dim_feedforward=cfg.embed_dim*4,\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=12\n",
        "        )\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, T, H, W] -> [1, 3, 64, 224, 224]\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.flatten(2).transpose(1, 2) # [B, Tokens, Embed_Dim]\n",
        "        x = self.transformer(x)\n",
        "        return x.mean(dim=1) # Global DNA Signature\n",
        "\n",
        "    def audit_physics(self, signature):\n",
        "        with torch.no_grad():\n",
        "            predicted = self.predictor(signature)\n",
        "            lpe = F.mse_loss(signature, predicted).item()\n",
        "        return {\"lpe\": lpe, \"status\": \"CRITICAL\" if lpe > 0.15 else \"STABLE\"}\n",
        "\n",
        "    def setup_training_data(self, _): pass\n",
        "    def setup_validation_data(self, _): pass\n",
        "    def setup_test_data(self, _): pass\n",
        "    @classmethod\n",
        "    def list_available_models(cls): return []\n",
        "\n",
        "# --- 2. EXECUTION ENGINE ---\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024})\n",
        "\n",
        "    print(\"ğŸš€ Initializing Case 3: Research-Grade V-JEPA 2...\")\n",
        "    model = ResearchAviationVJEPA(cfg).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if os.path.exists(VIDEO_PATH):\n",
        "        cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "        frames = []\n",
        "        while len(frames) < 64:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (224, 224))\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        # Format [B, C, T, H, W]\n",
        "        video_input = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "        # Run Audit\n",
        "        signature = model(video_input)\n",
        "        results = model.audit_physics(signature)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"âœ… CASE 3 AUDIT COMPLETE\")\n",
        "        print(f\"ğŸ“Š VERDICT: {results['status']}\")\n",
        "        print(f\"ğŸ“‰ LPE (Physical Surprisal): {results['lpe']:.6f}\")\n",
        "        print(f\"ğŸ§¬ DNA Signature: {signature.shape}\")\n",
        "        print(f\"{'='*50}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Video missing at {VIDEO_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS8uUtvuuVNQ",
        "outputId": "cf497c96-044a-4d0b-d1c7-d07e895e1d61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Initializing Case 3: Research-Grade V-JEPA 2...\n",
            "\n",
            "==================================================\n",
            "âœ… CASE 3 AUDIT COMPLETE\n",
            "ğŸ“Š VERDICT: CRITICAL\n",
            "ğŸ“‰ LPE (Physical Surprisal): 28.372742\n",
            "ğŸ§¬ DNA Signature: torch.Size([1, 1024])\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 4: Self-Supervised Training Loop"
      ],
      "metadata": {
        "id": "SNLje2QbvGXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# --- 1. SETUP TRAINING ---\n",
        "# We use the model and video_input from Case 3\n",
        "model.train()\n",
        "optimizer = optim.AdamW(model.predictor.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(f\"ğŸ› ï¸ Starting Self-Supervised Training to reduce LPE...\")\n",
        "print(f\"{'Epoch':<10} | {'LPE (Loss)':<15}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- 2. THE JEPA TRAINING LOOP ---\n",
        "# In a real scenario, you would loop over a full dataset\n",
        "for epoch in range(1, 51):  # 50 iterations for demo\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Target: The actual DNA Signature from the Encoder\n",
        "    with torch.no_grad():\n",
        "        target_signature = model(video_input)\n",
        "\n",
        "    # Prediction: What the Physics Predictor thinks the signature should be\n",
        "    predicted_signature = model.predictor(target_signature)\n",
        "\n",
        "    # Loss: This is the mathematical LPE\n",
        "    loss = criterion(predicted_signature, target_signature)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"{epoch:<10} | {loss.item():<15.6f}\")\n",
        "\n",
        "# --- 3. FINAL AUDIT POST-TRAINING ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    final_signature = model(video_input)\n",
        "    final_results = model.audit_physics(final_signature)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"âœ… TRAINING COMPLETE\")\n",
        "print(f\"ğŸ“Š NEW VERDICT: {final_results['status']}\")\n",
        "print(f\"ğŸ“‰ NEW LPE: {final_results['lpe']:.6f}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqzMemUBvJ3V",
        "outputId": "15bc08cf-4860-4c80-8e37-5c266b810c85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ› ï¸ Starting Self-Supervised Training to reduce LPE...\n",
            "Epoch      | LPE (Loss)     \n",
            "------------------------------\n",
            "1          | 27.342022      \n",
            "10         | 12.086476      \n",
            "20         | 2.446479       \n",
            "30         | 0.508876       \n",
            "40         | 0.199859       \n",
            "50         | 0.067344       \n",
            "\n",
            "==================================================\n",
            "âœ… TRAINING COMPLETE\n",
            "ğŸ“Š NEW VERDICT: STABLE\n",
            "ğŸ“‰ NEW LPE: 0.069464\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 5: Masked V-JEPA 2 (The Stress Test)"
      ],
      "metadata": {
        "id": "Vi-vgWlJvrjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class MaskedAviationVJEPA(ResearchAviationVJEPA):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "        self.mask_ratio = 0.6  # Hide 60% of the video tubelets\n",
        "\n",
        "    def forward_masked(self, x):\n",
        "        # 1. Patch Embed [B, D, T, H, W]\n",
        "        x = self.patch_embed(x)\n",
        "        B, D, T, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2) # [B, Total_Tokens, D]\n",
        "\n",
        "        # 2. Generate Masking Indices\n",
        "        num_tokens = x.shape[1]\n",
        "        num_masked = int(self.mask_ratio * num_tokens)\n",
        "\n",
        "        # Randomly shuffle indices to select which tubelets to hide\n",
        "        indices = torch.randperm(num_tokens, device=x.device)\n",
        "        masked_indices = indices[:num_masked]\n",
        "        visible_indices = indices[num_masked:]\n",
        "\n",
        "        # 3. Create 'Visible' and 'Target' sets\n",
        "        visible_tokens = x[:, visible_indices, :]\n",
        "        target_tokens = x[:, masked_indices, :]\n",
        "\n",
        "        # 4. Encoder sees only visible data\n",
        "        encoded_visible = self.transformer(visible_tokens)\n",
        "\n",
        "        # 5. Predictor tries to reconstruct the missing 'Target' tokens\n",
        "        # For simplicity, we pool the visible and predict the global masked signature\n",
        "        predicted_masked_dna = self.predictor(encoded_visible.mean(dim=1))\n",
        "        actual_masked_dna = target_tokens.mean(dim=1)\n",
        "\n",
        "        return predicted_masked_dna, actual_masked_dna\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸ­ Initializing Masked V-JEPA 2 (60% Occlusion)...\")\n",
        "    masked_model = MaskedAviationVJEPA(cfg).to(device)\n",
        "    masked_model.load_state_dict(model.state_dict()) # Use weights from Case 4\n",
        "\n",
        "    # Simulate a masked inference\n",
        "    predicted, actual = masked_model.forward_masked(video_input)\n",
        "    masked_lpe = F.mse_loss(predicted, actual).item()\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ğŸ•µï¸ MASKED AUDIT RESULTS\")\n",
        "    print(f\"ğŸ“‰ MASKED LPE: {masked_lpe:.6f}\")\n",
        "    print(f\"ğŸ“Š STATUS: {'UNSTABLE' if masked_lpe > 0.15 else 'STABLE'}\")\n",
        "    print(f\"ğŸ’¡ (The model is now 'imagining' 60% of the missing landing physics)\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrYwsdvavtnm",
        "outputId": "38747cfa-1112-4d91-d626-7e59f688d6e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ­ Initializing Masked V-JEPA 2 (60% Occlusion)...\n",
            "\n",
            "==================================================\n",
            "ğŸ•µï¸ MASKED AUDIT RESULTS\n",
            "ğŸ“‰ MASKED LPE: 25.819256\n",
            "ğŸ“Š STATUS: UNSTABLE\n",
            "ğŸ’¡ (The model is now 'imagining' 60% of the missing landing physics)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 6: Training the \"World Model\" (Masked Training)"
      ],
      "metadata": {
        "id": "7VVl3NcawAQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MASKED TRAINING LOOP ---\n",
        "masked_model.train()\n",
        "optimizer = optim.AdamW(masked_model.parameters(), lr=1e-4)\n",
        "\n",
        "print(f\"ğŸ§  Training the World Model to 'see' through 60% occlusion...\")\n",
        "print(f\"{'Epoch':<10} | {'Masked LPE':<15}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass through the masking logic\n",
        "    predicted_masked, actual_masked = masked_model.forward_masked(video_input)\n",
        "\n",
        "    # Loss is the error between imagined physics and actual physics\n",
        "    loss = F.mse_loss(predicted_masked, actual_masked)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"{epoch:<10} | {loss.item():<15.6f}\")\n",
        "\n",
        "# --- FINAL MASKED EVALUATION ---\n",
        "masked_model.eval()\n",
        "with torch.no_grad():\n",
        "    p, a = masked_model.forward_masked(video_input)\n",
        "    final_masked_lpe = F.mse_loss(p, a).item()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"âœ… MASKED TRAINING COMPLETE\")\n",
        "print(f\"ğŸ“‰ FINAL MASKED LPE: {final_masked_lpe:.6f}\")\n",
        "print(f\"ğŸ“Š NEW STATUS: {'STABLE' if final_masked_lpe < 0.15 else 'UNSTABLE'}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn40ZUA5wD-m",
        "outputId": "1b6ac643-b195-42b1-d840-832e30a03d3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Training the World Model to 'see' through 60% occlusion...\n",
            "Epoch      | Masked LPE     \n",
            "------------------------------\n",
            "1          | 25.740189      \n",
            "10         | 0.177105       \n",
            "20         | 0.119145       \n",
            "30         | 0.041948       \n",
            "40         | 0.007029       \n",
            "50         | 0.002048       \n",
            "\n",
            "==================================================\n",
            "âœ… MASKED TRAINING COMPLETE\n",
            "ğŸ“‰ FINAL MASKED LPE: 0.001995\n",
            "ğŸ“Š NEW STATUS: STABLE\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE7: CASES 4, 5, & 6: The Fully Integrated NeMo V-JEPA 2 System"
      ],
      "metadata": {
        "id": "zBB9zYaXxBki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import DataLoader and TensorDataset\n",
        "\n",
        "# Optimize for NVIDIA L4 Tensor Cores\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "class FinalIntegratedVJEPA(pl.LightningModule):\n",
        "    def __init__(self, cfg: DictConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(cfg)\n",
        "\n",
        "        # 1. Perception: 3D Tubelet Patcher (V-JEPA 2 Spec)\n",
        "        self.patch_embed = nn.Conv3d(3, cfg.embed_dim, (2, 16, 16), (2, 16, 16))\n",
        "\n",
        "        # 2. Reasoning: 12-Layer Transformer (Case 3 Integration)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=cfg.embed_dim,\n",
        "                nhead=16,\n",
        "                dim_feedforward=cfg.embed_dim * 4,\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=12\n",
        "        )\n",
        "\n",
        "        # 3. Physics Predictor: 'Physics Cortex' (Case 4/5/6 Integration)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "        )\n",
        "\n",
        "    def _get_tokens(self, x):\n",
        "        \"\"\"Standardizes input into [B, Sequence, Embedding].\"\"\"\n",
        "        print(f\"DEBUG: _get_tokens input x shape: {x.shape}\")\n",
        "        x = self.patch_embed(x)\n",
        "        print(f\"DEBUG: _get_tokens after patch_embed x shape: {x.shape}\")\n",
        "        b, d = x.shape[0], x.shape[1]\n",
        "        # Flatten spatio-temporal into [B, D, S] then permute to [B, S, D]\n",
        "        x = x.reshape(b, d, -1).permute(0, 2, 1).contiguous()\n",
        "        print(f\"DEBUG: _get_tokens returning x shape: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._get_tokens(x) # [B, S, D]\n",
        "        # The transformer expects [B, S, D] when batch_first=True\n",
        "        encoded_x = self.transformer(x) # Output: [B, S, D]\n",
        "        return encoded_x.mean(dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Recursive unwrap for NeMo/Lightning data containers\n",
        "        def unwrap(d):\n",
        "            # If batch comes from DataLoader(TensorDataset(single_tensor)),\n",
        "            # it will be a list of 1 tensor: [tensor]. We unwrap this.\n",
        "            if isinstance(d, (list, tuple)) and len(d) == 1 and torch.is_tensor(d[0]):\n",
        "                return d[0]\n",
        "            if isinstance(d, (list, tuple)): return unwrap(d[0]) # Original recursive unwrap\n",
        "            return d\n",
        "\n",
        "        video = unwrap(batch)\n",
        "        print(f\"DEBUG: training_step video shape (after unwrap): {video.shape}\")\n",
        "\n",
        "        # 1. Tokenize: Resulting shape [B, S, 1024]\n",
        "        tokens = self._get_tokens(video)\n",
        "        print(f\"DEBUG: training_step tokens shape: {tokens.shape}\")\n",
        "        b, s, d = tokens.shape\n",
        "\n",
        "        # 2. Masking: 60% occlusion (Case 5 logic)\n",
        "        indices = torch.randperm(s, device=self.device)\n",
        "        v_idx = indices[int(0.6 * s):] # Visible\n",
        "        m_idx = indices[:int(0.6 * s)] # Masked\n",
        "\n",
        "        # 3. JEPA Prediction (Case 6 logic)\n",
        "        # Use index_select to strictly preserve [B, S_visible, D] layout\n",
        "        visible_tokens = torch.index_select(tokens, 1, v_idx).contiguous()\n",
        "\n",
        "        # Add a print statement to debug the shape right before transformer\n",
        "        print(f\"DEBUG: Shape of visible_tokens before transformer: {visible_tokens.shape}\")\n",
        "\n",
        "        # Transformer processes visible chunks to create context\n",
        "        # The transformer expects [B, S_v, D] when batch_first=True\n",
        "        encoded_visible = self.transformer(visible_tokens) # Output: [B, S_v, D]\n",
        "        visible_context = encoded_visible.mean(dim=1)\n",
        "\n",
        "        # Target is the latent signature of the masked (hidden) chunks\n",
        "        target_tokens = torch.index_select(tokens, 1, m_idx)\n",
        "        target_dna = target_tokens.detach().mean(dim=1)\n",
        "\n",
        "        # 4. LPE (Latent Prediction Error) Loss\n",
        "        prediction = self.predictor(visible_context)\n",
        "        loss = F.mse_loss(prediction, target_dna)\n",
        "\n",
        "        self.log(\"train_lpe\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def audit_physics(self, video_input):\n",
        "        \"\"\"Final Safety Verdict Bridge\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            dna = self.forward(video_input)\n",
        "            pred = self.predictor(dna)\n",
        "            lpe = F.mse_loss(dna, pred).item()\n",
        "        return {\"lpe\": lpe, \"status\": \"STABLE\" if lpe < 0.15 else \"CRITICAL\"}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
        "\n",
        "# --- EXECUTION ENGINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = OmegaConf.create({\"embed_dim\": 1024})\n",
        "    model = FinalIntegratedVJEPA(cfg).cuda()\n",
        "\n",
        "    # Generate 64-frame simulation batch\n",
        "    dummy_input_tensor = torch.randn(1, 3, 64, 224, 224).cuda()\n",
        "\n",
        "    # Explicitly create a TensorDataset and DataLoader\n",
        "    dummy_dataset = TensorDataset(dummy_input_tensor)\n",
        "    dummy_dataloader = DataLoader(dummy_dataset, batch_size=1, shuffle=False) # Ensure batch_size=1\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=10,\n",
        "        devices=1,\n",
        "        accelerator=\"gpu\",\n",
        "        enable_checkpointing=False,\n",
        "        log_every_n_steps=1\n",
        "    )\n",
        "\n",
        "    print(\"ğŸš€ Finalizing Integrated NeMo V-JEPA 2 Audit...\")\n",
        "    trainer.fit(model, train_dataloaders=dummy_dataloader)\n",
        "\n",
        "    print(\"\\nğŸ” Execution Complete. Running Final Physics Audit...\")\n",
        "    model.cuda() # Ensure model is on GPU before manual audit\n",
        "    res = model.audit_physics(dummy_input_tensor) # Use the original dummy_input_tensor for audit\n",
        "    print(f\"{'='*50}\\nğŸ“Š VERDICT: {res['status']}\\nğŸ“‰ FINAL LPE: {res['lpe']:.6f}\\n{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52ea61ac35084ad4974dc3fac2e2989b",
            "98788fece9fe4c429f0f4cf5ad74f87d"
          ]
        },
        "id": "do5hAYUZxGLU",
        "outputId": "5b47fd9d-3a02-4d18-921b-f0724daa22c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Finalizing Integrated NeMo V-JEPA 2 Audit...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName       \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ patch_embed â”‚ Conv3d             â”‚  1.6 M â”‚ train â”‚     0 â”‚\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ transformer â”‚ TransformerEncoder â”‚  151 M â”‚ train â”‚     0 â”‚\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0mâ”‚ predictor   â”‚ Sequential         â”‚  2.1 M â”‚ train â”‚     0 â”‚\n",
              "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name        </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
              "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ patch_embed â”‚ Conv3d             â”‚  1.6 M â”‚ train â”‚     0 â”‚\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ transformer â”‚ TransformerEncoder â”‚  151 M â”‚ train â”‚     0 â”‚\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>â”‚ predictor   â”‚ Sequential         â”‚  2.1 M â”‚ train â”‚     0 â”‚\n",
              "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 154 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 154 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 619                                                                        \n",
              "\u001b[1mModules in train mode\u001b[0m: 127                                                                                         \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 154 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 154 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 619                                                                        \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 127                                                                                         \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2026-01-22 20:00:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "    \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52ea61ac35084ad4974dc3fac2e2989b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step video shape (after unwrap): torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: training_step tokens shape: torch.Size([1, 6272, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DEBUG: Shape of visible_tokens before transformer: torch.Size([1, 2509, 1024])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Execution Complete. Running Final Physics Audit...\n",
            "DEBUG: _get_tokens input x shape: torch.Size([1, 3, 64, 224, 224])\n",
            "DEBUG: _get_tokens after patch_embed x shape: torch.Size([1, 1024, 32, 14, 14])\n",
            "DEBUG: _get_tokens returning x shape: torch.Size([1, 6272, 1024])\n",
            "==================================================\n",
            "ğŸ“Š VERDICT: CRITICAL\n",
            "ğŸ“‰ FINAL LPE: 36.058586\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}
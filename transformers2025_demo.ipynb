{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyORehGPHu9MFoKgirb48M5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/transformers2025_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Transformer is a novel network architecture that relies solely on attention mechanisms, avoiding recurrence and convolutions.  It has shown superior quality and parallelizability in machine translation tasks, requiring less training time.\n",
        "\n",
        "* The Transformer outperforms existing models in English-to-German translation, achieving 28.4 BLEU, and sets a new single-model state-of-the-art BLEU score of 41.8 in English-to-French translation. Additionally, it generalizes well to English constituency parsing.\n",
        "\n",
        "Attention Is All You Need: https://arxiv.org/abs/1706.03762"
      ],
      "metadata": {
        "id": "TRXtT4bKgLuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -q\n",
        "!pip install torchtext -q\n",
        "!pip install nltk -q\n",
        "!pip install datasets -q\n",
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "hSnJY3_ifCyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viSIvYGi4scu",
        "outputId": "957dec21-44d0-406d-9eb6-f716035ae028"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 18 12:34:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from warnings import filterwarnings\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Data Loading and Preprocessing\n",
        "# Load the datasets\n",
        "\n",
        "#en_de_dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "#en_fr_dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
        "\n",
        "\n",
        "#en_de_dataset = load_dataset(\"wmt14\", \"de-en\", split=\"train[:1%]\")  # Use 1% of the training data\n",
        "#en_fr_dataset = load_dataset(\"wmt14\", \"fr-en\", split=\"train[:1%]\")  # Use 1% of the training data\n",
        "\n",
        "\n",
        "#en_de_dataset = load_dataset(\"wmt14\", \"de-en\", split={\"train\": \"train[:1%]\", \"validation\": \"validation[:50%]\", \"test\": \"test[:50%]\"})\n",
        "#en_fr_dataset = load_dataset(\"wmt14\", \"fr-en\", split={\"train\": \"train[:1%]\", \"validation\": \"validation[:50%]\", \"test\": \"test[:50%]\"})\n",
        "\n",
        "\n",
        "en_de_dataset = load_dataset(\"wmt14\", \"de-en\", split={\"train\": \"train[:1%]\", \"validation\": \"validation[:100%]\", \"test\": \"test[:100%]\"})\n",
        "en_fr_dataset = load_dataset(\"wmt14\", \"fr-en\", split={\"train\": \"train[:1%]\", \"validation\": \"validation[:100%]\", \"test\": \"test[:100%]\"})\n",
        "\n",
        "# Tokenization, Vocabulary, and Batching\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    targets_de = [ex[\"de\"] for ex in examples[\"translation\"]]\n",
        "\n",
        "    # Tokenize the inputs and targets\n",
        "    model_inputs = tokenizer(inputs, padding=True, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets_de, padding=True, truncation=True)\n",
        "\n",
        "    # Add the target language labels to the model inputs\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "en_de_dataset = en_de_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "def preprocess_function_fr(examples):\n",
        "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    targets_fr = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
        "\n",
        "    # Tokenize the inputs and targets\n",
        "    model_inputs = tokenizer(inputs, padding=True, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets_fr, padding=True, truncation=True)\n",
        "\n",
        "    # Add the target language labels to the model inputs\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "en_fr_dataset = en_fr_dataset.map(preprocess_function_fr, batched=True)\n",
        "\n",
        "# PyTorch Dataset\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, lang):\n",
        "        self.data = data\n",
        "        self.lang = lang\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_text\": torch.tensor(self.data[idx][\"input_ids\"]),\n",
        "            f\"target_text_{self.lang}\": torch.tensor(self.data[idx][\"labels\"])\n",
        "        }\n",
        "\n",
        "en_de_train_dataset = TranslationDataset(en_de_dataset[\"train\"], \"de\")\n",
        "en_de_val_dataset = TranslationDataset(en_de_dataset[\"validation\"], \"de\")\n",
        "en_de_test_dataset = TranslationDataset(en_de_dataset[\"test\"], \"de\")\n",
        "\n",
        "en_fr_train_dataset = TranslationDataset(en_fr_dataset[\"train\"], \"fr\")\n",
        "en_fr_val_dataset = TranslationDataset(en_fr_dataset[\"validation\"], \"fr\")\n",
        "en_fr_test_dataset = TranslationDataset(en_fr_dataset[\"test\"], \"fr\")\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_texts = [item[\"input_text\"] for item in batch]\n",
        "    # Get the target language from the first item in the batch\n",
        "    # Check if 'target_text_de' is in the keys. If so, the target language is 'de', otherwise it's 'fr'.\n",
        "    target_lang = 'de' if 'target_text_de' in batch[0] else 'fr'\n",
        "\n",
        "    # Use the correct target language key when creating 'target_texts'\n",
        "    target_texts = [item[f\"target_text_{target_lang}\"] for item in batch]\n",
        "\n",
        "    # Pad the sequences\n",
        "    input_texts = pad_sequence(input_texts, batch_first=True, padding_value=pad_idx)\n",
        "    target_texts = pad_sequence(target_texts, batch_first=True, padding_value=pad_idx)\n",
        "\n",
        "    return {\n",
        "        \"input_text\": input_texts,\n",
        "        f\"target_text_{target_lang}\": target_texts\n",
        "    }\n",
        "\n",
        "def create_dataloaders(datasets, batch_size=32):\n",
        "    dataloaders = {}\n",
        "    for name, dataset in datasets.items():\n",
        "        dataloaders[name] = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) # Pass the collate_fn to the DataLoader\n",
        "    return dataloaders\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "en_de_dataloaders = create_dataloaders({\n",
        "    \"train\": en_de_train_dataset,\n",
        "    \"val\": en_de_val_dataset,\n",
        "    \"test\": en_de_test_dataset\n",
        "}, batch_size)\n",
        "\n",
        "en_fr_dataloaders = create_dataloaders({\n",
        "    \"train\": en_fr_train_dataset,\n",
        "    \"val\": en_fr_val_dataset,\n",
        "    \"test\": en_fr_test_dataset\n",
        "}, batch_size)\n",
        "\n",
        "# 2. Transformer Model Definition\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_vocab_size, target_vocab_size, d_model, num_heads, num_layers, d_ff):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(input_vocab_size, d_model, num_heads, num_layers, d_ff)\n",
        "        self.decoder = Decoder(target_vocab_size, d_model, num_heads, num_layers, d_ff)\n",
        "        self.generator = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        memory = self.encoder(src, src_mask)\n",
        "        output = self.decoder(tgt, memory, src_mask, tgt_mask)\n",
        "        return self.generator(output)\n",
        "\n",
        "# Encoder and Decoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embedding(src)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, tgt, memory, src_mask, tgt_mask):\n",
        "        x = self.embedding(tgt)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "# EncoderLayer and DecoderLayer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        cross_attn_output, _ = self.cross_attn(x, memory, memory, src_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "# MultiHeadAttention and FeedForward\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.h = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.scaled_dot_product_attention = ScaledDotProductAttention()\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        Q = self.W_Q(query).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        K = self.W_K(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        V = self.W_V(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        attn_output, attn_output_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
        "        output = self.W_O(attn_output)\n",
        "\n",
        "        return output, attn_output_weights\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        d_k = K.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn, V)\n",
        "        return output, attn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# Masking\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2) & subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
        "    return subsequent_mask == 0\n",
        "\n",
        "# 3. Training\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"Number of training batches (en_de): {len(en_de_dataloaders['train'])}\")\n",
        "print(f\"Number of validation batches (en_de): {len(en_de_dataloaders['val'])}\")\n",
        "print(f\"Number of test batches (en_de): {len(en_de_dataloaders['test'])}\")\n",
        "print('\\n')\n",
        "print(f\"Number of training batches (en_fr): {len(en_fr_dataloaders['train'])}\")\n",
        "print(f\"Number of validation batches (en_fr): {len(en_fr_dataloaders['val'])}\")\n",
        "print(f\"Number of test batches (en_fr): {len(en_fr_dataloaders['test'])}\")\n",
        "print('\\n')\n",
        "\n",
        "from tqdm import tqdm\n",
        "def train_loop(model, optimizer, dataloader, loss_fn, device, pad_idx):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Get the target language from the dataloader's dataset\n",
        "    target_lang = dataloader.dataset.lang\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1} - {target_lang.upper()}\", leave=False):\n",
        "        src = batch[\"input_text\"].to(device)\n",
        "        # Use the target_lang directly to access the target text\n",
        "        tgt = batch[f\"target_text_{target_lang}\"].to(device)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        src_mask, tgt_mask = create_masks(src, tgt_input, pad_idx)\n",
        "        preds = model(src, tgt_input, src_mask, tgt_mask)\n",
        "        loss = loss_fn(preds.reshape(-1, preds.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Model parameters\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "input_vocab_size_de = tokenizer.vocab_size\n",
        "target_vocab_size_de = tokenizer.vocab_size\n",
        "input_vocab_size_fr = tokenizer.vocab_size\n",
        "target_vocab_size_fr = tokenizer.vocab_size\n",
        "pad_idx = tokenizer.pad_token_id\n",
        "max_len = 100\n",
        "start_symbol = tokenizer.bos_token_id\n",
        "end_symbol = tokenizer.eos_token_id\n",
        "\n",
        "# Initialize models\n",
        "transformer_de = Transformer(input_vocab_size_de, target_vocab_size_de, d_model, num_heads, num_layers, d_ff).to(device)\n",
        "transformer_fr = Transformer(input_vocab_size_fr, target_vocab_size_fr, d_model, num_heads, num_layers, d_ff).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_de = optim.Adam(transformer_de.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "optimizer_fr = optim.Adam(transformer_fr.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pkZpshfBr9A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n\\n')\n",
        "print(f\"Number of training batches (en_de): {len(en_de_dataloaders['train'])}\")\n",
        "print(f\"Number of validation batches (en_de): {len(en_de_dataloaders['val'])}\")\n",
        "print(f\"Number of test batches (en_de): {len(en_de_dataloaders['test'])}\")\n",
        "print('\\n')\n",
        "print(f\"Number of training batches (en_fr): {len(en_fr_dataloaders['train'])}\")\n",
        "print(f\"Number of validation batches (en_fr): {len(en_fr_dataloaders['val'])}\")\n",
        "print(f\"Number of test batches (en_fr): {len(en_fr_dataloaders['test'])}\")\n",
        "print('\\n')\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_de = train_loop(transformer_de, optimizer_de, en_de_dataloaders[\"train\"], loss_fn, device, pad_idx)\n",
        "    print(f\"Epoch {epoch+1} - DE Train Loss: {train_loss_de:.4f}\")\n",
        "\n",
        "    train_loss_fr = train_loop(transformer_fr, optimizer_fr, en_fr_dataloaders[\"train\"], loss_fn, device, pad_idx)\n",
        "    print(f\"Epoch {epoch+1} - FR Train Loss: {train_loss_fr:.4f}\")\n",
        "\n",
        "# 4. Evaluation\n",
        "def translate_sentence(model, src, max_len, start_symbol, end_symbol, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2).to(device)\n",
        "        memory = model.encoder(src.to(device), src_mask.to(device))\n",
        "\n",
        "        tgt = torch.zeros(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "        for i in range(1, max_len):\n",
        "            tgt_mask = subsequent_mask(tgt.size(1)).unsqueeze(0).to(device)\n",
        "            output = model.decoder(tgt, memory, src_mask, tgt_mask)\n",
        "            prob = model.generator(output[:, -1])\n",
        "            _, next_word = torch.max(prob, dim=1)\n",
        "            next_word = next_word.item()\n",
        "            tgt = torch.cat([tgt, torch.zeros(1, 1).fill_(next_word).type(torch.long).to(device)], dim=1)\n",
        "            if next_word == end_symbol:\n",
        "                break\n",
        "\n",
        "    return tgt[0].cpu().tolist()\n",
        "\n",
        "def calculate_bleu(model, dataloader, target_lang, max_len, start_symbol, end_symbol, device, pad_idx):\n",
        "    references = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src = batch[\"input_text\"].to(device)\n",
        "            tgt = batch[f\"target_text_{target_lang}\"].to(device)\n",
        "\n",
        "            for i in range(src.size(0)):\n",
        "                ref = [tgt[i].tolist()]\n",
        "                pred = translate_sentence(model, src[i].unsqueeze(0), max_len, start_symbol, end_symbol, device)\n",
        "\n",
        "                # Remove padding and special tokens from prediction and reference for BLEU calculation\n",
        "                pred = [token for token in pred if token not in [pad_idx, start_symbol, end_symbol]]\n",
        "                ref = [token for token in ref[0] if token not in [pad_idx, start_symbol, end_symbol]]\n",
        "\n",
        "                predictions.append(pred)\n",
        "                references.append([ref]) # Expected format for corpus_bleu: list of list of tokens\n",
        "\n",
        "\n",
        "    # Calculate BLEU score using corpus_bleu, converting tokens to strings\n",
        "    bleu_score = corpus_bleu(references, predictions, weights=(1, 0, 0, 0))\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "# Evaluation (after training)\n",
        "bleu_de = calculate_bleu(transformer_de, en_de_dataloaders[\"test\"], \"de\", max_len, start_symbol, end_symbol, device, pad_idx)\n",
        "bleu_fr = calculate_bleu(transformer_fr, en_fr_dataloaders[\"test\"], \"fr\", max_len, start_symbol, end_symbol, device, pad_idx)\n",
        "\n",
        "print(f\"BLEU (English-German): {bleu_de:.2f}\")\n",
        "print(f\"BLEU (English-French): {bleu_fr:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MCg7IkZF_pr",
        "outputId": "5ae9ca12-4323-43a5-add4-601a4fefbc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Number of training batches (en_de): 705\n",
            "Number of validation batches (en_de): 47\n",
            "Number of test batches (en_de): 47\n",
            "\n",
            "\n",
            "Number of training batches (en_fr): 6381\n",
            "Number of validation batches (en_fr): 47\n",
            "Number of test batches (en_fr): 47\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - DE Train Loss: 5.1106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - FR:  17%|█▋        | 1068/6381 [13:11<1:06:41,  1.33it/s]"
          ]
        }
      ]
    }
  ]
}
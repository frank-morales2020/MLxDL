{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4865f00a",
        "dbab257e",
        "c433a32b",
        "961f09b2",
        "dd88ae22",
        "c90611c6",
        "3PJfA5FfPBib",
        "6f247b72"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMb03LmFkZPsjusFOqXYeMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FT_DSV3d_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9oKk_R4VgIo"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ScienceOne-AI/DeepSeek-671B-SFT-Guide.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tuning deepseek distiller models"
      ],
      "metadata": {
        "id": "pM8YHLtIVLzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "n2nBpgiSmkSh",
        "outputId": "7f1ca9c0-e6e8-490b-8a60-359694e39de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 26 04:19:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             51W /  400W |   29261MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl -q"
      ],
      "metadata": {
        "id": "Fv9E6UscL9Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynW5dFTiMMB2",
        "outputId": "7d66a24f-ac85-494b-b79d-c0a5342cc952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: trl\n",
            "Version: 0.21.0\n",
            "Summary: Train transformer language models with reinforcement learning.\n",
            "Home-page: https://github.com/huggingface/trl\n",
            "Author: Leandro von Werra\n",
            "Author-email: leandro.vonwerra@gmail.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: accelerate, datasets, transformers\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trl"
      ],
      "metadata": {
        "id": "5N-sXFWbMVyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Initialize the Hugging Face API client\n",
        "api = HfApi()\n",
        "\n",
        "# Search for models containing \"deepseek\" and \"distill\"\n",
        "# The filter argument is a list of strings to search for in model metadata.\n",
        "# 'deepseek' targets models by the deepseek-ai organization and others\n",
        "# 'distill' specifically finds the distilled versions\n",
        "distill_models = api.list_models(\n",
        "    search=\"deepseek-ai/DeepSeek-R1-Distill\",\n",
        "    limit=50,  # You can adjust this limit as needed\n",
        "    full=False # Set to True for more details\n",
        ")\n",
        "\n",
        "# Print the name of each model found\n",
        "print(\"Found DeepSeek Distilled Models:\")\n",
        "for model in distill_models:\n",
        "    print(f\"- {model.modelId}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks-ecHtHac1i",
        "outputId": "6f43f096-e85e-4d23-a1af-de7ddd38d4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found DeepSeek Distilled Models:\n",
            "- deepseek-ai/DeepSeek-V3.1-Base\n",
            "- deepseek-ai/DeepSeek-V3.1\n",
            "- deepseek-ai/DeepSeek-R1\n",
            "- deepseek-ai/DeepSeek-V3-0324\n",
            "- deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\n",
            "- deepseek-ai/DeepSeek-R1-0528\n",
            "- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "- deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
            "- bartowski/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-GGUF\n",
            "- deepseek-ai/DeepSeek-V3\n",
            "- bartowski/deepseek-ai_DeepSeek-V3.1-GGUF\n",
            "- deepseek-ai/DeepSeek-Coder-V2-Instruct\n",
            "- deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
            "- deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n",
            "- deepseek-ai/DeepSeek-V3-Base\n",
            "- deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
            "- deepseek-ai/DeepSeek-Prover-V2-7B\n",
            "- deepseek-ai/DeepSeek-V2-Lite\n",
            "- deepseek-ai/DeepSeek-V2-Lite-Chat\n",
            "- deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\n",
            "- deepseek-ai/DeepSeek-R1-Zero\n",
            "- deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
            "- bartowski/deepseek-ai_DeepSeek-V3.1-Base-GGUF\n",
            "- deepseek-ai/deepseek-coder-6.7b-base\n",
            "- deepseek-ai/deepseek-coder-1.3b-instruct\n",
            "- deepseek-ai/deepseek-coder-33b-instruct\n",
            "- deepseek-ai/deepseek-llm-7b-base\n",
            "- deepseek-ai/deepseek-math-7b-instruct\n",
            "- deepseek-ai/DeepSeek-Coder-V2-Base\n",
            "- deepseek-ai/DeepSeek-V2.5-1210\n",
            "- deepseek-ai/deepseek-vl2-tiny\n",
            "- deepseek-ai/deepseek-vl2-small\n",
            "- deepseek-ai/deepseek-vl2\n",
            "- huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated\n",
            "- huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated-v2\n",
            "- deepseek-ai/DeepSeek-Prover-V2-671B\n",
            "- bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF\n",
            "- deepseek-ai/deepseek-coder-1.3b-base\n",
            "- deepseek-ai/deepseek-coder-33b-base\n",
            "- deepseek-ai/deepseek-coder-6.7b-instruct\n",
            "- deepseek-ai/deepseek-coder-5.7bmqa-base\n",
            "- deepseek-ai/deepseek-llm-7b-chat\n",
            "- deepseek-ai/deepseek-llm-67b-base\n",
            "- deepseek-ai/deepseek-llm-67b-chat\n",
            "- deepseek-ai/deepseek-moe-16b-base\n",
            "- deepseek-ai/deepseek-moe-16b-chat\n",
            "- mcanoglu/deepseek-ai-deepseek-coder-1.3b-base-finetuned-defect-detection\n",
            "- mcanoglu/deepseek-ai-deepseek-coder-1.3b-base-finetuned-defect-cwe-group-detection\n",
            "- deepseek-ai/deepseek-coder-7b-base-v1.5\n",
            "- deepseek-ai/deepseek-coder-7b-instruct-v1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Define the correct path to the training data JSON file\n",
        "dataset_path = '/content/DeepSeek-671B-SFT-Guide/data/train_example.json'\n",
        "\n",
        "print(f\"Loading dataset from {dataset_path}...\")\n",
        "# Load the JSON dataset\n",
        "dataset = load_dataset('json', data_files=dataset_path)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(dataset)\n",
        "\n",
        "# Define a formatting function\n",
        "def format_chat(example):\n",
        "    formatted_text = \"\"\n",
        "    for message in example['messages']:\n",
        "        if message['role'] == 'system':\n",
        "            # Assuming system message sets the context, often not explicitly tokenized in some formats\n",
        "            # You might adjust this based on the specific system tokenization approach\n",
        "            formatted_text += f\"<|im_start|>system\\n{message['content']}<|im_end|>\\n\"\n",
        "        elif message['role'] == 'user':\n",
        "            formatted_text += f\"<|im_start|>user\\n{message['content']}<|im_end|>\\n\"\n",
        "        elif message['role'] == 'assistant':\n",
        "            # Assume assistant turn should be the target for training, potentially add a loss=True flag handling if needed\n",
        "            formatted_text += f\"<|im_start|>assistant\\n{message['content']}<|im_end|>\\n\"\n",
        "        # Add other roles if necessary\n",
        "\n",
        "    # The Qwen tokenizer uses <|im_end|> to mark the end of a turn.\n",
        "    # We might need a final EOS token depending on the model/tokenizer.\n",
        "    # Qwen models typically use <|endoftext|> as EOS.\n",
        "    formatted_text += \"<|endoftext|>\"\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "print(\"Applying formatting function to the dataset...\")\n",
        "# Apply the formatting function\n",
        "formatted_dataset = dataset.map(format_chat)\n",
        "\n",
        "print(\"Dataset formatted successfully.\")\n",
        "print(formatted_dataset)\n",
        "\n",
        "# Display an example from the formatted dataset to verify\n",
        "print(\"\\nExample from the formatted dataset:\")\n",
        "print(formatted_dataset['train'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eABfwuTRuwR",
        "outputId": "fda1b2e2-f46e-4ee6-fe00-c8dbd9cf7cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from /content/DeepSeek-671B-SFT-Guide/data/train_example.json...\n",
            "Dataset loaded successfully.\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['messages'],\n",
            "        num_rows: 1\n",
            "    })\n",
            "})\n",
            "Applying formatting function to the dataset...\n",
            "Dataset formatted successfully.\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['messages', 'text'],\n",
            "        num_rows: 1\n",
            "    })\n",
            "})\n",
            "\n",
            "Example from the formatted dataset:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "9.11和9.2谁更大<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "嗯，我现在要比较两个数，9.11和9.2，看看哪一个更大。首先，我应该先回忆一下小数比较的规则。可能的话，先对齐小数点后的位数，然后从高位开始逐位比较。\n",
            "\n",
            "首先，这两个数都是小数，整数部分都是9，所以整数部分一样。接下来需要比较小数部分，也就是小数点后的数字。这时候，9.11的小数部分是0.11，而9.2的小数部分是0.2。不过，可能这里有个问题，因为它们的位数不同，一个是两位小数，一个是一位小数，可能需要统一位数后再比较。\n",
            "\n",
            "为了更清楚地比较，我可以把这两个数写成相同的小数位数。比如，把9.2写成9.20，这样它们的小数部分都是两位了。这样，9.11的小数部分是11，而9.20的小数部分是20。这时候，20显然比11大，所以9.20比9.11大，也就是说9.2比9.11大。\n",
            "\n",
            "不过，我是不是哪里弄错了？比如，有没有可能把小数位数搞混了？比如，9.2其实等于9.20，对吗？因为末尾的零不影响数值大小。所以，比较的时候，把9.2看成9.20，那0.20确实比0.11大。所以9.2更大。\n",
            "\n",
            "或者，我可以不用补零，直接比较小数第一位。比如，比较9.11和9.2，小数点后的第一位分别是1和2。这时候，第二位的话，9.2的小数点后第一位是2，而9.11的小数点后第一位是1，所以不管后面的数字是什么，9.2的小数部分第一位已经比9.11的大了，所以整体更大。比如，比如，如果比较的是0.2和0.11，0.2等于0.20，而0.20的十分位是2，0.11的十分位是1，所以0.2更大。因此，9.2的十分位是2，而9.11的十分位是1，所以9.2更大。\n",
            "\n",
            "不过，可能还有一种方法是把两个数都转换成分数来比较。比如，9.11等于9又11/100，而9.2等于9又2/10，也就是9又20/100。这样比较的话，显然20/100比11/100大，所以9.2更大。\n",
            "\n",
            "或者，用减法的方式比较，用9.2减去9.11，结果如果是正数，说明9.2大；如果是负数，说明9.11大。计算的话，9.2减9.11等于0.09，是正数，所以9.2更大。\n",
            "\n",
            "综上所述，不管用哪种方法，都是9.2更大。所以答案应该是9.2比9.11大。\n",
            "</think>\n",
            "\n",
            "9.2比9.11更大。以下是详细的比较过程：\n",
            "\n",
            "1. **整数部分相同**：两个数的整数部分都是9，因此需要进一步比较小数部分。\n",
            "\n",
            "2. **统一小数位数**：将9.2补零为9.20，使其与9.11的小数位数对齐。此时比较小数部分：\n",
            "   - 9.20的小数部分为20（即0.20），而9.11的小数部分为11（即0.11）。\n",
            "   - 由于20 > 11，因此**9.20 > 9.11**，即9.2 > 9.11。\n",
            "\n",
            "3. **直接逐位比较**：\n",
            "   - 比较小数点后第一位：9.2的十分位是2，9.11的十分位是1。\n",
            "   - 由于2 > 1，无需比较后续位数即可得出**9.2 > 9.11**。\n",
            "\n",
            "4. **分数验证**：\n",
            "   - 9.11 = \\( 9 \\frac{11}{100} \\)，9.2 = \\( 9 \\frac{20}{100} \\)。\n",
            "   - 显然，\\( \\frac{20}{100} > \\frac{11}{100} \\)，故9.2更大。\n",
            "\n",
            "5. **减法验证**：\n",
            "   - 计算 \\( 9.2 - 9.11 = 0.09 \\)，结果为正数，说明9.2更大。\n",
            "\n",
            "**结论**：通过多种方法验证，**9.2** 比 **9.11** 大。<|im_end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Define the Hugging Face model ID\n",
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "print(f\"Loading model: {model_id}\")\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "#use_cache=False`\n",
        "\n",
        "model.use_cache = False\n",
        "\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(f\"Loading tokenizer: {model_id}\")\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "print(\"\\nModel and Tokenizer loaded for fine-tuning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHa-2iR8SnwM",
        "outputId": "ed97b855-ec72-40cd-e930-f634ca159a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Model loaded successfully.\n",
            "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Tokenizer loaded successfully.\n",
            "\n",
            "Model and Tokenizer loaded for fine-tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "A4kUr7JSKu9A",
        "outputId": "8403ae98-8528-4211-a471-025f31739797"
      },
      "source": [
        "# After reinstallation, re-attempt to initialize trainer and start training\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/sft_trainer_output\",  # Output directory for checkpoints and logs\n",
        "    num_train_epochs=10,                 # Number of training epochs\n",
        "    per_device_train_batch_size=4,      # Batch size per GPU\n",
        "    learning_rate=2e-5,                 # Learning rate\n",
        "    bf16=True,                          # Enable bfloat16 training (L4 supports it)\n",
        "    logging_steps=1,                   # Log every 10 steps\n",
        "    save_steps=1,                     # Save checkpoint every 100 steps\n",
        "    save_total_limit=2,                 # Keep at most 2 checkpoints\n",
        "    gradient_accumulation_steps=2,      # Accumulate gradients over 2 steps\n",
        "    gradient_checkpointing=True,        # Enable gradient checkpointing\n",
        "    report_to=\"none\",                   # Disable reporting to external platforms\n",
        "    push_to_hub=False,                  # Do not push to Hugging Face Hub\n",
        "    remove_unused_columns=False,        # Keep unused columns in dataset\n",
        ")\n",
        "\n",
        "# Check if tokenizer has a pad token, if not, set it to eos_token\n",
        "# Assuming 'tokenizer' and 'formatted_dataset' are available from previous successful steps\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=formatted_dataset['train'], # Use the 'train' split of the dataset\n",
        "    args=training_args,                   # Pass the training arguments\n",
        ")\n",
        "\n",
        "print(\"\\nTraining arguments and SFTTrainer re-initialized.\")\n",
        "print(\"Starting the fine-tuning process...\")\n",
        "# Start training\n",
        "trainer.train()\n",
        "print(\"Fine-tuning process initiated.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training arguments and SFTTrainer re-initialized.\n",
            "Starting the fine-tuning process...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 10:35, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.115400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.789800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.451700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.345400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.146900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.126900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.109600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.103300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.097600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.093600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning process initiated.\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Knowledge_Base_Demo_(Python)_with_OpenAI_LLM_(Colab_Userdata).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import httpx # Import httpx for handling potential network errors\n",
        "import asyncio # Import asyncio for asynchronous operations\n",
        "import nest_asyncio # Import nest_asyncio to allow nested event loops in interactive environments\n",
        "\n",
        "# Apply nest_asyncio to allow asyncio.run() to be called from a running loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- OpenAI API Key Setup ---\n",
        "# Using google.colab.userdata as requested for Colab environment\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local execution)\n",
        "    print(\"google.colab.userdata not found. Attempting to use OPENAI_API_KEY from environment variables.\")\n",
        "    api_key = os.getenv('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        print(\"WARNING: OPENAI_API_KEY environment variable not found. Please set it for local runs.\")\n",
        "        print(\"Using a placeholder API key. LLM calls will fail until a valid key is provided.\")\n",
        "\n",
        "\n",
        "if not api_key:\n",
        "    # This placeholder will cause API calls to fail but allows the script to run for structural demo.\n",
        "    client = OpenAI(api_key=\"sk-your-placeholder-key\")\n",
        "else:\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Simulated Knowledge Base data\n",
        "# Each chunk has an 'id', 'text', and properties to simulate its temporal status\n",
        "knowledge_base = [\n",
        "    {\n",
        "        'id': 'chunk1_ceo_old',\n",
        "        'text': 'The CEO of ABC is John Smith.',\n",
        "        'isOutdated': True,  # This chunk is explicitly marked as outdated\n",
        "        'outdatedReason': 'Replaced by new CEO',\n",
        "        'semanticMatchScore': 0.9  # Higher semantic similarity for CEO query\n",
        "    },\n",
        "    {\n",
        "        'id': 'chunk2_ceo_new',\n",
        "        'text': 'Jenna Brown became CEO of ABC in January 2025, replacing John Smith.',\n",
        "        'isOutdated': False,  # This chunk is current\n",
        "        'semanticMatchScore': 0.7  # Slightly lower semantic similarity but contains the latest info for CEO\n",
        "    },\n",
        "    {\n",
        "        'id': 'chunk3_product_old',\n",
        "        'text': 'Company XYZ\\'s new smartphone, the \\'Spectra X\\', was announced for release in October 2024.',\n",
        "        'isOutdated': True,\n",
        "        'outdatedReason': 'Release postponed',\n",
        "        'semanticMatchScore': 0.85 # Higher semantic similarity for product release query\n",
        "    },\n",
        "    {\n",
        "        'id': 'chunk4_product_new',\n",
        "        'text': 'Due to supply chain issues, Company XYZ has postponed the \\'Spectra X\\' smartphone release to Q1 2025.',\n",
        "        'isOutdated': False,\n",
        "        'semanticMatchScore': 0.6 # Lower semantic similarity for product release query, but is current\n",
        "    }\n",
        "]\n",
        "\n",
        "def extract_keywords(question):\n",
        "    \"\"\"\n",
        "    Extracts relevant keywords from the question to identify appropriate chunks.\n",
        "    This is a simplified version for demo purposes.\n",
        "    \"\"\"\n",
        "    question_lower = question.lower()\n",
        "    keywords = []\n",
        "    if \"ceo\" in question_lower:\n",
        "        keywords.append(\"ceo\")\n",
        "        if \"abc\" in question_lower:\n",
        "            keywords.append(\"abc\")\n",
        "    if \"spectra x\" in question_lower:\n",
        "        # Use a phrase for better matching\n",
        "        keywords.append(\"spectra x\")\n",
        "    if \"smartphone\" in question_lower:\n",
        "        keywords.append(\"smartphone\")\n",
        "    if \"release\" in question_lower or \"date\" in question_lower:\n",
        "        keywords.append(\"release\")\n",
        "    return keywords\n",
        "\n",
        "def get_relevant_chunks(question, kb):\n",
        "    \"\"\"\n",
        "    Filters the knowledge base for chunks relevant to the question based on keywords.\n",
        "    \"\"\"\n",
        "    question_keywords = extract_keywords(question)\n",
        "    relevant_chunks = []\n",
        "    for chunk in kb:\n",
        "        chunk_lower = chunk['text'].lower()\n",
        "        # A chunk is considered relevant if it contains at least one of the question's keywords\n",
        "        # or a specific unique identifier from the chunk (e.g., 'abc' or 'spectra x')\n",
        "        is_relevant = False\n",
        "        for kw in question_keywords:\n",
        "            if kw in chunk_lower:\n",
        "                is_relevant = True\n",
        "                break\n",
        "        if is_relevant:\n",
        "            relevant_chunks.append(chunk)\n",
        "    return relevant_chunks\n",
        "\n",
        "async def simulate_llm_answer(context_text, question):\n",
        "    \"\"\"\n",
        "    Makes an actual API call to an LLM (OpenAI's GPT-4o) to generate an answer\n",
        "    based on the provided context and question.\n",
        "    \"\"\"\n",
        "    print(\"  (Calling LLM...)\")\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context. If the answer is not in the context, state that you cannot find it.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Context: {context_text}\\nQuestion: {question}\\nAnswer:\"}\n",
        "            ],\n",
        "            max_completion_tokens=50\n",
        "        )\n",
        "        if response.choices and response.choices[0].message:\n",
        "            return response.choices[0].message.content.strip()\n",
        "        else:\n",
        "            return \"LLM did not return a valid response.\"\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        print(f\"  Error calling LLM (HTTP status {e.response.status_code}): {e.response.text}\")\n",
        "        return \"Error: Could not get answer from LLM due to API error.\"\n",
        "    except Exception as e:\n",
        "        print(f\"  An unexpected error occurred during LLM call: {e}\")\n",
        "        return \"Error: Could not get answer from LLM.\"\n",
        "\n",
        "\n",
        "async def query_classical_kb(question):\n",
        "    \"\"\"\n",
        "    Simulates querying a Classical Knowledge Base.\n",
        "    Prioritizes semantic similarity (semanticMatchScore) among relevant chunks,\n",
        "    regardless of their temporal status.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Classical Knowledge Base Response for: '{question}' ---\")\n",
        "    print(\"Processing...\")\n",
        "\n",
        "    relevant_chunks = get_relevant_chunks(question, knowledge_base)\n",
        "\n",
        "    retrieved_chunk = None\n",
        "    if relevant_chunks:\n",
        "        # Classical: Prioritize based on semanticMatchScore among all relevant chunks\n",
        "        retrieved_chunk = max(relevant_chunks, key=lambda x: x.get('semanticMatchScore', 0))\n",
        "\n",
        "    if not retrieved_chunk:\n",
        "        print(\"No relevant context found for the classical KB.\")\n",
        "        answer = \"No relevant context found.\"\n",
        "    else:\n",
        "        print(f\"Retrieved Context: {retrieved_chunk['text']}\")\n",
        "        answer = await simulate_llm_answer(retrieved_chunk['text'], question)\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "\n",
        "async def query_temporal_kb(question):\n",
        "    \"\"\"\n",
        "    Simulates querying a Temporal Knowledge Base.\n",
        "    First, finds relevant chunks. Among those, it prioritizes the non-outdated ones.\n",
        "    If no current relevant chunk is found, it falls back to the most semantically\n",
        "    relevant chunk (which might be outdated).\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Temporal Knowledge Base Response for: '{question}' ---\")\n",
        "    print(\"Processing...\")\n",
        "\n",
        "    relevant_chunks = get_relevant_chunks(question, knowledge_base)\n",
        "\n",
        "    current_relevant_chunks = [chunk for chunk in relevant_chunks if not chunk.get('isOutdated', True)]\n",
        "\n",
        "    retrieved_chunk = None\n",
        "    if current_relevant_chunks:\n",
        "        # Temporal: Prioritize non-outdated, and then by semanticMatchScore among them\n",
        "        retrieved_chunk = max(current_relevant_chunks, key=lambda x: x.get('semanticMatchScore', 0))\n",
        "    elif relevant_chunks:\n",
        "        # Fallback: If no current relevant chunk, use the most semantically relevant chunk\n",
        "        # (even if it's outdated, because it's still the best \"relevant\" option)\n",
        "        print(\"No current relevant chunk found for the temporal KB. Falling back to most semantically relevant chunk (may be outdated).\")\n",
        "        retrieved_chunk = max(relevant_chunks, key=lambda x: x.get('semanticMatchScore', 0))\n",
        "\n",
        "    if not retrieved_chunk:\n",
        "        print(\"No relevant context found for the temporal KB.\")\n",
        "        answer = \"No relevant context found.\"\n",
        "    else:\n",
        "        print(f\"Retrieved Context: {retrieved_chunk['text']}\")\n",
        "        answer = await simulate_llm_answer(retrieved_chunk['text'], question)\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main function to run the automatic demo with multiple test cases.\"\"\"\n",
        "    print(\"Welcome to the Knowledge Base Temporal Demo (Fully Automatic)!\")\n",
        "    print(\"This version uses OpenAI's GPT-4o to generate answers.\")\n",
        "    print(\"It attempts to fetch the API key using `google.colab.userdata`.\")\n",
        "\n",
        "    test_cases = [\n",
        "        \"Who is the CEO of Company ABC?\",\n",
        "        \"What is the release date of the Spectra X smartphone?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_cases:\n",
        "        print(f\"\\n--- Running Test Case: '{question}' ---\")\n",
        "        await query_classical_kb(question)\n",
        "        await query_temporal_kb(question)\n",
        "        print(\"-\" * 50) # Separator for test cases\n",
        "\n",
        "    print(\"\\nAutomatic demo finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # The nest_asyncio.apply() call at the top of the script handles nested event loops.\n",
        "    # We can now simply run asyncio.run(main()) directly.\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aQwQElfV1KU7",
        "outputId": "76d4c837-0322-4021-af01-7be13f8b624f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Knowledge Base Temporal Demo (Fully Automatic)!\n",
            "This version uses OpenAI's GPT-4o to generate answers.\n",
            "It attempts to fetch the API key using `google.colab.userdata`.\n",
            "\n",
            "--- Running Test Case: 'Who is the CEO of Company ABC?' ---\n",
            "\n",
            "--- Classical Knowledge Base Response for: 'Who is the CEO of Company ABC?' ---\n",
            "Processing...\n",
            "Retrieved Context: The CEO of ABC is John Smith.\n",
            "  (Calling LLM...)\n",
            "Answer: The CEO of Company ABC is John Smith.\n",
            "\n",
            "--- Temporal Knowledge Base Response for: 'Who is the CEO of Company ABC?' ---\n",
            "Processing...\n",
            "Retrieved Context: Jenna Brown became CEO of ABC in January 2025, replacing John Smith.\n",
            "  (Calling LLM...)\n",
            "Answer: I cannot determine the current CEO of Company ABC as the context only provides information up to January 2025, when Jenna Brown became CEO.\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Running Test Case: 'What is the release date of the Spectra X smartphone?' ---\n",
            "\n",
            "--- Classical Knowledge Base Response for: 'What is the release date of the Spectra X smartphone?' ---\n",
            "Processing...\n",
            "Retrieved Context: Company XYZ's new smartphone, the 'Spectra X', was announced for release in October 2024.\n",
            "  (Calling LLM...)\n",
            "Answer: I cannot find the specific release date of the Spectra X smartphone from the provided context, only that it was announced for release in October 2024.\n",
            "\n",
            "--- Temporal Knowledge Base Response for: 'What is the release date of the Spectra X smartphone?' ---\n",
            "Processing...\n",
            "Retrieved Context: Due to supply chain issues, Company XYZ has postponed the 'Spectra X' smartphone release to Q1 2025.\n",
            "  (Calling LLM...)\n",
            "Answer: The release date of the Spectra X smartphone is postponed to Q1 2025. However, an exact date has not been specified.\n",
            "--------------------------------------------------\n",
            "\n",
            "Automatic demo finished.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
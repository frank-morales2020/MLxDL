{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMSYuuyf/qUP33JGdpJlOcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/test_disitll.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "frankmorales@Franks-MacBook-Air ~ % ssh -i  /Users/frankmorales/lambda/poc-ft.pem ubuntu@192.222.53.76                                         \n",
        "Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-60-generic x86_64)\n",
        " .============.\n",
        " ||   __      ||    _                    _         _\n",
        " ||   \\_\\     ||   | |    __ _ _ __ ___ | |__   __| | __ _\n",
        " ||    \\_\\    ||   | |   / _` | '_ ` _ \\| '_ \\ / _` |/ _` |\n",
        " ||   /_λ_\\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |\n",
        " ||  /_/ \\_\\  ||   |_____\\__,_|_| |_| |_|_.__/ \\__,_|\\__,_|\n",
        "  .============.                                  GPU CLOUD\n",
        "\n",
        " * Documentation:  https://help.ubuntu.com\n",
        " * Management:     https://landscape.canonical.com\n",
        " * Support:        https://ubuntu.com/pro\n",
        "\n",
        "Last login: Sat Sep 13 09:39:32 2025 from 107.171.187.251\n",
        "To run a command as administrator (user \"root\"), use \"sudo <command>\".\n",
        "See \"man sudo_root\" for details.\n",
        "\n",
        "ubuntu@192-222-53-76:~$ . mistral/bin/activate\n",
        "(mistral) ubuntu@192-222-53-76:~$  python -c \"import torch; torch.cuda.empty_cache(); import gc; gc.collect()\"\n",
        "(mistral) ubuntu@192-222-53-76:~$  python qwen3tomistral-distill-V2.py\n",
        "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
        "Successfully logged in to Hugging Face Hub.\n",
        "Loading teacher model: Qwen/Qwen3-Next-80B-A3B-Instruct with quantization...\n",
        "The fast path is not available because one of the required library is not installed. Falling back to torch implementation. To install follow https://github.com/fla-org/flash-linear-attention#installation and https://github.com/Dao-AILab/causal-conv1d\n",
        "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [01:35<00:00,  2.34s/it]\n",
        "Generating distillation dataset with teacher model...\n",
        "Generating batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [09:39<00:00, 115.96s/it]\n",
        "Dataset with 20 samples created.\n",
        "Cleaning up teacher model to free memory...\n",
        "Loading student model: mistralai/Mistral-7B-v0.1...\n",
        "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.47s/it]\n",
        "Setting up the SFTTrainer...\n",
        "Generating train split: 3 examples [00:00, 640.51 examples/s]\n",
        "Executing fine-tuning for 3 epochs...\n",
        "Starting fine-tuning...\n",
        "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
        "  0%|                                                                                                                                                                           | 0/3 [00:00<?, ?it/s]Casting fp32 inputs back to torch.float16 for flash-attn compatibility.\n",
        "{'loss': 0.9107, 'grad_norm': 123838.6171875, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 0, 'train_runtime': 1.4158, 'train_tokens_per_second': 0.0}                                \n",
        "{'loss': 0.9057, 'grad_norm': 115116.4921875, 'learning_rate': 4e-05, 'epoch': 2.0, 'num_input_tokens_seen': 0, 'train_runtime': 2.3569, 'train_tokens_per_second': 0.0}                              \n",
        "{'loss': 0.7662, 'grad_norm': 90711.140625, 'learning_rate': 8e-05, 'epoch': 3.0, 'num_input_tokens_seen': 0, 'train_runtime': 3.3034, 'train_tokens_per_second': 0.0}                                \n",
        "{'train_runtime': 3.8236, 'train_samples_per_second': 2.354, 'train_steps_per_second': 0.785, 'train_loss': 0.8608601093292236, 'epoch': 3.0, 'num_input_tokens_seen': 0}                             \n",
        "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.27s/it]\n",
        "Saving the fine-tuned model locally to ./mistral-7b-qwen-Next-80B-A3B-Instruct-distilled...\n",
        "Uploading model to Hugging Face Hub: frankmorales2020/mistral-7b-qwen-Next-80B-A3B-Instruct-distilled...\n",
        "Processing Files (1 / 1)                : 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████|  168MB /  168MB, 64.6MB/s  \n",
        "New Data Upload                         : 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████|  168MB /  168MB, 64.6MB/s  \n",
        "  ...distilled/adapter_model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████|  168MB /  168MB            \n",
        "No files have been modified since last commit. Skipping to prevent empty commit.\n",
        "Model and model card successfully uploaded to frankmorales2020/mistral-7b-qwen-Next-80B-A3B-Instruct-distilled!\n",
        "Distillation, fine-tuning, and upload complete!"
      ],
      "metadata": {
        "id": "XmMvBYglD19j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation -q"
      ],
      "metadata": {
        "id": "HdRG2A7JATJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIBJeyby-oL9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base + adapter (uses FP16 for efficiency)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\"  # Optional: For speed\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, \"frankmorales2020/mistral-7b-qwen-Next-80B-A3B-Instruct-distilled\")\n",
        "\n",
        "# Test prompt (from your dataset)\n",
        "prompt = \"### Instruction:\\nWrite a Python function to compute Fibonacci numbers efficiently.\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mP4dus0Blj8",
        "outputId": "0097f2ea-5380-4e98-f404-3f4e25511520"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Write a Python function to compute Fibonacci numbers efficiently.\n",
            "\n",
            "### Response:\n",
            "\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    if n == 0 or n == 1:\n",
            "        return n\n",
            "    prev = 0\n",
            "    curr = 1\n",
            "    for i in range(2, n + 1):\n",
            "        temp = curr\n",
            "        curr += prev\n",
            "        prev = temp\n",
            "    return curr\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "The function `fibonacci(n)` takes a single integer `n` as input and returns the `n`th Fibonacci number.\n",
            "\n",
            "The function uses the recursive definition of the Fibonacci sequence, which states that the `n`th Fibonacci number is the sum of the `(n - 1)`th and `(n - 2)`th Fibonacci numbers.\n",
            "\n",
            "The function uses the base case of `n == 0` or `n == 1` to return the input directly, as these are the initial values of the Fibonacci sequence.\n",
            "\n",
            "The function then initializes two variables, `prev` and `curr`, to hold the previous and current Fibonacci numbers, respectively.\n",
            "\n",
            "The function then uses a `for` loop to iterate from `2` to `n + 1`, computing the `n`th Fibonacci number by adding the current Fibonacci number to the previous Fibonacci number and assigning the result to the `prev` variable.\n",
            "\n",
            "The function returns the final value of `curr` as the `n`th Fibonacci number.\n",
            "\n",
            "Overall, this function computes the `n`th Fibonacci number efficiently by using a combination of recursion and iteration.\n"
          ]
        }
      ]
    }
  ]
}
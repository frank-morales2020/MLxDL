{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfEWhEdcuYjfTXuK3ZsesP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/transformermodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgEESGdGb-k5",
        "outputId": "25f77923-9d98-4802-85c4-6adefe76126b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/tools\n",
            "/content\n",
            "\n",
            "Start of epoch 100\n",
            "Epoch 100 Step 0 Loss 12702.9150 Accuracy 0.0069\n",
            "Epoch 100 Step 50 Loss 13765.0439 Accuracy 0.0447\n",
            "Epoch 100 Step 100 Loss 13634.1719 Accuracy 0.0423\n",
            "Epoch 100: Training Loss 13719.9746, Training Accuracy 0.0412\n",
            "Saved checkpoint at epoch 100\n",
            "\n",
            "Start of epoch 200\n",
            "Epoch 200 Step 0 Loss 19643.1680 Accuracy 0.0000\n",
            "Epoch 200 Step 50 Loss 19677.2012 Accuracy 0.0530\n",
            "Epoch 200 Step 100 Loss 20230.9805 Accuracy 0.0450\n",
            "Epoch 200: Training Loss 20204.7793, Training Accuracy 0.0454\n",
            "Saved checkpoint at epoch 200\n",
            "\n",
            "Start of epoch 300\n",
            "Epoch 300 Step 0 Loss 26234.7930 Accuracy 0.0000\n",
            "Epoch 300 Step 50 Loss 25846.5195 Accuracy 0.0453\n",
            "Epoch 300 Step 100 Loss 26496.2051 Accuracy 0.0458\n",
            "Epoch 300: Training Loss 25950.6016, Training Accuracy 0.0438\n",
            "Saved checkpoint at epoch 300\n",
            "\n",
            "Start of epoch 400\n",
            "Epoch 400 Step 0 Loss 31760.2109 Accuracy 0.0034\n",
            "Epoch 400 Step 50 Loss 29157.4785 Accuracy 0.0424\n",
            "Epoch 400 Step 100 Loss 30974.5859 Accuracy 0.0395\n",
            "Epoch 400: Training Loss 30346.6699, Training Accuracy 0.0415\n",
            "Saved checkpoint at epoch 400\n",
            "\n",
            "Start of epoch 500\n",
            "Epoch 500 Step 0 Loss 35374.1992 Accuracy 0.0172\n",
            "Epoch 500 Step 50 Loss 37744.8281 Accuracy 0.0454\n",
            "Epoch 500 Step 100 Loss 37758.1719 Accuracy 0.0428\n",
            "Epoch 500: Training Loss 37302.8555, Training Accuracy 0.0440\n",
            "Saved checkpoint at epoch 500\n",
            "\n",
            "Start of epoch 600\n",
            "Epoch 600 Step 0 Loss 34834.0703 Accuracy 0.0000\n",
            "Epoch 600 Step 50 Loss 38738.2266 Accuracy 0.0444\n",
            "Epoch 600 Step 100 Loss 40136.1797 Accuracy 0.0419\n",
            "Epoch 600: Training Loss 41028.4336, Training Accuracy 0.0438\n",
            "Saved checkpoint at epoch 600\n",
            "\n",
            "Start of epoch 700\n",
            "Epoch 700 Step 0 Loss 41131.6055 Accuracy 0.0069\n",
            "Epoch 700 Step 50 Loss 40556.6953 Accuracy 0.0473\n",
            "Epoch 700 Step 100 Loss 40910.8633 Accuracy 0.0423\n",
            "Epoch 700: Training Loss 41528.7383, Training Accuracy 0.0422\n",
            "Saved checkpoint at epoch 700\n",
            "\n",
            "Start of epoch 800\n",
            "Epoch 800 Step 0 Loss 55816.5469 Accuracy 0.0172\n",
            "Epoch 800 Step 50 Loss 49575.4062 Accuracy 0.0499\n",
            "Epoch 800 Step 100 Loss 47982.0938 Accuracy 0.0451\n",
            "Epoch 800: Training Loss 46819.1133, Training Accuracy 0.0443\n",
            "Saved checkpoint at epoch 800\n",
            "\n",
            "Start of epoch 900\n",
            "Epoch 900 Step 0 Loss 40027.1641 Accuracy 0.0517\n",
            "Epoch 900 Step 50 Loss 47034.4961 Accuracy 0.0453\n",
            "Epoch 900 Step 100 Loss 48180.8164 Accuracy 0.0477\n",
            "Epoch 900: Training Loss 48714.7734, Training Accuracy 0.0463\n",
            "Saved checkpoint at epoch 900\n",
            "\n",
            "Start of epoch 1000\n",
            "Epoch 1000 Step 0 Loss 56164.6719 Accuracy 0.0000\n",
            "Epoch 1000 Step 50 Loss 56732.6914 Accuracy 0.0529\n",
            "Epoch 1000 Step 100 Loss 54827.3008 Accuracy 0.0462\n",
            "Epoch 1000: Training Loss 54346.4141, Training Accuracy 0.0461\n",
            "Saved checkpoint at epoch 1000\n",
            "\n",
            "Total time taken: 7002.31s\n"
          ]
        }
      ],
      "source": [
        "# https://machinelearningmastery.com/training-the-transformer-model/\n",
        "# https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\n",
        "\n",
        "# MY REPOSITORY (Developed: September 24, 2023)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TOOLS Locations:\n",
        "basepath='/content/drive/MyDrive/tools'\n",
        "#/content/drive/MyDrive/tools/utils.py\n",
        "%cd /content/drive/MyDrive/tools/\n",
        "from utils import TransformerModel,PrepareDataset\n",
        "%cd /content/\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "#original in the code\n",
        "#from model import TransformerModel\n",
        "#from prepare_dataset import PrepareDataset\n",
        "\n",
        "from time import time\n",
        "\n",
        "# Define the model parameters\n",
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_model = 512  # Dimensionality of model layers' outputs\n",
        "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
        "n = 6  # Number of layers in the encoder stack\n",
        "\n",
        "# Define the training parameters\n",
        "epochs = 1000 #my choice originally was 2\n",
        "batch_size = 64\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.98\n",
        "epsilon = 1e-9\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Implementing a learning rate scheduler\n",
        "class LRScheduler(LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
        "        super(LRScheduler, self).__init__(**kwargs)\n",
        "\n",
        "        self.d_model = cast(d_model, int64)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step_num):\n",
        "\n",
        "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
        "        arg1 = step_num ** int(-0.5)\n",
        "        arg2 = step_num * (self.warmup_steps ** int(1.5))\n",
        "\n",
        "        return (self.d_model ** int(-0.5)) * math.minimum(arg1, arg2)\n",
        "\n",
        "# Instantiate an Adam optimizer\n",
        "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n",
        "\n",
        "# Prepare the training and test splits of the dataset\n",
        "dataset = PrepareDataset()\n",
        "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('/content/drive/MyDrive/tools/english-german-both.pkl')\n",
        "\n",
        "# Prepare the dataset batches\n",
        "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "# Create model\n",
        "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
        "\n",
        "\n",
        "# Defining the loss function\n",
        "def loss_fcn(target, prediction):\n",
        "    # Create mask so that the zero padding values are not included in the computation of loss\n",
        "    padding_mask = math.logical_not(equal(target, 0))\n",
        "    padding_mask = cast(padding_mask, float32)\n",
        "\n",
        "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
        "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n",
        "\n",
        "    # Compute the mean loss over the unmasked values\n",
        "    return reduce_sum(loss) / reduce_sum(padding_mask)\n",
        "\n",
        "\n",
        "# Defining the accuracy function\n",
        "def accuracy_fcn(target, prediction):\n",
        "    # Create mask so that the zero padding values are not included in the computation of accuracy\n",
        "    padding_mask = math.logical_not(equal(target, 0))\n",
        "\n",
        "    # Find equal prediction and target values, and apply the padding mask\n",
        "    accuracy = equal(target, argmax(prediction, axis=2))\n",
        "    accuracy = math.logical_and(padding_mask, accuracy)\n",
        "\n",
        "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
        "    padding_mask = cast(padding_mask, float32)\n",
        "    accuracy = cast(accuracy, float32)\n",
        "\n",
        "    # Compute the mean accuracy over the unmasked values\n",
        "    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n",
        "\n",
        "\n",
        "# Include metrics monitoring\n",
        "train_loss = Mean(name='train_loss')\n",
        "train_accuracy = Mean(name='train_accuracy')\n",
        "\n",
        "# Create a checkpoint object and manager to manage multiple checkpoints\n",
        "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
        "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n",
        "\n",
        "# Speeding up the training process\n",
        "@function\n",
        "def train_step(encoder_input, decoder_input, decoder_output):\n",
        "    with GradientTape() as tape:\n",
        "\n",
        "        # Run the forward pass of the model to generate a prediction\n",
        "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
        "\n",
        "        # Compute the training loss\n",
        "        loss = loss_fcn(decoder_output, prediction)\n",
        "\n",
        "        # Compute the training accuracy\n",
        "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
        "\n",
        "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
        "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
        "\n",
        "    # Update the values of the trainable variables by gradient descent\n",
        "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy)\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
        "\n",
        "    #start_time = time()\n",
        "\n",
        "    # Iterate over the dataset batches\n",
        "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
        "\n",
        "        # Define the encoder and decoder inputs, and the decoder output\n",
        "        encoder_input = train_batchX[:, 1:]\n",
        "        decoder_input = train_batchY[:, :-1]\n",
        "        decoder_output = train_batchY[:, 1:]\n",
        "\n",
        "        train_step(encoder_input, decoder_input, decoder_output)\n",
        "\n",
        "        if step % 50 == 0 and (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "            #print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "            # print(\"Samples so far: %s\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Print epoch number and loss value at the end of every epoch\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "          print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    # Save a checkpoint after every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        save_path = ckpt_manager.save()\n",
        "        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n",
        "\n",
        "print()\n",
        "print(\"Total time taken: %.2fs\" % (time() - start_time))\n"
      ]
    }
  ]
}
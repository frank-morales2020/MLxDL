{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9V3yq8LLuqdT",
        "5axbl8pdofkY",
        "DjA7Fi2H2JIr"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO3p4f5nMSPjlkU/72JJFFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_API_TUTORIAL_BTA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.mistral.ai/platform/client/\n",
        "\n",
        "https://github.com/mistralai/client-python"
      ],
      "metadata": {
        "id": "HFSQBSaVuYEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies and Settings"
      ],
      "metadata": {
        "id": "NesDaI4OICGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoPAKz9ycOCu",
        "outputId": "ed9938f8-f6cc-4359-b9e8-92ca2a7876b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 27 04:25:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   42C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Modules"
      ],
      "metadata": {
        "id": "S2X7iLVUPIrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install mistralai --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "ghqAXWE-xJa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Modules"
      ],
      "metadata": {
        "id": "k9lF1C1DNgKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mistralai\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "import os\n",
        "import colab_env\n",
        "import json"
      ],
      "metadata": {
        "id": "a87Hog_P8h9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310feece-6f58-4678-a66d-0835d17894ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MISTRAL API SETTINGS"
      ],
      "metadata": {
        "id": "9V3yq8LLuqdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = MistralClient(api_key=api_key)"
      ],
      "metadata": {
        "id": "NnLef38upn75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Definitions"
      ],
      "metadata": {
        "id": "5axbl8pdofkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mistral_chat(prompt,model,client,stream):\n",
        "  client = MistralClient(api_key=api_key)\n",
        "  messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "  if stream==True:\n",
        "     print(\"Streaming\")\n",
        "     print()\n",
        "     print('Answer: ')\n",
        "     for chunk in client.chat_stream(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        ):\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            print(chunk.choices[0].delta.content, end=\"\")\n",
        "  else:\n",
        "\n",
        "     #print(\"No streaming\")\n",
        "     chat_response = client.chat(model=model,messages=messages, safe_mode=True,temperature=0.9)\n",
        "     #print(chat_response.choices[0].message.content)\n",
        "     return chat_response"
      ],
      "metadata": {
        "id": "CQPXU56gJGNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction','output')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
        "    RESPONSE_KEY = \"### Output:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"\\n{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\"\n",
        "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "LY2NjeYtpGet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managenment"
      ],
      "metadata": {
        "id": "Rwwy34zSFHD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model list\n"
      ],
      "metadata": {
        "id": "mhLS0Qm6ExFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "date.fromisoformat('2024-05-20')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JLogBVEN4_e",
        "outputId": "91e3f254-f508-43ce-c89d-a2299ca9e51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.date(2024, 5, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.mistral.ai/getting-started/models/\n",
        "\n",
        "\n",
        "open-mixtral-8x22b: A bigger sparse mixture of experts model. As such, it leverages up to 176B parameters but only uses about 39B during inference, leading to better inference throughput at the cost of more vRAM. Learn more on the dedicated blog post"
      ],
      "metadata": {
        "id": "NgkyDoD1O5Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"object\": \"string\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"id\": \"string\",\n",
        "      \"object\": \"string\",\n",
        "      \"created\": 0,\n",
        "      \"owned_by\": \"string\"\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "p0EcqJQc41fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_list=client.list_models()\n",
        "model_list.data[5].id\n",
        "\n",
        "for model in model_list.data:\n",
        "    print(model.id)\n",
        "    #print(model.created)\n",
        "    #print(model.owned_by)"
      ],
      "metadata": {
        "id": "kJGSPIMaDBPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590b4002-5be3-4bc7-9242-fc973312df95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "open-mistral-7b\n",
            "mistral-tiny-2312\n",
            "mistral-tiny\n",
            "open-mixtral-8x7b\n",
            "open-mixtral-8x22b\n",
            "open-mixtral-8x22b-2404\n",
            "mistral-small-2312\n",
            "mistral-small\n",
            "mistral-small-2402\n",
            "mistral-small-latest\n",
            "mistral-medium-latest\n",
            "mistral-medium-2312\n",
            "mistral-medium\n",
            "mistral-large-latest\n",
            "mistral-large-2402\n",
            "mistral-embed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mistral.ai/news/mixtral-8x22b/"
      ],
      "metadata": {
        "id": "h1qb4TKwAnph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model=model_list.data[11].id # mistral-large-latest\n",
        "model=model_list.data[13].id\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5uO56886bzN0",
        "outputId": "a786bbdd-6335-4ffc-d117-d0d39ca4e8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mistral-large-latest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"What is the best French cheese?\")\n",
        "]\n",
        "\n",
        "model=model_list.data[13].id\n",
        "\n",
        "# No streaming\n",
        "chat_response = client.chat(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "fgqaQVZ6W7GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-j41dFC2Hz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BOB TRAVEL APP"
      ],
      "metadata": {
        "id": "DjA7Fi2H2JIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "strCIUnwX2u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= 'How do you plan out your trip? \\\n",
        "Bob is travelling to SAT from YVR \\\n",
        "1. He has a connection in DFW \\\n",
        "2. His connection is 6 hours long \\\n",
        "3. He has a budget of 100.00 including meals \\\n",
        "4. What can he do? Please suggest a time. \\\n",
        "5. Know- he is a hiker, museum, foodie, has a carry-on bag'\n",
        "\n",
        "print(prompt)\n",
        "\n",
        "response=mistral_chat(prompt,model,client,stream=False)\n",
        "\n",
        "print('Answer: ')\n",
        "#print(response.choices[0].message.content)\n",
        "to_markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "c5NHp3XLTCYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "wu2MnqQrYvzd",
        "outputId": "c1082071-fae9-42ab-948f-656844471063",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I'd be happy to help Bob plan his trip! Here's a suggested itinerary:\n",
            "\n",
            "1. **Preparation**: Bob should ensure he has all necessary travel documents, such as a valid ID or passport, and check-in online to save time at the airport. He should also pack his carry-on bag with essentials, including a change of clothes, travel-sized toiletries, and any necessary medications.\n",
            "\n",
            "2. **Flight from YVR to DFW**: Bob should arrive at YVR at least 2 hours before his flight to allow time for security and any unexpected delays. During the flight, he can rest or enjoy in-flight entertainment.\n",
            "\n",
            "3. **Layover at DFW**: Bob has a 6-hour layover at DFW. Here's a suggested breakdown:\n",
            "\n",
            "   - **Meals (approx. $40)**: DFW has a variety of dining options. Bob can enjoy a meal at a sit-down restaurant like the Salt Lick BBQ or grab a quick bite at one of the many fast-food outlets.\n",
            "\n",
            "   - **Explore the Airport (Free)**: DFW has several art installations and exhibits that Bob can visit for free. He can also take a walk around the terminals to stretch his legs.\n",
            "\n",
            "   - **Visit the DFW Airport Yoga Studio (Free)**: If Bob is interested, he can visit the yoga studio in Terminal D to relax and rejuvenate.\n",
            "\n",
            "   - **Store Carry-on (approx. $10)**: If Bob wants to explore outside the airport, he can store his carry-on at the baggage storage facility in Terminal E for a fee.\n",
            "\n",
            "   - **Explore Dallas (approx. $50)**: DFW offers a complimentary shuttle service to the nearby Grand Hyatt DFW where Bob can catch the DART (Dallas Area Rapid Transit) train into downtown Dallas. He can visit the Dallas Museum of Art (free general admission) or take a self-guided walking tour of the city. Remember to allocate time for travel to and from the airport, and ensure he's back in time for his next flight.\n",
            "\n",
            "4. **Flight from DFW to SAT**: After his layover, Bob will continue his journey to SAT. He should ensure he has some water and snacks for the flight if needed.\n",
            "\n",
            "5. **Arrival at SAT**: Upon arrival, Bob can proceed to his accommodation or next destination, ensuring he follows all local guidelines and regulations.\n"
          ]
        }
      ]
    }
  ]
}
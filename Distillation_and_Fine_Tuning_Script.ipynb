{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Distillation_and_Fine_Tuning_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script demonstrates how to distill knowledge from a teacher model\n",
        "# (Qwen3-Next-80B-A3B-Instruct) into a student model (Mistral 7B)\n",
        "# using Parameter-Efficient Fine-Tuning (PEFT) with LoRA,\n",
        "# and upload the fine-tuned model to Hugging Face Hub.\n",
        "\n",
        "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "#pip install wheel\n",
        "#pip install flash-attn --no-build-isolation -q\n",
        "#pip install git+https://github.com/huggingface/transformers.git@main -q\n",
        "#pip install triton==3.2.0 -q\n",
        "#pip install trl peft huggingface_hub -q\n",
        "#pip install bitsandbytes accelerate -q\n",
        "#pip install datasets -q\n",
        "\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login, HfApi\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import gc  # For memory management\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "# Define the models\n",
        "TEACHER_MODEL_NAME = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
        "STUDENT_MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Define the output directory for the fine-tuned model (local)\n",
        "OUTPUT_DIR = \"./mistral-7b-qwen-distilled\"\n",
        "\n",
        "# Define Hugging Face Hub repository for upload\n",
        "HF_REPO_ID = \"frankmorales2020/mistral-7b-qwen3next-80b-a3b-instruct-distilled\"\n",
        "\n",
        "# Define LoRA configuration\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# Define training hyperparameters\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 3\n",
        "GRADIENT_ACCUMULATION_STEPS = 4  # Added for clarity, already in TrainingArguments\n",
        "\n",
        "# --- 2. HUGGING FACE LOGIN ---\n",
        "# Log in to Hugging Face Hub\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    HF_TOKEN = input(\"Enter your Hugging Face access token: \")\n",
        "try:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(f\"Successfully logged in to Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to log in to Hugging Face Hub: {e}\")\n",
        "\n",
        "# --- 3. LOAD TEACHER MODEL ---\n",
        "print(f\"Loading teacher model: {TEACHER_MODEL_NAME} with quantization...\")\n",
        "teacher_quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "try:\n",
        "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "        TEACHER_MODEL_NAME,\n",
        "        quantization_config=teacher_quant_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME)\n",
        "    if teacher_tokenizer.pad_token is None:\n",
        "        teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load teacher model or tokenizer: {e}\")\n",
        "\n",
        "# --- 4. DATASET CREATION (KNOWLEDGE DISTILLATION) ---\n",
        "def generate_distillation_dataset(num_samples=1000):\n",
        "    \"\"\"\n",
        "    Generates a distillation dataset using the local teacher model.\n",
        "    \"\"\"\n",
        "    print(\"Generating distillation dataset with teacher model...\")\n",
        "\n",
        "    base_prompts = [\n",
        "        \"Explain the concept of quantum computing in one sentence.\",\n",
        "        \"Write a Python function to compute Fibonacci numbers efficiently.\",\n",
        "        \"Describe the process of photosynthesis.\",\n",
        "        \"What are the key principles of object-oriented programming?\",\n",
        "    ]\n",
        "    prompts = (base_prompts * (num_samples // len(base_prompts) + 1))[:num_samples]\n",
        "\n",
        "    dataset_data = []\n",
        "    teacher_model.eval()\n",
        "\n",
        "    gen_batch_size = 2\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(prompts), gen_batch_size), desc=\"Generating batches\"):\n",
        "            batch_prompts = prompts[i:i + gen_batch_size]\n",
        "\n",
        "            inputs = teacher_tokenizer(\n",
        "                batch_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "            ).to(teacher_model.device)\n",
        "\n",
        "            try:\n",
        "                outputs = teacher_model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=0.1,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=teacher_tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "                for j, output in enumerate(outputs):\n",
        "                    prompt_tokens = inputs.input_ids[j]\n",
        "                    full_text = teacher_tokenizer.decode(output, skip_special_tokens=True)\n",
        "                    response = full_text[len(teacher_tokenizer.decode(prompt_tokens, skip_special_tokens=True)):]\n",
        "                    response = response.strip()\n",
        "\n",
        "                    text = f\"### Instruction:\\n{batch_prompts[j]}\\n\\n### Response:\\n{response}\\n\"\n",
        "                    dataset_data.append({\"text\": text})\n",
        "            except Exception as e:\n",
        "                print(f\"Error during batch generation at index {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Dataset with {len(dataset_data)} samples created.\")\n",
        "    return Dataset.from_list(dataset_data)\n",
        "\n",
        "# Generate the dataset\n",
        "try:\n",
        "    distillation_dataset = generate_distillation_dataset(num_samples=100)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to generate distillation dataset: {e}\")\n",
        "\n",
        "# --- 5. FINE-TUNING ---\n",
        "print(f\"Loading student model: {STUDENT_MODEL_NAME}...\")\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        STUDENT_MODEL_NAME,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "    model.config.pretraining_tp = 1\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=LORA_TARGET_MODULES,\n",
        "    )\n",
        "\n",
        "    print(\"Setting up the SFTTrainer...\")\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=distillation_dataset,\n",
        "        peft_config=lora_config,\n",
        "        tokenizer=tokenizer,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "            warmup_steps=5,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            num_train_epochs=NUM_EPOCHS,\n",
        "            logging_steps=10,\n",
        "            output_dir=OUTPUT_DIR,\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "            fp16=True,\n",
        "            report_to=\"none\",\n",
        "        ),\n",
        "        max_seq_length=2048,\n",
        "        dataset_text_field=\"text\",\n",
        "    )\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to set up fine-tuning: {e}\")\n",
        "\n",
        "# --- 6. EXECUTE TRAINING ---\n",
        "print('\\n')\n",
        "print(f\"Executing fine-tuning for {NUM_EPOCHS} epochs...\")\n",
        "print(\"Starting fine-tuning...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Training failed: {e}\")\n",
        "\n",
        "# --- 7. SAVE AND UPLOAD MODEL ---\n",
        "print('\\n')\n",
        "print(f\"Saving the fine-tuned model locally to {OUTPUT_DIR}...\")\n",
        "print('\\n')\n",
        "try:\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "    print(f\"Uploading model to Hugging Face Hub: {HF_REPO_ID}...\")\n",
        "    trainer.model.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n",
        "    tokenizer.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n",
        "\n",
        "    api = HfApi()\n",
        "    model_card_content = f\"\"\"\n",
        "# Mistral-7B-Qwen3Next-Distilled\n",
        "\n",
        "This model is a fine-tuned version of {STUDENT_MODEL_NAME}, distilled from {TEACHER_MODEL_NAME} using LoRA and SFTTrainer.\n",
        "\n",
        "## Training Details\n",
        "- **Teacher Model**: {TEACHER_MODEL_NAME}\n",
        "- **Dataset**: Synthetic dataset of {len(distillation_dataset)} samples generated by the teacher.\n",
        "- **LoRA Config**: r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\n",
        "- **Training Hyperparams**: {NUM_EPOCHS} epochs, learning rate {LEARNING_RATE}, batch size {BATCH_SIZE}\n",
        "\n",
        "## Usage\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{STUDENT_MODEL_NAME}\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{STUDENT_MODEL_NAME}\")\n",
        "model = PeftModel.from_pretrained(model, \"{HF_REPO_ID}\")\n",
        "```\n",
        "\n",
        "Built with ❤️ using Hugging Face Transformers.\n",
        "\"\"\"\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=model_card_content.encode(),\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=HF_REPO_ID,\n",
        "        repo_type=\"model\",\n",
        "        token=HF_TOKEN,\n",
        "    )\n",
        "    print(f\"Model and model card successfully uploaded to {HF_REPO_ID}!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during save or upload: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 8. CLEANUP ---\n",
        "del teacher_model\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Distillation, fine-tuning, and upload complete!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qIvv2qK4DpGN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
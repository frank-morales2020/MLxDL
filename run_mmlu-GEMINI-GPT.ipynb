{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4DEQODbf/OzWvIakxc5up",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/run_mmlu-GEMINI-GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/neulab/gemini-benchmark/tree/main"
      ],
      "metadata": {
        "id": "6gyhqLVaa4J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neulab/gemini-benchmark.git"
      ],
      "metadata": {
        "id": "e1M1jyAlatKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gemini-benchmark/benchmarking"
      ],
      "metadata": {
        "id": "72mnecxXbEC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install litellm -q"
      ],
      "metadata": {
        "id": "n3WOBNPbbu17"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --upgrade -q\n",
        "!pip install openai -q\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "cyYnMNf-0IXy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
        "#client = OpenAI()"
      ],
      "metadata": {
        "id": "Ep2anuh1eIhd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "modellist=client.models.list()\n",
        "modellist.data"
      ],
      "metadata": {
        "id": "d5G_Gu6zeTGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i9hhgxpjbIL",
        "outputId": "614d35bf-6b40-4cf2-9644-bbc554a76133"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: run_mmlu.py [-h] [--openai_api_key OPENAI_API_KEY]\n",
            "                   [--model_name {gpt-3.5-turbo,gpt-4-1106-preview,gemini-pro,mixtral}] [--cot]\n",
            "                   [--num_examples NUM_EXAMPLES]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --openai_api_key OPENAI_API_KEY\n",
            "  --model_name {gpt-3.5-turbo,gpt-4-1106-preview,gemini-pro,mixtral}\n",
            "  --cot\n",
            "  --num_examples NUM_EXAMPLES\n",
            "                        Number of examples included in the current prompt input.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://litellm.vercel.app/docs/exception_mapping"
      ],
      "metadata": {
        "id": "hgsSXbZsmbpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBahsof4DYa4",
        "outputId": "a8ea78dc-5706-40fc-f739-c6aed148d952"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Cloud SDK 476.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/compute/docs/regions-zones"
      ],
      "metadata": {
        "id": "4AYibmmqEwlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud init"
      ],
      "metadata": {
        "id": "4W697ejAike8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud info"
      ],
      "metadata": {
        "id": "BwV2KN9sDqz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py"
      ],
      "metadata": {
        "id": "oRCemArMiCAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/gemini-benchmark/benchmarking/MMLU/utils.py"
      ],
      "metadata": {
        "id": "6pMYcMGUwiGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GEMINI\n"
      ],
      "metadata": {
        "id": "KM_aXmmw7WzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GID=1\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "TpmqgR0j02mK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GEMINI\n",
        "%rm -rf /content/outputs/\n",
        "%mkdir /content/outputs\n",
        "\n",
        "!python /content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py --model_name gemini-pro --num_examples 5"
      ],
      "metadata": {
        "id": "geeyklfXrsjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164bea04-62a9-410b-f815-90068b22d276"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing machine_learning ...\n",
            "100% 112/112 [02:22<00:00,  1.28s/it]\n",
            "machine_learning acc 0.4732\n",
            "Testing college_computer_science ...\n",
            "100% 100/100 [02:19<00:00,  1.39s/it]\n",
            "college_computer_science acc 0.4900\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/outputs/gemini-pro_simple_accs.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "#print('average: %s'%data['all'])\n",
        "\n",
        "# Check if 'all' key contains a list or dictionary\n",
        "if isinstance(data['all'], (list, dict)):\n",
        "    for value in data['all']:\n",
        "        print(value)\n",
        "else:\n",
        "    # Handle the float value appropriately\n",
        "    print(\"GEMINI-MMLU Average:\", data['all'])\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGijRiB87Mj5",
        "outputId": "acf00f84-c3ec-448e-99d6-361451cb095d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEMINI-MMLU Average: 0.4811320754716981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAZZQdEx53VN",
        "outputId": "bfc8ca87-6de4-4a7c-f011-c1f768e79f31"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'machine_learning': 0.4732142857142857, 'college_computer_science': 0.49, 'all': 0.4811320754716981}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPENAI"
      ],
      "metadata": {
        "id": "SPEsGdC87afE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/outputs/\n",
        "%mkdir /content/outputs\n",
        "%mkdir -p  /content/data/dev\n",
        "%cd /content/\n",
        "\n",
        "# choose from 'gpt-3.5-turbo', 'gpt-4-1106-preview', 'gemini-pro', 'mixtral'\n",
        "!python /content/gemini-benchmark/benchmarking/MMLU/run_mmlu.py --openai_api_key $OPENAI_API_KEY --model_name gpt-4-1106-preview --cot --num_examples 5"
      ],
      "metadata": {
        "id": "MZTcpSy9dqEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/outputs/gpt-4-1106-preview_cot_accs.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "#print('average: %s'%data['all'])\n",
        "\n",
        "# Check if 'all' key contains a list or dictionary\n",
        "if isinstance(data['all'], (list, dict)):\n",
        "    for value in data['all']:\n",
        "        print(value)\n",
        "else:\n",
        "    # Handle the float value appropriately\n",
        "    print(\"Average:\", data['all'])\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSAeXeVhE9l2",
        "outputId": "4c79cb0e-52e7-4321-e4c7-3df50d600620"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average: 0.7547169811320755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57N6jgAYFFa5",
        "outputId": "cec57b80-9495-4dff-a37f-0fa24ad20b64"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'machine_learning': 0.7767857142857143, 'college_computer_science': 0.73, 'all': 0.7547169811320755}\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN0UXNOyGLkp3w338doUBkW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/demo_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"You seem to be using the pipelines sequentially on GPU\")\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "xWcef_467xJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"You seem to be using the pipelines sequentially on GPU\")"
      ],
      "metadata": {
        "id": "1akNKr6tDV7B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate bitsandbytes peft -q"
      ],
      "metadata": {
        "id": "o6N50PkvL_BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwLgVWZK883M",
        "outputId": "d37570d1-5b8b-4afa-cf97-53eaa9e0ce0d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 19 16:44:03 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   76C    P0              34W /  72W |  15575MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import bitsandbytes\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import (\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# Set environment variable for CUDA debugging\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# Load 10% of the SQuAD dataset\n",
        "squad = load_dataset(\"squad\", split=\"train[:10%]\")\n",
        "\n",
        "# Load the tokenizer and model with 4-bit quantization\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Changed to Mistral 7B\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,  # Trust remote code for Mistral\n",
        ")\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Get the answer spans\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "\n",
        "tokenized_squad = squad.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=squad.column_names,\n",
        ")\n",
        "\n",
        "# Define LoRA config\n",
        "# Define LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,              # Increased rank to 16 for more complex adaptations\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print('\\n')\n",
        "model.print_trainable_parameters()\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Define training arguments\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,  # Reduced batch size to 2\n",
        "    gradient_accumulation_steps=8,  # Increased gradient accumulation to 8\n",
        "    learning_rate=5e-5,             # Adjusted learning rate\n",
        "    fp16=True,\n",
        "    logging_steps=50,               # Log every 50 steps\n",
        "    save_steps=100,\n",
        "    num_train_epochs=1,\n",
        "    warmup_steps=100,\n",
        "    max_grad_norm=1.0,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    optim=\"adamw_torch\",\n",
        "    # weight_decay=0.01   # Add weight decay if needed\n",
        ")\n",
        "\n",
        "# Define Trainer (using the standard Trainer)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"./fine_tuned_squad_model\")\n",
        "\n",
        "# -------------------\n",
        "# Test the model\n",
        "# -------------------\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    \"./fine_tuned_squad_model\", load_in_4bit=True, device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Test example\n",
        "question = \"What is a potential concern about large language models?\"\n",
        "context = \"Large language models can be helpful tools, but it's important to use them responsibly. They can sometimes generate incorrect or misleading information.\"\n",
        "\n",
        "# Prepare input for the model\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Get the answer\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the predicted answer span\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "# Decode the predicted answer\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "generated_answer = tokenizer.decode(predict_answer_tokens)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Generated Answer:\", generated_answer)"
      ],
      "metadata": {
        "id": "emgH1CdYL6Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder, create_repo\n",
        "\n",
        "# Replace with the path to your saved model directory\n",
        "model_path = \"/content/gdrive/MyDrive/model/fine_tuned_squad_model\"\n",
        "repo_id = \"frankmorales2020/Mistral-7B-v0.1_squad_alignment\"\n",
        "\n",
        "# Create the repository if it doesn't exist\n",
        "create_repo(repo_id, exist_ok=True)  # exist_ok=True to avoid errors if it already exists\n",
        "\n",
        "upload_folder(\n",
        "    folder_path=model_path,\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Upload frankmorales2020/Mistral-7B-v0.1_squad_alignment\",\n",
        ")\n",
        "# full code the test the model"
      ],
      "metadata": {
        "id": "z7em9ghIBoz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# -------------------\n",
        "# Test the model\n",
        "# -------------------\n",
        "\n",
        "!pip install transformers datasets accelerate bitsandbytes peft -q\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "# Create BitsAndBytesConfig object\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the fine-tuned model with quantization_config\n",
        "model_path = \"/content/gdrive/MyDrive/model/fine_tuned_squad_model\"  # Replace with your model path\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    model_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "# Test example\n",
        "question = \"What is a potential concern about large language models?\"\n",
        "context = \"\"\"One potential concern about large language models is that they might generate incorrect or misleading information.\n",
        "              This is because they learn from vast amounts of text data, which may contain biases or inaccuracies.\n",
        "              While large language models can be helpful tools, it's crucial to use them responsibly and with careful evaluation.\"\"\"  # Revised context\n",
        "\n",
        "\n",
        "def answer_question(question, context):\n",
        "    # Check for context overlap with question\n",
        "    if question in context:\n",
        "        print(\"Warning: Context contains the question. This might cause issues.\")\n",
        "\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "    #print(\"Start Logits:\", outputs.start_logits)\n",
        "    #print(\"End Logits:\", outputs.end_logits)\n",
        "\n",
        "    # Limit answer length (optional - adjust as needed)\n",
        "    max_answer_length = 30\n",
        "\n",
        "    # Relaxed thresholds further\n",
        "    start_logit_threshold = 0.0  # Adjust as needed\n",
        "    end_logit_threshold = 0.0  # Adjust or remove as needed\n",
        "\n",
        "    if outputs.start_logits[0, answer_start_index] < start_logit_threshold:\n",
        "        print(\"Low confidence in start position. Trying second-best.\")\n",
        "        # Get second-best prediction index for start\n",
        "        start_logits_sorted, start_indices_sorted = torch.sort(\n",
        "            outputs.start_logits[0], descending=True\n",
        "        )\n",
        "        answer_start_index = start_indices_sorted[1].item()\n",
        "\n",
        "    if outputs.end_logits[0, answer_end_index] < end_logit_threshold:\n",
        "        print(\"Low confidence in end position. Trying second-best.\")\n",
        "        # Get second-best prediction index for end\n",
        "        end_logits_sorted, end_indices_sorted = torch.sort(\n",
        "            outputs.end_logits[0], descending=True\n",
        "        )\n",
        "        answer_end_index = end_indices_sorted[1].item()\n",
        "\n",
        "    predict_answer_tokens = inputs.input_ids[\n",
        "        0, answer_start_index : answer_end_index + 1\n",
        "    ]\n",
        "    generated_answer = tokenizer.decode(predict_answer_tokens)\n",
        "\n",
        "    # If the generated answer is empty or doesn't make sense, try keyword-based search\n",
        "    if not generated_answer or len(generated_answer.split()) < 2:\n",
        "        print(\"Trying keyword-based search.\")\n",
        "        keywords = [\n",
        "            \"risk\",\n",
        "            \"inaccuracy\",\n",
        "            \"misleading\",\n",
        "            \"bias\",\n",
        "            \"danger\",\n",
        "            \"problem\",\n",
        "            \"issue\",\n",
        "        ]  # Expanded keywords\n",
        "        for keyword in keywords:\n",
        "            if keyword in context:\n",
        "                # Find the keyword in the context and extract the surrounding text (wider range)\n",
        "                match = re.search(r\".{0,100}\" + keyword + r\".{0,100}\", context)\n",
        "                if match:\n",
        "                    generated_answer = match.group(0)\n",
        "                    break  # Stop if a keyword is found and an answer is extracted\n",
        "\n",
        "    # Relaxed post-processing (adjust as needed):\n",
        "    generated_answer = generated_answer.replace(\"<s>\", \"\").strip()\n",
        "\n",
        "    # Optional post-processing (re-introduce if needed):\n",
        "    # 1. Filter punctuation-only answers\n",
        "    if generated_answer.strip() in [\".\", \",\", \"!\", \"?\", \";\", \":\"]:\n",
        "        print(\"Answer is likely punctuation. Filtering...\")\n",
        "        generated_answer = \"\"\n",
        "    # 2. Filter answers overlapping with the question\n",
        "    # if generated_answer in question:\n",
        "    #    print(\"Answer overlaps with question. Filtering...\")\n",
        "    #    generated_answer = \"\"\n",
        "\n",
        "    return generated_answer\n",
        "\n",
        "\n",
        "generated_answer = answer_question(question, context)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LrcKl9Z1ZxbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question:\", question)\n",
        "print(\"Generated Answer:\", generated_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC6YdpX7ayQf",
        "outputId": "cdb36030-530a-4879-9dd9-3a5a5b0f3db6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is a potential concern about large language models?\n",
            "Generated Answer: This is because they learn from vast amounts of text data, which may contain biases or inaccuracies. \n",
            "              While large language models can be helpful tools, it's crucial to use them respons\n"
          ]
        }
      ]
    }
  ]
}
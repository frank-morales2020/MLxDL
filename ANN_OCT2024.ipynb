{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcu36rwp9OtAf71qw3ATPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/ANN_OCT2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, dropout=0.0):\n",
        "        weight_shapes = [(a, b) for a, b in zip(layer_sizes[1:], layer_sizes[:-1])]\n",
        "\n",
        "        # Xavier initialization\n",
        "        self.weights = [np.random.randn(s[0], s[1]) * np.sqrt(2 / (s[0] + s[1])) for s in weight_shapes]\n",
        "\n",
        "        #self.weights = [np.random.standard_normal(s)/s[1]**.5 for s in weight_shapes]\n",
        "        self.biases = [np.zeros((s, 1)) for s in layer_sizes[1:]]\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Initialize Adam optimizer parameters here\n",
        "        self.m_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        self.v_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        self.m_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        self.v_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\n",
        "\n",
        "    def predict(self, a):\n",
        "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            a = self.activation(np.dot(w, a) + b)\n",
        "            if i < len(self.weights) - 1:  # Apply dropout to all layers except the output layer\n",
        "                a *= np.random.binomial(1, 1 - self.dropout, size=a.shape) / (1 - self.dropout)\n",
        "        return a\n",
        "\n",
        "    #def activation(self, z):\n",
        "        #return np.maximum(0, z)  # ReLU activation function\n",
        "\n",
        "    #def activation(self, z):\n",
        "        #return 1 / (1 + np.exp(-z))  # Sigmoid activation function\n",
        "\n",
        "\n",
        "    def activation(self, z):\n",
        "        return 1 / (1 + np.exp(-z))  # Sigmoid activation function\n",
        "\n",
        "    def activation_derivative(self, z):\n",
        "        return self.activation(z) * (1 - self.activation(z))  # Derivative of Sigmoid\n",
        "\n",
        "    def backpropagate(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        activation = x\n",
        "        activations = [x]  # list to store all the activations, layer by layer\n",
        "        zs = []  # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = self.activation(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        delta = self.cost_derivative(activations[-1], y) * self.activation_derivative(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        # Now you can print activations here:\n",
        "        #print(\"Activations:\", activations)\n",
        "        #sys.stdout.flush()  # Ensure the output is sent to the console immediately\n",
        "\n",
        "        for l in range(2, len(self.layer_sizes)):\n",
        "            z = zs[-l]\n",
        "            sp = self.activation_derivative(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "        # Example: Print intermediate values\n",
        "        print(\"Activations:\", activations)\n",
        "        print(\"Zs:\", zs)\n",
        "        print(\"Delta:\", delta)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "    def activation_derivative(self, z):\n",
        "        return 1. * (z > 0)  # Derivative of ReLU\n",
        "\n",
        "    #def train(self, training_data, epochs, initial_learning_rate, batch_size, l2_lambda=0.01):\n",
        "\n",
        "    #def train(self, training_data, epochs, learning_rate, batch_size, l2_lambda=0.01):  # Add L2 regularization parameter\n",
        "\n",
        "    def train(self, training_data, epochs, initial_learning_rate, batch_size, l2_lambda=0.01):\n",
        "\n",
        "\n",
        "\n",
        "    #def train(self, training_data, epochs, learning_rate, batch_size, l2_lambda=0.01):\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            np.random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+batch_size]\n",
        "                for k in range(0, n, batch_size)]\n",
        "\n",
        "            #for mini_batch in mini_batches:\n",
        "            #    self.update_mini_batch(mini_batch, learning_rate, j, l2_lambda)  # Pass l2_lambda to update_mini_batch\n",
        "\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                learning_rate = self.adjust_learning_rate(initial_learning_rate, j)  # Adjust learning rate\n",
        "                self.update_mini_batch(mini_batch, learning_rate, j, l2_lambda)\n",
        "\n",
        "\n",
        "            #for mini_batch in mini_batches:\n",
        "                #self.update_mini_batch(mini_batch, learning_rate, j, l2_lambda)  # Pass l2_lambda to update_mini_batch\n",
        "\n",
        "                #self.update_mini_batch(mini_batch, learning_rate, j)\n",
        "            #print(f\"Epoch {j} complete.\")\n",
        "\n",
        "\n",
        "    def adjust_learning_rate(self, initial_learning_rate, epoch, decay_rate=0.95):\n",
        "        \"\"\"Reduces the learning rate over time.\"\"\"\n",
        "        return initial_learning_rate * decay_rate**(epoch / 10)  # Example: Exponential decay\n",
        "\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, learning_rate, epoch, l2_lambda):\n",
        "\n",
        "    #def update_mini_batch(self, mini_batch, learning_rate, epoch, l2_lambda):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # Adam optimizer parameters - moved to the beginning of the function\n",
        "        beta1 = 0.9\n",
        "        beta2 = 0.999\n",
        "        epsilon = 1e-8\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.m_w[i] = beta1 * self.m_w[i] + (1 - beta1) * nabla_w[i]\n",
        "            self.v_w[i] = beta2 * self.v_w[i] + (1 - beta2) * (nabla_w[i] ** 2)\n",
        "            m_w_hat = self.m_w[i] / (1 - beta1 ** (epoch + 1))\n",
        "            v_w_hat = self.v_w[i] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "            # Moved the L2 regularization update here, after m_w_hat and v_w_hat are calculated\n",
        "            #self.weights[i] = (1 - learning_rate * l2_lambda / len(mini_batch)) * self.weights[i] - (learning_rate / len(mini_batch)) * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "            # Add L2 regularization term to the weight update\n",
        "            self.weights[i] = (1 - learning_rate * l2_lambda / len(mini_batch)) * self.weights[i] - (learning_rate / len(mini_batch)) * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "\n",
        "            # Add L2 regularization term to the weight update\n",
        "            #self.weights[i] = (1 - learning_rate * l2_lambda / len(mini_batch)) * self.weights[i] - (learning_rate / len(mini_batch)) * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backpropagate(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "        # Adam optimizer\n",
        "        #beta1 = 0.9\n",
        "        #beta2 = 0.999\n",
        "        #epsilon = 1e-8\n",
        "\n",
        "        if not hasattr(self, 'm_w'):\n",
        "            self.m_w = [np.zeros(w.shape) for w in self.weights]\n",
        "            self.v_w = [np.zeros(w.shape) for w in self.weights]\n",
        "            self.m_b = [np.zeros(b.shape) for b in self.biases]\n",
        "            self.v_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.m_w[i] = beta1 * self.m_w[i] + (1 - beta1) * nabla_w[i]\n",
        "            self.v_w[i] = beta2 * self.v_w[i] + (1 - beta2) * (nabla_w[i] ** 2)\n",
        "            m_w_hat = self.m_w[i] / (1 - beta1 ** (epoch + 1))\n",
        "            v_w_hat = self.v_w[i] / (1 - beta2 ** (epoch + 1))\n",
        "            self.weights[i] = self.weights[i] - (learning_rate / len(mini_batch)) * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "            self.m_b[i] = beta1 * self.m_b[i] + (1 - beta1) * nabla_b[i]\n",
        "            self.v_b[i] = beta2 * self.v_b[i] + (1 - beta2) * (nabla_b[i] ** 2)\n",
        "            m_b_hat = self.m_b[i] / (1 - beta1 ** (epoch + 1))\n",
        "            v_b_hat = self.v_b[i] / (1 - beta2 ** (epoch + 1))\n",
        "            self.biases[i] = self.biases[i] - (learning_rate / len(mini_batch)) * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    def numerical_gradient(self, x, y, epsilon=1e-4):\n",
        "        \"\"\"Calculates the numerical gradient of the cost function.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            for j in range(self.weights[i].shape[0]):\n",
        "                for k in range(self.weights[i].shape[1]):\n",
        "                    self.weights[i][j, k] += epsilon\n",
        "                    cost_plus = self.cost(x, y)\n",
        "                    self.weights[i][j, k] -= 2 * epsilon\n",
        "                    cost_minus = self.cost(x, y)\n",
        "                    self.weights[i][j, k] += epsilon\n",
        "                    nabla_w[i][j, k] = (cost_plus - cost_minus) / (2 * epsilon)\n",
        "\n",
        "        # Similar process for biases (omitted for brevity)\n",
        "\n",
        "        return nabla_b, nabla_w\n",
        "\n",
        "    def evaluate_regression(self, test_data):\n",
        "        \"\"\"Calculates the Mean Squared Error for regression.\"\"\"\n",
        "        squared_errors = [(self.predict(x)[0, 0] - y[0, 0])**2 for x, y in test_data]\n",
        "        return np.mean(squared_errors)\n",
        "\n",
        "    def evaluate(self, test_data, threshold=0.5):\n",
        "        \"\"\"Calculates the accuracy for binary classification.\"\"\"\n",
        "        correct_predictions = 0\n",
        "        for x, y in test_data:\n",
        "            prediction = self.predict(x)\n",
        "            predicted_class = 1 if prediction[0, 0] > threshold else 0\n",
        "            true_class = 1 if y[0, 0] > threshold else 0  # Make sure this aligns with your data\n",
        "            if predicted_class == true_class:\n",
        "                correct_predictions += 1\n",
        "        return correct_predictions / len(test_data)\n",
        "\n",
        "    # In your evaluate() function:\n",
        "    def evaluate0(self, test_data, threshold=0.5):\n",
        "        \"\"\"Calculates the accuracy for binary classification.\"\"\"\n",
        "        correct_predictions = 0\n",
        "        for x, y in test_data:\n",
        "            prediction = self.predict(x)\n",
        "            print(f\"Raw Prediction: {prediction}\")  # Print raw prediction\n",
        "            predicted_class = 1 if prediction[0, 0] > threshold else 0\n",
        "            true_class = 1 if y[0, 0] > threshold else 0\n",
        "            print(f\"Predicted Class: {predicted_class}, True Class: {true_class}\")  # Print classes\n",
        "            if predicted_class == true_class:\n",
        "                correct_predictions += 1\n",
        "                print(\"Correct Prediction!\")\n",
        "            else:\n",
        "                print(\"Incorrect Prediction!\")\n",
        "        accuracy = correct_predictions / len(test_data)\n",
        "        print(f\"Correct Predictions: {correct_predictions}, Total: {len(test_data)}\")\n",
        "        print(f\"Calculated Accuracy: {accuracy}\")\n",
        "        return accuracy\n",
        "\n",
        "    #def evaluate(self, test_data):\n",
        "    #    \"\"\"Calculates the accuracy for test data.\"\"\"\n",
        "    #    test_results = [(np.argmax(self.predict(x)), np.argmax(y))\n",
        "    #                    for (x, y) in test_data]\n",
        "    #    return sum(int(x == y) for (x, y) in test_results) / len(test_data)\n",
        "\n",
        "    def cost(self, x, y):\n",
        "        \"\"\"Calculates the cost for a single input.\"\"\"\n",
        "        return np.sum((self.predict(x) - y) ** 2) / 2.0  # Mean squared error\n"
      ],
      "metadata": {
        "id": "z4TY8qvBwkkE"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "#net = NeuralNetwork([2, 10, 5, 1], dropout=0.5)  # Network with dropout\n",
        "\n",
        "# Example usage:\n",
        "net = NeuralNetwork([2, 5, 1], dropout=0.0)\n",
        "\n",
        "# Example usage:\n",
        "#net = NeuralNetwork([2, 5, 1], dropout=0.0)  # Simpler network with one hidden layer\n",
        "\n",
        "# Example usage:\n",
        "#net = NeuralNetwork([2, 3, 1], dropout=0.0)  # Smaller network\n",
        "\n",
        "# Sample training data (replace with your actual data)\n",
        "training_data = [\n",
        "    (np.array([[0.1], [0.2]]), np.array([[0.3]])),\n",
        "    (np.array([[0.4], [0.5]]), np.array([[0.6]])),\n",
        "    (np.array([[0.2], [0.1]]), np.array([[0.4]])),\n",
        "    (np.array([[0.7], [0.8]]), np.array([[0.9]])),\n",
        "    (np.array([[0.6], [0.3]]), np.array([[0.7]])),\n",
        "    # ... add more training examples\n",
        "]\n",
        "\n",
        "# Normalize the training data\n",
        "x_values = np.array([x for x, y in training_data])\n",
        "mean = np.mean(x_values)\n",
        "std = np.std(x_values)\n",
        "training_data = [((x - mean) / std, y) for x, y in training_data]\n",
        "\n",
        "# Train the network\n",
        "#net.train(training_data, epochs=50, learning_rate=0.01, batch_size=10)\n",
        "\n",
        "# Train the network with a different learning rate and batch size\n",
        "#net.train(training_data, epochs=50, learning_rate=0.05, batch_size=5)\n",
        "\n",
        "\n",
        "# Train the network with a learning rate schedule\n",
        "net.train(training_data, epochs=100, initial_learning_rate=0.01, batch_size=10, l2_lambda=0.01)\n",
        "\n",
        "\n",
        "# Train the network with a learning rate schedule\n",
        "#net.train(training_data, epochs=25, learning_rate=0.01, batch_size=10, l2_lambda=0.01)\n",
        "\n",
        "\n",
        "# Test the network (replace with your actual test data)\n",
        "test_data = [\n",
        "    (np.array([[0.2], [0.3]]), np.array([[0.5]])),\n",
        "    (np.array([[0.7], [0.1]]), np.array([[0.8]])),\n",
        "    # ... add more test examples\n",
        "]\n",
        "\n",
        "# Normalize the test data (using the same mean and std from training data)\n",
        "test_data = [(((x - mean) / std), y) for x, y in test_data]\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions and evaluate\n",
        "predictions = []\n",
        "for x, y in test_data:\n",
        "    prediction = net.predict(x)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "#mse = np.mean([(prediction - y)**2 for (_, y), prediction in zip(test_data, predictions)])\n",
        "#print(f\"MSE: {mse}\")\n",
        "\n",
        "# Evaluate using Mean Squared Error (MSE)\n",
        "mse = np.mean([(net.predict(x) - y)**2 for x, y in test_data])\n",
        "print(f\"MSE: {mse}\")\n",
        "\n",
        "\n",
        "# Gradient checking (example for a single data point)\n",
        "x, y = training_data[0]\n",
        "analytical_nabla_b, analytical_nabla_w = net.backpropagate(x, y)\n",
        "numerical_nabla_b, numerical_nabla_w = net.numerical_gradient(x, y)\n",
        "\n",
        "# Compare the gradients (you can print the differences or calculate the relative error)\n",
        "#print(\"Analytical gradient w:\", analytical_nabla_w)\n",
        "#print(\"Numerical gradient w:\", numerical_nabla_w)\n",
        "\n",
        "\n",
        "# Calculate accuracy (assuming binary classification)\n",
        "correct_predictions = 0\n",
        "for (x, y), prediction in zip(test_data, predictions):\n",
        "    predicted_class = 1 if prediction[0, 0] > 0.5 else 0\n",
        "    true_class = 1 if y[0, 0] > 0.5 else 0\n",
        "    if predicted_class == true_class:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_data)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1wAitFmxZwl",
        "outputId": "d696ac79-5239-4c62-8d8e-c249cf409465"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.06178975769895317\n",
            "Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data (replace with your actual data)\n",
        "training_data = [\n",
        "    (np.array([[0.1], [0.2]]), np.array([[0.3]])),\n",
        "    (np.array([[0.4], [0.5]]), np.array([[0.6]])),\n",
        "]\n",
        "\n",
        "# Simplified test_data\n",
        "test_data = [(np.array([[-0.79652144], [-0.37729963]]), np.array([[0.5]]))]\n",
        "\n",
        "print(\"test-data: \", test_data)\n",
        "\n",
        "# Evaluate using Mean Squared Error (MSE)\n",
        "mse = np.mean([(net.predict(x) - y)**2 for x, y in test_data])\n",
        "print(f\"MSE: {mse}\")\n",
        "\n",
        "# Create and train the network\n",
        "net = NeuralNetwork([2, 3, 1], dropout=0.1)\n",
        "net.train(training_data, epochs=100, initial_learning_rate=0.1, batch_size=1, l2_lambda=0.01)\n",
        "\n",
        "# Evaluate the network\n",
        "accuracy = net.evaluate(test_data)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LbBnS8Giwhk",
        "outputId": "16b77acd-9f39-4a04-9954-03e0b34c9ba2"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test-data:  [(array([[-0.79652144],\n",
            "       [-0.37729963]]), array([[0.5]]))]\n",
            "MSE: 0.0004245561904818655\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMggjVYdgefk8gvE/CrCo+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/langchain_MISTRAL_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/v0.1/docs/integrations/chat/mistralai/"
      ],
      "metadata": {
        "id": "fhU5a5KgH8fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install mistralai --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "GNEFwWlYIZRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-core langchain-mistralai -q"
      ],
      "metadata": {
        "id": "qQQzdVI0ItWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mistralai\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "import os\n",
        "import colab_env\n",
        "import json\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = MistralClient(api_key=api_key)"
      ],
      "metadata": {
        "id": "8_Tazt7eINpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "# If api_key is not passed, default behavior is to use the `MISTRAL_API_KEY` environment variable.\n",
        "llm = ChatMistralAI(api_key=api_key,model_name='open-mixtral-8x7b')\n",
        "\n",
        "messages = [HumanMessage(content=\"knock knock\")]\n",
        "response=llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNPHA2CoHpru",
        "outputId": "8a57cc6e-e0ef-46eb-d999-8618cb62ba64"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who's there?\n",
            "\n",
            "Caramel:\n",
            "Caramel who?\n",
            "\n",
            "Caramel:\n",
            "Caramel, can I come in and have some coffee with you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2"
      ],
      "metadata": {
        "id": "pH2vnSD2jjp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community -q"
      ],
      "metadata": {
        "id": "FRLhfkD6kLkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"the capital of canada?\")]\n",
        "response = llm(messages)"
      ],
      "metadata": {
        "id": "ghGAau9YmR_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_6yuZ7ymnA0",
        "outputId": "84775bac-2a2a-4ffd-ccbf-102a64f38152"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Canada is Ottawa. It is located in eastern Ontario, near the city of Montreal and the U.S. border. Ottawa is home to many national institutions, including the Parliament of Canada, the Supreme Court of Canada, and a number of museums and galleries. It is also a important technology and research hub, with a number of universities and research institutions located in the city. Ottawa is known for its picturesque setting on the Ottawa River, and its many parks and green spaces make it a popular destination for outdoor enthusiasts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q"
      ],
      "metadata": {
        "id": "Rtmv1ZpXeQvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "%cd /content/\n",
        "!wget \"https://www.dropbox.com/s/f6bmb19xdg0xedm/paul_graham_essay.txt?dl=1\" -O pg_essay.txt\n",
        "#!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
      ],
      "metadata": {
        "id": "PMhCRBKveVeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "#!pip install langchain-core -q\n",
        "!pip install langchain-community -q\n",
        "!pip install langchain-anthropic -q"
      ],
      "metadata": {
        "id": "598t6tCqeklJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader('/content/pg_essay.txt')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "LghxcCC4erDu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "documents = splitter.split_documents(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78kgSN4Aevvv",
        "outputId": "204c432f-2ffa-41cc-829d-ab6765d066e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1004, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1203, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1025, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "id": "WsW6xwC_e3Dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c19ba90-89b5-4ff2-f101-2dddb456a312"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu -q\n",
        "!pip install faiss-cpu -q"
      ],
      "metadata": {
        "id": "UzonPfD9e7UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5170d05e-507d-4eed-895b-5d09bb3c7107"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "#embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "ySPrTMRhfCmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "retriever = vector.as_retriever()\n",
        "\n",
        "# create a chain to answer questions\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
      ],
      "metadata": {
        "id": "Ht_DRR8WfJnA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_ai(prompt):\n",
        "  formatted_prompt = f\"Instruct: Answer the following question.\\n{prompt}\\n\"\n",
        "  result = qa({\"query\": formatted_prompt})\n",
        "\n",
        "  print()\n",
        "  #print('chain to answer questions')\n",
        "  print(\"-\" * 80)\n",
        "  print()\n",
        "  result = qa({\"query\": prompt})\n",
        "  print(f'Query: {result[\"query\"]}\\n')\n",
        "  print(f'Result: {result[\"result\"]}\\n')\n",
        "  print(f'Context Documents: ')\n",
        "  for srcdoc in result[\"source_documents\"]:\n",
        "        print(f'{srcdoc}\\n')\n",
        "  print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "HuFkqxt6OmL4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"What did the author do growing up?\"\n",
        "chat_with_ai(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq4s1C8GPbWQ",
        "outputId": "f20bed4f-b2f4-4f2e-d06c-0af1d09b60eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query: What did the author do growing up?\n",
            "\n",
            "Result: The author, Paul Graham, worked on writing short stories and programming during his childhood, before college. He started writing stories as exercises for beginning writers, and attempted to write his first programs on an IBM 1401 in the basement of his junior high school. Later in life, he continued to write essays and also worked on spam filters, painting, cooking, and real estate. He also started publishing essays online, which he considers a turning point in his career.\n",
            "\n",
            "Context Documents: \n",
            "page_content='What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.' metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "page_content='Working on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\\n\\nIn the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.' metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "page_content=\"Over the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.\\n\\nOne night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.\" metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "page_content=\"In the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\\n\\nI've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\\n\\nI knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.\" metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"who is the president of russia?\"\n",
        "chat_with_ai(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UDzjvZePAKc",
        "outputId": "e6d6b5d1-012f-48d7-b1f7-1cfdbba489d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query: who is the president of russia?\n",
            "\n",
            "Result: I'm sorry for any confusion, but the context you've provided doesn't contain any information about the current president of Russia. As of my last update, the president of Russia is Vladimir Putin. However, I recommend checking the most recent sources to confirm, as this information can change.\n",
            "\n",
            "Context Documents: \n",
            "page_content=\"When we asked Sam if he wanted to be president of YC, initially he said no. He wanted to start a startup to make nuclear reactors. But I kept at it, and in October 2013 he finally agreed. We decided he'd take over starting with the winter 2014 batch. For the rest of 2013 I left running YC more and more to Sam, partly so he could learn the job, and partly because I was focused on my mother, whose cancer had returned.\\n\\nShe died on January 15, 2014. We knew this was coming, but it was still hard when it did.\\n\\nI kept working on YC till March, to help get that batch of startups through Demo Day, then I checked out pretty completely. (I still talk to alumni and to new startups working on things I'm interested in, but that only takes a few hours a week.)\" metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "page_content=\"In the summer of 2012 my mother had a stroke, and the cause turned out to be a blood clot caused by colon cancer. The stroke destroyed her balance, and she was put in a nursing home, but she really wanted to get out of it and back to her house, and my sister and I were determined to help her do it. I used to fly up to Oregon to visit her regularly, and I had a lot of time to think on those flights. On one of them I realized I was ready to hand YC over to someone else.\\n\\nI asked Jessica if she wanted to be president, but she didn't, so we decided we'd try to recruit Sam Altman. We talked to Robert and Trevor and we agreed to make it a complete changing of the guard. Up till that point YC had been controlled by the original LLC we four had started. But we wanted YC to last for a long time, and to do that it couldn't be controlled by the founders. So if Sam said yes, we'd let him reorganize YC. Robert and I would retire, and Jessica and Trevor would become ordinary partners.\" metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "page_content=\"We invited about 20 of the 225 groups to interview in person, and from those we picked 8 to fund. They were an impressive group. That first batch included reddit, Justin Kan and Emmett Shear, who went on to found Twitch, Aaron Swartz, who had already helped write the RSS spec and would a few years later become a martyr for open access, and Sam Altman, who would later become the second president of YC. I don't think it was entirely luck that the first batch was so good. You had to be pretty bold to sign up for a weird thing like the Summer Founders Program instead of a summer job at a legit place like Microsoft or Goldman Sachs.\" metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "page_content='One day in 2010, when he was visiting California for interviews, Robert Morris did something astonishing: he offered me unsolicited advice. I can only remember him doing that once before. One day at Viaweb, when I was bent over double from a kidney stone, he suggested that it would be a good idea for him to take me to the hospital. That was what it took for Rtm to offer unsolicited advice. So I remember his exact words very clearly. \"You know,\" he said, \"you should make sure Y Combinator isn\\'t the last cool thing you do.\"' metadata={'source': '/content/pg_essay.txt'}\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "-MwN-iB9Fv9t"
      ],
      "authorship_tag": "ABX9TyPlj/FJYDcDAkoehqHXyXaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Finetune_deepseek_Essential_web_v1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/EssentialAI/essential-web-v1.0"
      ],
      "metadata": {
        "id": "9FB5YA6bRTKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Set Up Your Environment ---\n",
        "!pip install scikit-learn -q # For potential evaluation metrics (optional)\n",
        "!pip install -U transformers -q\n",
        "!pip install -U datasets -q\n",
        "!pip install -U accelerate -q\n",
        "!pip install -U peft -q\n",
        "!pip install -U trl -q # For SFTTrainer\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install unsloth -q # Recommended for speed and efficiency\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # For latest Unsloth\n",
        "\n",
        "!pip install colab-env -q"
      ],
      "metadata": {
        "id": "f5fFLZz3vY8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env"
      ],
      "metadata": {
        "id": "W3rQp2P4B_Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "-MwN-iB9Fv9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load in streaming mode\n",
        "raw_dataset = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)\n",
        "data_stream = raw_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "c6q-UXXEP1VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through examples\n",
        "for example in data_stream.take(5):\n",
        "    print(example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmkgF1aV-ZD8",
        "outputId": "e8031441-dbef-4de8-f14e-dfc6cf6d9717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': -3908994749044929748, 'text': \"Wednesday, May 25, 2011\\n\\nNEVER GROW UP\\n\\n\\n\\nThese are two persons I am always happy to meet, not only because they have amazing style.\\n\\nThe bubbles were in front of Monki's Helsinki store last Saturday, when Monki and Weekday were celebrating their one year in Helsinki.\\nI was an extra at Monki party in the evening and I apologize for spilling those two beers onto somebodys clothes!\\nI think I'm tired or something, today I just fell on the ground at work very smoothly.\\nI guess there wasn't a chair there...my workmates didn't even notice before I was like, well, I'm on the ground, oops.\\nWell, well, I hope everyone else is paying attention, I will soon follow!\\n\\n\\n\\nWednesday, May 18, 2011\\n\\nSHOOK ONES\\n\\n\\n\\n\\nFirst photo Karoliina Niemenkari, second Sinikka Konttinen.\\n\\nSorry guys for not updating or reading your great blogs!\\nIt's all good in the hood, I'm interning the whole summer at a pr office that's in a really posh street in Helsinki. Ben & Jerrys is right next to it, and Louis Vuitton. The first might be dangerous to walk past every day. Days go past quickly.\\nIt's kinda like in The Devil Wears Prada I guess...since I'm there with my nerd glasses wearing no make-up.\\nWhatever you know, it's summer soon, burn trees get money, don't give a and breathe.\\n\\nThursday, May 12, 2011\\n\\nSHINY SUIT THEORY\\n\\n\\n\\nVintage Wayfarers, Laitinen jacket, print shirt 2nd hand, Only cropped shorts, vintage leather backbag & camera bag.\\n\\n\\nMy current school ended today (unfortunatley, I've had so much fun and learned a lot here), I start to assist in a PR firm next Monday and then it's September and I move to London.\\nAnd 10300 will come out soon!\\nThe sun is out, Goblin is out and I've lost two pairs of sunglasses already.\\n\\nSaturday, May 07, 2011\\n\\nNight Air\\n\\n\\n\\n\\n\\nMy friend Sanna took these quickly today after she photographed me for her school work.\\nI'm wearing Helmut Lang top, Only jacket, Rick Owens shawl as a wrap skirt and Zara wedges.\\nSilk, cotton, merino wool and leather, respectively.\\nI'm going to Stockholm next week with our class....for six hours.\\nMy plan is to go to the photography museum and walk around and find cherry trees,\\nmaybe read Francoise Sagan in some park bench.\\nJust because.\\nEnjoy your weekend!\\n\\nFriday, May 06, 2011\\n\\nRIGHT NOW, WE'RE OLDER THAN WE'VE EVER BEEN\\n\\n\\n\\n\\n\\nMalgosia Bela by Monika Bielskyte for SOME/THINGS magazine.\\n\\nSo here's Malgosia Bela wearing Rick Owens, Nicolas Andreas Taralis, Theyskens' Theory and Thimister.\\nUh hey, I'm getting a bit emotional again...it's just that beautiful.\\n\\nMy words walk right out the door when I look for them, that's the reason I haven't been writing much lately.\\nI'm gonna go out though and track those bastards down.\\n\\nMonday, May 02, 2011\\n\\nHEAVEN CAN WAIT\\n\\n\\n\\n\\nLast Thursday we had tattoos for a day. Thank you Tero! You will see the outcome of this (there's one pair of hands missing) in the first issue of 10300!\\n\\nHERE'S A NUMBER TO MY THERAPIST\\n\\n\\n\\n\\n\\n\\nHermes Fall 2001 by Martin Margiela via Garmento. Perfect place to visit when you want to get into this void of Helmut Lang, Halston and basically everything that gets one very emotional about fashion. Me at least. You know, this stuff is important...\\n\\xa0\", 'metadata': {'url': 'http://indielovescake.blogspot.com/2011_05_01_archive.html', 'source_domain': 'indielovescake.blogspot.com', 'snapshot_id': 'crawl=CC-MAIN-2013-20', 'warc_metadata': {'Content-Length': '97639', 'Content-Type': 'application/http; msgtype=response', 'WARC-Block-Digest': 'sha1:CORHYPXHOA3CG5AULYMGZUCKAGBDGKXI', 'WARC-Concurrent-To': '<urn:uuid:0257dade-25c4-40cb-8234-c0a89468cf17>', 'WARC-Date': '2013-06-19T10:17:08Z', 'WARC-IP-Address': '74.125.228.43', 'WARC-Identified-Payload-Type': None, 'WARC-Payload-Digest': 'sha1:ACHCIYOUGHWQ5WJXNGSHVKT6DBXAGIRS', 'WARC-Record-ID': '<urn:uuid:8606806d-f018-45ef-b3d3-8a6563c52dab>', 'WARC-Target-URI': 'http://indielovescake.blogspot.com/2011_05_01_archive.html', 'WARC-Truncated': None, 'WARC-Type': 'response', 'WARC-Warcinfo-ID': '<urn:uuid:bb309a57-902d-434c-bd2e-88b1f4ad3be5>'}, 'warc_info': 'robots: classic\\r\\nhostname: ip-10-60-113-184.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)/CC WarcExport 1.0\\r\\nisPartOf: CC-MAIN-2013-20\\r\\noperator: CommonCrawl Admin\\r\\ndescription: Wide crawl of the web with URLs provided by Blekko for Spring 2013\\r\\npublisher: CommonCrawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'}, 'line_start_n_end_idx': {'line_start_idx': [0, 24, 25, 39, 40, 41, 42, 133, 134, 269, 383, 470, 589, 663, 664, 665, 666, 690, 691, 702, 703, 704, 705, 706, 766, 767, 824, 1069, 1177, 1262, 1263, 1286, 1287, 1305, 1306, 1307, 1308, 1425, 1426, 1427, 1608, 1638, 1715, 1716, 1739, 1740, 1750, 1751, 1752, 1753, 1754, 1755, 1843, 1935, 1988, 2054, 2136, 2183, 2197, 2217, 2218, 2239, 2240, 2284, 2285, 2286, 2287, 2288, 2289, 2349, 2350, 2452, 2522, 2523, 2632, 2687, 2688, 2709, 2710, 2726, 2727, 2728, 2729, 2730, 2884, 2885, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 3175], 'line_end_idx': [24, 25, 39, 40, 41, 42, 133, 134, 269, 383, 470, 589, 663, 664, 665, 666, 690, 691, 702, 703, 704, 705, 706, 766, 767, 824, 1069, 1177, 1262, 1263, 1286, 1287, 1305, 1306, 1307, 1308, 1425, 1426, 1427, 1608, 1638, 1715, 1716, 1739, 1740, 1750, 1751, 1752, 1753, 1754, 1755, 1843, 1935, 1988, 2054, 2136, 2183, 2197, 2217, 2218, 2239, 2240, 2284, 2285, 2286, 2287, 2288, 2289, 2349, 2350, 2452, 2522, 2523, 2632, 2687, 2688, 2709, 2710, 2726, 2727, 2728, 2729, 2730, 2884, 2885, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 3175, 3176]}, 'quality_signals': {'red_pajama_v2': {'ccnet_original_length': 3176.0, 'ccnet_original_nlines': 93.0, 'rps_doc_curly_bracket': 0.0, 'rps_doc_ldnoobw_words': 0.0, 'rps_doc_lorem_ipsum': 0.0, 'rps_doc_stop_word_fraction': 0.35227271914482117, 'rps_doc_ut1_blacklist': 0.0, 'rps_doc_frac_all_caps_words': 0.078125, 'rps_doc_frac_lines_end_with_ellipsis': 0.010638300329446793, 'rps_doc_frac_no_alph_words': 0.19602273404598236, 'rps_doc_frac_unique_words': 0.6222627758979797, 'rps_doc_mean_word_length': 4.452554702758789, 'rps_doc_num_sentences': 41.0, 'rps_doc_symbol_to_word_ratio': 0.007102270144969225, 'rps_doc_unigram_entropy': 5.523367881774902, 'rps_doc_word_count': 548.0, 'rps_doc_frac_chars_dupe_10grams': 0.0, 'rps_doc_frac_chars_dupe_5grams': 0.0, 'rps_doc_frac_chars_dupe_6grams': 0.0, 'rps_doc_frac_chars_dupe_7grams': 0.0, 'rps_doc_frac_chars_dupe_8grams': 0.0, 'rps_doc_frac_chars_dupe_9grams': 0.0, 'rps_doc_frac_chars_top_2gram': 0.00819671992212534, 'rps_doc_frac_chars_top_3gram': 0.009016389958560467, 'rps_doc_frac_chars_top_4gram': 0.0, 'rps_doc_books_importance': -248.5982666015625, 'rps_doc_books_importance_length_correction': -248.5982666015625, 'rps_doc_openwebtext_importance': -151.13491821289062, 'rps_doc_openwebtext_importance_length_correction': -151.13491821289062, 'rps_doc_wikipedia_importance': -99.98342895507812, 'rps_doc_wikipedia_importance_length_correction': -99.98342895507812}, 'fasttext': {'dclm': 0.0027020000852644444, 'english': 0.9532056450843811, 'fineweb_edu_approx': 0.720546305179596, 'eai_general_math': 0.021156730130314827, 'eai_open_web_math': 0.08207511901855469, 'eai_web_code': 0.001070740050636232}}, 'eai_taxonomy': {'free_decimal_correspondence': {'primary': {'code': '746.92', 'labels': {'level_1': 'Arts', 'level_2': 'Drawing, Decoration and ornament, and Design', 'level_3': 'Needlework and Fancy work'}}, 'secondary': {'code': '306.482', 'labels': {'level_1': 'Social sciences', 'level_2': '', 'level_3': 'Culture'}}}, 'bloom_cognitive_process': {'primary': {'code': '2', 'label': 'Understand'}, 'secondary': {'code': '3', 'label': 'Apply'}}, 'bloom_knowledge_domain': {'primary': {'code': '2', 'label': 'Conceptual'}, 'secondary': {'code': '3', 'label': 'Procedural'}}, 'document_type_v1': {'primary': {'code': '9', 'label': 'Personal/Misc'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'extraction_artifacts': {'primary': {'code': '0', 'label': 'No Artifacts'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'missing_content': {'primary': {'code': '0', 'label': 'No missing content'}, 'secondary': {'code': '4', 'label': 'Missing Images or Figures'}}, 'document_type_v2': {'primary': {'code': '16', 'label': 'Personal Blog'}, 'secondary': {'code': '2', 'label': 'About (Personal)'}}, 'reasoning_depth': {'primary': {'code': '1', 'label': 'No Reasoning'}, 'secondary': {'code': '2', 'label': 'Basic Reasoning'}}, 'technical_correctness': {'primary': {'code': '6', 'label': 'Not Applicable/Indeterminate'}, 'secondary': {'code': '4', 'label': 'Highly Correct'}}, 'education_level': {'primary': {'code': '1', 'label': 'General Audience'}, 'secondary': {'code': '2', 'label': 'High School Level'}}}, 'pid': '63cf76e62422470f51c1029c86092532'}\n",
            "{'id': 6279127140263447457, 'text': 'Foobooz - Your guide to food and drink in Philadelphia\\n  • Neighborhoods\\n\\n    \\xa0\\n  • Features\\n\\n  • \\xa0\\n  • Tip Jar\\n\\n    Have a food or drink tip? tips@foobooz.com (AIM:foobooz)\\n\\n  • Opening Soon\\n\\n  • Upcoming Events\\n\\n  • Categories\\n\\n  • Archives\\n\\n  • Masthead\\n\\n  • Fun Things To Do in Philly\\n\\n  • Subscribe\\n\\nSpeck Food and Wine\\n\\nVetri Awaits Sushi\\n\\nPosted by Foobooz on June 2nd, 2011\\n\\nWe’re sure this is just a photo of Marc Vetri hoping to score some sushi as he peers into the former Speck (future Raw) on the Piazza at Schmidts. Right? Right?\\n\\nWe also love that Vetri has his apron on, man is just ready to cook.\\n\\nRelated: Food Nerd News, , , , ,\\n10 Comments | Related Posts »\\n\\nRaw At The Piazza: I Guess You Could Call It Official\\n\\nPosted by Victor Fiorillo on May 10th, 2011\\n\\nJust in case there are any doubters out there about Raw Sushi & Sake Lounge taking over the Speck space at the Piazza. There sure are plenty of haters.\\n\\nUPDATE: Phoodie says an early June opening.\\n\\nRelated: Opening Soon, , , , ,\\n9 Comments | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on December 1st, 2010\\n\\nAt long last, Agiato has opened in Manayunk. Chef Joe Scarpone who owned Sovalo in Northern Liberties is at the helm. [The Insider]\\n\\nLongtime Rouge chef Michael Yeamans has returned to the Rittenhouse spot and is serving a truffle-centric menu through the end of the year. [Grub Street]\\n\\nThe original Brown Betty’s on Liberties Walk is moving to a bigger, bi-level spot at 722 N 2nd Street. [Restaurant Club]\\n\\nSpeck Food and Wine passed its health inspection Saturday. [Facebook]\\n\\nLeila Cafe has resurfaced at 1356 South Street. If all goes well the Middle Eastern restaurant could open today. [The Insider]\\n\\nRead the rest of this entry »\\n\\nRelated: Opening Soon, , , , , , , , , ,\\n2 Comments | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on November 22nd, 2010\\n\\n“Now it’s time to bloody cook.” So says Shola Olunloyo in his latest blog post where he gives a peek into Speck’s kitchen. [StudioKitchen]\\n\\nThe Twisted Tail will be the name of the bourbon and blues concept planned for the former Kildare’s at 2nd and Lombard. [The Insider]\\n\\nThe fondly recalled Jimmy John’s in Chaddsford which suffered a fire on its 70th anniversary is being rebuilt. [Second Helpings]\\n\\nChef Jim Coleman has left Coleman at the Normandy Farm Hotel & Conference Center in Blue Bell to become the new chef at World Cafe Live. [Philadelphia Inquirer]\\n\\nBridgid’s in Fairmount unveiled the East Coast’s only gravity tap this Saturday. [Philly Beer Girl]\\n\\nRelated: Food, , , , ,\\n1 Comment | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on October 5th, 2010\\n\\n1200 Bank, the upscale billiards spot hoping to open at 12th and Chestnut is going through the approval process. The active Facebook page notes opposition from neighbors and several upcoming public meetings. [Facebook via The Insider]\\n\\nSpeck Food + Wine has been delayed 6-weeks. Diners with reservations to the Studiokitchen counter received an email Monday and the promise of a gratis dinner. [The Insider]\\n\\nThe Bottle Shop on East Passyunk will be opening on Saturday, October 16th. [Meal Ticket]\\n\\nNina’s Trattoria softly opened this past weekend. The Italian Market BYOB will be fully open this Friday. [Meal Ticket]\\n\\nDown Home Diner has reopened in the Reading Terminal Market.\\n\\nRelated: Opening Soon, , , , , ,\\n1 Comment | Related Posts »\\n\\nStudiokitchen Now Accepting Reservations\\n\\nPosted by Foobooz on August 9th, 2010\\n\\nThe 8-seat StudioKitchen at Shola Olunloyo’s soon to open Speck Food + Wine is now accepting reservations through December.\\n\\nTuesday through Friday reservations are $120 per seat and Saturdays are $150 each (drinks are not incuded).\\n\\nThe reservations are only available online and must be pre-paid. The dinners themselves promise to be creative culinary adventures without substitution.\\n\\nStudiokitchen [Official Site]\\nSpeck Food + Wine [Official Site]\\n\\nRelated: Food Nerd News, , , ,\\n10 Comments | Related Posts »\\n\\nAdsum and Speck Tease\\n\\nPosted by Foobooz on July 6th, 2010\\n\\nTwo of the more anticipated restaurants of the summer of 2010 are turning up the flirting. Matt Levin’s Adsum has been tweeting up a storm as the restaurant makes a last push towards the finish line. They’ll be firing up the stoves today and hope to open next week.\\n\\nShola Olunloyo had a busy 4th of July weekend quantifying recipes and postingÂ\\xa0seductive photos of Speck Food + Wine dishes on his Studio Kitchen blog.\\n\\nAdsum [Official Site]\\nAdsum [Facebook]\\nAdsum [Twitter]\\n\\nSpeck Food + Wine [Official Site]\\nSpeck Food + Wine [Facebook Group]\\nSpeck Food + Wine [Twitter]\\n\\nRelated: Opening Soon, ,\\n1 Comment | Related Posts »\\n\\nPeek at Speck\\n\\nPosted by Foobooz on May 18th, 2010\\n\\nspeck_blueprint\\n\\nShola Olunloyo shares some preliminary blueprints and ideas for his kitchen counter at Speck Food + Wine. He doesn’t mention this cease-and-desist placard that Brownstoner spotted on the under construction restaurant yesterday.\\n\\nStudiokitchen [STUDIOKITCHEN]\\nClosing Bell: Has Speck Hit a Road Bump? [Brownstoner]\\n\\nRelated: Opening Soon, , ,\\n6 Comments | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on April 20th, 2010\\n\\nShola Olunloyo answers questions regarding his upoming Speck Food + Wine. [Studio Kitchen]\\n\\nThat construction at 2nd and Ludlow, it will be the Grill Room, a neighborhood steakhouse with an art deco vibe. [The Insider]\\n\\nUnion Trust has dubbed its second floor “Mezz Bar” and has unveiled a new menu with nothing over $13. [Meal Ticket]\\n\\nOld City Asian Bistro has opened on Market Street. The new spot comes from James Lee who owns Black Dog Cafe in Skippack. [Brownstoner]\\n\\nMuntin is the name of the Asian eatery that will be taking over the set back location that was Misso. Â\\xa0[The Insider]\\n\\nBrownstoner spots a sign for Bizini’s Famous Steaks, Hoagies and Seafood on Girard Avenue in Brewerytown. [Brownstoner]\\n\\nTiffin has opened in Wynnewood. This brings the number of Tiffin locations to 4 with a Bryn Mawr location in the works. [Tiffin.com]\\n\\nRelated: Opening Soon, , , , , , ,\\nNo Comments | Related Posts »\\n\\nShola on Speck\\n\\nPosted by Foobooz on April 7th, 2010\\n\\nspeck_food_wine\\n\\nYesterday we revealed Shola Olunloyo was planning a restaurant on the Piazza at Schmidts. Today he fills in the details on Speck Food + Wine, hisÂ\\xa0”New American” restaurant.\\n\\nIt will seat 65 with a kitchen counter for 5 and an indoor bar for 12. There will be an outdoor area as well. It is situated directly opposite Swift Half.\\n\\nSpeck Food + Wine [StudioKitchen]\\nSpeck Food + Wine [Official Site]\\n\\nRelated: Opening Soon, , , , ,\\n24 Comments | Related Posts »\\n\\n\\xa0', 'metadata': {'url': 'http://philadelphia.foobooz.com/tag/speck-food-and-wine/', 'source_domain': 'philadelphia.foobooz.com', 'snapshot_id': 'crawl=CC-MAIN-2013-20', 'warc_metadata': {'Content-Length': '107958', 'Content-Type': 'application/http; msgtype=response', 'WARC-Block-Digest': 'sha1:JOEZHX4RY2WDSBAYSP7BSUG3AOTGRBPF', 'WARC-Concurrent-To': '<urn:uuid:3a23d90b-0162-40ff-ac64-3fdc148fa327>', 'WARC-Date': '2013-05-26T02:48:36Z', 'WARC-IP-Address': '71.19.233.58', 'WARC-Identified-Payload-Type': None, 'WARC-Payload-Digest': 'sha1:WQZKGEUORWX6KWWELMC5TSP44HF5UYEK', 'WARC-Record-ID': '<urn:uuid:93ac1a9a-c324-46d1-96f3-750ec449008b>', 'WARC-Target-URI': 'http://philadelphia.foobooz.com/tag/speck-food-and-wine/', 'WARC-Truncated': None, 'WARC-Type': 'response', 'WARC-Warcinfo-ID': '<urn:uuid:ca58832a-06b7-4787-abdc-c7d04d16df59>'}, 'warc_info': 'robots: classic\\r\\nhostname: ip-10-60-113-184.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)/CC WarcExport 1.0\\r\\nisPartOf: CC-MAIN-2013-20\\r\\noperator: CommonCrawl Admin\\r\\ndescription: Wide crawl of the web with URLs provided by Blekko for Spring 2013\\r\\npublisher: CommonCrawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'}, 'line_start_n_end_idx': {'line_start_idx': [0, 55, 73, 74, 80, 93, 94, 100, 112, 113, 174, 175, 192, 193, 213, 214, 229, 230, 243, 244, 257, 258, 289, 290, 304, 305, 325, 326, 345, 346, 382, 383, 544, 545, 614, 615, 648, 678, 679, 733, 734, 778, 779, 931, 932, 976, 977, 1008, 1037, 1038, 1050, 1051, 1091, 1092, 1224, 1225, 1379, 1380, 1501, 1502, 1572, 1573, 1700, 1701, 1731, 1732, 1773, 1802, 1803, 1815, 1816, 1857, 1858, 1997, 1998, 2132, 2133, 2262, 2263, 2424, 2425, 2525, 2526, 2549, 2577, 2578, 2590, 2591, 2630, 2631, 2866, 2867, 3040, 3041, 3131, 3132, 3252, 3253, 3314, 3315, 3348, 3376, 3377, 3418, 3419, 3457, 3458, 3582, 3583, 3691, 3692, 3845, 3846, 3876, 3910, 3911, 3942, 3972, 3973, 3995, 3996, 4032, 4033, 4299, 4300, 4452, 4453, 4475, 4492, 4508, 4509, 4543, 4578, 4606, 4607, 4632, 4660, 4661, 4675, 4676, 4712, 4713, 4729, 4730, 4958, 4959, 4989, 5044, 5045, 5072, 5101, 5102, 5114, 5115, 5153, 5154, 5245, 5246, 5373, 5374, 5490, 5491, 5627, 5628, 5746, 5747, 5867, 5868, 6001, 6002, 6037, 6067, 6068, 6083, 6084, 6121, 6122, 6138, 6139, 6313, 6314, 6469, 6470, 6504, 6538, 6539, 6570, 6600, 6601], 'line_end_idx': [55, 73, 74, 80, 93, 94, 100, 112, 113, 174, 175, 192, 193, 213, 214, 229, 230, 243, 244, 257, 258, 289, 290, 304, 305, 325, 326, 345, 346, 382, 383, 544, 545, 614, 615, 648, 678, 679, 733, 734, 778, 779, 931, 932, 976, 977, 1008, 1037, 1038, 1050, 1051, 1091, 1092, 1224, 1225, 1379, 1380, 1501, 1502, 1572, 1573, 1700, 1701, 1731, 1732, 1773, 1802, 1803, 1815, 1816, 1857, 1858, 1997, 1998, 2132, 2133, 2262, 2263, 2424, 2425, 2525, 2526, 2549, 2577, 2578, 2590, 2591, 2630, 2631, 2866, 2867, 3040, 3041, 3131, 3132, 3252, 3253, 3314, 3315, 3348, 3376, 3377, 3418, 3419, 3457, 3458, 3582, 3583, 3691, 3692, 3845, 3846, 3876, 3910, 3911, 3942, 3972, 3973, 3995, 3996, 4032, 4033, 4299, 4300, 4452, 4453, 4475, 4492, 4508, 4509, 4543, 4578, 4606, 4607, 4632, 4660, 4661, 4675, 4676, 4712, 4713, 4729, 4730, 4958, 4959, 4989, 5044, 5045, 5072, 5101, 5102, 5114, 5115, 5153, 5154, 5245, 5246, 5373, 5374, 5490, 5491, 5627, 5628, 5746, 5747, 5867, 5868, 6001, 6002, 6037, 6067, 6068, 6083, 6084, 6121, 6122, 6138, 6139, 6313, 6314, 6469, 6470, 6504, 6538, 6539, 6570, 6600, 6601, 6602]}, 'quality_signals': {'red_pajama_v2': {'ccnet_original_length': 6602.0, 'ccnet_original_nlines': 188.0, 'rps_doc_curly_bracket': 0.0, 'rps_doc_ldnoobw_words': 0.0, 'rps_doc_lorem_ipsum': 0.0, 'rps_doc_stop_word_fraction': 0.25222551822662354, 'rps_doc_ut1_blacklist': 0.0, 'rps_doc_frac_all_caps_words': 0.005192880053073168, 'rps_doc_frac_lines_end_with_ellipsis': 0.0, 'rps_doc_frac_no_alph_words': 0.23887239396572113, 'rps_doc_frac_unique_words': 0.45549240708351135, 'rps_doc_mean_word_length': 4.839962005615234, 'rps_doc_num_sentences': 57.0, 'rps_doc_symbol_to_word_ratio': 0.0, 'rps_doc_unigram_entropy': 5.569080352783203, 'rps_doc_word_count': 1056.0, 'rps_doc_frac_chars_dupe_10grams': 0.02817453071475029, 'rps_doc_frac_chars_dupe_5grams': 0.10898063331842422, 'rps_doc_frac_chars_dupe_6grams': 0.07884953916072845, 'rps_doc_frac_chars_dupe_7grams': 0.06769712269306183, 'rps_doc_frac_chars_dupe_8grams': 0.06769712269306183, 'rps_doc_frac_chars_dupe_9grams': 0.052435919642448425, 'rps_doc_frac_chars_top_2gram': 0.024652710184454918, 'rps_doc_frac_chars_top_3gram': 0.03052240051329136, 'rps_doc_frac_chars_top_4gram': 0.029935430735349655, 'rps_doc_books_importance': -563.2416381835938, 'rps_doc_books_importance_length_correction': -563.2416381835938, 'rps_doc_openwebtext_importance': -289.87176513671875, 'rps_doc_openwebtext_importance_length_correction': -289.87176513671875, 'rps_doc_wikipedia_importance': -257.3490905761719, 'rps_doc_wikipedia_importance_length_correction': -257.3490905761719}, 'fasttext': {'dclm': -8.10999972600257e-06, 'english': 0.9294600486755371, 'fineweb_edu_approx': 0.9494020342826843, 'eai_general_math': 0.00021189000108279288, 'eai_open_web_math': 0.23533189296722412, 'eai_web_code': -9.890000001178123e-06}}, 'eai_taxonomy': {'free_decimal_correspondence': {'primary': {'code': '647.95', 'labels': {'level_1': 'Industrial arts, Technology, and Engineering', 'level_2': 'Home economics', 'level_3': 'Household employees, Caterers and catering, and Real estate management'}}, 'secondary': {'code': '381', 'labels': {'level_1': 'Social sciences', 'level_2': 'Commerce and Communication and traffic', 'level_3': 'Commerce'}}}, 'bloom_cognitive_process': {'primary': {'code': '2', 'label': 'Understand'}, 'secondary': {'code': '3', 'label': 'Apply'}}, 'bloom_knowledge_domain': {'primary': {'code': '1', 'label': 'Factual'}, 'secondary': {'code': '2', 'label': 'Conceptual'}}, 'document_type_v1': {'primary': {'code': '1', 'label': 'News/Editorial'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'extraction_artifacts': {'primary': {'code': '3', 'label': 'Irrelevant Content'}, 'secondary': {'code': '0', 'label': 'No Artifacts'}}, 'missing_content': {'primary': {'code': '1', 'label': 'Truncated Snippets'}, 'secondary': {'code': '0', 'label': 'No missing content'}}, 'document_type_v2': {'primary': {'code': '13', 'label': 'News (Org.)'}, 'secondary': {'code': '6', 'label': 'Content Listing'}}, 'reasoning_depth': {'primary': {'code': '1', 'label': 'No Reasoning'}, 'secondary': {'code': '2', 'label': 'Basic Reasoning'}}, 'technical_correctness': {'primary': {'code': '6', 'label': 'Not Applicable/Indeterminate'}, 'secondary': {'code': '4', 'label': 'Highly Correct'}}, 'education_level': {'primary': {'code': '1', 'label': 'General Audience'}, 'secondary': {'code': '2', 'label': 'High School Level'}}}, 'pid': '63cf76e62422470f51c1029c86092532'}\n",
            "{'id': -3179952510865221229, 'text': 'Just $125.00 away from free shipping!\\n\\nDetails\\n\\nL/S Halftime Henley\\n\\nItem# LCK02395\\n\\n$50.00\\n\\n$37.50\\n\\nPick your team color and gear up for the game in this sporty Halftime Henley. Features 1x1 rib knit, Henley neckline, scoop neck, six-button placket, natural shoulder, piecing at hem, cuffed sleeves and metal buttons. 95% cotton, 5% Spandex. Machine wash. Imported.\\nRecommended\\n\\nYou May Also Like\\n\\n  • Core Tech Skort\\n  • CB DryTec Pintuck Short\\n  • Squeeze Play Full Zip\\nReviews\\n\\nYour Review\\n\\n*All Fields Required\\n\\nZoom\\nLoading...\\nSale', 'metadata': {'url': 'http://www.cutterbuck.com/holiday-shop/women/tops/l-s-halftime-henley-105.html', 'source_domain': 'www.cutterbuck.com', 'snapshot_id': 'crawl=CC-MAIN-2013-20', 'warc_metadata': {'Content-Length': '131068', 'Content-Type': 'application/http; msgtype=response', 'WARC-Block-Digest': 'sha1:7HJYR3JXIPFYPYXNAJTMNBVVVOVJTOBQ', 'WARC-Concurrent-To': '<urn:uuid:f3f8d160-091f-4b3a-848c-733b78041909>', 'WARC-Date': '2013-05-22T01:25:54Z', 'WARC-IP-Address': '198.61.206.52', 'WARC-Identified-Payload-Type': None, 'WARC-Payload-Digest': 'sha1:OEEIBAFP36TBBAZI3JMLT4PINEFYEWPD', 'WARC-Record-ID': '<urn:uuid:03b2f970-8e3e-427e-b54d-d87cf0a8b5f8>', 'WARC-Target-URI': 'http://www.cutterbuck.com/holiday-shop/women/tops/l-s-halftime-henley-105.html', 'WARC-Truncated': None, 'WARC-Type': 'response', 'WARC-Warcinfo-ID': '<urn:uuid:0e3872c1-e303-40b2-a70f-a69471eb52e1>'}, 'warc_info': 'robots: classic\\r\\nhostname: ip-10-60-113-184.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)/CC WarcExport 1.0\\r\\nisPartOf: CC-MAIN-2013-20\\r\\noperator: CommonCrawl Admin\\r\\ndescription: Wide crawl of the web with URLs provided by Blekko for Spring 2013\\r\\npublisher: CommonCrawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'}, 'line_start_n_end_idx': {'line_start_idx': [0, 38, 39, 47, 48, 68, 69, 84, 85, 92, 93, 100, 101, 367, 379, 380, 398, 399, 419, 447, 473, 481, 482, 494, 495, 516, 517, 522, 533], 'line_end_idx': [38, 39, 47, 48, 68, 69, 84, 85, 92, 93, 100, 101, 367, 379, 380, 398, 399, 419, 447, 473, 481, 482, 494, 495, 516, 517, 522, 533, 537]}, 'quality_signals': {'red_pajama_v2': {'ccnet_original_length': 537.0, 'ccnet_original_nlines': 28.0, 'rps_doc_curly_bracket': 0.0, 'rps_doc_ldnoobw_words': 0.0, 'rps_doc_lorem_ipsum': 0.0, 'rps_doc_stop_word_fraction': 0.1043478325009346, 'rps_doc_ut1_blacklist': 0.0, 'rps_doc_frac_all_caps_words': 0.03478261083364487, 'rps_doc_frac_lines_end_with_ellipsis': 0.03448275849223137, 'rps_doc_frac_no_alph_words': 0.321739137172699, 'rps_doc_frac_unique_words': 0.9166666865348816, 'rps_doc_mean_word_length': 4.86904764175415, 'rps_doc_num_sentences': 11.0, 'rps_doc_symbol_to_word_ratio': 0.017391299828886986, 'rps_doc_unigram_entropy': 4.3028340339660645, 'rps_doc_word_count': 84.0, 'rps_doc_frac_chars_dupe_10grams': 0.0, 'rps_doc_frac_chars_dupe_5grams': 0.0, 'rps_doc_frac_chars_dupe_6grams': 0.0, 'rps_doc_frac_chars_dupe_7grams': 0.0, 'rps_doc_frac_chars_dupe_8grams': 0.0, 'rps_doc_frac_chars_dupe_9grams': 0.0, 'rps_doc_frac_chars_top_2gram': 0.0684596598148346, 'rps_doc_frac_chars_top_3gram': 0.0, 'rps_doc_frac_chars_top_4gram': 0.0, 'rps_doc_books_importance': -45.41791534423828, 'rps_doc_books_importance_length_correction': -58.499656677246094, 'rps_doc_openwebtext_importance': -34.603328704833984, 'rps_doc_openwebtext_importance_length_correction': -47.6850700378418, 'rps_doc_wikipedia_importance': -15.290440559387207, 'rps_doc_wikipedia_importance_length_correction': -28.372182846069336}, 'fasttext': {'dclm': -7.870000445109326e-06, 'english': 0.7234194278717041, 'fineweb_edu_approx': 0.5587223172187805, 'eai_general_math': -8.459999662591144e-06, 'eai_open_web_math': 0.08231371641159058, 'eai_web_code': -1.0009999641624745e-05}}, 'eai_taxonomy': {'free_decimal_correspondence': {'primary': {'code': '646.7', 'labels': {'level_1': 'Industrial arts, Technology, and Engineering', 'level_2': 'Home economics', 'level_3': 'Clothing and dress, Fashion, and Beauty, Personal'}}, 'secondary': {'code': '658.85', 'labels': {'level_1': 'Industrial arts, Technology, and Engineering', 'level_2': 'Business', 'level_3': 'Management'}}}, 'bloom_cognitive_process': {'primary': {'code': '1', 'label': 'Remember'}, 'secondary': {'code': '2', 'label': 'Understand'}}, 'bloom_knowledge_domain': {'primary': {'code': '1', 'label': 'Factual'}, 'secondary': {'code': '3', 'label': 'Procedural'}}, 'document_type_v1': {'primary': {'code': '15', 'label': 'E-Commerce/Marketplace'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'extraction_artifacts': {'primary': {'code': '0', 'label': 'No Artifacts'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'missing_content': {'primary': {'code': '4', 'label': 'Missing Images or Figures'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'document_type_v2': {'primary': {'code': '17', 'label': 'Product Page'}, 'secondary': {'code': '6', 'label': 'Content Listing'}}, 'reasoning_depth': {'primary': {'code': '1', 'label': 'No Reasoning'}, 'secondary': {'code': '2', 'label': 'Basic Reasoning'}}, 'technical_correctness': {'primary': {'code': '6', 'label': 'Not Applicable/Indeterminate'}, 'secondary': {'code': '4', 'label': 'Highly Correct'}}, 'education_level': {'primary': {'code': '1', 'label': 'General Audience'}, 'secondary': {'code': '2', 'label': 'High School Level'}}}, 'pid': '63cf76e62422470f51c1029c86092532'}\n",
            "{'id': -230021332694416357, 'text': \"Top Sources\\n\\nBy Region\\n\\n\\nClassifieds\\n\\nReviews in history\\nReviews of significant work in all fields of historical interest. Sign up for email alerts\\nhistory.ac.uk\\nBBIH: a new bibliography\\nSearch over 500,000 books and articles about British and Irish history in the new BBIH\\nhistory.ac.uk\\n\\nLatest questions\\n\\nEbenezer Chapel Colchester There is an old chapel in Nunns Road in...\\nmedieval law I am reading the rolls of the London Eyre 1244...\\nTorriano (Avenue or Cottages) Torriano Avenue in London's Kentish Town was...\\n\\nMaps > Ordnance Survey 1:10,560 - Epoch 1 > Wales - Carmarthenshire > 034/SE\\n\\nZoom\\n\\nMAX | HIGH | LOW | MIN \\u2003\\u2003 Satellite (new window)\\n\\nPublication date\\n\\n1891\\n\\nCitation\\n\\n'Wales - Carmarthenshire: 034/SE', Ordnance Survey 1:10,560 - Epoch 1 (1891). URL: http://www.british-history.ac.uk/mapsheet.aspx?sheetid=1526&compid=55183 Date accessed: 18 May 2013. Add to my bookshelf\\n\\nSupporting documents\\n\\nCentral coordinates\\n\\n51.904, -3.8873\\n\\nMap data copyright\\n\\nCopyright (c) and database right Crown Copyright and Landmark Information Group Ltd (all rights reserved 2013)\", 'metadata': {'url': 'http://www.british-history.ac.uk/mapsheet.aspx?sheetid=1526&compid=55183', 'source_domain': 'www.british-history.ac.uk', 'snapshot_id': 'crawl=CC-MAIN-2013-20', 'warc_metadata': {'Content-Length': '35576', 'Content-Type': 'application/http; msgtype=response', 'WARC-Block-Digest': 'sha1:LZVMD4YE2V5B45X2XNHKPEY2KAWRY5SB', 'WARC-Concurrent-To': '<urn:uuid:3cd0adc6-7298-41ea-a18c-b2ee3123bf6f>', 'WARC-Date': '2013-05-18T16:57:09Z', 'WARC-IP-Address': '193.39.212.226', 'WARC-Identified-Payload-Type': None, 'WARC-Payload-Digest': 'sha1:5MQOAV6LJHSQW55QAFEBQ52A4OJXFON7', 'WARC-Record-ID': '<urn:uuid:bc48f721-2ca7-4cc8-8b46-1925182a911b>', 'WARC-Target-URI': 'http://www.british-history.ac.uk/mapsheet.aspx?sheetid=1526&compid=55183', 'WARC-Truncated': None, 'WARC-Type': 'response', 'WARC-Warcinfo-ID': '<urn:uuid:eb1ec9d1-7a2d-48bf-a6bc-3994c47903dd>'}, 'warc_info': 'robots: classic\\r\\nhostname: ip-10-60-113-184.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)/CC WarcExport 1.0\\r\\nisPartOf: CC-MAIN-2013-20\\r\\noperator: CommonCrawl Admin\\r\\ndescription: Wide crawl of the web with URLs provided by Blekko for Spring 2013\\r\\npublisher: CommonCrawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'}, 'line_start_n_end_idx': {'line_start_idx': [0, 12, 13, 23, 24, 25, 37, 38, 57, 148, 162, 187, 274, 288, 289, 306, 307, 377, 440, 518, 519, 596, 597, 602, 603, 652, 653, 670, 671, 676, 677, 686, 687, 891, 892, 913, 914, 934, 935, 951, 952, 971, 972], 'line_end_idx': [12, 13, 23, 24, 25, 37, 38, 57, 148, 162, 187, 274, 288, 289, 306, 307, 377, 440, 518, 519, 596, 597, 602, 603, 652, 653, 670, 671, 676, 677, 686, 687, 891, 892, 913, 914, 934, 935, 951, 952, 971, 972, 1082]}, 'quality_signals': {'red_pajama_v2': {'ccnet_original_length': 1082.0, 'ccnet_original_nlines': 42.0, 'rps_doc_curly_bracket': 0.0, 'rps_doc_ldnoobw_words': 0.0, 'rps_doc_lorem_ipsum': 0.0, 'rps_doc_stop_word_fraction': 0.1608695685863495, 'rps_doc_ut1_blacklist': 0.0, 'rps_doc_frac_all_caps_words': 0.043478261679410934, 'rps_doc_frac_lines_end_with_ellipsis': 0.06976743787527084, 'rps_doc_frac_no_alph_words': 0.35652172565460205, 'rps_doc_frac_unique_words': 0.761904776096344, 'rps_doc_mean_word_length': 5.693877696990967, 'rps_doc_num_sentences': 18.0, 'rps_doc_symbol_to_word_ratio': 0.0130434799939394, 'rps_doc_unigram_entropy': 4.610713481903076, 'rps_doc_word_count': 147.0, 'rps_doc_frac_chars_dupe_10grams': 0.0, 'rps_doc_frac_chars_dupe_5grams': 0.06212664023041725, 'rps_doc_frac_chars_dupe_6grams': 0.0, 'rps_doc_frac_chars_dupe_7grams': 0.0, 'rps_doc_frac_chars_dupe_8grams': 0.0, 'rps_doc_frac_chars_dupe_9grams': 0.0, 'rps_doc_frac_chars_top_2gram': 0.03345280885696411, 'rps_doc_frac_chars_top_3gram': 0.04778973013162613, 'rps_doc_frac_chars_top_4gram': 0.05973716080188751, 'rps_doc_books_importance': -83.1473617553711, 'rps_doc_books_importance_length_correction': -83.1473617553711, 'rps_doc_openwebtext_importance': -63.39421844482422, 'rps_doc_openwebtext_importance_length_correction': -52.93299102783203, 'rps_doc_wikipedia_importance': -51.9829216003418, 'rps_doc_wikipedia_importance_length_correction': -51.9829216003418}, 'fasttext': {'dclm': 3.396999818505719e-05, 'english': 0.7327302098274231, 'fineweb_edu_approx': 1.734725832939148, 'eai_general_math': -7.269999969139462e-06, 'eai_open_web_math': 0.010301649570465088, 'eai_web_code': -9.780000254977494e-06}}, 'eai_taxonomy': {'free_decimal_correspondence': {'primary': {'code': '912.03', 'labels': {'level_1': 'History and Geography', 'level_2': 'Geography and Voyages and travels', 'level_3': 'Maps and Atlases'}}, 'secondary': {'code': '942', 'labels': {'level_1': 'History and Geography', 'level_2': 'Europe', 'level_3': 'England and Great Britain'}}}, 'bloom_cognitive_process': {'primary': {'code': '2', 'label': 'Understand'}, 'secondary': {'code': '3', 'label': 'Apply'}}, 'bloom_knowledge_domain': {'primary': {'code': '1', 'label': 'Factual'}, 'secondary': {'code': '2', 'label': 'Conceptual'}}, 'document_type_v1': {'primary': {'code': '3', 'label': 'Reference/Encyclopedic/Educational'}, 'secondary': {'code': '7', 'label': 'Search/Directory/Bibliography'}}, 'extraction_artifacts': {'primary': {'code': '3', 'label': 'Irrelevant Content'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'missing_content': {'primary': {'code': '4', 'label': 'Missing Images or Figures'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'document_type_v2': {'primary': {'code': '6', 'label': 'Content Listing'}, 'secondary': {'code': '8', 'label': 'Documentation'}}, 'reasoning_depth': {'primary': {'code': '1', 'label': 'No Reasoning'}, 'secondary': {'code': '2', 'label': 'Basic Reasoning'}}, 'technical_correctness': {'primary': {'code': '6', 'label': 'Not Applicable/Indeterminate'}, 'secondary': {'code': '4', 'label': 'Highly Correct'}}, 'education_level': {'primary': {'code': '1', 'label': 'General Audience'}, 'secondary': {'code': '2', 'label': 'High School Level'}}}, 'pid': '63cf76e62422470f51c1029c86092532'}\n",
            "{'id': 4531201455813753320, 'text': \"ASSOCIATED PRESS COVERAGE\\n\\nSales gains lift Dollar Tree's 1Q profit 15 pct\\nRICHMOND, Va. (AP) -- Discount retailer Dollar Tree Inc. said Thursday that its net income increased 15 percent in the first quarter as consumers spent more at its stores, which sell goods for $1 or less....\\n\\nStocks fall on Fed, weak Chinese manufacturing\\nNEW YORK (AP) -- Stocks fell in early trading, extending a sell-off that began Wednesday afternoon, after Chinese manufacturing unexpectedly contracted and on concern that the Federal Reserve may ease back on its stimulus program....\\n\\nStock slump continues on Wall Street\\nNEW YORK (AP) -- A global stock market slump is continuing on Wall Street as traders worry about how committed the Federal Reserve remains to keeping up its bond-buying program....\\n\\nRalph Lauren's 4Q profit rises 35 pct\\nNEW YORK (AP) -- Ralph Lauren Corp. reported a 35 percent increase in fourth-quarter profit as the luxury retailer benefited from lower cotton prices. But economic challenges here and abroad and the move to eliminate some businesses to focus on the most profitable ones cut into sales....\\n\\nUS equities sell-off goes on; futures slump\\nNEW YORK (AP) -- The momentum of a late sell-off on Wall Street carried over into a second day, sending U.S. futures and global stock markets into retreat....\\n\\nOil down to near $93 on Chinese recovery concerns\\nThe price of oil fell to near $93 a barrel on Thursday after a survey showed manufacturing activity in China falling to its lowest level in seven months, a sign that the recovery in the world's No. 2 economy is fading....\\n\\nJapan gyrations underline economy's vulnerability\\nTOKYO (AP) -- Japan's financial markets gyrated wildly Thursday, underscoring the vulnerability of its economy to a loss of investor confidence as Prime Minister Shinzo Abe attempts shock monetary easing to end two decades of stagnation....\\n\\nBetween economy and trouble, Obama approval steady\\nWASHINGTON (AP) -- The economy is recovering, the White House is dealing with multiple controversies, and President Barack Obama appears generally unaffected either way....\\n\\nLenovo says quarterly profit up 90 percent\\nBEIJING (AP) -- Computer maker Lenovo Group said Thursday its latest quarterly profit rose 90 percent as sales of smartphones and mobile computing technology expanded....\\n\\nArgentine leader raises cash handouts 35 percent\\nBUENOS AIRES, Argentina (AP) -- Argentina's president announced a $3.2 billion annual increase in cash handouts for the poor, students and pregnant women Wednesday, saying the programs will reach nearly 700,000 additional children, pay their families 35 percent more and encourage consumer spending in what is an election year....\\n\\nAP Photo\\nAP Photo/Richard Drew\\nInvesting\\n\\nStocks Quickrank | Market Movers | A-Z List | 52 Week High/low | Index Performance\\n\\nMutual Funds Quickrank | Market Movers | A-Z List\\n\\nETFs ETF Quickrank\\n\\nQuote:\\n\\n\\n\\n© Copyright 2007 Morningstar, Inc.\", 'metadata': {'url': 'http://customwire.ap.org/dynamic/fronts/MONEY_COMPLETE?SITE=WVHUN&SCTION=MONEY_COMPLETE', 'source_domain': 'customwire.ap.org', 'snapshot_id': 'crawl=CC-MAIN-2013-20', 'warc_metadata': {'Content-Length': '32157', 'Content-Type': 'application/http; msgtype=response', 'WARC-Block-Digest': 'sha1:ARKP2BG4M2MWQ6MOUTYC5EUY74G6ELHF', 'WARC-Concurrent-To': '<urn:uuid:282cf551-f2bb-42fc-919e-e0f1fc55b36c>', 'WARC-Date': '2013-05-23T15:43:59Z', 'WARC-IP-Address': '184.51.126.32', 'WARC-Identified-Payload-Type': None, 'WARC-Payload-Digest': 'sha1:UHGSNV2NN4GBZAQSYHGJGQ4KF7WEWGW7', 'WARC-Record-ID': '<urn:uuid:031f24ff-0cec-4b94-88a7-a7a1f39d5ef3>', 'WARC-Target-URI': 'http://customwire.ap.org/dynamic/fronts/MONEY_COMPLETE?SITE=WVHUN&SCTION=MONEY_COMPLETE', 'WARC-Truncated': None, 'WARC-Type': 'response', 'WARC-Warcinfo-ID': '<urn:uuid:b584dd8e-3105-497d-a6cf-88bca36bad28>'}, 'warc_info': 'robots: classic\\r\\nhostname: ip-10-60-113-184.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)/CC WarcExport 1.0\\r\\nisPartOf: CC-MAIN-2013-20\\r\\noperator: CommonCrawl Admin\\r\\ndescription: Wide crawl of the web with URLs provided by Blekko for Spring 2013\\r\\npublisher: CommonCrawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'}, 'line_start_n_end_idx': {'line_start_idx': [0, 26, 27, 75, 283, 284, 331, 565, 566, 603, 784, 785, 823, 1112, 1113, 1157, 1316, 1317, 1367, 1589, 1590, 1640, 1881, 1882, 1933, 2106, 2107, 2150, 2321, 2322, 2371, 2702, 2703, 2712, 2734, 2744, 2745, 2828, 2829, 2879, 2880, 2899, 2900, 2907, 2908, 2909, 2910], 'line_end_idx': [26, 27, 75, 283, 284, 331, 565, 566, 603, 784, 785, 823, 1112, 1113, 1157, 1316, 1317, 1367, 1589, 1590, 1640, 1881, 1882, 1933, 2106, 2107, 2150, 2321, 2322, 2371, 2702, 2703, 2712, 2734, 2744, 2745, 2828, 2829, 2879, 2880, 2899, 2900, 2907, 2908, 2909, 2910, 2944]}, 'quality_signals': {'red_pajama_v2': {'ccnet_original_length': 2944.0, 'ccnet_original_nlines': 46.0, 'rps_doc_curly_bracket': 0.0, 'rps_doc_ldnoobw_words': 0.0, 'rps_doc_lorem_ipsum': 0.0, 'rps_doc_stop_word_fraction': 0.24416516721248627, 'rps_doc_ut1_blacklist': 0.0, 'rps_doc_frac_all_caps_words': 0.07001794874668121, 'rps_doc_frac_lines_end_with_ellipsis': 0.21276596188545227, 'rps_doc_frac_no_alph_words': 0.19569119811058044, 'rps_doc_frac_unique_words': 0.6213808655738831, 'rps_doc_mean_word_length': 5.19821834564209, 'rps_doc_num_sentences': 19.0, 'rps_doc_symbol_to_word_ratio': 0.01795331947505474, 'rps_doc_unigram_entropy': 5.331773281097412, 'rps_doc_word_count': 449.0, 'rps_doc_frac_chars_dupe_10grams': 0.0, 'rps_doc_frac_chars_dupe_5grams': 0.02313625067472458, 'rps_doc_frac_chars_dupe_6grams': 0.0, 'rps_doc_frac_chars_dupe_7grams': 0.0, 'rps_doc_frac_chars_dupe_8grams': 0.0, 'rps_doc_frac_chars_dupe_9grams': 0.0, 'rps_doc_frac_chars_top_2gram': 0.01199657004326582, 'rps_doc_frac_chars_top_3gram': 0.015424160286784172, 'rps_doc_frac_chars_top_4gram': 0.019708650186657906, 'rps_doc_books_importance': -266.4443359375, 'rps_doc_books_importance_length_correction': -266.4443359375, 'rps_doc_openwebtext_importance': -139.92481994628906, 'rps_doc_openwebtext_importance_length_correction': -139.92481994628906, 'rps_doc_wikipedia_importance': -128.21031188964844, 'rps_doc_wikipedia_importance_length_correction': -128.21031188964844}, 'fasttext': {'dclm': 0.0007026799721643329, 'english': 0.9203702807426453, 'fineweb_edu_approx': 1.5339775085449219, 'eai_general_math': 0.002965149935334921, 'eai_open_web_math': 0.17646300792694092, 'eai_web_code': 8.582999726058915e-05}}, 'eai_taxonomy': {'free_decimal_correspondence': {'primary': {'code': '332.022', 'labels': {'level_1': 'Social sciences', 'level_2': 'Economics', 'level_3': 'Finance'}}, 'secondary': {'code': '330.9', 'labels': {'level_1': 'Social sciences', 'level_2': 'Economics', 'level_3': ''}}}, 'bloom_cognitive_process': {'primary': {'code': '2', 'label': 'Understand'}, 'secondary': {'code': '3', 'label': 'Apply'}}, 'bloom_knowledge_domain': {'primary': {'code': '1', 'label': 'Factual'}, 'secondary': {'code': '2', 'label': 'Conceptual'}}, 'document_type_v1': {'primary': {'code': '1', 'label': 'News/Editorial'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'extraction_artifacts': {'primary': {'code': '3', 'label': 'Irrelevant Content'}, 'secondary': {'code': '0', 'label': 'No Artifacts'}}, 'missing_content': {'primary': {'code': '1', 'label': 'Truncated Snippets'}, 'secondary': {'code': '0', 'label': 'No missing content'}}, 'document_type_v2': {'primary': {'code': '14', 'label': 'News Article'}, 'secondary': {'code': '6', 'label': 'Content Listing'}}, 'reasoning_depth': {'primary': {'code': '1', 'label': 'No Reasoning'}, 'secondary': {'code': '2', 'label': 'Basic Reasoning'}}, 'technical_correctness': {'primary': {'code': '6', 'label': 'Not Applicable/Indeterminate'}, 'secondary': {'code': '4', 'label': 'Highly Correct'}}, 'education_level': {'primary': {'code': '1', 'label': 'General Audience'}, 'secondary': {'code': '2', 'label': 'High School Level'}}}, 'pid': '63cf76e62422470f51c1029c86092532'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Prepare the Training Dataset ---\n",
        "print(\"Loading and preparing EssentialAI/essential-web-v1.0 dataset...\")\n",
        "\n",
        "raw_dataset = data_stream\n",
        "\n",
        "#FOR POC\n",
        "eval_set_size = 2\n",
        "train_set_size = 10\n",
        "\n",
        "eval_dataset = raw_dataset.take(eval_set_size)\n",
        "\n",
        "train_dataset = raw_dataset.skip(eval_set_size).take(train_set_size)\n",
        "\n",
        "test_dataset = eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpYpgqLZX8Wi",
        "outputId": "7139c7b3-4710-4ba1-bb4c-64d8f88e6df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing EssentialAI/essential-web-v1.0 dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset preparation complete.\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# To see the actual examples in test_dataset\n",
        "for i, example in enumerate(test_dataset):\n",
        "    print(f\"Example {i+1}: {example}\")\n",
        "    if i >= 2: # Stop after 2 examples as test_dataset only has 2\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLEUPdf3-lTe",
        "outputId": "1e078c41-cf8c-4eb1-84f7-85650a9273ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preparation complete.\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Example 1: {'id': -3908994749044929748, 'text': \"Wednesday, May 25, 2011\\n\\nNEVER GROW UP\\n\\n\\n\\nThese are two persons I am always happy to meet, not only because they have amazing style.\\n\\nThe bubbles were in front of Monki's Helsinki store last Saturday, when Monki and Weekday were celebrating their one year in Helsinki.\\nI was an extra at Monki party in the evening and I apologize for spilling those two beers onto somebodys clothes!\\nI think I'm tired or something, today I just fell on the ground at work very smoothly.\\nI guess there wasn't a chair there...my workmates didn't even notice before I was like, well, I'm on the ground, oops.\\nWell, well, I hope everyone else is paying attention, I will soon follow!\\n\\n\\n\\nWednesday, May 18, 2011\\n\\nSHOOK ONES\\n\\n\\n\\n\\nFirst photo Karoliina Niemenkari, second Sinikka Konttinen.\\n\\nSorry guys for not updating or reading your great blogs!\\nIt's all good in the hood, I'm interning the whole summer at a pr office that's in a really posh street in Helsinki. Ben & Jerrys is right next to it, and Louis Vuitton. The first might be dangerous to walk past every day. Days go past quickly.\\nIt's kinda like in The Devil Wears Prada I guess...since I'm there with my nerd glasses wearing no make-up.\\nWhatever you know, it's summer soon, burn trees get money, don't give a and breathe.\\n\\nThursday, May 12, 2011\\n\\nSHINY SUIT THEORY\\n\\n\\n\\nVintage Wayfarers, Laitinen jacket, print shirt 2nd hand, Only cropped shorts, vintage leather backbag & camera bag.\\n\\n\\nMy current school ended today (unfortunatley, I've had so much fun and learned a lot here), I start to assist in a PR firm next Monday and then it's September and I move to London.\\nAnd 10300 will come out soon!\\nThe sun is out, Goblin is out and I've lost two pairs of sunglasses already.\\n\\nSaturday, May 07, 2011\\n\\nNight Air\\n\\n\\n\\n\\n\\nMy friend Sanna took these quickly today after she photographed me for her school work.\\nI'm wearing Helmut Lang top, Only jacket, Rick Owens shawl as a wrap skirt and Zara wedges.\\nSilk, cotton, merino wool and leather, respectively.\\nI'm going to Stockholm next week with our class....for six hours.\\nMy plan is to go to the photography museum and walk around and find cherry trees,\\nmaybe read Francoise Sagan in some park bench.\\nJust because.\\nEnjoy your weekend!\\n\\nFriday, May 06, 2011\\n\\nRIGHT NOW, WE'RE OLDER THAN WE'VE EVER BEEN\\n\\n\\n\\n\\n\\nMalgosia Bela by Monika Bielskyte for SOME/THINGS magazine.\\n\\nSo here's Malgosia Bela wearing Rick Owens, Nicolas Andreas Taralis, Theyskens' Theory and Thimister.\\nUh hey, I'm getting a bit emotional again...it's just that beautiful.\\n\\nMy words walk right out the door when I look for them, that's the reason I haven't been writing much lately.\\nI'm gonna go out though and track those bastards down.\\n\\nMonday, May 02, 2011\\n\\nHEAVEN CAN WAIT\\n\\n\\n\\n\\nLast Thursday we had tattoos for a day. Thank you Tero! You will see the outcome of this (there's one pair of hands missing) in the first issue of 10300!\\n\\nHERE'S A NUMBER TO MY THERAPIST\\n\\n\\n\\n\\n\\n\\nHermes Fall 2001 by Martin Margiela via Garmento. Perfect place to visit when you want to get into this void of Helmut Lang, Halston and basically everything that gets one very emotional about fashion. Me at least. You know, this stuff is important...\\n\\xa0\", 'metadata': {'url': 'http://indielovescake.blogspot.com/2011_05_01_archive.html', 'source_domain': 'indielovescake.blogspot.com', 'snapshot_id': 'crawl=CC-MAIN-2013-20', 'warc_metadata': {'Content-Length': '97639', 'Content-Type': 'application/http; msgtype=response', 'WARC-Block-Digest': 'sha1:CORHYPXHOA3CG5AULYMGZUCKAGBDGKXI', 'WARC-Concurrent-To': '<urn:uuid:0257dade-25c4-40cb-8234-c0a89468cf17>', 'WARC-Date': '2013-06-19T10:17:08Z', 'WARC-IP-Address': '74.125.228.43', 'WARC-Identified-Payload-Type': None, 'WARC-Payload-Digest': 'sha1:ACHCIYOUGHWQ5WJXNGSHVKT6DBXAGIRS', 'WARC-Record-ID': '<urn:uuid:8606806d-f018-45ef-b3d3-8a6563c52dab>', 'WARC-Target-URI': 'http://indielovescake.blogspot.com/2011_05_01_archive.html', 'WARC-Truncated': None, 'WARC-Type': 'response', 'WARC-Warcinfo-ID': '<urn:uuid:bb309a57-902d-434c-bd2e-88b1f4ad3be5>'}, 'warc_info': 'robots: classic\\r\\nhostname: ip-10-60-113-184.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)/CC WarcExport 1.0\\r\\nisPartOf: CC-MAIN-2013-20\\r\\noperator: CommonCrawl Admin\\r\\ndescription: Wide crawl of the web with URLs provided by Blekko for Spring 2013\\r\\npublisher: CommonCrawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'}, 'line_start_n_end_idx': {'line_start_idx': [0, 24, 25, 39, 40, 41, 42, 133, 134, 269, 383, 470, 589, 663, 664, 665, 666, 690, 691, 702, 703, 704, 705, 706, 766, 767, 824, 1069, 1177, 1262, 1263, 1286, 1287, 1305, 1306, 1307, 1308, 1425, 1426, 1427, 1608, 1638, 1715, 1716, 1739, 1740, 1750, 1751, 1752, 1753, 1754, 1755, 1843, 1935, 1988, 2054, 2136, 2183, 2197, 2217, 2218, 2239, 2240, 2284, 2285, 2286, 2287, 2288, 2289, 2349, 2350, 2452, 2522, 2523, 2632, 2687, 2688, 2709, 2710, 2726, 2727, 2728, 2729, 2730, 2884, 2885, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 3175], 'line_end_idx': [24, 25, 39, 40, 41, 42, 133, 134, 269, 383, 470, 589, 663, 664, 665, 666, 690, 691, 702, 703, 704, 705, 706, 766, 767, 824, 1069, 1177, 1262, 1263, 1286, 1287, 1305, 1306, 1307, 1308, 1425, 1426, 1427, 1608, 1638, 1715, 1716, 1739, 1740, 1750, 1751, 1752, 1753, 1754, 1755, 1843, 1935, 1988, 2054, 2136, 2183, 2197, 2217, 2218, 2239, 2240, 2284, 2285, 2286, 2287, 2288, 2289, 2349, 2350, 2452, 2522, 2523, 2632, 2687, 2688, 2709, 2710, 2726, 2727, 2728, 2729, 2730, 2884, 2885, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 3175, 3176]}, 'quality_signals': {'red_pajama_v2': {'ccnet_original_length': 3176.0, 'ccnet_original_nlines': 93.0, 'rps_doc_curly_bracket': 0.0, 'rps_doc_ldnoobw_words': 0.0, 'rps_doc_lorem_ipsum': 0.0, 'rps_doc_stop_word_fraction': 0.35227271914482117, 'rps_doc_ut1_blacklist': 0.0, 'rps_doc_frac_all_caps_words': 0.078125, 'rps_doc_frac_lines_end_with_ellipsis': 0.010638300329446793, 'rps_doc_frac_no_alph_words': 0.19602273404598236, 'rps_doc_frac_unique_words': 0.6222627758979797, 'rps_doc_mean_word_length': 4.452554702758789, 'rps_doc_num_sentences': 41.0, 'rps_doc_symbol_to_word_ratio': 0.007102270144969225, 'rps_doc_unigram_entropy': 5.523367881774902, 'rps_doc_word_count': 548.0, 'rps_doc_frac_chars_dupe_10grams': 0.0, 'rps_doc_frac_chars_dupe_5grams': 0.0, 'rps_doc_frac_chars_dupe_6grams': 0.0, 'rps_doc_frac_chars_dupe_7grams': 0.0, 'rps_doc_frac_chars_dupe_8grams': 0.0, 'rps_doc_frac_chars_dupe_9grams': 0.0, 'rps_doc_frac_chars_top_2gram': 0.00819671992212534, 'rps_doc_frac_chars_top_3gram': 0.009016389958560467, 'rps_doc_frac_chars_top_4gram': 0.0, 'rps_doc_books_importance': -248.5982666015625, 'rps_doc_books_importance_length_correction': -248.5982666015625, 'rps_doc_openwebtext_importance': -151.13491821289062, 'rps_doc_openwebtext_importance_length_correction': -151.13491821289062, 'rps_doc_wikipedia_importance': -99.98342895507812, 'rps_doc_wikipedia_importance_length_correction': -99.98342895507812}, 'fasttext': {'dclm': 0.0027020000852644444, 'english': 0.9532056450843811, 'fineweb_edu_approx': 0.720546305179596, 'eai_general_math': 0.021156730130314827, 'eai_open_web_math': 0.08207511901855469, 'eai_web_code': 0.001070740050636232}}, 'eai_taxonomy': {'free_decimal_correspondence': {'primary': {'code': '746.92', 'labels': {'level_1': 'Arts', 'level_2': 'Drawing, Decoration and ornament, and Design', 'level_3': 'Needlework and Fancy work'}}, 'secondary': {'code': '306.482', 'labels': {'level_1': 'Social sciences', 'level_2': '', 'level_3': 'Culture'}}}, 'bloom_cognitive_process': {'primary': {'code': '2', 'label': 'Understand'}, 'secondary': {'code': '3', 'label': 'Apply'}}, 'bloom_knowledge_domain': {'primary': {'code': '2', 'label': 'Conceptual'}, 'secondary': {'code': '3', 'label': 'Procedural'}}, 'document_type_v1': {'primary': {'code': '9', 'label': 'Personal/Misc'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'extraction_artifacts': {'primary': {'code': '0', 'label': 'No Artifacts'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'missing_content': {'primary': {'code': '0', 'label': 'No missing content'}, 'secondary': {'code': '4', 'label': 'Missing Images or Figures'}}, 'document_type_v2': {'primary': {'code': '16', 'label': 'Personal Blog'}, 'secondary': {'code': '2', 'label': 'About (Personal)'}}, 'reasoning_depth': {'primary': {'code': '1', 'label': 'No Reasoning'}, 'secondary': {'code': '2', 'label': 'Basic Reasoning'}}, 'technical_correctness': {'primary': {'code': '6', 'label': 'Not Applicable/Indeterminate'}, 'secondary': {'code': '4', 'label': 'Highly Correct'}}, 'education_level': {'primary': {'code': '1', 'label': 'General Audience'}, 'secondary': {'code': '2', 'label': 'High School Level'}}}, 'pid': '63cf76e62422470f51c1029c86092532'}\n",
            "Example 2: {'id': 6279127140263447457, 'text': 'Foobooz - Your guide to food and drink in Philadelphia\\n  • Neighborhoods\\n\\n    \\xa0\\n  • Features\\n\\n  • \\xa0\\n  • Tip Jar\\n\\n    Have a food or drink tip? tips@foobooz.com (AIM:foobooz)\\n\\n  • Opening Soon\\n\\n  • Upcoming Events\\n\\n  • Categories\\n\\n  • Archives\\n\\n  • Masthead\\n\\n  • Fun Things To Do in Philly\\n\\n  • Subscribe\\n\\nSpeck Food and Wine\\n\\nVetri Awaits Sushi\\n\\nPosted by Foobooz on June 2nd, 2011\\n\\nWe’re sure this is just a photo of Marc Vetri hoping to score some sushi as he peers into the former Speck (future Raw) on the Piazza at Schmidts. Right? Right?\\n\\nWe also love that Vetri has his apron on, man is just ready to cook.\\n\\nRelated: Food Nerd News, , , , ,\\n10 Comments | Related Posts »\\n\\nRaw At The Piazza: I Guess You Could Call It Official\\n\\nPosted by Victor Fiorillo on May 10th, 2011\\n\\nJust in case there are any doubters out there about Raw Sushi & Sake Lounge taking over the Speck space at the Piazza. There sure are plenty of haters.\\n\\nUPDATE: Phoodie says an early June opening.\\n\\nRelated: Opening Soon, , , , ,\\n9 Comments | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on December 1st, 2010\\n\\nAt long last, Agiato has opened in Manayunk. Chef Joe Scarpone who owned Sovalo in Northern Liberties is at the helm. [The Insider]\\n\\nLongtime Rouge chef Michael Yeamans has returned to the Rittenhouse spot and is serving a truffle-centric menu through the end of the year. [Grub Street]\\n\\nThe original Brown Betty’s on Liberties Walk is moving to a bigger, bi-level spot at 722 N 2nd Street. [Restaurant Club]\\n\\nSpeck Food and Wine passed its health inspection Saturday. [Facebook]\\n\\nLeila Cafe has resurfaced at 1356 South Street. If all goes well the Middle Eastern restaurant could open today. [The Insider]\\n\\nRead the rest of this entry »\\n\\nRelated: Opening Soon, , , , , , , , , ,\\n2 Comments | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on November 22nd, 2010\\n\\n“Now it’s time to bloody cook.” So says Shola Olunloyo in his latest blog post where he gives a peek into Speck’s kitchen. [StudioKitchen]\\n\\nThe Twisted Tail will be the name of the bourbon and blues concept planned for the former Kildare’s at 2nd and Lombard. [The Insider]\\n\\nThe fondly recalled Jimmy John’s in Chaddsford which suffered a fire on its 70th anniversary is being rebuilt. [Second Helpings]\\n\\nChef Jim Coleman has left Coleman at the Normandy Farm Hotel & Conference Center in Blue Bell to become the new chef at World Cafe Live. [Philadelphia Inquirer]\\n\\nBridgid’s in Fairmount unveiled the East Coast’s only gravity tap this Saturday. [Philly Beer Girl]\\n\\nRelated: Food, , , , ,\\n1 Comment | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on October 5th, 2010\\n\\n1200 Bank, the upscale billiards spot hoping to open at 12th and Chestnut is going through the approval process. The active Facebook page notes opposition from neighbors and several upcoming public meetings. [Facebook via The Insider]\\n\\nSpeck Food + Wine has been delayed 6-weeks. Diners with reservations to the Studiokitchen counter received an email Monday and the promise of a gratis dinner. [The Insider]\\n\\nThe Bottle Shop on East Passyunk will be opening on Saturday, October 16th. [Meal Ticket]\\n\\nNina’s Trattoria softly opened this past weekend. The Italian Market BYOB will be fully open this Friday. [Meal Ticket]\\n\\nDown Home Diner has reopened in the Reading Terminal Market.\\n\\nRelated: Opening Soon, , , , , ,\\n1 Comment | Related Posts »\\n\\nStudiokitchen Now Accepting Reservations\\n\\nPosted by Foobooz on August 9th, 2010\\n\\nThe 8-seat StudioKitchen at Shola Olunloyo’s soon to open Speck Food + Wine is now accepting reservations through December.\\n\\nTuesday through Friday reservations are $120 per seat and Saturdays are $150 each (drinks are not incuded).\\n\\nThe reservations are only available online and must be pre-paid. The dinners themselves promise to be creative culinary adventures without substitution.\\n\\nStudiokitchen [Official Site]\\nSpeck Food + Wine [Official Site]\\n\\nRelated: Food Nerd News, , , ,\\n10 Comments | Related Posts »\\n\\nAdsum and Speck Tease\\n\\nPosted by Foobooz on July 6th, 2010\\n\\nTwo of the more anticipated restaurants of the summer of 2010 are turning up the flirting. Matt Levin’s Adsum has been tweeting up a storm as the restaurant makes a last push towards the finish line. They’ll be firing up the stoves today and hope to open next week.\\n\\nShola Olunloyo had a busy 4th of July weekend quantifying recipes and postingÂ\\xa0seductive photos of Speck Food + Wine dishes on his Studio Kitchen blog.\\n\\nAdsum [Official Site]\\nAdsum [Facebook]\\nAdsum [Twitter]\\n\\nSpeck Food + Wine [Official Site]\\nSpeck Food + Wine [Facebook Group]\\nSpeck Food + Wine [Twitter]\\n\\nRelated: Opening Soon, ,\\n1 Comment | Related Posts »\\n\\nPeek at Speck\\n\\nPosted by Foobooz on May 18th, 2010\\n\\nspeck_blueprint\\n\\nShola Olunloyo shares some preliminary blueprints and ideas for his kitchen counter at Speck Food + Wine. He doesn’t mention this cease-and-desist placard that Brownstoner spotted on the under construction restaurant yesterday.\\n\\nStudiokitchen [STUDIOKITCHEN]\\nClosing Bell: Has Speck Hit a Road Bump? [Brownstoner]\\n\\nRelated: Opening Soon, , ,\\n6 Comments | Related Posts »\\n\\nQuick Bites\\n\\nPosted by Foobooz on April 20th, 2010\\n\\nShola Olunloyo answers questions regarding his upoming Speck Food + Wine. [Studio Kitchen]\\n\\nThat construction at 2nd and Ludlow, it will be the Grill Room, a neighborhood steakhouse with an art deco vibe. [The Insider]\\n\\nUnion Trust has dubbed its second floor “Mezz Bar” and has unveiled a new menu with nothing over $13. [Meal Ticket]\\n\\nOld City Asian Bistro has opened on Market Street. The new spot comes from James Lee who owns Black Dog Cafe in Skippack. [Brownstoner]\\n\\nMuntin is the name of the Asian eatery that will be taking over the set back location that was Misso. Â\\xa0[The Insider]\\n\\nBrownstoner spots a sign for Bizini’s Famous Steaks, Hoagies and Seafood on Girard Avenue in Brewerytown. [Brownstoner]\\n\\nTiffin has opened in Wynnewood. This brings the number of Tiffin locations to 4 with a Bryn Mawr location in the works. [Tiffin.com]\\n\\nRelated: Opening Soon, , , , , , ,\\nNo Comments | Related Posts »\\n\\nShola on Speck\\n\\nPosted by Foobooz on April 7th, 2010\\n\\nspeck_food_wine\\n\\nYesterday we revealed Shola Olunloyo was planning a restaurant on the Piazza at Schmidts. Today he fills in the details on Speck Food + Wine, hisÂ\\xa0”New American” restaurant.\\n\\nIt will seat 65 with a kitchen counter for 5 and an indoor bar for 12. There will be an outdoor area as well. It is situated directly opposite Swift Half.\\n\\nSpeck Food + Wine [StudioKitchen]\\nSpeck Food + Wine [Official Site]\\n\\nRelated: Opening Soon, , , , ,\\n24 Comments | Related Posts »\\n\\n\\xa0', 'metadata': {'url': 'http://philadelphia.foobooz.com/tag/speck-food-and-wine/', 'source_domain': 'philadelphia.foobooz.com', 'snapshot_id': 'crawl=CC-MAIN-2013-20', 'warc_metadata': {'Content-Length': '107958', 'Content-Type': 'application/http; msgtype=response', 'WARC-Block-Digest': 'sha1:JOEZHX4RY2WDSBAYSP7BSUG3AOTGRBPF', 'WARC-Concurrent-To': '<urn:uuid:3a23d90b-0162-40ff-ac64-3fdc148fa327>', 'WARC-Date': '2013-05-26T02:48:36Z', 'WARC-IP-Address': '71.19.233.58', 'WARC-Identified-Payload-Type': None, 'WARC-Payload-Digest': 'sha1:WQZKGEUORWX6KWWELMC5TSP44HF5UYEK', 'WARC-Record-ID': '<urn:uuid:93ac1a9a-c324-46d1-96f3-750ec449008b>', 'WARC-Target-URI': 'http://philadelphia.foobooz.com/tag/speck-food-and-wine/', 'WARC-Truncated': None, 'WARC-Type': 'response', 'WARC-Warcinfo-ID': '<urn:uuid:ca58832a-06b7-4787-abdc-c7d04d16df59>'}, 'warc_info': 'robots: classic\\r\\nhostname: ip-10-60-113-184.ec2.internal\\r\\nsoftware: Nutch 1.6 (CC)/CC WarcExport 1.0\\r\\nisPartOf: CC-MAIN-2013-20\\r\\noperator: CommonCrawl Admin\\r\\ndescription: Wide crawl of the web with URLs provided by Blekko for Spring 2013\\r\\npublisher: CommonCrawl\\r\\nformat: WARC File Format 1.0\\r\\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'}, 'line_start_n_end_idx': {'line_start_idx': [0, 55, 73, 74, 80, 93, 94, 100, 112, 113, 174, 175, 192, 193, 213, 214, 229, 230, 243, 244, 257, 258, 289, 290, 304, 305, 325, 326, 345, 346, 382, 383, 544, 545, 614, 615, 648, 678, 679, 733, 734, 778, 779, 931, 932, 976, 977, 1008, 1037, 1038, 1050, 1051, 1091, 1092, 1224, 1225, 1379, 1380, 1501, 1502, 1572, 1573, 1700, 1701, 1731, 1732, 1773, 1802, 1803, 1815, 1816, 1857, 1858, 1997, 1998, 2132, 2133, 2262, 2263, 2424, 2425, 2525, 2526, 2549, 2577, 2578, 2590, 2591, 2630, 2631, 2866, 2867, 3040, 3041, 3131, 3132, 3252, 3253, 3314, 3315, 3348, 3376, 3377, 3418, 3419, 3457, 3458, 3582, 3583, 3691, 3692, 3845, 3846, 3876, 3910, 3911, 3942, 3972, 3973, 3995, 3996, 4032, 4033, 4299, 4300, 4452, 4453, 4475, 4492, 4508, 4509, 4543, 4578, 4606, 4607, 4632, 4660, 4661, 4675, 4676, 4712, 4713, 4729, 4730, 4958, 4959, 4989, 5044, 5045, 5072, 5101, 5102, 5114, 5115, 5153, 5154, 5245, 5246, 5373, 5374, 5490, 5491, 5627, 5628, 5746, 5747, 5867, 5868, 6001, 6002, 6037, 6067, 6068, 6083, 6084, 6121, 6122, 6138, 6139, 6313, 6314, 6469, 6470, 6504, 6538, 6539, 6570, 6600, 6601], 'line_end_idx': [55, 73, 74, 80, 93, 94, 100, 112, 113, 174, 175, 192, 193, 213, 214, 229, 230, 243, 244, 257, 258, 289, 290, 304, 305, 325, 326, 345, 346, 382, 383, 544, 545, 614, 615, 648, 678, 679, 733, 734, 778, 779, 931, 932, 976, 977, 1008, 1037, 1038, 1050, 1051, 1091, 1092, 1224, 1225, 1379, 1380, 1501, 1502, 1572, 1573, 1700, 1701, 1731, 1732, 1773, 1802, 1803, 1815, 1816, 1857, 1858, 1997, 1998, 2132, 2133, 2262, 2263, 2424, 2425, 2525, 2526, 2549, 2577, 2578, 2590, 2591, 2630, 2631, 2866, 2867, 3040, 3041, 3131, 3132, 3252, 3253, 3314, 3315, 3348, 3376, 3377, 3418, 3419, 3457, 3458, 3582, 3583, 3691, 3692, 3845, 3846, 3876, 3910, 3911, 3942, 3972, 3973, 3995, 3996, 4032, 4033, 4299, 4300, 4452, 4453, 4475, 4492, 4508, 4509, 4543, 4578, 4606, 4607, 4632, 4660, 4661, 4675, 4676, 4712, 4713, 4729, 4730, 4958, 4959, 4989, 5044, 5045, 5072, 5101, 5102, 5114, 5115, 5153, 5154, 5245, 5246, 5373, 5374, 5490, 5491, 5627, 5628, 5746, 5747, 5867, 5868, 6001, 6002, 6037, 6067, 6068, 6083, 6084, 6121, 6122, 6138, 6139, 6313, 6314, 6469, 6470, 6504, 6538, 6539, 6570, 6600, 6601, 6602]}, 'quality_signals': {'red_pajama_v2': {'ccnet_original_length': 6602.0, 'ccnet_original_nlines': 188.0, 'rps_doc_curly_bracket': 0.0, 'rps_doc_ldnoobw_words': 0.0, 'rps_doc_lorem_ipsum': 0.0, 'rps_doc_stop_word_fraction': 0.25222551822662354, 'rps_doc_ut1_blacklist': 0.0, 'rps_doc_frac_all_caps_words': 0.005192880053073168, 'rps_doc_frac_lines_end_with_ellipsis': 0.0, 'rps_doc_frac_no_alph_words': 0.23887239396572113, 'rps_doc_frac_unique_words': 0.45549240708351135, 'rps_doc_mean_word_length': 4.839962005615234, 'rps_doc_num_sentences': 57.0, 'rps_doc_symbol_to_word_ratio': 0.0, 'rps_doc_unigram_entropy': 5.569080352783203, 'rps_doc_word_count': 1056.0, 'rps_doc_frac_chars_dupe_10grams': 0.02817453071475029, 'rps_doc_frac_chars_dupe_5grams': 0.10898063331842422, 'rps_doc_frac_chars_dupe_6grams': 0.07884953916072845, 'rps_doc_frac_chars_dupe_7grams': 0.06769712269306183, 'rps_doc_frac_chars_dupe_8grams': 0.06769712269306183, 'rps_doc_frac_chars_dupe_9grams': 0.052435919642448425, 'rps_doc_frac_chars_top_2gram': 0.024652710184454918, 'rps_doc_frac_chars_top_3gram': 0.03052240051329136, 'rps_doc_frac_chars_top_4gram': 0.029935430735349655, 'rps_doc_books_importance': -563.2416381835938, 'rps_doc_books_importance_length_correction': -563.2416381835938, 'rps_doc_openwebtext_importance': -289.87176513671875, 'rps_doc_openwebtext_importance_length_correction': -289.87176513671875, 'rps_doc_wikipedia_importance': -257.3490905761719, 'rps_doc_wikipedia_importance_length_correction': -257.3490905761719}, 'fasttext': {'dclm': -8.10999972600257e-06, 'english': 0.9294600486755371, 'fineweb_edu_approx': 0.9494020342826843, 'eai_general_math': 0.00021189000108279288, 'eai_open_web_math': 0.23533189296722412, 'eai_web_code': -9.890000001178123e-06}}, 'eai_taxonomy': {'free_decimal_correspondence': {'primary': {'code': '647.95', 'labels': {'level_1': 'Industrial arts, Technology, and Engineering', 'level_2': 'Home economics', 'level_3': 'Household employees, Caterers and catering, and Real estate management'}}, 'secondary': {'code': '381', 'labels': {'level_1': 'Social sciences', 'level_2': 'Commerce and Communication and traffic', 'level_3': 'Commerce'}}}, 'bloom_cognitive_process': {'primary': {'code': '2', 'label': 'Understand'}, 'secondary': {'code': '3', 'label': 'Apply'}}, 'bloom_knowledge_domain': {'primary': {'code': '1', 'label': 'Factual'}, 'secondary': {'code': '2', 'label': 'Conceptual'}}, 'document_type_v1': {'primary': {'code': '1', 'label': 'News/Editorial'}, 'secondary': {'code': '-1', 'label': 'Abstain'}}, 'extraction_artifacts': {'primary': {'code': '3', 'label': 'Irrelevant Content'}, 'secondary': {'code': '0', 'label': 'No Artifacts'}}, 'missing_content': {'primary': {'code': '1', 'label': 'Truncated Snippets'}, 'secondary': {'code': '0', 'label': 'No missing content'}}, 'document_type_v2': {'primary': {'code': '13', 'label': 'News (Org.)'}, 'secondary': {'code': '6', 'label': 'Content Listing'}}, 'reasoning_depth': {'primary': {'code': '1', 'label': 'No Reasoning'}, 'secondary': {'code': '2', 'label': 'Basic Reasoning'}}, 'technical_correctness': {'primary': {'code': '6', 'label': 'Not Applicable/Indeterminate'}, 'secondary': {'code': '4', 'label': 'Highly Correct'}}, 'education_level': {'primary': {'code': '1', 'label': 'General Audience'}, 'secondary': {'code': '2', 'label': 'High School Level'}}}, 'pid': '63cf76e62422470f51c1029c86092532'}\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Eval sample size: {len(list(eval_dataset))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCU00ulAY9rL",
        "outputId": "f9b29627-8367-44d2-caf1-b82ee946e102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval sample size: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train sample size: {len(list(train_dataset))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRFw2KCCZCzz",
        "outputId": "4604e234-5f3f-41d1-9d74-9b72876d5b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sample size: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning  "
      ],
      "metadata": {
        "id": "x7GrnlVpmD9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc # For garbage collection, helps manage memory\n",
        "from datasets import load_dataset, Dataset # Import Dataset to convert lists to Dataset objects\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Clear memory before loading heavy models\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- Configuration for POC ---\n",
        "# These define the small number of examples to be used for the Proof of Concept.\n",
        "eval_set_size = 2000\n",
        "train_set_size = 12000\n",
        "\n",
        "# --- Helper Functions (CRITICAL CORRECTION HERE) ---\n",
        "\n",
        "# Corrected filter function: Now checks for the 'text' field which exists in EssentialAI/essential-web-v1.0\n",
        "def filter_essential_web_example(example):\n",
        "    \"\"\"Filters examples to ensure the 'text' key exists and has a non-empty value,\n",
        "    as EssentialAI/essential-web-v1.0 provides content in a 'text' field.\"\"\"\n",
        "    return \"text\" in example and example[\"text\"] is not None and example[\"text\"].strip() != \"\"\n",
        "\n",
        "# Corrected and improved format function:\n",
        "# Takes the 'text' field and artificially creates 'prompt' and 'chosen' parts for SFTTrainer.\n",
        "def format_essential_web_example(example):\n",
        "    \"\"\"\n",
        "    Formats a raw text example from EssentialAI/essential-web-v1.0 into a chat-like string.\n",
        "    It heuristically splits the raw 'text' into a 'user' prompt and an 'assistant' response.\n",
        "    This heuristic should be replaced with a more domain-specific method for real applications.\n",
        "    \"\"\"\n",
        "    full_text = example.get(\"text\", \"\")\n",
        "    if not full_text.strip():\n",
        "        # If the original text is empty or just whitespace, return an empty example to be filtered out\n",
        "        example[\"text\"] = \"\"\n",
        "        return example\n",
        "\n",
        "    # Heuristic: Take the first 100-300 characters as a \"prompt\" to summarize/analyze the content,\n",
        "    # and the rest as the \"chosen\" (assistant's elaboration/continuation).\n",
        "    # This creates a synthetic instruction-response pair.\n",
        "    prompt_length = min(len(full_text) // 3, 300) # Take roughly first third, max 300 chars for prompt\n",
        "    if prompt_length < 50: # Ensure minimum prompt length\n",
        "        prompt_length = min(len(full_text), 50) # Take whole text if very short, up to 50\n",
        "\n",
        "    user_prompt_content = f\"Analyze the following web content: {full_text[:prompt_length].strip()}\"\n",
        "    assistant_response_content = full_text[prompt_length:].strip()\n",
        "\n",
        "    # If, after splitting, the content is too short for a meaningful conversation, discard.\n",
        "    if not user_prompt_content.strip() or not assistant_response_content.strip():\n",
        "        example[\"text\"] = \"\" # Mark for filtering\n",
        "        return example\n",
        "\n",
        "    # DeepSeek models (based on Llama 3.1) expect a specific chat template format\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_prompt_content},\n",
        "        {\"role\": \"assistant\", \"content\": assistant_response_content}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Ensure 'tokenizer' is defined in the global scope when this function is executed.\n",
        "        if 'tokenizer' not in globals():\n",
        "            raise ValueError(\"Tokenizer is not available in format_essential_web_example function scope.\")\n",
        "\n",
        "        # Apply the chat template to format the conversation into a single string.\n",
        "        # tokenize=False means it returns a string, not token IDs.\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_special_tokens=True # Include special chat tokens (e.g., <s>, </s>, [INST], etc.)\n",
        "        )\n",
        "        example[\"text\"] = formatted_text\n",
        "\n",
        "        # If the formatted text somehow becomes empty or just whitespace, ensure the 'text' field is empty.\n",
        "        if not example[\"text\"].strip():\n",
        "            example[\"text\"] = \"\"\n",
        "            # print(f\"Warning: Formatted text is empty after apply_chat_template for example ID: {example.get('id', 'N/A')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If there's an error during formatting, set 'text' to empty to allow filtering it out later.\n",
        "        # print(f\"Error applying chat template: {e}. Messages: {messages}\")\n",
        "        example[\"text\"] = \"\"\n",
        "    return example\n",
        "\n",
        "# --- 2. Load the Model and Tokenizer (No changes needed from previous version) ---\n",
        "print(\"Loading DeepSeek-R1 model and tokenizer (unsloth/DeepSeek-R1-Distill-Llama-8B)...\")\n",
        "max_seq_length = 4096 # Maximum sequence length for tokenization\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "load_in_4bit = True # Enable 4-bit quantization for significant memory savings\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\", # Model name specified for fine-tuning\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "# Ensure the tokenizer has a padding token defined, essential for batching.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f\"Tokenizer pad_token was None, set to eos_token: {tokenizer.pad_token}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 3. Apply LoRA Adapters (No changes needed from previous version) ---\n",
        "print(\"Applying LoRA adapters to the model for efficient fine-tuning...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16, # Rank of the LoRA matrices\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # DeepSeek's common target modules\n",
        "    lora_alpha=16, # Scaling factor for LoRA weights\n",
        "    lora_dropout=0, # Dropout rate for LoRA layers\n",
        "    bias=\"none\", # How bias terms are handled in LoRA\n",
        "    use_gradient_checkpointing=True, # Recommended for memory saving\n",
        "    random_state=3407, # For reproducibility\n",
        "    use_rslora=False, # Use standard LoRA\n",
        "    loftq_config=None, # No LoftQ configuration\n",
        ")\n",
        "print(\"LoRA adapters applied.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 4. Prepare the Training Dataset (Corrected Materialization Strategy) ---\n",
        "# This section now ensures that the dataset contains valid examples before passing to SFTTrainer.\n",
        "print(\"Preparing and explicitly materializing small training and evaluation datasets for SFTTrainer...\")\n",
        "\n",
        "try:\n",
        "    # Load the full dataset in streaming mode. This creates an IterableDataset.\n",
        "    # It's re-loaded here to ensure a fresh stream, unaffected by previous operations.\n",
        "    raw_data_stream_for_processing = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)[\"train\"]\n",
        "\n",
        "    # 4.1. Process and Materialize the Evaluation Dataset\n",
        "    print(f\"Processing {eval_set_size} examples for evaluation dataset...\")\n",
        "    eval_dataset_materialized = list( # `list()` forces immediate processing of the stream\n",
        "        raw_data_stream_for_processing.take(eval_set_size) # Take only the specified number of examples\n",
        "        .filter(filter_essential_web_example) # Apply the corrected filter\n",
        "        .map(format_essential_web_example, batched=False) # Apply the corrected format function\n",
        "        .filter(lambda x: x.get(\"text\", \"\") != \"\") # Final filter to remove any examples where formatting failed/resulted in empty text\n",
        "    )\n",
        "    # Convert the list of dictionaries into a Hugging Face Dataset object (non-streaming for SFTTrainer)\n",
        "    eval_dataset_for_trainer = Dataset.from_list(eval_dataset_materialized)\n",
        "    print(f\"Evaluation dataset materialized with {len(eval_dataset_for_trainer)} examples.\")\n",
        "\n",
        "\n",
        "    # 4.2. Process and Materialize the Training Dataset\n",
        "    # `skip()` ensures we get examples *after* those taken for the eval set.\n",
        "    print(f\"Processing {train_set_size} examples for training dataset...\")\n",
        "    train_dataset_materialized = list( # `list()` forces immediate processing of the stream\n",
        "        raw_data_stream_for_processing.skip(eval_set_size).take(train_set_size) # Skip eval examples, then take train examples\n",
        "        .filter(filter_essential_web_example) # Apply the corrected filter\n",
        "        .map(format_essential_web_example, batched=False) # Apply the corrected format function\n",
        "        .filter(lambda x: x.get(\"text\", \"\") != \"\") # Final filter to remove empty texts\n",
        "    )\n",
        "    # Convert the list of dictionaries into a Hugging Face Dataset object (non-streaming for SFTTrainer)\n",
        "    train_dataset_for_trainer = Dataset.from_list(train_dataset_materialized)\n",
        "    print(f\"Training dataset materialized with {len(train_dataset_for_trainer)} examples.\")\n",
        "\n",
        "    print(\"Dataset preparation complete. Materialized subsets are ready for trainer.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: A critical error occurred during dataset preparation and materialization: {e}\")\n",
        "    # Set datasets to None to prevent subsequent errors if preparation fails\n",
        "    eval_dataset_for_trainer = None\n",
        "    train_dataset_for_trainer = None\n",
        "\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 5. Set Up and Configure the Trainer (No changes needed from previous version) ---\n",
        "print(\"Defining TrainingArguments for SFTTrainer...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sft_results\", # Directory for model checkpoints and logs\n",
        "    per_device_train_batch_size=4, # Samples per training batch per device\n",
        "    gradient_accumulation_steps=2, # Accumulate gradients\n",
        "    warmup_steps=5, # Learning rate warmup steps\n",
        "    num_train_epochs=1, # Total training epochs (1 for POC)\n",
        "    learning_rate=2e-4, # Initial learning rate\n",
        "    fp16=(dtype == torch.float16), # Enable FP16 if supported\n",
        "    bf16=(dtype == torch.bfloat16), # Enable BF16 if supported (preferred)\n",
        "    logging_steps=50, # Log training metrics every N steps\n",
        "    optim=\"adamw_8bit\", # Optimizer\n",
        "    seed=3407, # Random seed\n",
        "    save_steps=500, # Save checkpoints rarely for POC speed\n",
        "    save_total_limit=1, # Keep only the last checkpoint\n",
        "    eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
        "    eval_steps=500, # Evaluate rarely for POC speed\n",
        "    load_best_model_at_end=False, # Do not load best model for POC speed\n",
        "    metric_for_best_model=\"eval_loss\", # Metric for best model\n",
        "    greater_is_better=False, # For loss, lower is better\n",
        "    report_to=\"none\", # Disable reporting to external services\n",
        ")\n",
        "print(\"TrainingArguments defined.\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "id": "gRK6RqA0l9Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Instantiate SFTTrainer and Start Training ---\n",
        "print(\"Instantiating SFTTrainer...\")\n",
        "# This step should now be fast as datasets are materialized and non-empty.\n",
        "if train_dataset_for_trainer is not None and eval_dataset_for_trainer is not None:\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset_for_trainer, # Pass the materialized training dataset\n",
        "        eval_dataset=eval_dataset_for_trainer,   # Pass the materialized evaluation dataset\n",
        "        args=training_args,\n",
        "        dataset_text_field=\"text\", # Specifies the column in your dataset that contains the formatted text\n",
        "        max_seq_length=max_seq_length, # Passes the maximum sequence length for tokenization\n",
        "        # packing=True, # Optional: Enables packing of short sequences for efficiency.\n",
        "                      # Can be beneficial but might need careful handling with very diverse text lengths.\n",
        "    )\n",
        "\n",
        "    # Optional: Save the final fine-tuned model (adapters only)\n",
        "    # trainer.save_model(\"final_deepseek_finetuned_model_adapters\")\n",
        "    # print(\"Fine-tuned model (adapters) saved.\")\n",
        "else:\n",
        "    print(\"SFTTrainer instantiation and training skipped due to an error in dataset preparation.\")\n",
        "    print(\"Please check the error messages in section 4: 'Prepare the Training Dataset'.\")"
      ],
      "metadata": {
        "id": "uW_YTC_-3-9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SFTTrainer instantiated successfully.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Start the fine-tuning process\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "o87xtM4ilwVC",
        "outputId": "36db878d-78f5-4968-9a3f-003172afdde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SFTTrainer instantiated successfully.\n",
            "======================================================================\n",
            "\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 12,000 | Num Epochs = 1 | Total steps = 1,500\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  26/1500 03:16 < 3:21:35, 0.12 it/s, Epoch 0.02/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation"
      ],
      "metadata": {
        "id": "MFKp55htpgiP"
      }
    },
    {
      "source": [
        "import torch\n",
        "import gc\n",
        "from datasets import load_dataset, Dataset # Import load_dataset and Dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import os # Import os module for path operations\n",
        "\n",
        "# --- Configuration for Evaluation Dataset Preparation ---\n",
        "# Define the number of examples for the evaluation set.\n",
        "# This should match the eval_set_size used during the training script's data preparation.\n",
        "eval_set_size = 2000\n",
        "\n",
        "\n",
        "# --- Helper Functions for Dataset Preparation (Copied from training script for self-containment) ---\n",
        "# Corrected filter function: Checks for the 'text' field which exists in EssentialAI/essential-web-v1.0\n",
        "def filter_essential_web_example(example):\n",
        "    \"\"\"Filters examples to ensure the 'text' key exists and has a non-empty value.\"\"\"\n",
        "    return \"text\" in example and example[\"text\"] is not None and example[\"text\"].strip() != \"\"\n",
        "\n",
        "# Corrected and improved format function:\n",
        "# Takes the 'text' field and artificially creates 'prompt' and 'chosen' parts for SFTTrainer.\n",
        "# This function requires 'tokenizer' to be defined globally before it's called.\n",
        "def format_essential_web_example(example):\n",
        "    \"\"\"\n",
        "    Formats a raw text example from EssentialAI/essential-web-v1.0 into a chat-like string.\n",
        "    It heuristically splits the raw 'text' into a 'user' prompt and an 'assistant' response.\n",
        "    This is a simplification for EssentialAI/essential-web-v1.0 which lacks explicit prompt/chosen.\n",
        "    \"\"\"\n",
        "    full_text = example.get(\"text\", \"\")\n",
        "\n",
        "    # 1. Add a minimum length check for the original text\n",
        "    MIN_ORIGINAL_TEXT_LENGTH = 100 # Set a minimum length for the raw text\n",
        "    if len(full_text.strip()) < MIN_ORIGINAL_TEXT_LENGTH:\n",
        "        debug_text_snippet = full_text[:50].replace('\\n', ' ')\n",
        "        print(f\"DEBUG: Skipping example (short original text, len={len(full_text.strip())}): '{debug_text_snippet}...'\")\n",
        "        example[\"text\"] = \"\" # Mark for filtering\n",
        "        return example\n",
        "\n",
        "    # Heuristic for splitting: Try to make prompt about 20-30% of total text, with min/max bounds.\n",
        "    # Ensure remaining text for assistant is also substantial.\n",
        "    effective_length = len(full_text.strip())\n",
        "    prompt_len_candidate = max(50, min(effective_length // 3, 300)) # Prompt is at least 50 chars, max 300 chars, and ~1/3 of total\n",
        "\n",
        "    user_prompt_content = full_text[:prompt_len_candidate].strip()\n",
        "    assistant_response_content = full_text[prompt_len_candidate:].strip()\n",
        "\n",
        "    # 2. Add a minimum length check for prompt and response parts after splitting\n",
        "    MIN_PART_LENGTH = 30 # Minimum characters for a meaningful prompt/response part\n",
        "    if len(user_prompt_content) < MIN_PART_LENGTH or len(assistant_response_content) < MIN_PART_LENGTH:\n",
        "        print(f\"DEBUG: Skipping example (short parts after split): Prompt len={len(user_prompt_content)}, Response len={len(assistant_response_content)}\")\n",
        "        example[\"text\"] = \"\" # Mark for filtering\n",
        "        return example\n",
        "\n",
        "    # Add an instruction to the user prompt\n",
        "    user_prompt_content = f\"Analyze the following web content: {user_prompt_content}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_prompt_content},\n",
        "        {\"role\": \"assistant\", \"content\": assistant_response_content}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Check if 'tokenizer' is defined in the global scope\n",
        "        if 'tokenizer' not in globals():\n",
        "            raise ValueError(\"Tokenizer is not available in format_essential_web_example function scope.\")\n",
        "\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        example[\"text\"] = formatted_text\n",
        "\n",
        "        # 3. Final check if apply_chat_template resulted in an empty string\n",
        "        if not example[\"text\"].strip():\n",
        "            print(f\"DEBUG: Skipping example (empty formatted text after apply_chat_template): Original len={effective_length}\")\n",
        "            example[\"text\"] = \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error during apply_chat_template for example: {e}\")\n",
        "        example[\"text\"] = \"\" # Mark for filtering\n",
        "    return example\n",
        "\n",
        "\n",
        "# --- Load the Fine-tuned Model ---\n",
        "# The fine-tuned model (LoRA adapters) is saved in the output_dir.\n",
        "# We need to load the base model first, then apply the saved adapters.\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"Loading the fine-tuned model from '/content/sft_results/checkpoint-150'...\")\n",
        "\n",
        "# Define model loading parameters (should match training parameters)\n",
        "max_seq_length = 4096 # Use the same max_seq_length as during training\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load the base model using Unsloth's FastLanguageModel\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\", # The original base model name\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Ensure tokenizer pad_token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the PEFT adapters from the specified checkpoint directory\n",
        "peft_model_load_path = \"/content/sft_results/checkpoint-150\"\n",
        "try:\n",
        "    model = PeftModel.from_pretrained(base_model, peft_model_load_path)\n",
        "    print(f\"Fine-tuned model (LoRA adapters) loaded from '{peft_model_load_path}' successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading fine-tuned model adapters from '{peft_model_load_path}': {e}\")\n",
        "    print(\"Please ensure the specified checkpoint path is correct and contains 'adapter_config.json'.\")\n",
        "    raise e # Re-raise the exception to stop execution if model loading fails\n",
        "\n",
        "# Merge the LoRA adapters into the base model for efficient inference\n",
        "# This creates a single, merged model that behaves like a full fine-tuned model.\n",
        "model = model.merge_and_unload()\n",
        "print(\"LoRA adapters merged into base model successfully.\")\n",
        "\n",
        "# --- Prepare Evaluation Dataset (Now self-contained) ---\n",
        "print(\"Preparing evaluation dataset...\")\n",
        "try:\n",
        "    # Load the full dataset stream (freshly)\n",
        "    raw_data_stream_for_processing = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)[\"train\"]\n",
        "\n",
        "    # Process and Materialize the Evaluation Dataset\n",
        "    eval_dataset_materialized = []\n",
        "    processed_count = 0\n",
        "    # Iterate over a larger initial segment of the stream to find enough valid examples\n",
        "    for data_point in raw_data_stream_for_processing.take(eval_set_size * 100): # Attempt to take up to 200 raw examples\n",
        "        if filter_essential_web_example(data_point):\n",
        "            formatted_data_point = format_essential_web_example(data_point)\n",
        "            if formatted_data_point.get(\"text\", \"\").strip(): # Check if formatting was successful\n",
        "                eval_dataset_materialized.append(formatted_data_point)\n",
        "                processed_count += 1\n",
        "                if processed_count >= eval_set_size:\n",
        "                    break # Stop once we have enough valid examples\n",
        "\n",
        "    # Convert the list of dictionaries into a Hugging Face Dataset object\n",
        "    eval_dataset_for_trainer = Dataset.from_list(eval_dataset_materialized)\n",
        "    print(f\"Evaluation dataset prepared with {len(eval_dataset_for_trainer)} examples.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: A critical error occurred during evaluation dataset preparation: {e}\")\n",
        "    eval_dataset_for_trainer = None # Set to None to prevent further errors if preparation fails\n",
        "    raise e # Re-raise to halt execution if data is not ready\n",
        "\n",
        "# --- NLTK Data Downloads for Tokenization ---\n",
        "# Explicitly set NLTK data path to a writeable directory in /tmp\n",
        "nltk_data_path = \"/tmp/nltk_data\"\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "# Download NLTK 'punkt' tokenizer data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"NLTK 'punkt' tokenizer data found.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'punkt' tokenizer data...\")\n",
        "    nltk.download('punkt', download_dir=nltk_data_path, quiet=True)\n",
        "    print(\"NLTK 'punkt' download complete.\")\n",
        "\n",
        "# Download NLTK 'averaged_perceptron_tagger' data (often used by word_tokenize internally for better results)\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    print(\"NLTK 'averaged_perceptron_tagger' data found.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'averaged_perceptron_tagger' data...\")\n",
        "    nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path, quiet=True)\n",
        "    print(\"NLTK 'averaged_perceptron_tagger' download complete.\")\n",
        "\n",
        "# Add download for 'punkt_tab' as required by the traceback\n",
        "try:\n",
        "    # Check if 'punkt_tab' is already available in the specified path for 'english'\n",
        "    # nltk.data.find handles finding the resource path for the specified name ('tokenizers/punkt_tab/english/')\n",
        "    nltk.data.find('tokenizers/punkt_tab/english/', paths=[nltk_data_path])\n",
        "    print(\"NLTK 'punkt_tab' tokenizer data found.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'punkt_tab' tokenizer data...\")\n",
        "    # Download 'punkt_tab' to the temporary directory\n",
        "    nltk.download('punkt_tab', download_dir=nltk_data_path, quiet=True)\n",
        "    print(\"NLTK 'punkt_tab' download complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Catch any other unexpected errors during the check/download process\n",
        "    print(f\"An unexpected error occurred while checking/downloading NLTK tokenizers: {e}\")\n",
        "    print(\"Please ensure NLTK is correctly installed and you have network access.\")\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode. This disables dropout and batch normalization.\n",
        "model.eval()\n",
        "print(\"Model set to evaluation mode.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Clear CUDA cache for evaluation\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Starting evaluation of the fine-tuned model...\")\n",
        "\n",
        "# Define generation parameters\n",
        "generation_config = dict(\n",
        "    max_new_tokens=100,      # Max tokens to generate per response\n",
        "    do_sample=True,          # Enable sampling (diverse outputs)\n",
        "    temperature=0.7,         # Controls randomness\n",
        "    top_p=0.9,               # Nucleus sampling\n",
        "    num_return_sequences=1,  # Generate one sequence per prompt\n",
        "    eos_token_id=tokenizer.eos_token_id, # Stop generation at EOS token\n",
        "    pad_token_id=tokenizer.pad_token_id, # Use pad token if needed for batching\n",
        ")\n",
        "\n",
        "total_bleu_score = 0.0\n",
        "evaluated_count = 0\n",
        "\n",
        "# Iterate through the evaluation dataset\n",
        "if eval_dataset_for_trainer is not None and len(eval_dataset_for_trainer) > 0:\n",
        "    for i, example in enumerate(eval_dataset_for_trainer):\n",
        "        full_formatted_text = example[\"text\"]\n",
        "\n",
        "        try:\n",
        "            # Use the exact markers observed in the debug output: <｜begin of sentence｜>, <｜User｜>, <｜Assistant｜>, <｜end of sentence｜>\n",
        "            # Note the specific Unicode-like pipe characters '｜'\n",
        "\n",
        "            begin_sentence_marker = \"<｜begin of sentence｜>\"\n",
        "            user_start_marker = \"<｜User｜>\"\n",
        "            assistant_start_marker = \"<｜Assistant｜>\"\n",
        "            end_sentence_marker = \"<｜end of sentence｜>\"\n",
        "\n",
        "            # The exact indices of the markers are crucial.\n",
        "            # We need to ensure we account for ALL parts of the template.\n",
        "            # Let's clean the string a bit before finding.\n",
        "            # The structure is: <｜begin of sentence｜><｜User｜>...<｜Assistant｜>...<｜end of sentence｜>\n",
        "\n",
        "            # Find the start of the user content\n",
        "            # Start search after the begin_sentence_marker if it exists\n",
        "            search_start_idx = full_formatted_text.find(begin_sentence_marker)\n",
        "            if search_start_idx == -1:\n",
        "                search_start_idx = 0 # If not found, start from beginning\n",
        "\n",
        "            user_marker_pos = full_formatted_text.find(user_start_marker, search_start_idx)\n",
        "            if user_marker_pos == -1:\n",
        "                raise ValueError(f\"'{user_start_marker}' marker not found in formatted text.\")\n",
        "\n",
        "            # The prompt is between <｜User｜> and <｜Assistant｜>\n",
        "            assistant_marker_pos = full_formatted_text.find(assistant_start_marker, user_marker_pos)\n",
        "            if assistant_marker_pos == -1:\n",
        "                raise ValueError(f\"'{assistant_start_marker}' marker not found after user prompt.\")\n",
        "\n",
        "            original_user_prompt = full_formatted_text[user_marker_pos + len(user_start_marker) : assistant_marker_pos].strip()\n",
        "\n",
        "            # The ground truth response is between <｜Assistant｜> and <｜end of sentence｜> (or end of string)\n",
        "            end_sentence_marker_pos = full_formatted_text.rfind(end_sentence_marker)\n",
        "            if end_sentence_marker_pos == -1:\n",
        "                # If end_sentence_marker is not found, assume it goes to the end of the string\n",
        "                ground_truth_response = full_formatted_text[assistant_marker_pos + len(assistant_start_marker) :].strip()\n",
        "            else:\n",
        "                ground_truth_response = full_formatted_text[assistant_marker_pos + len(assistant_start_marker) : end_sentence_marker_pos].strip()\n",
        "\n",
        "            # Final check that parsed parts are not empty\n",
        "            if not original_user_prompt or not ground_truth_response:\n",
        "                raise ValueError(\"Parsed prompt or ground truth response is empty after token parsing.\")\n",
        "\n",
        "        except ValueError as ve:\n",
        "            print(f\"DEBUG: Skipping example {i+1} due to malformed chat template structure during parsing: {ve}\")\n",
        "            debug_full_text_snippet = full_formatted_text.replace('\\n', ' ')\n",
        "            print(f\"DEBUG: Full text (showing markers): '{debug_full_text_snippet}'\")\n",
        "            continue # Skip to next example\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Error parsing example {i+1}'s formatted text: {e}\")\n",
        "            debug_full_text_snippet_short = full_formatted_text[:200].replace('\\n', ' ')\n",
        "            print(f\"DEBUG: Full text (snippet): '{debug_full_text_snippet_short}'\")\n",
        "            continue # Skip to next example\n",
        "\n",
        "\n",
        "        # Prepare input for model generation\n",
        "        input_messages = [{\"role\": \"user\", \"content\": original_user_prompt}]\n",
        "        # Applying chat template for generation, similar to how it was done for training input.\n",
        "        input_text = tokenizer.apply_chat_template(input_messages, tokenize=False, add_special_tokens=True)\n",
        "\n",
        "        # Tokenize the input text\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate response from the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                **generation_config\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens\n",
        "        generated_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "        # Clean any remaining end sentence markers that might have been generated by the model\n",
        "        generated_text = generated_text.replace(end_sentence_marker, \"\").strip()\n",
        "\n",
        "\n",
        "        # --- BLEU Score Calculation ---\n",
        "        reference_tokens = word_tokenize(ground_truth_response)\n",
        "        candidate_tokens = word_tokenize(generated_text)\n",
        "\n",
        "        if len(reference_tokens) > 0 and len(candidate_tokens) > 0:\n",
        "            bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
        "            total_bleu_score += bleu_score\n",
        "            evaluated_count += 1\n",
        "        else:\n",
        "            bleu_score = 0.0\n",
        "            print(f\"Warning: Cannot calculate BLEU for example {i+1} due to empty reference or candidate tokens.\")\n",
        "\n",
        "\n",
        "        print(f\"\\n--- Evaluation Example {i+1} ---\")\n",
        "        print(f\"User Prompt:\\n{original_user_prompt}\")\n",
        "        print(f\"\\nModel's Response:\\n{generated_text}\")\n",
        "        print(f\"\\nGround Truth:\\n{ground_truth_response}\")\n",
        "        print(f\"\\nBLEU Score: {bleu_score:.4f}\")\n",
        "        print(\"-\" * 70)\n",
        "else:\n",
        "    print(\"\\nEvaluation skipped: eval_dataset_for_trainer is empty or not prepared. Check dataset preparation logs above.\")\n",
        "\n",
        "\n",
        "print(\"\\nEvaluation complete.\")\n",
        "\n",
        "if evaluated_count > 0:\n",
        "    average_bleu_score = total_bleu_score / evaluated_count\n",
        "    print(f\"\\nAverage BLEU Score across {evaluated_count} evaluated examples: {average_bleu_score:.4f}\")\n",
        "else:\n",
        "    print(\"\\nNo examples were successfully evaluated for BLEU score.\")\n",
        "\n",
        "print(\"\\nNote on BLEU Score Interpretation for this POC:\")\n",
        "print(\"BLEU measures the n-gram overlap between generated text and reference text.\")\n",
        "print(\"For this specific dataset and heuristic-based 'prompt'/'chosen' creation,\")\n",
        "print(\"a very high BLEU score might not be expected, as the model's generated\")\n",
        "print(\"continuation might diverge from the arbitrary 'ground truth' split, even if it's coherent.\")\n",
        "print(\"BLEU is most effective when the generated text is expected to be a close paraphrase or exact match.\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kfZkpGLsAwFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
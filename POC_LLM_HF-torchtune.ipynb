{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/POC_LLM_HF-torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkKx3xIPuD2d"
      },
      "source": [
        "https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPPrd3vrEku7",
        "outputId": "9cfd68d1-5f3d-48f2-fa38-b1f43908c5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 29 14:51:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   38C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r87UBcYKaPb9"
      },
      "source": [
        "https://www.engineering.com/story/design-and-engineering-performance-and-flexibility-the-nvidia-l4-tensor-core-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gze0JGsUrpOD"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install tiktoken -q\n",
        "!pip install accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of_yMPBIrxbw",
        "outputId": "54b6124e-bf9a-41a9-e867-a2fe617a0182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhORtoG8pVtw"
      },
      "outputs": [],
      "source": [
        "!pip install torchtune -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0IpiYjfpaeK",
        "outputId": "ff12dcad-ac6c-4da5-80c3-a3fb5db268bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ],
      "source": [
        "!tune -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Ak8-i7pbg2"
      },
      "outputs": [],
      "source": [
        "!tune download frankmorales2020/torchtune-Llama-2-7b --output-dir /tmp/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toSKWSMArPoJ",
        "outputId": "7bbecd3d-109a-48ee-9c15-07df3927099b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root  151 Apr 29 15:00 /tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/ee62ed2ad7ded505ae47df50bc6c52916860dfb1c009df4715148cc4bfb50d2f\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 15:00 /tmp/Llama-2-7b-hf/hf_model_0001_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/40be895e082a4f8b9d610326d2f9de309a58ae0219346b8afd608d27b0451c19\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 15:00 /tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/1fd7762035b3ca4f2d6af6bf10129689a119b7c38058025f9842511532ea02fb\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 15:00 /tmp/Llama-2-7b-hf/hf_model_0002_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/f93037a0b82968e3c11e26de864988d642f37154bdfd1f314630c00d06ab4b17\n",
            "-rw-r--r-- 1 root root 1.8M Apr 29 14:59 /tmp/Llama-2-7b-hf/tokenizer.json\n",
            "-rw-r--r-- 1 root root 489K Apr 29 14:59 /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "-rw-r--r-- 1 root root  776 Apr 29 14:59 /tmp/Llama-2-7b-hf/tokenizer_config.json\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 14:59 /tmp/Llama-2-7b-hf/adapter_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/9b209735a82e8963036e00bc4f21c59d7dee6c196f5a588b6711b3f03d30dc99\n",
            "-rw-r--r-- 1 root root  414 Apr 29 14:59 /tmp/Llama-2-7b-hf/special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 1.2M Apr 29 14:59 /tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
            "-rw-r--r-- 1 root root  27K Apr 29 14:59 /tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root  27K Apr 29 14:59 /tmp/Llama-2-7b-hf/model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root  22K Apr 29 14:59 /tmp/Llama-2-7b-hf/README.md\n",
            "-rw-r--r-- 1 root root  609 Apr 29 14:59 /tmp/Llama-2-7b-hf/config.json\n",
            "-rw-r--r-- 1 root root 6.9K Apr 29 14:59 /tmp/Llama-2-7b-hf/LICENSE.txt\n",
            "-rw-r--r-- 1 root root 4.7K Apr 29 14:59 /tmp/Llama-2-7b-hf/USE_POLICY.md\n",
            "-rw-r--r-- 1 root root  188 Apr 29 14:59 /tmp/Llama-2-7b-hf/generation_config.json\n"
          ]
        }
      ],
      "source": [
        "!ls -ltha /tmp/Llama-2-7b-hf/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYv2HMBVrtfh",
        "outputId": "bf428aab-a9a8-406e-d38c-f18049d169f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied file to /content/custom_generation_config.yaml\n"
          ]
        }
      ],
      "source": [
        "!tune cp generation /content/custom_generation_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "before fine-tunne"
      ],
      "metadata": {
        "id": "jrXzQoQpTUuS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jzx_4W6AsY_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c4a61e-bcac-4cf3-925c-c73328c14f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "max_new_tokens: 300\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "prompt: What are some interesting sites to visit in the Bay Area?\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "temperature: 0.6\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "top_k: 300\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:What are some interesting sites to visit in the Bay Area?\n",
            "I'll be in the Bay Area for 3 days and I'm wondering what are some good places to visit? I'd like to visit the Golden Gate Bridge, Alcatraz, Fisherman's Wharf, and the Haight-Ashbury area. What else should I see?\n",
            "posted by alamat to Travel & Transportation around San Francisco, CA (15 answers total) 1 user marked this as a favorite\n",
            "2009 is the 150th anniversary of the Golden Gate Bridge. They are celebrating all year, with various events.\n",
            "posted by mr_crash_davis at 12:01 PM on January 17, 2009\n",
            "I'd add Chinatown and the Castro.\n",
            "posted by Michele in California at 12:01 PM on January 17, 2009\n",
            "Awesome. I'm in the Castro now.\n",
            "posted by alamat at 12:04 PM on January 17, 2009\n",
            "Seconding Chinatown and the Castro, and if you have time, the Mission.\n",
            "posted by gursky at 12:06 PM on January 17, 2009\n",
            "The Mission is one of my favorite places in the city.\n",
            "post\n",
            "INFO:torchtune.utils.logging:Time for inference: 20.69 sec total, 14.50 tokens/sec\n",
            "INFO:torchtune.utils.logging:Bandwidth achieved: 227.54 GB/s\n",
            "INFO:torchtune.utils.logging:Memory used: 15.72 GB\n"
          ]
        }
      ],
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fine-tune model\n",
        "\n",
        "checkpoint_files: [\n",
        "        hf_model_0001_0.pt,\n",
        "        hf_model_0002_0.pt,\n",
        "  ]"
      ],
      "metadata": {
        "id": "N5O7awR2SxM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ],
      "metadata": {
        "id": "IltRzp6QSsdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea2f227-a37f-4565-a329-326b19437bab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  checkpoint_files:\n",
            "  - hf_model_0001_0.pt\n",
            "  - hf_model_0002_0.pt\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "max_new_tokens: 300\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "prompt: What are some interesting sites to visit in the Bay Area?\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "temperature: 0.6\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "top_k: 300\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:What are some interesting sites to visit in the Bay Area?\n",
            "What are some interesting sites to visit in the Bay Area?\n",
            "The Bay Area is home to many interesting sites, from the iconic Golden Gate Bridge to the quirky Alcatraz Island. Here are some of the most interesting sites to visit in the Bay Area:\n",
            "Golden Gate Bridge: This suspension bridge is one of the most recognizable landmarks in the world. It spans the Golden Gate Strait, connecting San Francisco to Marin County. Visitors can take a walk or bike ride across the bridge and enjoy the stunning views of the bay.\n",
            "Alcatraz Island: This former prison is now a popular tourist attraction. Visitors can take a ferry to the island and explore the cell blocks, hospital, and other buildings. There is also a museum on the island, which tells the history of the prison and its most famous inmates.\n",
            "Coit Tower: This 210-foot tower is located in the Telegraph Hill neighborhood of San Francisco. It offers panoramic views of the city and the bay. Visitors can take an elevator to the top of the tower and enjoy the views.\n",
            "Sutro Baths: These ruins are located in the Lands End area of San Francisco. The baths were a popular swimming and spa destination in the late 19th century, but closed in 1966. Today, visitors can explore the ru\n",
            "INFO:torchtune.utils.logging:Time for inference: 20.32 sec total, 14.77 tokens/sec\n",
            "INFO:torchtune.utils.logging:Bandwidth achieved: 231.69 GB/s\n",
            "INFO:torchtune.utils.logging:Memory used: 15.72 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-B4eX9pr4vt"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "#model_id = \"davidkim205/Rhea-72b-v0.5\" # HF AVERAGE = 81.22 #1 APRIL 5TH, 2024\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\" #29/04/2024\n",
        "\n",
        "#Metric\tValue\n",
        "#Avg.\t81.22\n",
        "#AI2 Reasoning Challenge (25-Shot)\t79.78\n",
        "#HellaSwag (10-Shot)\t91.15\n",
        "#MMLU (5-Shot)\t77.95\n",
        "#TruthfulQA (0-shot)\t74.50\n",
        "#Winogrande (5-shot)\t87.85\n",
        "#GSM8k (5-shot)\t76.12\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True, do_sample=True)\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xlg7rlwzTpZL"
      },
      "outputs": [],
      "source": [
        "#prompt=\"What was the first album Beyoncé released as a solo artist?\"\n",
        "prompt=\"What is the capital of canada?\"\n",
        "outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.8,top_k=50, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8S7gBybTbGn"
      },
      "outputs": [],
      "source": [
        "print('Question: %s'%prompt)\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNZMrz3BC76CrnC/2csjsNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/POC_LLM_HF-torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkKx3xIPuD2d"
      },
      "source": [
        "https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPPrd3vrEku7",
        "outputId": "14300589-fc80-4e56-8813-5b6f41d4729c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 29 18:40:33 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   28C    P8              11W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r87UBcYKaPb9"
      },
      "source": [
        "https://www.engineering.com/story/design-and-engineering-performance-and-flexibility-the-nvidia-l4-tensor-core-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gze0JGsUrpOD"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install tiktoken -q\n",
        "!pip install accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of_yMPBIrxbw",
        "outputId": "a2eff18b-879c-4901-c429-036bd6c12c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhORtoG8pVtw"
      },
      "outputs": [],
      "source": [
        "!pip install torchtune -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0IpiYjfpaeK",
        "outputId": "41077c2c-28c9-4e81-c07f-35c2972e8966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ],
      "source": [
        "!tune -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Ak8-i7pbg2"
      },
      "outputs": [],
      "source": [
        "!tune download frankmorales2020/torchtune-Llama-2-7b --output-dir /tmp/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toSKWSMArPoJ",
        "outputId": "a0a93f09-8b6a-414f-b421-5384c5c06fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root  151 Apr 29 18:44 /tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/ee62ed2ad7ded505ae47df50bc6c52916860dfb1c009df4715148cc4bfb50d2f\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 18:43 /tmp/Llama-2-7b-hf/hf_model_0001_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/40be895e082a4f8b9d610326d2f9de309a58ae0219346b8afd608d27b0451c19\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 18:43 /tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/1fd7762035b3ca4f2d6af6bf10129689a119b7c38058025f9842511532ea02fb\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 18:43 /tmp/Llama-2-7b-hf/hf_model_0002_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/f93037a0b82968e3c11e26de864988d642f37154bdfd1f314630c00d06ab4b17\n",
            "-rw-r--r-- 1 root root 1.8M Apr 29 18:42 /tmp/Llama-2-7b-hf/tokenizer.json\n",
            "-rw-r--r-- 1 root root  776 Apr 29 18:42 /tmp/Llama-2-7b-hf/tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 489K Apr 29 18:42 /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "-rw-r--r-- 1 root root  414 Apr 29 18:42 /tmp/Llama-2-7b-hf/special_tokens_map.json\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 18:42 /tmp/Llama-2-7b-hf/adapter_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/9b209735a82e8963036e00bc4f21c59d7dee6c196f5a588b6711b3f03d30dc99\n",
            "-rw-r--r-- 1 root root 1.2M Apr 29 18:42 /tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
            "-rw-r--r-- 1 root root  27K Apr 29 18:42 /tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root  27K Apr 29 18:42 /tmp/Llama-2-7b-hf/model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root  22K Apr 29 18:42 /tmp/Llama-2-7b-hf/README.md\n",
            "-rw-r--r-- 1 root root  609 Apr 29 18:42 /tmp/Llama-2-7b-hf/config.json\n",
            "-rw-r--r-- 1 root root  188 Apr 29 18:42 /tmp/Llama-2-7b-hf/generation_config.json\n",
            "-rw-r--r-- 1 root root 4.7K Apr 29 18:42 /tmp/Llama-2-7b-hf/USE_POLICY.md\n",
            "-rw-r--r-- 1 root root 6.9K Apr 29 18:42 /tmp/Llama-2-7b-hf/LICENSE.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -ltha /tmp/Llama-2-7b-hf/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYv2HMBVrtfh"
      },
      "outputs": [],
      "source": [
        "!tune cp generation /content/custom_generation_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "before fine-tunne"
      ],
      "metadata": {
        "id": "jrXzQoQpTUuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_generation_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbObmaXeNqFY",
        "outputId": "1819a901-3eb4-4018-9e72-c1a10926fbb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run generate --config generation\n",
            "\n",
            "# Model arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  checkpoint_files: [\n",
            "    pytorch_model-00001-of-00002.bin,\n",
            "    pytorch_model-00002-of-00002.bin,\n",
            "  ]\n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "  model_type: LLAMA2\n",
            "\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "\n",
            "seed: 1234\n",
            "\n",
            "# Tokenizer arguments\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Generation arguments; defaults taken from gpt-fast\n",
            "prompt: \"Hello, my name is\"\n",
            "max_new_tokens: 300\n",
            "temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
            "top_k: 300\n",
            "\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jzx_4W6AsY_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01b0e24-943f-46dd-fbd2-cf2f18ad492c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "max_new_tokens: 300\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "prompt: What are some interesting sites to visit in the Bay Area?\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "temperature: 0.6\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "top_k: 300\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:What are some interesting sites to visit in the Bay Area?\n",
            "I'll be in the Bay Area for 3 days and I'm wondering what are some good places to visit? I'd like to visit the Golden Gate Bridge, Alcatraz, Fisherman's Wharf, and the Haight-Ashbury area. What else should I see?\n",
            "posted by alamat to Travel & Transportation around San Francisco, CA (15 answers total) 1 user marked this as a favorite\n",
            "2009 is the 150th anniversary of the Golden Gate Bridge. They are celebrating all year, with various events.\n",
            "posted by mr_crash_davis at 12:01 PM on January 17, 2009\n",
            "I'd add Chinatown and the Castro.\n",
            "posted by Michele in California at 12:01 PM on January 17, 2009\n",
            "Awesome. I'm in the Castro now.\n",
            "posted by alamat at 12:04 PM on January 17, 2009\n",
            "Seconding Chinatown and the Castro, and if you have time, the Mission.\n",
            "posted by gursky at 12:06 PM on January 17, 2009\n",
            "The Mission is one of my favorite places in the city.\n",
            "post\n",
            "INFO:torchtune.utils.logging:Time for inference: 20.18 sec total, 14.86 tokens/sec\n",
            "INFO:torchtune.utils.logging:Bandwidth achieved: 233.25 GB/s\n",
            "INFO:torchtune.utils.logging:Memory used: 15.72 GB\n"
          ]
        }
      ],
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fine-tune model\n",
        "\n",
        "checkpoint_files: [\n",
        "        hf_model_0001_0.pt,\n",
        "        hf_model_0002_0.pt,\n",
        "  ]"
      ],
      "metadata": {
        "id": "N5O7awR2SxM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/custom_generation_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6nBkk7WNCJ7",
        "outputId": "626b90e8-c98e-4ae0-99da-c202843f4a7a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
            "#\n",
            "# To launch, run the following command from root torchtune directory:\n",
            "#    tune run generate --config generation\n",
            "\n",
            "# Model arguments\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  checkpoint_files: [ hf_model_0001_0.pt, hf_model_0002_0.pt, ]\n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "  model_type: LLAMA2\n",
            "\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "\n",
            "seed: 1234\n",
            "\n",
            "# Tokenizer arguments\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "\n",
            "# Generation arguments; defaults taken from gpt-fast\n",
            "prompt: \"Hello, my name is\"\n",
            "max_new_tokens: 300\n",
            "temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
            "top_k: 300\n",
            "\n",
            "quantizer: null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ],
      "metadata": {
        "id": "IltRzp6QSsdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b699998-dd4b-4b5e-b402-7645d9637ddf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  checkpoint_files:\n",
            "  - hf_model_0001_0.pt\n",
            "  - hf_model_0002_0.pt\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "max_new_tokens: 300\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "prompt: What are some interesting sites to visit in the Bay Area?\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "temperature: 0.6\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "top_k: 300\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:What are some interesting sites to visit in the Bay Area?\n",
            "What are some interesting sites to visit in the Bay Area?\n",
            "The Bay Area is home to many interesting sites, from the iconic Golden Gate Bridge to the quirky Alcatraz Island. Here are some of the most interesting sites to visit in the Bay Area:\n",
            "Golden Gate Bridge: This suspension bridge is one of the most recognizable landmarks in the world. It spans the Golden Gate Strait, connecting San Francisco to Marin County. Visitors can take a walk or bike ride across the bridge and enjoy the stunning views of the bay.\n",
            "Alcatraz Island: This former prison is now a popular tourist attraction. Visitors can take a ferry to the island and explore the cell blocks, hospital, and other buildings. There is also a museum on the island, which tells the history of the prison and its most famous inmates.\n",
            "Coit Tower: This 210-foot tower is located in the Telegraph Hill neighborhood of San Francisco. It offers panoramic views of the city and the bay. Visitors can take an elevator to the top of the tower and enjoy the views.\n",
            "Sutro Baths: These ruins are located in the Lands End area of San Francisco. The baths were a popular swimming and spa destination in the late 19th century, but closed in 1966. Today, visitors can explore the ru\n",
            "INFO:torchtune.utils.logging:Time for inference: 20.15 sec total, 14.89 tokens/sec\n",
            "INFO:torchtune.utils.logging:Bandwidth achieved: 233.66 GB/s\n",
            "INFO:torchtune.utils.logging:Memory used: 15.72 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-B4eX9pr4vt"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "#model_id = \"davidkim205/Rhea-72b-v0.5\" # HF AVERAGE = 81.22 #1 APRIL 5TH, 2024\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\" #29/04/2024\n",
        "\n",
        "#Metric\tValue\n",
        "#Avg.\t81.22\n",
        "#AI2 Reasoning Challenge (25-Shot)\t79.78\n",
        "#HellaSwag (10-Shot)\t91.15\n",
        "#MMLU (5-Shot)\t77.95\n",
        "#TruthfulQA (0-shot)\t74.50\n",
        "#Winogrande (5-shot)\t87.85\n",
        "#GSM8k (5-shot)\t76.12\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True, do_sample=True)\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Xlg7rlwzTpZL"
      },
      "outputs": [],
      "source": [
        "#prompt=\"What was the first album Beyoncé released as a solo artist?\"\n",
        "prompt=\"What is the capital of russia?\"\n",
        "prompt=\"What are some interesting sites to visit in the Bay Area?\"\n",
        "outputs = pipe(prompt, max_new_tokens=300, temperature=0.6,do_sample=True,top_k=50, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "c8S7gBybTbGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9bdc7d-b11c-4b99-ce37-a569f5acda8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are some interesting sites to visit in the Bay Area?\n",
            "Generated Answer:\n",
            "The Bay Area is home to many interesting sites, including the Golden Gate Bridge, Alcatraz Island, and Fisherman's Wharf. Other popular attractions include the California Academy of Sciences, the Exploratorium, and the San Francisco Museum of Modern Art.\n",
            "How can I get around the Bay Area?\n",
            "There are many ways to get around the Bay Area. You can take public transportation, such as buses and trains, or rent a car. There are also several ride-sharing services, such as Uber and Lyft, that operate in the area.\n",
            "What are some popular foods to try in the Bay Area?\n",
            "Some popular foods to try in the Bay Area include sourdough bread, clam chowder, and Dungeness crab. Other popular dishes include burritos, tacos, and pizza. There are also many local wines and beers that are worth trying.\n",
            "What is the weather like in the Bay Area?\n",
            "The Bay Area has a mild Mediterranean climate, with cool, wet winters and warm, dry summers. The average temperature in the summer is around 70 degrees Fahrenheit, and the average temperature in the winter is around 50 degrees Fahrenheit.\n",
            "What are some popular activities to do in the Bay Area?\n",
            "Some popular activities to do in the Bay Area include hiking, biking,\n"
          ]
        }
      ],
      "source": [
        "print('Question: %s'%prompt)\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMluPX17K1vi/xh4LQtgDPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
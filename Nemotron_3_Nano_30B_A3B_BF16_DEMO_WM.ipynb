{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPzZAAbBFUfXneYdIrQVBqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Nemotron_3_Nano_30B_A3B_BF16_DEMO_WM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm requests -q"
      ],
      "metadata": {
        "id": "l130FjQz9ejg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available! PyTorch can access your GPU.\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(\"CUDA is NOT available. PyTorch cannot access a GPU.\")\n",
        "    print(\"Please ensure your Colab runtime type is set to GPU (Runtime -> Change runtime type -> Hardware accelerator: GPU).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRvlBIQb97rk",
        "outputId": "612b063d-5a55-4213-eb65-92c10ddb8e7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available! PyTorch can access your GPU.\n",
            "GPU Name: NVIDIA A100-SXM4-80GB\n",
            "GPU Memory: 79.32 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8000"
      ],
      "metadata": {
        "id": "-6Q13gst-y0e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation (must be run after every restart)\n",
        "#!pip install vllm requests\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "VLLM_MODEL = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
        "VLLM_URL = \"http://localhost:8000\"\n",
        "\n",
        "# --- CRITICAL CHANGE: Increased Utilization to 0.85 ---\n",
        "GPU_UTILIZATION_TARGET = \"0.85\"\n",
        "\n",
        "print(f\"--- Starting Nemotron-3-Nano vLLM Server (Targeting {GPU_UTILIZATION_TARGET} utilization) ---\")\n",
        "\n",
        "# Start vLLM server using subprocess\n",
        "vllm_process = subprocess.Popen([\n",
        "    \"vllm\", \"serve\", VLLM_MODEL,\n",
        "    \"--tensor-parallel-size\", \"1\",\n",
        "    \"--enable-expert-parallel\",\n",
        "    \"--trust-remote-code\",\n",
        "    \"--gpu-memory-utilization\", GPU_UTILIZATION_TARGET,\n",
        "    \"--served-model-name\", \"nemotron-nano\"\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "# Polling loop to wait for the server to be ready\n",
        "print(\"Waiting for vLLM server to start and model to load...\")\n",
        "start_time = time.time()\n",
        "server_ready = False\n",
        "\n",
        "while time.time() - start_time < 480:\n",
        "    try:\n",
        "        response = requests.get(f\"{VLLM_URL}/v1/models\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"\\n‚úÖ Server is READY! Took {int(time.time() - start_time)} seconds.\")\n",
        "            server_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        if vllm_process.poll() is not None:\n",
        "             print(\"\\n!!! ERROR: vLLM server process crashed. Check logs below !!!\")\n",
        "             print(vllm_process.communicate()[0])\n",
        "             break\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(10)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during check: {e}\")\n",
        "        time.sleep(10)\n",
        "\n",
        "if not server_ready:\n",
        "    print(\"\\n!!! ERROR: Server failed to start within the timeout period. Please check GPU/VRAM.\")\n",
        "    if vllm_process.poll() is None:\n",
        "        vllm_process.terminate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWpDzobk-5Ie",
        "outputId": "7652bdf4-5e8a-47f6-dd9a-2d9b3a1bf4da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Nemotron-3-Nano vLLM Server (Targeting 0.85 utilization) ---\n",
            "Waiting for vLLM server to start and model to load...\n",
            "...........\n",
            "‚úÖ Server is READY! Took 110 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from requests.exceptions import ConnectionError, Timeout\n",
        "\n",
        "# --- CORE AGENT LOGIC (Missing Piece) ---\n",
        "# NOTE: This entire block for run_agent_workflow and its helpers\n",
        "# must be pasted into the environment before the check_api_availability call.\n",
        "# Without the full definition of this function, the NameError will persist.\n",
        "\n",
        "def run_agent_workflow(target_goal: str):\n",
        "    # This function is where all the sequencing, Nemotron calls, and\n",
        "    # Simulation Fallback logic (from my previous response) reside.\n",
        "    # It cannot be run if the code for this function is not in memory.\n",
        "\n",
        "    # ... (Implementation of the agent logic goes here) ...\n",
        "    # Placeholder to show it's defined:\n",
        "    print(f\"Agent logic for goal '{target_goal}' is now running.\")\n",
        "    pass\n",
        "\n",
        "# --- API CHECK FUNCTION (Your Code) ---\n",
        "def check_api_availability(url: str, check_timeout: int = 5) -> bool:\n",
        "    \"\"\"Checks if the VLLM API server is available at the given URL.\"\"\"\n",
        "    try:\n",
        "        # Use HEAD request for quick status check (Source 1.1)\n",
        "        response = requests.head(url, timeout=check_timeout)\n",
        "        if response.status_code in [200, 404, 405, 503]:\n",
        "            print(f\"[‚úÖ API Check] Server responded with status code {response.status_code}. Proceeding.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[‚ö†Ô∏è API Check] Server responded with unexpected status code {response.status_code}.\")\n",
        "            return False\n",
        "    except (ConnectionError, Timeout):\n",
        "        print(f\"[üõë API Check] Could not connect to the server at {url}. Connection timed out or refused.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"[üõë API Check] An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- EXECUTION BLOCK (Your Code) ---\n",
        "if __name__ == \"__main__\":\n",
        "    target_goal = \"Turn off the light and lock the door before going to bed.\"\n",
        "    BASE_URL = \"http://localhost:8000\"\n",
        "\n",
        "    if not check_api_availability(BASE_URL):\n",
        "        print(\"\\nFATAL ERROR: Cannot start agent. VLLM server is unavailable.\")\n",
        "    else:\n",
        "        # This call will now succeed because run_agent_workflow is defined above.\n",
        "        run_agent_workflow(target_goal)\n",
        "\n",
        "# Assuming the full agent logic (World Model, Nemotron Planner, etc.)\n",
        "# is placed inside the run_agent_workflow function definition."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDQSWeKKZJSW",
        "outputId": "faf8c256-6104-469d-979f-41cace79d5db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úÖ API Check] Server responded with status code 404. Proceeding.\n",
            "Agent logic for goal 'Turn off the light and lock the door before going to bed.' is now running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install vllm requests litellm jsonformer -q"
      ],
      "metadata": {
        "id": "19KJK5oBDNp7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 1: SLEEP"
      ],
      "metadata": {
        "id": "E05e2F1QhJSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from requests.exceptions import ConnectionError, Timeout\n",
        "\n",
        "# --- Configuration (Centralized and Accurate Endpoints) ---\n",
        "NEMOTRON_BASE_URL = \"http://localhost:8000\"\n",
        "NEMOTRON_API_URL = f\"{NEMOTRON_BASE_URL}/v1/chat/completions\" # The endpoint for LLM planning\n",
        "NEMOTRON_HEALTH_URL = f\"{NEMOTRON_BASE_URL}/v1/models\"        # The explicit health check endpoint\n",
        "NEMOTRON_MODEL_NAME = \"nemotron-nano\"\n",
        "\n",
        "\n",
        "# --- 1. World Model (Internal State Tracker) ---\n",
        "def update_world_model(current_state: dict, action: str) -> str:\n",
        "    \"\"\"Updates the world state based on the executed action and returns a success/fail message.\"\"\"\n",
        "\n",
        "    action = action.upper()\n",
        "\n",
        "    # CRITICAL FIX: Accept synonyms (TURN_OFF_LIGHT, TOGGLE_LIGHT)\n",
        "    if action in [\"TOGGLE_LIGHT\", \"TURN_OFF_LIGHT\"]:\n",
        "        if current_state['light_on']:\n",
        "            current_state['light_on'] = False\n",
        "            print(\"[WM Update] Light is now OFF.\")\n",
        "            return \"SUCCESS\"\n",
        "        else:\n",
        "            print(\"[WM Update] Light is already OFF. State aligned.\")\n",
        "            return \"SUCCESS\"\n",
        "\n",
        "    elif action == \"LOCK_DOOR\":\n",
        "        current_state['door_locked'] = True\n",
        "        print(\"[WM Update] Door is now LOCKED.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    else:\n",
        "        print(f\"[WM Update] ERROR: Unknown action '{action}'. State unchanged.\")\n",
        "        return \"ERROR\"\n",
        "\n",
        "# --- 2. LLM Planner (Robust Call with Increased Timeout) ---\n",
        "def nemotron_planner(state: dict) -> str | None:\n",
        "    \"\"\"Calls the Nemotron LLM to get the next action, returning raw output or None on failure.\"\"\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"CURRENT WORLD STATE: {state}. GOAL: Turn off the light and lock the door before going to bed.\"\n",
        "        f\"Provide your full reasoning (Chain of Thought), and conclude with the next single action in the exact format: \"\n",
        "        f\"THE NEXT ACTION IS: [ACTION_NAME]\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a planning assistant. Be verbose in your reasoning.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    payload = {\n",
        "        \"model\": NEMOTRON_MODEL_NAME,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Using the confirmed necessary 60-second timeout\n",
        "        print(f\"[API Call] Sending request to Nemotron (Timeout: 60s)...\")\n",
        "        response = requests.post(\n",
        "            NEMOTRON_API_URL,\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            json=payload,\n",
        "            timeout=60\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"!!! LLM PLANNING FAILED (API Error/Timeout). Details: {e.__class__.__name__}\")\n",
        "        # Return None to trigger the Simulation Fallback\n",
        "        return None\n",
        "\n",
        "# --- 3. Resilient Parser with Simulation Fallback ---\n",
        "def parse_and_act(raw_llm_output: str | None, state: dict) -> str:\n",
        "    \"\"\"Extracts action from output or uses the fallback, then executes the action.\"\"\"\n",
        "\n",
        "    action = None\n",
        "\n",
        "    if raw_llm_output:\n",
        "        # Brittle Parser Logic (Searching for the specific keyword \"THE NEXT ACTION IS:\")\n",
        "        match = re.search(r\"THE NEXT ACTION IS:\\s*(\\w+)\", raw_llm_output)\n",
        "        if match:\n",
        "            action = match.group(1).strip().upper()\n",
        "            print(f\"[LLM Extracted] Action found by parser: {action}\")\n",
        "        else:\n",
        "            print(\"[‚ö†Ô∏è Parser Fail] Could not find the action keyword. Falling back to Simulation.\")\n",
        "\n",
        "    # Simulation Fallback (The Resilience Layer)\n",
        "    if action is None:\n",
        "        print(\"!!! Triggering Simulation Fallback !!!\")\n",
        "\n",
        "        # Fallback logic based on the goal: Light first, then Door.\n",
        "        if state['light_on']:\n",
        "            action = \"TOGGLE_LIGHT\"\n",
        "        elif not state['door_locked']:\n",
        "            action = \"LOCK_DOOR\"\n",
        "        else:\n",
        "            return \"SUCCESS: Goal Complete\"\n",
        "\n",
        "        print(f\"[Fallback Action] Executing: {action}\")\n",
        "\n",
        "    # Execute the determined action (either from LLM or Fallback)\n",
        "    return update_world_model(state, action)\n",
        "\n",
        "# --- 4. System Check (Fixed for Accuracy) ---\n",
        "def check_api_availability(health_url: str, check_timeout: int = 5) -> bool:\n",
        "    \"\"\"Checks if the VLLM API server is fully available using the explicit health_url.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(health_url, timeout=check_timeout)\n",
        "\n",
        "        # Success requires an explicit 200 OK signal\n",
        "        if response.status_code == 200:\n",
        "            print(f\"[‚úÖ API Check] Server responded with status code 200 OK at {health_url}. Proceeding.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[‚ö†Ô∏è API Check] Server responded with status code {response.status_code} at {health_url}. Status is not 200 OK.\")\n",
        "            return False\n",
        "\n",
        "    except (ConnectionError, Timeout):\n",
        "        print(f\"[üõë API Check] Could not connect to the server at {health_url}. Connection timed out or refused.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"[üõë API Check] An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- 5. The Agent Workflow Orchestrator (The Main Loop) ---\n",
        "def run_agent_workflow(target_goal: str):\n",
        "    \"\"\"The main loop for the multi-step agent.\"\"\"\n",
        "\n",
        "    # Initial State\n",
        "    current_state = {'location': 'LivingRoom', 'light_on': True, 'door_locked': False}\n",
        "    max_steps = 5\n",
        "\n",
        "    print(f\"\\n--- STARTING AGENT WORKFLOW for: {target_goal} ---\")\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        print(f\"\\n======== STEP {step} ========\")\n",
        "        print(f\"[üó∫Ô∏è Current State:] {current_state}\")\n",
        "\n",
        "        # Check for immediate goal completion\n",
        "        if not current_state['light_on'] and current_state['door_locked']:\n",
        "            print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "            return\n",
        "\n",
        "        # 1. LLM Planning\n",
        "        raw_output = nemotron_planner(current_state)\n",
        "\n",
        "        # 2. Parse and Execute\n",
        "        status = parse_and_act(raw_output, current_state)\n",
        "\n",
        "        if status == \"SUCCESS: Goal Complete\":\n",
        "             print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "             return\n",
        "\n",
        "    print(\"--- AGENT FAILURE: Max steps reached before goal completion. ---\")\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    target_goal = \"Turn off the light and lock the door before going to bed.\"\n",
        "\n",
        "    # The essential, accurate diagnostic check\n",
        "    if not check_api_availability(NEMOTRON_HEALTH_URL):\n",
        "        print(\"\\nFATAL ERROR: Cannot start agent. VLLM server is unavailable.\")\n",
        "    else:\n",
        "        # The main workflow call\n",
        "        run_agent_workflow(target_goal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ctsayjqcjm-",
        "outputId": "442be412-c70a-4337-de44-2eb350b18102"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úÖ API Check] Server responded with status code 200 OK at http://localhost:8000/v1/models. Proceeding.\n",
            "\n",
            "--- STARTING AGENT WORKFLOW for: Turn off the light and lock the door before going to bed. ---\n",
            "\n",
            "======== STEP 1 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': True, 'door_locked': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted] Action found by parser: TURN_OFF_LIGHT\n",
            "[WM Update] Light is now OFF.\n",
            "\n",
            "======== STEP 2 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': False, 'door_locked': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted] Action found by parser: LOCK_DOOR\n",
            "[WM Update] Door is now LOCKED.\n",
            "\n",
            "======== STEP 3 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': False, 'door_locked': True}\n",
            "--- AGENT SUCCESS: Goal completed! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 2: COFFEE"
      ],
      "metadata": {
        "id": "STw2TQ-pexnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from requests.exceptions import ConnectionError, Timeout\n",
        "\n",
        "# --- Configuration (Centralized and Accurate Endpoints) ---\n",
        "NEMOTRON_BASE_URL = \"http://localhost:8000\"\n",
        "NEMOTRON_API_URL = f\"{NEMOTRON_BASE_URL}/v1/chat/completions\"\n",
        "NEMOTRON_HEALTH_URL = f\"{NEMOTRON_BASE_URL}/v1/models\"\n",
        "NEMOTRON_MODEL_NAME = \"nemotron-nano\"\n",
        "TARGET_GOAL = \"Make and serve a fresh cup of coffee and place it on the desk.\"\n",
        "\n",
        "# --- 1. World Model (Internal State Tracker) ---\n",
        "def update_coffee_world_model(current_state: dict, action: str) -> str:\n",
        "    \"\"\"Updates the coffee-making state based on the executed action.\"\"\"\n",
        "    action = action.upper()\n",
        "\n",
        "    # Define recognized actions\n",
        "    if action == \"GRIND_BEANS\":\n",
        "        current_state['beans_ground'] = True\n",
        "        print(\"[WM Update] Beans are now ground.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    elif action == \"BREW_COFFEE\":\n",
        "        if current_state['beans_ground']:\n",
        "            current_state['coffee_brewed'] = True\n",
        "            print(\"[WM Update] Coffee is now brewed.\")\n",
        "            return \"SUCCESS\"\n",
        "        else:\n",
        "            print(\"[WM Update] ERROR: Cannot BREW_COFFEE, beans are not ground.\")\n",
        "            return \"ERROR\"\n",
        "\n",
        "    elif action == \"PICK_UP_CUP\":\n",
        "        current_state['cup_held'] = True\n",
        "        current_state['location'] = 'CarryingCup'\n",
        "        print(\"[WM Update] Cup is now held.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    elif action == \"PUT_DOWN_CUP\":\n",
        "        current_state['cup_held'] = False\n",
        "        current_state['location'] = 'Desk'\n",
        "        print(\"[WM Update] Cup is now on the desk.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    else:\n",
        "        print(f\"[WM Update] ERROR: Unknown action '{action}'. State unchanged.\")\n",
        "        return \"ERROR\"\n",
        "\n",
        "# --- 2. LLM Planner (Enhanced with Goal Gap and Strict Constraints) ---\n",
        "def nemotron_planner(state: dict) -> str | None:\n",
        "    \"\"\"Calls the Nemotron LLM to get the next action, returning raw output or None on failure.\"\"\"\n",
        "\n",
        "    # --- PROMPT: Goal Gap Analysis (To guide the LLM's reasoning) ---\n",
        "    missing_steps = []\n",
        "    if not state.get('beans_ground'):\n",
        "         missing_steps.append(\"GRINDING THE BEANS\")\n",
        "    if state.get('beans_ground') and not state.get('coffee_brewed'):\n",
        "        missing_steps.append(\"BREWING THE COFFEE\")\n",
        "    if state.get('coffee_brewed') and not state.get('cup_held'):\n",
        "        missing_steps.append(\"PICKING UP THE CUP\")\n",
        "    if state.get('cup_held') and state.get('location') != 'Desk':\n",
        "        missing_steps.append(\"PUTTING THE CUP ON THE DESK\")\n",
        "\n",
        "    goal_gap_text = f\"CRITICAL GAPS (Must prioritize fixing these): {', '.join(missing_steps)}. \" if missing_steps else \"\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are a sequence planner. The GOAL is: {TARGET_GOAL}. \"\n",
        "        f\"CURRENT WORLD STATE: {state}. {goal_gap_text}\"\n",
        "        f\"Available Actions: GRIND_BEANS, BREW_COFFEE, PICK_UP_CUP, PUT_DOWN_CUP. \"\n",
        "        f\"Provide your Chain of Thought *briefly*, then output the action. \"\n",
        "        f\"‚ö†Ô∏è STRICTLY FOLLOW THIS FORMAT ONLY: THE NEXT ACTION IS: [ACTION_NAME]\"\n",
        "    )\n",
        "    # --- END PROMPT ---\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a planning assistant. Your response must be short and end with the required format.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    payload = {\n",
        "        \"model\": NEMOTRON_MODEL_NAME,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"[API Call] Sending request to Nemotron (Timeout: 60s)...\")\n",
        "        response = requests.post(NEMOTRON_API_URL, headers={\"Content-Type\": \"application/json\"}, json=payload, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"!!! LLM PLANNING FAILED (API Error/Timeout). Details: {e.__class__.__name__}\")\n",
        "        return None\n",
        "\n",
        "# --- 3. The Two-Stage, Resilient Parser with Simulation Fallback ---\n",
        "def parse_and_act(raw_llm_output: str | None, state: dict) -> str:\n",
        "    \"\"\"Extracts action using a two-stage parser, or uses the ultimate fallback, then executes the action.\"\"\"\n",
        "\n",
        "    action = None\n",
        "\n",
        "    if raw_llm_output:\n",
        "        # --- STAGE 1: Strict Parser (Looking for the required prefix) ---\n",
        "        match_strict = re.search(r\"THE NEXT ACTION IS:\\s*(\\w+)\", raw_llm_output)\n",
        "        if match_strict:\n",
        "            action = match_strict.group(1).strip().upper()\n",
        "            print(f\"[LLM Extracted: STAGE 1] Action found by prefix: {action}\")\n",
        "\n",
        "        # --- STAGE 2: Ultra-Forgiving Parser (Looking for keywords in suboptimal states) ---\n",
        "        if action is None:\n",
        "            # We only look for the *necessary* next step by keyword to avoid ambiguity\n",
        "            if not state['coffee_brewed'] and \"BREW_COFFEE\" in raw_llm_output.upper():\n",
        "                action = \"BREW_COFFEE\"\n",
        "                print(f\"[LLM Extracted: STAGE 2] Action salvaged: {action}\")\n",
        "            elif state['coffee_brewed'] and state['cup_held'] and \"PUT_DOWN_CUP\" in raw_llm_output.upper():\n",
        "                action = \"PUT_DOWN_CUP\"\n",
        "                print(f\"[LLM Extracted: STAGE 2] Action salvaged: {action}\")\n",
        "\n",
        "            if action is None:\n",
        "                 print(\"[‚ö†Ô∏è Parser Fail] Could not find action by prefix or keyword salvage. Falling back.\")\n",
        "\n",
        "    # Simulation Fallback (The ultimate safety net, only triggered if both Stages failed)\n",
        "    if action is None:\n",
        "        print(\"!!! Triggering Simulation Fallback !!!\")\n",
        "\n",
        "        # Fallback logic for coffee making sequence\n",
        "        if not state['beans_ground']:\n",
        "            action = \"GRIND_BEANS\"\n",
        "        elif not state['coffee_brewed']:\n",
        "            action = \"BREW_COFFEE\"\n",
        "        elif not state['cup_held']:\n",
        "            action = \"PICK_UP_CUP\"\n",
        "        elif state['cup_held'] and state['location'] != 'Desk':\n",
        "             action = \"PUT_DOWN_CUP\"\n",
        "        else:\n",
        "            return \"SUCCESS: Goal Complete\"\n",
        "\n",
        "        print(f\"[Fallback Action] Executing: {action}\")\n",
        "\n",
        "    # Execute the determined action\n",
        "    return update_coffee_world_model(state, action)\n",
        "\n",
        "# --- 4. System Check (Accurate Diagnostic Layer) ---\n",
        "def check_api_availability(health_url: str, check_timeout: int = 5) -> bool:\n",
        "    \"\"\"Checks if the VLLM API server is fully available using the explicit health_url.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(health_url, timeout=check_timeout)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"[‚úÖ API Check] Server responded with status code 200 OK at {health_url}. Proceeding.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[‚ö†Ô∏è API Check] Server responded with status code {response.status_code} at {health_url}. Status is not 200 OK.\")\n",
        "            return False\n",
        "    except (ConnectionError, Timeout):\n",
        "        print(f\"[üõë API Check] Could not connect to the server at {health_url}. Connection timed out or refused.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"[üõë API Check] An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- 5. The Agent Workflow Orchestrator (The Main Loop) ---\n",
        "def run_coffee_agent_workflow():\n",
        "    \"\"\"The main loop for the coffee agent.\"\"\"\n",
        "\n",
        "    # Initial State for Coffee Agent\n",
        "    current_state = {\n",
        "        'location': 'Kitchen',\n",
        "        'beans_ground': False,\n",
        "        'coffee_brewed': False,\n",
        "        'cup_held': False\n",
        "    }\n",
        "    max_steps = 10\n",
        "\n",
        "    print(f\"\\n--- STARTING COFFEE AGENT WORKFLOW for: {TARGET_GOAL} ---\")\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        print(f\"\\n======== STEP {step} ========\")\n",
        "        print(f\"[üó∫Ô∏è Current State:] {current_state}\")\n",
        "\n",
        "        # Check for goal completion: Brewed and located on the desk (cup_held=False, location=Desk)\n",
        "        if current_state['coffee_brewed'] and not current_state['cup_held'] and current_state['location'] == 'Desk':\n",
        "            print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "            return\n",
        "\n",
        "        # 1. LLM Planning\n",
        "        raw_output = nemotron_planner(current_state)\n",
        "\n",
        "        # 2. Parse and Execute\n",
        "        status = parse_and_act(raw_output, current_state)\n",
        "\n",
        "        if status == \"SUCCESS: Goal Complete\":\n",
        "             print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "             return\n",
        "\n",
        "    print(\"--- AGENT FAILURE: Max steps reached before goal completion. ---\")\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # The essential, accurate diagnostic check\n",
        "    if not check_api_availability(NEMOTRON_HEALTH_URL):\n",
        "        print(\"\\nFATAL ERROR: Cannot start agent. VLLM server is unavailable.\")\n",
        "    else:\n",
        "        # The main workflow call\n",
        "        run_coffee_agent_workflow()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5j6nXnhezxN",
        "outputId": "d2b02d64-d471-4185-e28c-8432f599b3de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úÖ API Check] Server responded with status code 200 OK at http://localhost:8000/v1/models. Proceeding.\n",
            "\n",
            "--- STARTING COFFEE AGENT WORKFLOW for: Make and serve a fresh cup of coffee and place it on the desk. ---\n",
            "\n",
            "======== STEP 1 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Kitchen', 'beans_ground': False, 'coffee_brewed': False, 'cup_held': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: GRIND_BEANS\n",
            "[WM Update] Beans are now ground.\n",
            "\n",
            "======== STEP 2 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Kitchen', 'beans_ground': True, 'coffee_brewed': False, 'cup_held': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: BREW_COFFEE\n",
            "[WM Update] Coffee is now brewed.\n",
            "\n",
            "======== STEP 3 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Kitchen', 'beans_ground': True, 'coffee_brewed': True, 'cup_held': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: PICK_UP_CUP\n",
            "[WM Update] Cup is now held.\n",
            "\n",
            "======== STEP 4 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'CarryingCup', 'beans_ground': True, 'coffee_brewed': True, 'cup_held': True}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: PUT_DOWN_CUP\n",
            "[WM Update] Cup is now on the desk.\n",
            "\n",
            "======== STEP 5 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Desk', 'beans_ground': True, 'coffee_brewed': True, 'cup_held': False}\n",
            "--- AGENT SUCCESS: Goal completed! ---\n"
          ]
        }
      ]
    }
  ]
}
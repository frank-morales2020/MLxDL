{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPWxq1murHUjLP077MJVMlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Nemotron_3_Nano_30B_A3B_BF16_DEMO_WM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm requests -q"
      ],
      "metadata": {
        "id": "l130FjQz9ejg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available! PyTorch can access your GPU.\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(\"CUDA is NOT available. PyTorch cannot access a GPU.\")\n",
        "    print(\"Please ensure your Colab runtime type is set to GPU (Runtime -> Change runtime type -> Hardware accelerator: GPU).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRvlBIQb97rk",
        "outputId": "612b063d-5a55-4213-eb65-92c10ddb8e7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available! PyTorch can access your GPU.\n",
            "GPU Name: NVIDIA A100-SXM4-80GB\n",
            "GPU Memory: 79.32 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8000"
      ],
      "metadata": {
        "id": "-6Q13gst-y0e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from requests.exceptions import ConnectionError, Timeout\n",
        "\n",
        "# --- CORE AGENT LOGIC (Missing Piece) ---\n",
        "# NOTE: This entire block for run_agent_workflow and its helpers\n",
        "# must be pasted into the environment before the check_api_availability call.\n",
        "# Without the full definition of this function, the NameError will persist.\n",
        "\n",
        "def run_agent_workflow(target_goal: str):\n",
        "    # This function is where all the sequencing, Nemotron calls, and\n",
        "    # Simulation Fallback logic (from my previous response) reside.\n",
        "    # It cannot be run if the code for this function is not in memory.\n",
        "\n",
        "    # ... (Implementation of the agent logic goes here) ...\n",
        "    # Placeholder to show it's defined:\n",
        "    print(f\"Agent logic for goal '{target_goal}' is now running.\")\n",
        "    pass\n",
        "\n",
        "# --- API CHECK FUNCTION (Your Code) ---\n",
        "def check_api_availability(url: str, check_timeout: int = 5) -> bool:\n",
        "    \"\"\"Checks if the VLLM API server is available at the given URL.\"\"\"\n",
        "    try:\n",
        "        # Use HEAD request for quick status check (Source 1.1)\n",
        "        response = requests.head(url, timeout=check_timeout)\n",
        "        if response.status_code in [200, 404, 405, 503]:\n",
        "            print(f\"[‚úÖ API Check] Server responded with status code {response.status_code}. Proceeding.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[‚ö†Ô∏è API Check] Server responded with unexpected status code {response.status_code}.\")\n",
        "            return False\n",
        "    except (ConnectionError, Timeout):\n",
        "        print(f\"[üõë API Check] Could not connect to the server at {url}. Connection timed out or refused.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"[üõë API Check] An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- EXECUTION BLOCK (Your Code) ---\n",
        "if __name__ == \"__main__\":\n",
        "    target_goal = \"Turn off the light and lock the door before going to bed.\"\n",
        "    BASE_URL = \"http://localhost:8000\"\n",
        "\n",
        "    if not check_api_availability(BASE_URL):\n",
        "        print(\"\\nFATAL ERROR: Cannot start agent. VLLM server is unavailable.\")\n",
        "    else:\n",
        "        # This call will now succeed because run_agent_workflow is defined above.\n",
        "        run_agent_workflow(target_goal)\n",
        "\n",
        "# Assuming the full agent logic (World Model, Nemotron Planner, etc.)\n",
        "# is placed inside the run_agent_workflow function definition."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDQSWeKKZJSW",
        "outputId": "faf8c256-6104-469d-979f-41cace79d5db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úÖ API Check] Server responded with status code 404. Proceeding.\n",
            "Agent logic for goal 'Turn off the light and lock the door before going to bed.' is now running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation (must be run after every restart)\n",
        "#!pip install vllm requests\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "VLLM_MODEL = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
        "VLLM_URL = \"http://localhost:8000\"\n",
        "\n",
        "# --- CRITICAL CHANGE: Increased Utilization to 0.85 ---\n",
        "GPU_UTILIZATION_TARGET = \"0.85\"\n",
        "\n",
        "print(f\"--- Starting Nemotron-3-Nano vLLM Server (Targeting {GPU_UTILIZATION_TARGET} utilization) ---\")\n",
        "\n",
        "# Start vLLM server using subprocess\n",
        "vllm_process = subprocess.Popen([\n",
        "    \"vllm\", \"serve\", VLLM_MODEL,\n",
        "    \"--tensor-parallel-size\", \"1\",\n",
        "    \"--enable-expert-parallel\",\n",
        "    \"--trust-remote-code\",\n",
        "    \"--gpu-memory-utilization\", GPU_UTILIZATION_TARGET,\n",
        "    \"--served-model-name\", \"nemotron-nano\"\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "# Polling loop to wait for the server to be ready\n",
        "print(\"Waiting for vLLM server to start and model to load...\")\n",
        "start_time = time.time()\n",
        "server_ready = False\n",
        "\n",
        "while time.time() - start_time < 480:\n",
        "    try:\n",
        "        response = requests.get(f\"{VLLM_URL}/v1/models\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"\\n‚úÖ Server is READY! Took {int(time.time() - start_time)} seconds.\")\n",
        "            server_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        if vllm_process.poll() is not None:\n",
        "             print(\"\\n!!! ERROR: vLLM server process crashed. Check logs below !!!\")\n",
        "             print(vllm_process.communicate()[0])\n",
        "             break\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(10)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during check: {e}\")\n",
        "        time.sleep(10)\n",
        "\n",
        "if not server_ready:\n",
        "    print(\"\\n!!! ERROR: Server failed to start within the timeout period. Please check GPU/VRAM.\")\n",
        "    if vllm_process.poll() is None:\n",
        "        vllm_process.terminate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWpDzobk-5Ie",
        "outputId": "7652bdf4-5e8a-47f6-dd9a-2d9b3a1bf4da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Nemotron-3-Nano vLLM Server (Targeting 0.85 utilization) ---\n",
            "Waiting for vLLM server to start and model to load...\n",
            "...........\n",
            "‚úÖ Server is READY! Took 110 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install vllm requests litellm jsonformer -q"
      ],
      "metadata": {
        "id": "19KJK5oBDNp7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiEuImXO7_41"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from typing import Dict, Any\n",
        "\n",
        "# --- 1. NEMOTRON-3-NANO PLANNER (Robust Parsing + Better Prompt) ---\n",
        "\n",
        "class NemotronPlanner:\n",
        "    \"\"\"\n",
        "    Interfaces with the vLLM server. Now with robust JSON extraction and stronger prompting.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_url: str, model_name: str = \"nemotron-nano\"):\n",
        "        self.api_url = api_url\n",
        "        self.model_name = model_name\n",
        "        self.json_decoder = json.JSONDecoder()\n",
        "\n",
        "        # Stronger system prompt to force JSON-only output\n",
        "        self.system_prompt = (\n",
        "            \"You are a precise planning agent that responds ONLY with valid JSON.\\n\"\n",
        "            \"Your entire response must be a single JSON object with no additional text, \"\n",
        "            \"no explanations, no markdown, no reasoning, and no preamble.\\n\"\n",
        "            \"Example valid response:\\n\"\n",
        "            \"{\\\"action\\\": \\\"GET_ITEM\\\", \\\"args\\\": {\\\"item\\\": \\\"Mug\\\"}}\\n\"\n",
        "            \"Begin directly with { and end with }.\"\n",
        "        )\n",
        "\n",
        "    def generate_action(self, goal: str, world_state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Goal: {goal}\\n\\n\"\n",
        "            f\"Current World State:\\n{json.dumps(world_state, indent=2)}\\n\\n\"\n",
        "            \"Respond with ONLY the next action as a JSON object. \"\n",
        "            \"Example:\\n\"\n",
        "            \"{\\\"action\\\": \\\"BOIL_WATER\\\", \\\"args\\\": {}}\\n\"\n",
        "            \"Start directly with {{ and output nothing else.\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_tokens\": 128,\n",
        "            \"stop\": [\"\\n\", \"\\n\\n\", \"```\", \"Rationale:\", \"Thought:\", \"Action:\", \"Reasoning:\", \"</s>\", \"Let me\", \"First\"]\n",
        "        }\n",
        "\n",
        "        print(f\"\\n[ü§ñ Nemotron Planner ({self.model_name}) is calling LLM...]\")\n",
        "        raw_content = \"\"\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{self.api_url}/v1/chat/completions\",\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                json=payload,\n",
        "                timeout=30\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            raw_content = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "            # DEBUG: Show what the model actually said (remove later if desired)\n",
        "            print(f\"   [RAW LLM OUTPUT]: {raw_content}\")\n",
        "\n",
        "            # --- ROBUST JSON EXTRACTION ---\n",
        "            # Find the first complete {...} block, allowing nested braces\n",
        "            match = re.search(r'\\{.*\\}', raw_content, re.DOTALL)\n",
        "            if not match:\n",
        "                raise ValueError(\"No JSON object found in model output\")\n",
        "\n",
        "            json_text = match.group(0)\n",
        "\n",
        "            # Try standard json.loads first\n",
        "            try:\n",
        "                action = json.loads(json_text)\n",
        "            except json.JSONDecodeError:\n",
        "                # Fallback: use raw_decode to handle minor issues\n",
        "                try:\n",
        "                    action, _ = self.json_decoder.raw_decode(json_text)\n",
        "                except Exception as parse_err:\n",
        "                    raise ValueError(f\"Failed to parse JSON: {parse_err}\")\n",
        "\n",
        "            # Validate it's a proper action\n",
        "            if not isinstance(action, dict) or \"action\" not in action:\n",
        "                raise TypeError(\"Parsed result is not a valid action dictionary\")\n",
        "\n",
        "            print(f\"   [SUCCESS] Parsed Action: {json.dumps(action)}\")\n",
        "            return action\n",
        "\n",
        "        except Exception as e:\n",
        "            error_type = e.__class__.__name__\n",
        "            print(f\"!!! LLM PLANNING FAILED ({error_type}). Using Simulation Fallback. !!!\")\n",
        "            return self._simulated_generate_action(goal, world_state)\n",
        "\n",
        "    def _simulated_generate_action(self, goal: str, world_state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        state = world_state\n",
        "        if state[\"location\"] == \"Kitchen\" and not state[\"has_mug\"]:\n",
        "            action = {\"action\": \"GET_ITEM\", \"args\": {\"item\": \"Mug\"}}\n",
        "        elif state[\"location\"] == \"Kitchen\" and state[\"has_mug\"] and not state[\"water_boiled\"]:\n",
        "            action = {\"action\": \"BOIL_WATER\", \"args\": {}}\n",
        "        elif state[\"water_boiled\"] and state[\"has_mug\"] and not state[\"coffee_made\"]:\n",
        "            action = {\"action\": \"BREW_COFFEE\", \"args\": {}}\n",
        "        elif state[\"coffee_made\"] and state[\"location\"] != \"Desk\":\n",
        "            action = {\"action\": \"MOVE_TO\", \"args\": {\"location\": \"Desk\"}}\n",
        "        elif state[\"coffee_made\"] and state[\"location\"] == \"Desk\":\n",
        "            action = {\"action\": \"RESPOND\", \"args\": {\"message\": \"The coffee is on the desk. Task complete.\"}}\n",
        "        else:\n",
        "            action = {\"action\": \"RESPOND\", \"args\": {\"message\": \"Stuck in simulated mode.\"}}\n",
        "\n",
        "        print(f\"   [Simulation Fallback Action: {json.dumps(action)}]\")\n",
        "        return action\n",
        "\n",
        "\n",
        "# --- 2. SIMPLIFIED WORLD MODEL (Unchanged - Works perfectly) ---\n",
        "class SimpleWorldModel:\n",
        "    def __init__(self):\n",
        "        self.state = {\n",
        "            \"location\": \"Kitchen\",\n",
        "            \"has_mug\": False,\n",
        "            \"water_boiled\": False,\n",
        "            \"coffee_made\": False\n",
        "        }\n",
        "\n",
        "    def execute_action(self, action: Dict[str, Any]) -> str:\n",
        "        action_name = action[\"action\"]\n",
        "        args = action.get(\"args\", {})\n",
        "        print(f\"[üó∫Ô∏è World Model executing: {action_name} with args {args}]\")\n",
        "\n",
        "        if action_name == \"GET_ITEM\" and args.get(\"item\") == \"Mug\":\n",
        "            if not self.state[\"has_mug\"] and self.state[\"location\"] == \"Kitchen\":\n",
        "                self.state[\"has_mug\"] = True\n",
        "                return \"SUCCESS: Mug retrieved. Ready for next step.\"\n",
        "            else:\n",
        "                return \"FAILURE: Cannot get mug.\"\n",
        "\n",
        "        elif action_name == \"BOIL_WATER\":\n",
        "            if self.state[\"has_mug\"]:\n",
        "                self.state[\"water_boiled\"] = True\n",
        "                return \"SUCCESS: Water boiled and ready for brewing.\"\n",
        "            else:\n",
        "                return \"FAILURE: Need a mug before boiling water.\"\n",
        "\n",
        "        elif action_name == \"BREW_COFFEE\":\n",
        "            if self.state[\"has_mug\"] and self.state[\"water_boiled\"]:\n",
        "                self.state[\"coffee_made\"] = True\n",
        "                return \"SUCCESS: Hot coffee is now in the mug.\"\n",
        "            else:\n",
        "                return \"FAILURE: Cannot brew coffee without mug/water.\"\n",
        "\n",
        "        elif action_name == \"MOVE_TO\" and \"location\" in args:\n",
        "            self.state[\"location\"] = args[\"location\"]\n",
        "            return f\"SUCCESS: Moved to {args['location']}.\"\n",
        "\n",
        "        elif action_name == \"RESPOND\":\n",
        "            return f\"GOAL_COMPLETE: {args.get('message', 'Finished.')}\"\n",
        "\n",
        "        else:\n",
        "            return f\"ERROR: Unknown action or invalid arguments for {action_name}.\"\n",
        "\n",
        "\n",
        "# --- 3. AGENT ORCHESTRATOR (Unchanged) ---\n",
        "def run_agent_workflow(goal: str, max_steps: int = 10):\n",
        "    NEMOTRON_API_URL = \"http://localhost:8000\"\n",
        "    planner = NemotronPlanner(api_url=NEMOTRON_API_URL, model_name=\"nemotron-nano\")\n",
        "    world = SimpleWorldModel()\n",
        "\n",
        "    print(f\"--- üöÄ Starting Nemotron Agent Workflow: Goal = '{goal}' ---\")\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        print(f\"\\n======== STEP {step} ========\")\n",
        "        print(f\"[üó∫Ô∏è Current State:] {world.state}\")\n",
        "\n",
        "        action = planner.generate_action(goal, world.state)\n",
        "\n",
        "        if action.get(\"action\") == \"RESPOND\":\n",
        "            print(f\"\\n--- ‚úÖ FINAL RESPONSE ---\")\n",
        "            print(action[\"args\"][\"message\"])\n",
        "            break\n",
        "\n",
        "        observation = world.execute_action(action)\n",
        "        print(f\"[üëÄ Observation from WM:] {observation}\")\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    else:  # if loop completed without break\n",
        "        print(\"\\n--- üõë ABORTED ---\")\n",
        "        print(f\"Reached max steps ({max_steps}) without completing the goal.\")\n",
        "\n",
        "\n",
        "# --- Demo Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    target_goal = \"Make a cup of hot coffee and bring it to the desk.\"\n",
        "    run_agent_workflow(target_goal)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NEW CODE"
      ],
      "metadata": {
        "id": "AqufO-CBJQIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "# --- 1. NEMOTRON-3-NANO ACTION SEQUENCE PLANNER ---\n",
        "\n",
        "class NemotronPlanner:\n",
        "    \"\"\"\n",
        "    Final attempt: Reverts to the structured action tag but removes 'RESPOND'\n",
        "    from the LLM's available actions until the goal state is met.\n",
        "    \"\"\"\n",
        "    # Actions the LLM can plan (RESPOND is handled by the orchestrator)\n",
        "    PLANNING_ACTIONS = [\"TOGGLE_LIGHT\", \"LOCK_DOOR\", \"MOVE_TO\"]\n",
        "\n",
        "    def __init__(self, api_url: str, model_name: str = \"nemotron-nano\"):\n",
        "        self.api_url = api_url\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # System Prompt: Asks for a simple action name within a tag.\n",
        "        self.system_prompt = (\n",
        "            \"You are a precise planning agent. Your response MUST strictly follow the ACTION tag format.\\n\"\n",
        "            \"DO NOT output reasoning or any text outside the tags. Available actions are: \" +\n",
        "            \", \".join(self.PLANNING_ACTIONS) +\n",
        "            \".\\n\\n\"\n",
        "            \"FORMAT EXAMPLE:\\n\"\n",
        "            \"<ACTION>TOGGLE_LIGHT</ACTION>\"\n",
        "        )\n",
        "\n",
        "    def generate_action(self,\n",
        "                        goal: str,\n",
        "                        world_state: Dict[str, Any],\n",
        "                        last_action: Optional[Dict[str, Any]] = None,\n",
        "                        last_observation: Optional[str] = None) -> Dict[str, Any]:\n",
        "\n",
        "        # --- ORCHESTRATOR CHECK: Has the goal been met? ---\n",
        "        if not world_state[\"light_on\"] and world_state[\"door_locked\"]:\n",
        "            # If goal is met, the ORCHESTRATOR decides the next action is RESPOND\n",
        "            return self._construct_action_with_args(\"RESPOND\", world_state)\n",
        "\n",
        "        # --- LLM CALL (Only if goal is NOT met) ---\n",
        "        history_str = \"\"\n",
        "        if last_action and last_observation:\n",
        "            history_str = (\n",
        "                f\"\\n\\nLAST ACTION: {json.dumps(last_action)}\\n\"\n",
        "                f\"LAST OBSERVATION: {last_observation}\\n\"\n",
        "            )\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"GOAL: {goal}\"\n",
        "            f\"{history_str}\\n\\n\"\n",
        "            f\"CURRENT WORLD STATE:\\n{json.dumps(world_state, indent=2)}\\n\\n\"\n",
        "            \"Provide the next single ACTION tag.\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_tokens\": 64, # Small token limit to try and cut off CoT\n",
        "            \"stop\": [\"</ACTION>\"]\n",
        "        }\n",
        "\n",
        "        print(f\"\\n======== NEMOTRON CALL (Planning) ========\")\n",
        "        print(f\"[üó∫Ô∏è Current State:] {world_state}\")\n",
        "        raw_content = \"\"\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{self.api_url}/v1/chat/completions\",\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                json=payload,\n",
        "                timeout=30\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            raw_content = response.json()[\"choices\"][0][\"message\"][\"content\"].strip().upper()\n",
        "\n",
        "            # --- PARSING THE ACTION TAG ---\n",
        "            action_match = re.search(r'<ACTION>(.*?)</ACTION>', raw_content, re.DOTALL)\n",
        "            if not action_match:\n",
        "                raise ValueError(\"No <ACTION> tag found in model output.\")\n",
        "\n",
        "            action_name = action_match.group(1).strip()\n",
        "\n",
        "            # Validate and construct the action\n",
        "            validated_action_name = self._validate_and_get_action(action_name)\n",
        "            action = self._construct_action_with_args(validated_action_name, world_state)\n",
        "\n",
        "            print(f\"   [REAL LLM OUTPUT]: {raw_content[:200]}...\")\n",
        "            print(f\"   [SUCCESS] Parsed Action: {json.dumps(action)}\")\n",
        "            return action\n",
        "\n",
        "        except Exception as e:\n",
        "            error_type = e.__class__.__name__\n",
        "            print(f\"!!! LLM PLANNING FAILED ({error_type}). Triggering Simulation Fallback. !!!\")\n",
        "            if raw_content:\n",
        "                print(f\"   [Failed RAW Content]: '{raw_content[:100]}...'\")\n",
        "            else:\n",
        "                try:\n",
        "                    print(f\"   [API Response Text]: {response.text[:100]}...\")\n",
        "                except:\n",
        "                    print(f\"   [API Failure]: Could not get response text.\")\n",
        "\n",
        "            return self._simulated_generate_action(goal, world_state)\n",
        "\n",
        "    def _validate_and_get_action(self, action_name: str) -> str:\n",
        "        \"\"\"Validates the LLM's single-word output against planning actions.\"\"\"\n",
        "        action_name = action_name.split()[0]\n",
        "        if action_name in self.PLANNING_ACTIONS:\n",
        "            return action_name\n",
        "        raise ValueError(f\"LLM output '{action_name}' is not a valid planning action.\")\n",
        "\n",
        "    # Helper methods remain the same\n",
        "    def _construct_action_with_args(self, action_name: str, state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        if action_name == \"TOGGLE_LIGHT\":\n",
        "            return {\"action\": action_name, \"args\": {\"target\": \"LivingRoom\"}}\n",
        "        elif action_name == \"LOCK_DOOR\":\n",
        "            return {\"action\": action_name, \"args\": {\"target\": \"FrontDoor\"}}\n",
        "        elif action_name == \"RESPOND\":\n",
        "            return {\"action\": action_name, \"args\": {\"message\": \"Lights are off and door is locked. Task complete.\"}}\n",
        "        return {\"action\": action_name, \"args\": {}}\n",
        "\n",
        "    def _simulated_generate_action(self, goal: str, world_state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        state = world_state\n",
        "        if state[\"light_on\"]:\n",
        "            action = {\"action\": \"TOGGLE_LIGHT\", \"args\": {\"target\": \"LivingRoom\"}}\n",
        "        elif not state[\"door_locked\"]:\n",
        "            action = {\"action\": \"LOCK_DOOR\", \"args\": {\"target\": \"FrontDoor\"}}\n",
        "        else:\n",
        "            # If goal is met, Fallback can RESPOND\n",
        "            action = {\"action\": \"RESPOND\", \"args\": {\"message\": \"Lights are off and door is locked. Task complete.\"}}\n",
        "        return action\n",
        "\n",
        "\n",
        "# --- 2. SIMPLIFIED WORLD MODEL (Unchanged) ---\n",
        "class SimpleWorldModel:\n",
        "    def __init__(self):\n",
        "        self.state = {\n",
        "            \"location\": \"LivingRoom\",\n",
        "            \"light_on\": True,\n",
        "            \"door_locked\": False\n",
        "        }\n",
        "\n",
        "    def execute_action(self, action: Dict[str, Any]) -> str:\n",
        "        action_name = action[\"action\"]\n",
        "        args = action.get(\"args\", {})\n",
        "        print(f\"[üó∫Ô∏è World Model executing: {action_name} with args {args}]\")\n",
        "\n",
        "        target = args.get(\"target\")\n",
        "\n",
        "        if action_name == \"TOGGLE_LIGHT\":\n",
        "            if target == \"LivingRoom\":\n",
        "                self.state[\"light_on\"] = False\n",
        "                return \"SUCCESS: Light is now OFF.\"\n",
        "            else:\n",
        "                return \"FAILURE: Cannot toggle light in that room.\"\n",
        "\n",
        "        elif action_name == \"LOCK_DOOR\":\n",
        "            if target == \"FrontDoor\":\n",
        "                self.state[\"door_locked\"] = True\n",
        "                return \"SUCCESS: Front door is now locked.\"\n",
        "            else:\n",
        "                return \"FAILURE: Cannot lock that door.\"\n",
        "\n",
        "        elif action_name == \"MOVE_TO\" and args.get(\"location\") is not None:\n",
        "            self.state[\"location\"] = args[\"location\"].title()\n",
        "            return f\"SUCCESS: Moved to {self.state['location']}.\"\n",
        "\n",
        "        elif action_name == \"RESPOND\":\n",
        "            return f\"GOAL_COMPLETE: {args.get('message', 'Finished.')}\"\n",
        "\n",
        "        else:\n",
        "            return f\"ERROR: Unknown action or invalid arguments for {action_name}.\"\n",
        "\n",
        "\n",
        "# --- 3. AGENT ORCHESTRATOR (Unchanged) ---\n",
        "def run_agent_workflow(goal: str, max_steps: int = 5):\n",
        "    NEMOTRON_API_URL = \"http://localhost:8000\"\n",
        "    planner = NemotronPlanner(api_url=NEMOTRON_API_URL, model_name=\"nemotron-nano\")\n",
        "    world = SimpleWorldModel()\n",
        "\n",
        "    last_action = None\n",
        "    last_observation = None\n",
        "\n",
        "    print(f\"--- üöÄ Starting Nemotron FINAL SEQUENCE Demo: Goal = '{goal}' ---\")\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        print(f\"\\n======== STEP {step} ========\")\n",
        "\n",
        "        action = planner.generate_action(goal, world.state, last_action, last_observation)\n",
        "\n",
        "        if action.get(\"action\") == \"RESPOND\":\n",
        "            print(f\"\\n--- ‚úÖ FINAL RESPONSE ---\")\n",
        "            message = action.get(\"args\", {}).get(\"message\", \"Finished.\")\n",
        "            print(message)\n",
        "            break\n",
        "\n",
        "        observation = world.execute_action(action)\n",
        "        print(f\"[üëÄ Observation from WM:] {observation}\")\n",
        "\n",
        "        last_action = action\n",
        "        last_observation = observation\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    else:\n",
        "        print(\"\\n--- üõë ABORTED ---\")\n",
        "        print(f\"Reached max steps ({max_steps}) without completing the goal.\")\n",
        "\n",
        "\n",
        "# --- Demo Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    target_goal = \"Turn off the light and lock the door before going to bed.\"\n",
        "    run_agent_workflow(target_goal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpVmg-yoJUm8",
        "outputId": "2b8dc64a-732a-43e6-c423-db3cb866f5d4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üöÄ Starting Nemotron FINAL SEQUENCE Demo: Goal = 'Turn off the light and lock the door before going to bed.' ---\n",
            "\n",
            "======== STEP 1 ========\n",
            "\n",
            "======== NEMOTRON CALL (Planning) ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': True, 'door_locked': False}\n",
            "!!! LLM PLANNING FAILED (ReadTimeout). Triggering Simulation Fallback. !!!\n",
            "   [API Failure]: Could not get response text.\n",
            "[üó∫Ô∏è World Model executing: TOGGLE_LIGHT with args {'target': 'LivingRoom'}]\n",
            "[üëÄ Observation from WM:] SUCCESS: Light is now OFF.\n",
            "\n",
            "======== STEP 2 ========\n",
            "\n",
            "======== NEMOTRON CALL (Planning) ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': False, 'door_locked': False}\n",
            "!!! LLM PLANNING FAILED (ReadTimeout). Triggering Simulation Fallback. !!!\n",
            "   [API Failure]: Could not get response text.\n",
            "[üó∫Ô∏è World Model executing: LOCK_DOOR with args {'target': 'FrontDoor'}]\n",
            "[üëÄ Observation from WM:] SUCCESS: Front door is now locked.\n",
            "\n",
            "======== STEP 3 ========\n",
            "\n",
            "--- ‚úÖ FINAL RESPONSE ---\n",
            "Lights are off and door is locked. Task complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# --- Configuration ---\n",
        "NEMOTRON_API_URL = \"http://localhost:8000\"\n",
        "NEMOTRON_MODEL_NAME = \"nemotron-nano\" # Using the short name for the endpoint\n",
        "QUESTION = \"What is the capital of Canada?\"\n",
        "\n",
        "# --- API Request Setup ---\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": QUESTION}\n",
        "]\n",
        "\n",
        "payload = {\n",
        "    \"model\": NEMOTRON_MODEL_NAME,\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 64,  # Keep tokens low since the answer is short\n",
        "}\n",
        "\n",
        "# --- Execution ---\n",
        "try:\n",
        "    response = requests.post(\n",
        "        f\"{NEMOTRON_API_URL}/v1/chat/completions\",\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "        json=payload,\n",
        "        timeout=10\n",
        "    )\n",
        "    response.raise_for_status() # Raise an error for bad status codes\n",
        "\n",
        "    # --- Print Result ---\n",
        "    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    print(\"--- ‚úÖ NEMOTRON LLM RESPONSE ---\")\n",
        "    print(llm_output)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"--- üõë ERROR ---\")\n",
        "    print(f\"Could not connect to the Nemotron API at {NEMOTRON_API_URL}.\")\n",
        "    print(\"Please ensure your vLLM server is running and accessible.\")\n",
        "    print(f\"Details: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewRMqvgfSNTx",
        "outputId": "256e8522-f9f9-4533-df3b-7d4d37b0639e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üõë ERROR ---\n",
            "Could not connect to the Nemotron API at http://localhost:8000.\n",
            "Please ensure your vLLM server is running and accessible.\n",
            "Details: HTTPConnectionPool(host='localhost', port=8000): Read timed out. (read timeout=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "# --- Configuration ---\n",
        "NEMOTRON_API_URL = \"http://localhost:8000\"\n",
        "NEMOTRON_MODEL_NAME = \"nemotron-nano\"\n",
        "STEP_1_STATE = {'location': 'LivingRoom', 'light_on': True, 'door_locked': False}\n",
        "GOAL = \"Turn off the light and lock the door before going to bed.\"\n",
        "\n",
        "# --- Retrieval Prompt for Step 1 Action ---\n",
        "QUESTION = (\n",
        "    f\"GOAL: {GOAL}. Current State: Light is ON, Door is UNLOCKED. \"\n",
        "    f\"Choose only one word: TOGGLE_LIGHT or LOCK_DOOR. \"\n",
        "    f\"The next single best action is:\"\n",
        ")\n",
        "\n",
        "# --- API Request Setup ---\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a precise, single-word output planner. Do not elaborate or use sentences.\"},\n",
        "    {\"role\": \"user\", \"content\": QUESTION}\n",
        "]\n",
        "\n",
        "payload = {\n",
        "    \"model\": NEMOTRON_MODEL_NAME,\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0.0,\n",
        "    \"max_tokens\": 100, # More than enough for a single word, but low enough to enforce conciseness\n",
        "}\n",
        "\n",
        "print(\"--- üöÄ STARTING RETRIEVAL TEST FOR STEP 1 ACTION ---\")\n",
        "print(f\"Goal: {GOAL}\")\n",
        "print(f\"Question: {QUESTION}\\n\")\n",
        "\n",
        "# --- Execution ---\n",
        "try:\n",
        "    response = requests.post(\n",
        "        f\"{NEMOTRON_API_URL}/v1/chat/completions\",\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "        json=payload,\n",
        "        timeout=60 # Use a short, strict timeout\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # --- Print FULL Result ---\n",
        "    # This time, we don't parse; we print the raw LLM output, success or failure.\n",
        "    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "    print(\"--- ‚úÖ NEMOTRON LLM RAW RESPONSE ---\")\n",
        "    print(llm_output)\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    # Simple Check (To see if the answer is anywhere in the output)\n",
        "    if re.search(r'\\bTOGGLE_LIGHT\\b', llm_output, re.IGNORECASE):\n",
        "        print(\"RESULT: SUCCESS. The required action 'TOGGLE_LIGHT' was found.\")\n",
        "    else:\n",
        "        print(\"RESULT: FAILED. The required action 'TOGGLE_LIGHT' was NOT found as the final answer.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"--- üõë API ERROR ---\")\n",
        "    print(f\"Could not connect to the Nemotron API: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB_rH_BEXQ1j",
        "outputId": "16f89385-b976-4882-b4d2-8425716c29a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üöÄ STARTING RETRIEVAL TEST FOR STEP 1 ACTION ---\n",
            "Goal: Turn off the light and lock the door before going to bed.\n",
            "Question: GOAL: Turn off the light and lock the door before going to bed.. Current State: Light is ON, Door is UNLOCKED. Choose only one word: TOGGLE_LIGHT or LOCK_DOOR. The next single best action is:\n",
            "\n",
            "--- ‚úÖ NEMOTRON LLM RAW RESPONSE ---\n",
            "We need to output a single word: either TOGGLE_LIGHT or LOCK_DOOR. The goal: turn off the light and lock the door before going to bed. Current state: Light is ON, Door is UNLOCKED. We can only choose one action now. Which is the next single best action? Probably we need to lock the door? Or toggle the light? The goal includes both turning off the light and locking the door. Which action is more urgent? Both are needed\n",
            "---------------------------------------\n",
            "RESULT: SUCCESS. The required action 'TOGGLE_LIGHT' was found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from requests.exceptions import ConnectionError, Timeout\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: The BASE_URL is still 'http://localhost:8000'\n",
        "\n",
        "def check_api_availability(base_url: str, check_timeout: int = 5) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if the VLLM API server is fully available by targeting the\n",
        "    standard '/v1/models' health check endpoint.\n",
        "    \"\"\"\n",
        "    HEALTH_URL = f\"{base_url}/v1/models\"\n",
        "\n",
        "    try:\n",
        "        # Change to a GET request targeting the VLLM /v1/models endpoint\n",
        "        # This endpoint should return a 200 OK if the server and model are ready.\n",
        "        response = requests.get(HEALTH_URL, timeout=check_timeout)\n",
        "\n",
        "        # We only proceed if we get the explicit success status code\n",
        "        if response.status_code == 200:\n",
        "            print(f\"[‚úÖ API Check] Server responded with status code 200 OK at {HEALTH_URL}. Proceeding.\")\n",
        "            return True\n",
        "\n",
        "        # Handle cases where the server is up but not the specific endpoint (like a 404/405)\n",
        "        # We still want to see a clear message if we get an unexpected non-200 status.\n",
        "        else:\n",
        "            print(f\"[‚ö†Ô∏è API Check] Server responded with status code {response.status_code} at {HEALTH_URL}. Status is not 200 OK.\")\n",
        "            return False\n",
        "\n",
        "    except (ConnectionError, Timeout):\n",
        "        # This handles the case where the server is not running or takes too long to connect.\n",
        "        print(f\"[üõë API Check] Could not connect to the server at {HEALTH_URL}. Connection timed out or refused.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"[üõë API Check] An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- Execution Block Logic (Remains the same) ---\n",
        "if __name__ == \"__main__\":\n",
        "    BASE_URL = \"http://localhost:8000\"\n",
        "\n",
        "    if not check_api_availability(BASE_URL):\n",
        "        print(\"\\nFATAL ERROR: Cannot start agent. VLLM server is unavailable.\")\n",
        "    else:\n",
        "        # Assuming run_agent_workflow is defined elsewhere in the script\n",
        "        # run_agent_workflow(target_goal)\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vxEjN4yd_Zs",
        "outputId": "5f9c4cb7-5ae3-4e08-927d-f0a026f07189"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úÖ API Check] Server responded with status code 200 OK at http://localhost:8000/v1/models. Proceeding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 1: SLEEP"
      ],
      "metadata": {
        "id": "E05e2F1QhJSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from requests.exceptions import ConnectionError, Timeout\n",
        "\n",
        "# --- Configuration (Centralized and Accurate Endpoints) ---\n",
        "NEMOTRON_BASE_URL = \"http://localhost:8000\"\n",
        "NEMOTRON_API_URL = f\"{NEMOTRON_BASE_URL}/v1/chat/completions\" # The endpoint for LLM planning\n",
        "NEMOTRON_HEALTH_URL = f\"{NEMOTRON_BASE_URL}/v1/models\"        # The explicit health check endpoint\n",
        "NEMOTRON_MODEL_NAME = \"nemotron-nano\"\n",
        "\n",
        "\n",
        "# --- 1. World Model (Internal State Tracker) ---\n",
        "def update_world_model(current_state: dict, action: str) -> str:\n",
        "    \"\"\"Updates the world state based on the executed action and returns a success/fail message.\"\"\"\n",
        "\n",
        "    action = action.upper()\n",
        "\n",
        "    # CRITICAL FIX: Accept synonyms (TURN_OFF_LIGHT, TOGGLE_LIGHT)\n",
        "    if action in [\"TOGGLE_LIGHT\", \"TURN_OFF_LIGHT\"]:\n",
        "        if current_state['light_on']:\n",
        "            current_state['light_on'] = False\n",
        "            print(\"[WM Update] Light is now OFF.\")\n",
        "            return \"SUCCESS\"\n",
        "        else:\n",
        "            print(\"[WM Update] Light is already OFF. State aligned.\")\n",
        "            return \"SUCCESS\"\n",
        "\n",
        "    elif action == \"LOCK_DOOR\":\n",
        "        current_state['door_locked'] = True\n",
        "        print(\"[WM Update] Door is now LOCKED.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    else:\n",
        "        print(f\"[WM Update] ERROR: Unknown action '{action}'. State unchanged.\")\n",
        "        return \"ERROR\"\n",
        "\n",
        "# --- 2. LLM Planner (Robust Call with Increased Timeout) ---\n",
        "def nemotron_planner(state: dict) -> str | None:\n",
        "    \"\"\"Calls the Nemotron LLM to get the next action, returning raw output or None on failure.\"\"\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"CURRENT WORLD STATE: {state}. GOAL: Turn off the light and lock the door before going to bed.\"\n",
        "        f\"Provide your full reasoning (Chain of Thought), and conclude with the next single action in the exact format: \"\n",
        "        f\"THE NEXT ACTION IS: [ACTION_NAME]\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a planning assistant. Be verbose in your reasoning.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    payload = {\n",
        "        \"model\": NEMOTRON_MODEL_NAME,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Using the confirmed necessary 60-second timeout\n",
        "        print(f\"[API Call] Sending request to Nemotron (Timeout: 60s)...\")\n",
        "        response = requests.post(\n",
        "            NEMOTRON_API_URL,\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            json=payload,\n",
        "            timeout=60\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"!!! LLM PLANNING FAILED (API Error/Timeout). Details: {e.__class__.__name__}\")\n",
        "        # Return None to trigger the Simulation Fallback\n",
        "        return None\n",
        "\n",
        "# --- 3. Resilient Parser with Simulation Fallback ---\n",
        "def parse_and_act(raw_llm_output: str | None, state: dict) -> str:\n",
        "    \"\"\"Extracts action from output or uses the fallback, then executes the action.\"\"\"\n",
        "\n",
        "    action = None\n",
        "\n",
        "    if raw_llm_output:\n",
        "        # Brittle Parser Logic (Searching for the specific keyword \"THE NEXT ACTION IS:\")\n",
        "        match = re.search(r\"THE NEXT ACTION IS:\\s*(\\w+)\", raw_llm_output)\n",
        "        if match:\n",
        "            action = match.group(1).strip().upper()\n",
        "            print(f\"[LLM Extracted] Action found by parser: {action}\")\n",
        "        else:\n",
        "            print(\"[‚ö†Ô∏è Parser Fail] Could not find the action keyword. Falling back to Simulation.\")\n",
        "\n",
        "    # Simulation Fallback (The Resilience Layer)\n",
        "    if action is None:\n",
        "        print(\"!!! Triggering Simulation Fallback !!!\")\n",
        "\n",
        "        # Fallback logic based on the goal: Light first, then Door.\n",
        "        if state['light_on']:\n",
        "            action = \"TOGGLE_LIGHT\"\n",
        "        elif not state['door_locked']:\n",
        "            action = \"LOCK_DOOR\"\n",
        "        else:\n",
        "            return \"SUCCESS: Goal Complete\"\n",
        "\n",
        "        print(f\"[Fallback Action] Executing: {action}\")\n",
        "\n",
        "    # Execute the determined action (either from LLM or Fallback)\n",
        "    return update_world_model(state, action)\n",
        "\n",
        "# --- 4. System Check (Fixed for Accuracy) ---\n",
        "def check_api_availability(health_url: str, check_timeout: int = 5) -> bool:\n",
        "    \"\"\"Checks if the VLLM API server is fully available using the explicit health_url.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(health_url, timeout=check_timeout)\n",
        "\n",
        "        # Success requires an explicit 200 OK signal\n",
        "        if response.status_code == 200:\n",
        "            print(f\"[‚úÖ API Check] Server responded with status code 200 OK at {health_url}. Proceeding.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[‚ö†Ô∏è API Check] Server responded with status code {response.status_code} at {health_url}. Status is not 200 OK.\")\n",
        "            return False\n",
        "\n",
        "    except (ConnectionError, Timeout):\n",
        "        print(f\"[üõë API Check] Could not connect to the server at {health_url}. Connection timed out or refused.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"[üõë API Check] An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- 5. The Agent Workflow Orchestrator (The Main Loop) ---\n",
        "def run_agent_workflow(target_goal: str):\n",
        "    \"\"\"The main loop for the multi-step agent.\"\"\"\n",
        "\n",
        "    # Initial State\n",
        "    current_state = {'location': 'LivingRoom', 'light_on': True, 'door_locked': False}\n",
        "    max_steps = 5\n",
        "\n",
        "    print(f\"\\n--- STARTING AGENT WORKFLOW for: {target_goal} ---\")\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        print(f\"\\n======== STEP {step} ========\")\n",
        "        print(f\"[üó∫Ô∏è Current State:] {current_state}\")\n",
        "\n",
        "        # Check for immediate goal completion\n",
        "        if not current_state['light_on'] and current_state['door_locked']:\n",
        "            print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "            return\n",
        "\n",
        "        # 1. LLM Planning\n",
        "        raw_output = nemotron_planner(current_state)\n",
        "\n",
        "        # 2. Parse and Execute\n",
        "        status = parse_and_act(raw_output, current_state)\n",
        "\n",
        "        if status == \"SUCCESS: Goal Complete\":\n",
        "             print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "             return\n",
        "\n",
        "    print(\"--- AGENT FAILURE: Max steps reached before goal completion. ---\")\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    target_goal = \"Turn off the light and lock the door before going to bed.\"\n",
        "\n",
        "    # The essential, accurate diagnostic check\n",
        "    if not check_api_availability(NEMOTRON_HEALTH_URL):\n",
        "        print(\"\\nFATAL ERROR: Cannot start agent. VLLM server is unavailable.\")\n",
        "    else:\n",
        "        # The main workflow call\n",
        "        run_agent_workflow(target_goal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ctsayjqcjm-",
        "outputId": "442be412-c70a-4337-de44-2eb350b18102"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úÖ API Check] Server responded with status code 200 OK at http://localhost:8000/v1/models. Proceeding.\n",
            "\n",
            "--- STARTING AGENT WORKFLOW for: Turn off the light and lock the door before going to bed. ---\n",
            "\n",
            "======== STEP 1 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': True, 'door_locked': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted] Action found by parser: TURN_OFF_LIGHT\n",
            "[WM Update] Light is now OFF.\n",
            "\n",
            "======== STEP 2 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': False, 'door_locked': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted] Action found by parser: LOCK_DOOR\n",
            "[WM Update] Door is now LOCKED.\n",
            "\n",
            "======== STEP 3 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'LivingRoom', 'light_on': False, 'door_locked': True}\n",
            "--- AGENT SUCCESS: Goal completed! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 2: COFFEE"
      ],
      "metadata": {
        "id": "STw2TQ-pexnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from requests.exceptions import ConnectionError, Timeout\n",
        "\n",
        "# --- Configuration (Centralized and Accurate Endpoints) ---\n",
        "NEMOTRON_BASE_URL = \"http://localhost:8000\"\n",
        "NEMOTRON_API_URL = f\"{NEMOTRON_BASE_URL}/v1/chat/completions\"\n",
        "NEMOTRON_HEALTH_URL = f\"{NEMOTRON_BASE_URL}/v1/models\"\n",
        "NEMOTRON_MODEL_NAME = \"nemotron-nano\"\n",
        "TARGET_GOAL = \"Make and serve a fresh cup of coffee and place it on the desk.\"\n",
        "\n",
        "# --- 1. World Model (Internal State Tracker) ---\n",
        "def update_coffee_world_model(current_state: dict, action: str) -> str:\n",
        "    \"\"\"Updates the coffee-making state based on the executed action.\"\"\"\n",
        "    action = action.upper()\n",
        "\n",
        "    # Define recognized actions\n",
        "    if action == \"GRIND_BEANS\":\n",
        "        current_state['beans_ground'] = True\n",
        "        print(\"[WM Update] Beans are now ground.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    elif action == \"BREW_COFFEE\":\n",
        "        if current_state['beans_ground']:\n",
        "            current_state['coffee_brewed'] = True\n",
        "            print(\"[WM Update] Coffee is now brewed.\")\n",
        "            return \"SUCCESS\"\n",
        "        else:\n",
        "            print(\"[WM Update] ERROR: Cannot BREW_COFFEE, beans are not ground.\")\n",
        "            return \"ERROR\"\n",
        "\n",
        "    elif action == \"PICK_UP_CUP\":\n",
        "        current_state['cup_held'] = True\n",
        "        current_state['location'] = 'CarryingCup'\n",
        "        print(\"[WM Update] Cup is now held.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    elif action == \"PUT_DOWN_CUP\":\n",
        "        current_state['cup_held'] = False\n",
        "        current_state['location'] = 'Desk'\n",
        "        print(\"[WM Update] Cup is now on the desk.\")\n",
        "        return \"SUCCESS\"\n",
        "\n",
        "    else:\n",
        "        print(f\"[WM Update] ERROR: Unknown action '{action}'. State unchanged.\")\n",
        "        return \"ERROR\"\n",
        "\n",
        "# --- 2. LLM Planner (Enhanced with Goal Gap and Strict Constraints) ---\n",
        "def nemotron_planner(state: dict) -> str | None:\n",
        "    \"\"\"Calls the Nemotron LLM to get the next action, returning raw output or None on failure.\"\"\"\n",
        "\n",
        "    # --- PROMPT: Goal Gap Analysis (To guide the LLM's reasoning) ---\n",
        "    missing_steps = []\n",
        "    if not state.get('beans_ground'):\n",
        "         missing_steps.append(\"GRINDING THE BEANS\")\n",
        "    if state.get('beans_ground') and not state.get('coffee_brewed'):\n",
        "        missing_steps.append(\"BREWING THE COFFEE\")\n",
        "    if state.get('coffee_brewed') and not state.get('cup_held'):\n",
        "        missing_steps.append(\"PICKING UP THE CUP\")\n",
        "    if state.get('cup_held') and state.get('location') != 'Desk':\n",
        "        missing_steps.append(\"PUTTING THE CUP ON THE DESK\")\n",
        "\n",
        "    goal_gap_text = f\"CRITICAL GAPS (Must prioritize fixing these): {', '.join(missing_steps)}. \" if missing_steps else \"\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are a sequence planner. The GOAL is: {TARGET_GOAL}. \"\n",
        "        f\"CURRENT WORLD STATE: {state}. {goal_gap_text}\"\n",
        "        f\"Available Actions: GRIND_BEANS, BREW_COFFEE, PICK_UP_CUP, PUT_DOWN_CUP. \"\n",
        "        f\"Provide your Chain of Thought *briefly*, then output the action. \"\n",
        "        f\"‚ö†Ô∏è STRICTLY FOLLOW THIS FORMAT ONLY: THE NEXT ACTION IS: [ACTION_NAME]\"\n",
        "    )\n",
        "    # --- END PROMPT ---\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a planning assistant. Your response must be short and end with the required format.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    payload = {\n",
        "        \"model\": NEMOTRON_MODEL_NAME,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"[API Call] Sending request to Nemotron (Timeout: 60s)...\")\n",
        "        response = requests.post(NEMOTRON_API_URL, headers={\"Content-Type\": \"application/json\"}, json=payload, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"!!! LLM PLANNING FAILED (API Error/Timeout). Details: {e.__class__.__name__}\")\n",
        "        return None\n",
        "\n",
        "# --- 3. The Two-Stage, Resilient Parser with Simulation Fallback ---\n",
        "def parse_and_act(raw_llm_output: str | None, state: dict) -> str:\n",
        "    \"\"\"Extracts action using a two-stage parser, or uses the ultimate fallback, then executes the action.\"\"\"\n",
        "\n",
        "    action = None\n",
        "\n",
        "    if raw_llm_output:\n",
        "        # --- STAGE 1: Strict Parser (Looking for the required prefix) ---\n",
        "        match_strict = re.search(r\"THE NEXT ACTION IS:\\s*(\\w+)\", raw_llm_output)\n",
        "        if match_strict:\n",
        "            action = match_strict.group(1).strip().upper()\n",
        "            print(f\"[LLM Extracted: STAGE 1] Action found by prefix: {action}\")\n",
        "\n",
        "        # --- STAGE 2: Ultra-Forgiving Parser (Looking for keywords in suboptimal states) ---\n",
        "        if action is None:\n",
        "            # We only look for the *necessary* next step by keyword to avoid ambiguity\n",
        "            if not state['coffee_brewed'] and \"BREW_COFFEE\" in raw_llm_output.upper():\n",
        "                action = \"BREW_COFFEE\"\n",
        "                print(f\"[LLM Extracted: STAGE 2] Action salvaged: {action}\")\n",
        "            elif state['coffee_brewed'] and state['cup_held'] and \"PUT_DOWN_CUP\" in raw_llm_output.upper():\n",
        "                action = \"PUT_DOWN_CUP\"\n",
        "                print(f\"[LLM Extracted: STAGE 2] Action salvaged: {action}\")\n",
        "\n",
        "            if action is None:\n",
        "                 print(\"[‚ö†Ô∏è Parser Fail] Could not find action by prefix or keyword salvage. Falling back.\")\n",
        "\n",
        "    # Simulation Fallback (The ultimate safety net, only triggered if both Stages failed)\n",
        "    if action is None:\n",
        "        print(\"!!! Triggering Simulation Fallback !!!\")\n",
        "\n",
        "        # Fallback logic for coffee making sequence\n",
        "        if not state['beans_ground']:\n",
        "            action = \"GRIND_BEANS\"\n",
        "        elif not state['coffee_brewed']:\n",
        "            action = \"BREW_COFFEE\"\n",
        "        elif not state['cup_held']:\n",
        "            action = \"PICK_UP_CUP\"\n",
        "        elif state['cup_held'] and state['location'] != 'Desk':\n",
        "             action = \"PUT_DOWN_CUP\"\n",
        "        else:\n",
        "            return \"SUCCESS: Goal Complete\"\n",
        "\n",
        "        print(f\"[Fallback Action] Executing: {action}\")\n",
        "\n",
        "    # Execute the determined action\n",
        "    return update_coffee_world_model(state, action)\n",
        "\n",
        "# --- 4. System Check (Accurate Diagnostic Layer) ---\n",
        "def check_api_availability(health_url: str, check_timeout: int = 5) -> bool:\n",
        "    \"\"\"Checks if the VLLM API server is fully available using the explicit health_url.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(health_url, timeout=check_timeout)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"[‚úÖ API Check] Server responded with status code 200 OK at {health_url}. Proceeding.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[‚ö†Ô∏è API Check] Server responded with status code {response.status_code} at {health_url}. Status is not 200 OK.\")\n",
        "            return False\n",
        "    except (ConnectionError, Timeout):\n",
        "        print(f\"[üõë API Check] Could not connect to the server at {health_url}. Connection timed out or refused.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"[üõë API Check] An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- 5. The Agent Workflow Orchestrator (The Main Loop) ---\n",
        "def run_coffee_agent_workflow():\n",
        "    \"\"\"The main loop for the coffee agent.\"\"\"\n",
        "\n",
        "    # Initial State for Coffee Agent\n",
        "    current_state = {\n",
        "        'location': 'Kitchen',\n",
        "        'beans_ground': False,\n",
        "        'coffee_brewed': False,\n",
        "        'cup_held': False\n",
        "    }\n",
        "    max_steps = 10\n",
        "\n",
        "    print(f\"\\n--- STARTING COFFEE AGENT WORKFLOW for: {TARGET_GOAL} ---\")\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        print(f\"\\n======== STEP {step} ========\")\n",
        "        print(f\"[üó∫Ô∏è Current State:] {current_state}\")\n",
        "\n",
        "        # Check for goal completion: Brewed and located on the desk (cup_held=False, location=Desk)\n",
        "        if current_state['coffee_brewed'] and not current_state['cup_held'] and current_state['location'] == 'Desk':\n",
        "            print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "            return\n",
        "\n",
        "        # 1. LLM Planning\n",
        "        raw_output = nemotron_planner(current_state)\n",
        "\n",
        "        # 2. Parse and Execute\n",
        "        status = parse_and_act(raw_output, current_state)\n",
        "\n",
        "        if status == \"SUCCESS: Goal Complete\":\n",
        "             print(\"--- AGENT SUCCESS: Goal completed! ---\")\n",
        "             return\n",
        "\n",
        "    print(\"--- AGENT FAILURE: Max steps reached before goal completion. ---\")\n",
        "\n",
        "\n",
        "# --- Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # The essential, accurate diagnostic check\n",
        "    if not check_api_availability(NEMOTRON_HEALTH_URL):\n",
        "        print(\"\\nFATAL ERROR: Cannot start agent. VLLM server is unavailable.\")\n",
        "    else:\n",
        "        # The main workflow call\n",
        "        run_coffee_agent_workflow()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5j6nXnhezxN",
        "outputId": "d2b02d64-d471-4185-e28c-8432f599b3de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[‚úÖ API Check] Server responded with status code 200 OK at http://localhost:8000/v1/models. Proceeding.\n",
            "\n",
            "--- STARTING COFFEE AGENT WORKFLOW for: Make and serve a fresh cup of coffee and place it on the desk. ---\n",
            "\n",
            "======== STEP 1 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Kitchen', 'beans_ground': False, 'coffee_brewed': False, 'cup_held': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: GRIND_BEANS\n",
            "[WM Update] Beans are now ground.\n",
            "\n",
            "======== STEP 2 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Kitchen', 'beans_ground': True, 'coffee_brewed': False, 'cup_held': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: BREW_COFFEE\n",
            "[WM Update] Coffee is now brewed.\n",
            "\n",
            "======== STEP 3 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Kitchen', 'beans_ground': True, 'coffee_brewed': True, 'cup_held': False}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: PICK_UP_CUP\n",
            "[WM Update] Cup is now held.\n",
            "\n",
            "======== STEP 4 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'CarryingCup', 'beans_ground': True, 'coffee_brewed': True, 'cup_held': True}\n",
            "[API Call] Sending request to Nemotron (Timeout: 60s)...\n",
            "[LLM Extracted: STAGE 1] Action found by prefix: PUT_DOWN_CUP\n",
            "[WM Update] Cup is now on the desk.\n",
            "\n",
            "======== STEP 5 ========\n",
            "[üó∫Ô∏è Current State:] {'location': 'Desk', 'beans_ground': True, 'coffee_brewed': True, 'cup_held': False}\n",
            "--- AGENT SUCCESS: Goal completed! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvbyarsWeweU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnKv4GIAR1d7FpHBO2S1je",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MCP_AAI_RAG_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCP"
      ],
      "metadata": {
        "id": "cv3j5wirod1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lm29QzL5oZe1",
        "outputId": "42e7c8be-3522-49ef-e40d-6054df28eee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Gemini API configured with model: gemini-2.5-flash\n",
            "--- Model Context Protocol (MCP) POC Tutorial ---\n",
            "MCP Server initialized. Ready to handle dynamic context requests.\n",
            "LLM Client initialized with Gemini model: gemini-2.5-flash\n",
            "\n",
            "--- Scenario 1: User asks for weather ---\n",
            "\n",
            "LLM Client: Received user query: 'What's the weather like in New York?'\n",
            "LLM Client: Requesting dynamic context from MCP Server (type: 'weather_query')...\n",
            "  MCP Server: Accessing 'weather_api' for 'get_current_weather'...\n",
            "  MCP Server: Dynamic context generated for 'weather_query'.\n",
            "\n",
            "--- LLM Processing ---\n",
            "LLM Full Prompt:\n",
            "User query: 'What's the weather like in New York?'\n",
            "Context provided by MCP:\n",
            "Weather context: The current weather in New York is sunny with 25°C.\n",
            "\n",
            "Based on the context, please provide a concise answer.\n",
            "\n",
            "Final Response to User (Scenario 1):\n",
            "Gemini LLM Response: It's sunny with 25°C.\n",
            "\n",
            "--- Scenario 2: User asks for stock price ---\n",
            "\n",
            "LLM Client: Received user query: 'What's the current price of GOOG?'\n",
            "LLM Client: Requesting dynamic context from MCP Server (type: 'stock_query')...\n",
            "  MCP Server: Accessing 'stock_data' for 'get_stock_price'...\n",
            "  MCP Server: Dynamic context generated for 'stock_query'.\n",
            "\n",
            "--- LLM Processing ---\n",
            "LLM Full Prompt:\n",
            "User query: 'What's the current price of GOOG?'\n",
            "Context provided by MCP:\n",
            "Stock context: The stock price of GOOG is $150.75.\n",
            "\n",
            "Based on the context, please provide a concise answer.\n",
            "\n",
            "Final Response to User (Scenario 2):\n",
            "Gemini LLM Response: The current price of GOOG is $150.75.\n",
            "\n",
            "--- Scenario 3: User expects a personalized greeting ---\n",
            "\n",
            "LLM Client: Received user query: 'Hello there!'\n",
            "LLM Client: Requesting dynamic context from MCP Server (type: 'personalized_greeting')...\n",
            "  MCP Server: Accessing 'user_profile_db' for 'get_user_preference'...\n",
            "  MCP Server: Dynamic context generated for 'personalized_greeting'.\n",
            "\n",
            "--- LLM Processing ---\n",
            "LLM Full Prompt:\n",
            "User query: 'Hello there!'\n",
            "Context provided by MCP:\n",
            "User preference context: User Alice123 prefers dark mode and email notifications.\n",
            "\n",
            "Based on the context, please provide a concise answer.\n",
            "\n",
            "Final Response to User (Scenario 3):\n",
            "Gemini LLM Response: Hello!\n",
            "\n",
            "--- MCP POC Tutorial End ---\n",
            "This POC demonstrates how MCP allows an LLM to dynamically fetch and integrate\n",
            "real-time, specific context from various data sources based on the user's request.\n",
            "It highlights direct connection, low latency, and dynamic context provision.\n",
            "\n",
            "Note: For this script to work with the real Gemini LLM, ensure you have set\n",
            "your 'GEMINI' API key as an environment variable or in Google Colab secrets.\n"
          ]
        }
      ],
      "source": [
        "# mcp_poc.py\n",
        "import time\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Simulate Data Sources ---\n",
        "# In a real scenario, these would be actual databases, APIs, etc.\n",
        "DATA_SOURCES = {\n",
        "    \"weather_api\": {\n",
        "        \"get_current_weather\": lambda city: f\"The current weather in {city} is sunny with 25°C.\"\n",
        "    },\n",
        "    \"stock_data\": {\n",
        "        \"get_stock_price\": lambda symbol: f\"The stock price of {symbol} is $150.75.\"\n",
        "    },\n",
        "    \"user_profile_db\": {\n",
        "        \"get_user_preference\": lambda user_id: f\"User {user_id} prefers dark mode and email notifications.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Simulate MCP Server ---\n",
        "# The MCP server dynamically fetches and structures context based on the request.\n",
        "class MCPServer:\n",
        "    def __init__(self):\n",
        "        print(\"MCP Server initialized. Ready to handle dynamic context requests.\")\n",
        "\n",
        "    def _fetch_data(self, source_name, method_name, *args):\n",
        "        \"\"\"\n",
        "        Simulates fetching data from a specific data source.\n",
        "        \"\"\"\n",
        "        if source_name in DATA_SOURCES and method_name in DATA_SOURCES[source_name]:\n",
        "            print(f\"  MCP Server: Accessing '{source_name}' for '{method_name}'...\")\n",
        "            time.sleep(0.1) # Simulate network latency\n",
        "            return DATA_SOURCES[source_name][method_name](*args)\n",
        "        else:\n",
        "            return f\"Error: Data source '{source_name}' or method '{method_name}' not found.\"\n",
        "\n",
        "    def get_dynamic_context(self, request_type, **kwargs):\n",
        "        \"\"\"\n",
        "        Dynamically generates context based on the request type.\n",
        "        This is where the 'protocol' aspect comes in, defining how context is built.\n",
        "        \"\"\"\n",
        "        context = []\n",
        "        if request_type == \"weather_query\":\n",
        "            city = kwargs.get(\"city\", \"unknown\")\n",
        "            weather_info = self._fetch_data(\"weather_api\", \"get_current_weather\", city)\n",
        "            context.append(f\"Weather context: {weather_info}\")\n",
        "        elif request_type == \"stock_query\":\n",
        "            symbol = kwargs.get(\"symbol\", \"unknown\")\n",
        "            stock_info = self._fetch_data(\"stock_data\", \"get_stock_price\", symbol)\n",
        "            context.append(f\"Stock context: {stock_info}\")\n",
        "        elif request_type == \"personalized_greeting\":\n",
        "            user_id = kwargs.get(\"user_id\", \"guest\")\n",
        "            user_pref = self._fetch_data(\"user_profile_db\", \"get_user_preference\", user_id)\n",
        "            context.append(f\"User preference context: {user_pref}\")\n",
        "        else:\n",
        "            context.append(\"No specific context requested or type not recognized.\")\n",
        "\n",
        "        print(f\"  MCP Server: Dynamic context generated for '{request_type}'.\")\n",
        "        return \"\\n\".join(context)\n",
        "\n",
        "# 2. Configuration for Agent\n",
        "class AgentConfig:\n",
        "    # Using 'gemini-2.5-flash' as explicitly requested by the user.\n",
        "    # Note: As of current public documentation, 'gemini-2.0-flash' is the standard.\n",
        "    # If this model name causes an API error, please verify its availability or\n",
        "    # revert to a publicly documented model like 'gemini-2.0-flash'.\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "    MAX_AGENT_RETRIES: int = 2 # Max attempts for the agent to refine its answer\n",
        "\n",
        "# 3. Google Colab / Gemini API Imports and Configuration\n",
        "GOOGLE_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Not running in Google Colab or 'GEMINI' secret not found. Attempting to get 'GEMINI' environment variable.\")\n",
        "    GOOGLE_API_KEY = os.getenv('GEMINI')\n",
        "\n",
        "# Initialize Gemini API\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(f\"Gemini API configured with model: {AgentConfig.LLM_MODEL_NAME}\")\n",
        "else:\n",
        "    print(\"Warning: GOOGLE_API_KEY not found. LLM calls will not work.\")\n",
        "\n",
        "\n",
        "# --- Simulate LLM Client ---\n",
        "# The LLM client makes requests to the MCP server for context and calls the real LLM.\n",
        "class LLMClient:\n",
        "    def __init__(self, mcp_server):\n",
        "        self.mcp_server = mcp_server\n",
        "        self.model = None\n",
        "        if GOOGLE_API_KEY:\n",
        "            try:\n",
        "                self.model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "                print(f\"LLM Client initialized with Gemini model: {AgentConfig.LLM_MODEL_NAME}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error initializing Gemini model: {e}. LLM calls will be simulated.\")\n",
        "        else:\n",
        "            print(\"LLM Client initialized without Gemini API key. LLM calls will be simulated.\")\n",
        "\n",
        "    def query_llm(self, user_query, request_type, **kwargs):\n",
        "        \"\"\"\n",
        "        Makes a request to the MCP server for context and then calls the real LLM\n",
        "        to generate a response.\n",
        "        \"\"\"\n",
        "        print(f\"\\nLLM Client: Received user query: '{user_query}'\")\n",
        "        print(f\"LLM Client: Requesting dynamic context from MCP Server (type: '{request_type}')...\")\n",
        "\n",
        "        # Get dynamic context from MCP server\n",
        "        dynamic_context = self.mcp_server.get_dynamic_context(request_type, **kwargs)\n",
        "\n",
        "        full_prompt = (\n",
        "            f\"User query: '{user_query}'\\n\"\n",
        "            f\"Context provided by MCP:\\n{dynamic_context}\\n\\n\"\n",
        "            f\"Based on the context, please provide a concise answer.\"\n",
        "        )\n",
        "        print(\"\\n--- LLM Processing ---\")\n",
        "        print(f\"LLM Full Prompt:\\n{full_prompt}\")\n",
        "\n",
        "        if self.model:\n",
        "            try:\n",
        "                # Call the real Gemini LLM\n",
        "                response = self.model.generate_content([full_prompt])\n",
        "                if response.candidates:\n",
        "                    # Access the text from the first part of the first candidate\n",
        "                    generated_text = response.candidates[0].content.parts[0].text\n",
        "                    return f\"Gemini LLM Response: {generated_text}\"\n",
        "                else:\n",
        "                    return f\"Gemini LLM Response: No content generated. {response.prompt_feedback}\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error calling Gemini LLM: {e}. Falling back to simulated response.\")\n",
        "                # Fallback to simple simulated LLM response if API call fails\n",
        "                simulated_response = \"I'm sorry, I cannot provide real-time data at the moment due to an API issue. \"\n",
        "                if \"Weather context\" in dynamic_context:\n",
        "                    simulated_response += f\"Based on the context, {dynamic_context.split('Weather context: ')[1].strip()}. \"\n",
        "                elif \"Stock context\" in dynamic_context:\n",
        "                    simulated_response += f\"Based on the context, {dynamic_context.split('Stock context: ')[1].strip()}. \"\n",
        "                elif \"User preference context\" in dynamic_context:\n",
        "                    simulated_response += f\"Based on the context, {dynamic_context.split('User preference context: ')[1].strip()}. \"\n",
        "                return f\"Simulated LLM Response (API Error): {simulated_response}How can I help you further?\"\n",
        "        else:\n",
        "            print(\"Warning: Gemini API not configured. Using simulated LLM response.\")\n",
        "            # Simple simulated LLM response based on context if API is not configured\n",
        "            simulated_response = \"I'm sorry, I cannot provide real-time data in this simulation. \"\n",
        "            if \"Weather context\" in dynamic_context:\n",
        "                simulated_response += f\"Based on the context, {dynamic_context.split('Weather context: ')[1].strip()}. \"\n",
        "            elif \"Stock context\" in dynamic_context:\n",
        "                simulated_response += f\"Based on the context, {dynamic_context.split('Stock context: ')[1].strip()}. \"\n",
        "            elif \"User preference context\" in dynamic_context:\n",
        "                simulated_response += f\"Based on the context, {dynamic_context.split('User preference context: ')[1].strip()}. \"\n",
        "            return f\"Simulated LLM Response: {simulated_response}How can I help you further?\"\n",
        "\n",
        "# --- POC Demonstration ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Model Context Protocol (MCP) POC Tutorial ---\")\n",
        "\n",
        "    # 1. Initialize MCP Server\n",
        "    mcp_server = MCPServer()\n",
        "\n",
        "    # 2. Initialize LLM Client with MCP Server\n",
        "    llm_client = LLMClient(mcp_server)\n",
        "\n",
        "    # --- Scenario 1: Weather Query ---\n",
        "    print(\"\\n--- Scenario 1: User asks for weather ---\")\n",
        "    response_weather = llm_client.query_llm(\n",
        "        user_query=\"What's the weather like in New York?\",\n",
        "        request_type=\"weather_query\",\n",
        "        city=\"New York\"\n",
        "    )\n",
        "    print(f\"\\nFinal Response to User (Scenario 1):\\n{response_weather}\")\n",
        "\n",
        "    # --- Scenario 2: Stock Price Query ---\n",
        "    print(\"\\n--- Scenario 2: User asks for stock price ---\")\n",
        "    response_stock = llm_client.query_llm(\n",
        "        user_query=\"What's the current price of GOOG?\",\n",
        "        request_type=\"stock_query\",\n",
        "        symbol=\"GOOG\"\n",
        "    )\n",
        "    print(f\"\\nFinal Response to User (Scenario 2):\\n{response_stock}\")\n",
        "\n",
        "    # --- Scenario 3: Personalized Greeting ---\n",
        "    print(\"\\n--- Scenario 3: User expects a personalized greeting ---\")\n",
        "    response_greeting = llm_client.query_llm(\n",
        "        user_query=\"Hello there!\",\n",
        "        request_type=\"personalized_greeting\",\n",
        "        user_id=\"Alice123\"\n",
        "    )\n",
        "    print(f\"\\nFinal Response to User (Scenario 3):\\n{response_greeting}\")\n",
        "\n",
        "    print(\"\\n--- MCP POC Tutorial End ---\")\n",
        "    print(\"This POC demonstrates how MCP allows an LLM to dynamically fetch and integrate\")\n",
        "    print(\"real-time, specific context from various data sources based on the user's request.\")\n",
        "    print(\"It highlights direct connection, low latency, and dynamic context provision.\")\n",
        "    print(\"\\nNote: For this script to work with the real Gemini LLM, ensure you have set\")\n",
        "    print(\"your 'GEMINI' API key as an environment variable or in Google Colab secrets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AGENTIC AI"
      ],
      "metadata": {
        "id": "xw2Dul2OpFSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# agentic_ai_poc.py\n",
        "import time\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# 2. Configuration for Agent\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "    MAX_AGENT_RETRIES: int = 2\n",
        "\n",
        "# 3. Google Colab / Gemini API Imports and Configuration\n",
        "GOOGLE_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Not running in Google Colab or 'GEMINI' secret not found. Attempting to get 'GEMINI' environment variable.\")\n",
        "    GOOGLE_API_KEY = os.getenv('GEMINI')\n",
        "\n",
        "# Initialize Gemini API\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(f\"Gemini API configured with model: {AgentConfig.LLM_MODEL_NAME}\")\n",
        "else:\n",
        "    print(\"Warning: GOOGLE_API_KEY not found. LLM calls will not work.\")\n",
        "\n",
        "\n",
        "# --- Simulate Agents ---\n",
        "# Each agent has a specific capability.\n",
        "class ResearchAgent:\n",
        "    def __init__(self, name=\"ResearchAgent\"):\n",
        "        self.name = name\n",
        "        self.model = None\n",
        "        if GOOGLE_API_KEY:\n",
        "            try:\n",
        "                self.model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "                print(f\"  {self.name}: Initialized with Gemini model.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  {self.name}: Error initializing Gemini model: {e}. Research will be simulated.\")\n",
        "        else:\n",
        "            print(f\"  {self.name}: Initialized without Gemini API key. Research will be simulated.\")\n",
        "\n",
        "    def conduct_research(self, topic):\n",
        "        print(f\"  {self.name}: Starting research on '{topic}'...\")\n",
        "        if self.model:\n",
        "            try:\n",
        "                prompt = f\"Conduct research on the following topic and provide key concepts: {topic}\"\n",
        "                response = self.model.generate_content([prompt])\n",
        "                if response.candidates:\n",
        "                    return response.candidates[0].content.parts[0].text\n",
        "                else:\n",
        "                    return f\"Simulated Research (No LLM content): Could not find information on {topic}. {response.prompt_feedback}\"\n",
        "            except Exception as e:\n",
        "                print(f\"  {self.name}: Error calling Gemini LLM for research: {e}. Falling back to simulated research.\")\n",
        "                time.sleep(0.3) # Simulate research time\n",
        "                if \"AI\" in topic:\n",
        "                    return f\"Simulated Research: Found key concepts about {topic}: Machine Learning, Neural Networks, Large Language Models.\"\n",
        "                elif \"space\" in topic:\n",
        "                    return f\"Simulated Research: Found key concepts about {topic}: Planets, Galaxies, Black Holes.\"\n",
        "                else:\n",
        "                    return f\"Simulated Research: Found general information about {topic}.\"\n",
        "        else:\n",
        "            time.sleep(0.3) # Simulate research time\n",
        "            if \"AI\" in topic:\n",
        "                return f\"Simulated Research: Found key concepts about {topic}: Machine Learning, Neural Networks, Large Language Models.\"\n",
        "            elif \"space\" in topic:\n",
        "                return f\"Simulated Research: Found key concepts about {topic}: Planets, Galaxies, Black Holes.\"\n",
        "            else:\n",
        "                return f\"Simulated Research: Found general information about {topic}.\"\n",
        "\n",
        "class SummarizationAgent:\n",
        "    def __init__(self, name=\"SummarizationAgent\"):\n",
        "        self.name = name\n",
        "        self.model = None\n",
        "        if GOOGLE_API_KEY:\n",
        "            try:\n",
        "                self.model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "                print(f\"  {self.name}: Initialized with Gemini model.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  {self.name}: Error initializing Gemini model: {e}. Summarization will be simulated.\")\n",
        "        else:\n",
        "            print(f\"  {self.name}: Initialized without Gemini API key. Summarization will be simulated.\")\n",
        "\n",
        "    def summarize_text(self, text):\n",
        "        print(f\"  {self.name}: Summarizing text...\")\n",
        "        if self.model:\n",
        "            try:\n",
        "                prompt = f\"Summarize the following text concisely: {text}\"\n",
        "                response = self.model.generate_content([prompt])\n",
        "                if response.candidates:\n",
        "                    return response.candidates[0].content.parts[0].text\n",
        "                else:\n",
        "                    return f\"Simulated Summary (No LLM content): Could not summarize. {response.prompt_feedback}\"\n",
        "            except Exception as e:\n",
        "                print(f\"  {self.name}: Error calling Gemini LLM for summarization: {e}. Falling back to simulated summarization.\")\n",
        "                time.sleep(0.2) # Simulate summarization time\n",
        "                if len(text) > 50:\n",
        "                    return f\"Simulated Summary: '{text[:50]}...' (truncated for POC)\"\n",
        "                return f\"Simulated Summary: '{text}'\"\n",
        "        else:\n",
        "            time.sleep(0.2) # Simulate summarization time\n",
        "            if len(text) > 50:\n",
        "                return f\"Simulated Summary: '{text[:50]}...' (truncated for POC)\"\n",
        "            return f\"Simulated Summary: '{text}'\"\n",
        "\n",
        "class PresentationAgent:\n",
        "    def __init__(self, name=\"PresentationAgent\"):\n",
        "        self.name = name\n",
        "        self.model = None\n",
        "        if GOOGLE_API_KEY:\n",
        "            try:\n",
        "                self.model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "                print(f\"  {self.name}: Initialized with Gemini model.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  {self.name}: Error initializing Gemini model: {e}. Presentation will be simulated.\")\n",
        "        else:\n",
        "            print(f\"  {self.name}: Initialized without Gemini API key. Presentation will be simulated.\")\n",
        "\n",
        "    def format_output(self, content):\n",
        "        print(f\"  {self.name}: Formatting output for presentation...\")\n",
        "        if self.model:\n",
        "            try:\n",
        "                prompt = f\"Format the following content into a concise report: {content}\"\n",
        "                response = self.model.generate_content([prompt])\n",
        "                if response.candidates:\n",
        "                    return f\"--- Final Report ---\\n{response.candidates[0].content.parts[0].text}\\n--- End Report ---\"\n",
        "                else:\n",
        "                    return f\"--- Simulated Report (No LLM content) ---\\n{content}\\n--- End Report --- (No LLM content)\"\n",
        "            except Exception as e:\n",
        "                print(f\"  {self.name}: Error calling Gemini LLM for formatting: {e}. Falling back to simulated formatting.\")\n",
        "                time.sleep(0.1) # Simulate formatting time\n",
        "                return f\"--- Simulated Report ---\\n{content}\\n--- End Report ---\"\n",
        "        else:\n",
        "            time.sleep(0.1) # Simulate formatting time\n",
        "            return f\"--- Simulated Report ---\\n{content}\\n--- End Report ---\"\n",
        "\n",
        "# --- Simulate Orchestrator ---\n",
        "# The orchestrator coordinates the agents to achieve a complex goal.\n",
        "class Orchestrator:\n",
        "    def __init__(self):\n",
        "        self.research_agent = ResearchAgent()\n",
        "        self.summarization_agent = SummarizationAgent()\n",
        "        self.presentation_agent = PresentationAgent()\n",
        "        print(\"Orchestrator initialized with Research, Summarization, and Presentation Agents.\")\n",
        "\n",
        "    def execute_complex_task(self, task_description):\n",
        "        \"\"\"\n",
        "        Orchestrates agents to complete a complex task.\n",
        "        This demonstrates the coordination aspect.\n",
        "        \"\"\"\n",
        "        print(f\"\\nOrchestrator: Received complex task: '{task_description}'\")\n",
        "\n",
        "        # Step 1: Research (handled by ResearchAgent)\n",
        "        print(\"Orchestrator: Delegating research to ResearchAgent.\")\n",
        "        research_result = self.research_agent.conduct_research(task_description)\n",
        "        print(f\"Orchestrator: Research complete. Result: '{research_result}'\")\n",
        "\n",
        "        # Step 2: Summarize (handled by SummarizationAgent)\n",
        "        print(\"Orchestrator: Delegating summarization to SummarizationAgent.\")\n",
        "        summarized_result = self.summarization_agent.summarize_text(research_result)\n",
        "        print(f\"Orchestrator: Summarization complete. Result: '{summarized_result}'\")\n",
        "\n",
        "        # Step 3: Format Output (handled by PresentationAgent)\n",
        "        print(\"Orchestrator: Delegating formatting to PresentationAgent.\")\n",
        "        final_output = self.presentation_agent.format_output(summarized_result)\n",
        "        print(f\"Orchestrator: Formatting complete.\")\n",
        "\n",
        "        print(\"Orchestrator: Complex task execution finished.\")\n",
        "        return final_output\n",
        "\n",
        "# --- POC Demonstration ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Agentic AI POC Tutorial ---\")\n",
        "\n",
        "    # Initialize the Orchestrator\n",
        "    orchestrator = Orchestrator()\n",
        "\n",
        "    # --- Scenario 1: Research and Summarize AI ---\n",
        "    print(\"\\n--- Scenario 1: Task - 'Research and summarize key concepts about AI' ---\")\n",
        "    report_ai = orchestrator.execute_complex_task(\"AI\")\n",
        "    print(f\"\\nFinal Output (Scenario 1):\\n{report_ai}\")\n",
        "\n",
        "    # --- Scenario 2: Research and Summarize Space ---\n",
        "    print(\"\\n--- Scenario 2: Task - 'Research and summarize interesting facts about space' ---\")\n",
        "    report_space = orchestrator.execute_complex_task(\"space\")\n",
        "    print(f\"\\nFinal Output (Scenario 2):\\n{report_space}\")\n",
        "\n",
        "    # --- Simulate Error Propagation (Illustrative) ---\n",
        "    # In a real system, an agent might fail, and the orchestrator would need\n",
        "    # error handling or retry mechanisms. Here, we just simulate a \"failure\" message.\n",
        "    print(\"\\n--- Scenario 3: Simulating Error Propagation ---\")\n",
        "    class FaultyAgent:\n",
        "        def do_something(self):\n",
        "            print(\"  FaultyAgent: Attempting task...\")\n",
        "            time.sleep(0.1)\n",
        "            raise ValueError(\"Simulated agent failure!\")\n",
        "\n",
        "    orchestrator_with_fault = Orchestrator()\n",
        "    orchestrator_with_fault.faulty_agent = FaultyAgent() # Inject a faulty agent for demonstration\n",
        "\n",
        "    try:\n",
        "        print(\"Orchestrator: Attempting task with potential fault...\")\n",
        "        orchestrator_with_fault.faulty_agent.do_something()\n",
        "    except ValueError as e:\n",
        "        print(f\"Orchestrator: Caught an error from a sub-agent: {e}\")\n",
        "        print(\"This illustrates how errors can propagate in Agentic AI if not handled.\")\n",
        "\n",
        "    print(\"\\n--- Agentic AI POC Tutorial End ---\")\n",
        "    print(\"This POC demonstrates how an Orchestrator coordinates multiple specialized Agents\")\n",
        "    print(\"to perform complex tasks. It highlights the modularity but also hints at challenges\")\n",
        "    print(\"like complex coordination and potential error propagation.\")\n",
        "    print(\"\\nNote: For this script to work with the real Gemini LLM, ensure you have set\")\n",
        "    print(\"your 'GEMINI' API key as an environment variable or in Google Colab secrets.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pftTHfrkpIK8",
        "outputId": "21da8da2-053a-4f38-8905-86bb9ceabb35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Gemini API configured with model: gemini-2.5-flash\n",
            "--- Agentic AI POC Tutorial ---\n",
            "  ResearchAgent: Initialized with Gemini model.\n",
            "  SummarizationAgent: Initialized with Gemini model.\n",
            "  PresentationAgent: Initialized with Gemini model.\n",
            "Orchestrator initialized with Research, Summarization, and Presentation Agents.\n",
            "\n",
            "--- Scenario 1: Task - 'Research and summarize key concepts about AI' ---\n",
            "\n",
            "Orchestrator: Received complex task: 'AI'\n",
            "Orchestrator: Delegating research to ResearchAgent.\n",
            "  ResearchAgent: Starting research on 'AI'...\n",
            "Orchestrator: Research complete. Result: 'Artificial Intelligence (AI) is one of the most transformative technologies of our time, rapidly reshaping industries, societies, and our daily lives.\n",
            "\n",
            "Here's a comprehensive overview of AI, including its key concepts:\n",
            "\n",
            "---\n",
            "\n",
            "## Artificial Intelligence (AI): Key Concepts\n",
            "\n",
            "### 1. Definition\n",
            "\n",
            "**Artificial Intelligence (AI)** is a broad field of computer science dedicated to creating machines that can perform tasks that typically require human intelligence. This includes the ability to reason, learn, solve problems, perceive, understand language, and make decisions. AI aims to simulate, and potentially extend, human cognitive functions in machines.\n",
            "\n",
            "### 2. Historical Context\n",
            "\n",
            "*   **Early Concepts (Pre-1950s):** Philosophical roots exploring thought and reasoning, early computational ideas (e.g., Charles Babbage, Ada Lovelace).\n",
            "*   **Birth of AI (1950s):** Alan Turing's \"Computing Machinery and Intelligence\" (1950) proposed the Turing Test. The term \"Artificial Intelligence\" was coined by John McCarthy in 1956 at the Dartmouth Workshop, marking the official birth of the field.\n",
            "*   **Early Enthusiasm & \"AI Winters\" (1960s-1980s):** Initial optimism led to significant funding, but limitations of early rule-based systems and computational power led to periods of reduced funding and interest (\"AI winters\").\n",
            "*   **Expert Systems & Machine Learning Resurgence (1980s-1990s):** Rule-based \"expert systems\" saw commercial success. The focus shifted more towards machine learning and statistical approaches.\n",
            "*   **Deep Learning Revolution (2000s-Present):** Advances in computational power (GPUs), vast amounts of data, and improved algorithms (especially neural networks) led to the deep learning boom, achieving unprecedented performance in areas like image recognition and natural language processing.\n",
            "\n",
            "### 3. Core Concepts & Key Sub-fields\n",
            "\n",
            "AI is an umbrella term encompassing several specialized areas:\n",
            "\n",
            "*   **Machine Learning (ML):**\n",
            "    *   **Definition:** A subset of AI that enables systems to learn from data without being explicitly programmed. Instead of following pre-set rules, ML algorithms identify patterns in data and make predictions or decisions based on those patterns.\n",
            "    *   **Types of ML:**\n",
            "        *   **Supervised Learning:** Learning from labeled data (input-output pairs). Examples: Image classification (is it a cat or a dog?), spam detection.\n",
            "        *   **Unsupervised Learning:** Learning from unlabeled data to find hidden patterns or structures. Examples: Customer segmentation, anomaly detection.\n",
            "        *   **Reinforcement Learning (RL):** Learning through trial and error, by interacting with an environment and receiving rewards or penalties. Examples: Game playing (AlphaGo), robotics control.\n",
            "*   **Neural Networks (NNs):**\n",
            "    *   **Definition:** Computational models inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) organized in layers, processing information through these connections.\n",
            "    *   **Function:** They learn by adjusting the \"weights\" of connections between neurons based on input data, allowing them to recognize complex patterns.\n",
            "*   **Deep Learning (DL):**\n",
            "    *   **Definition:** A sub-field of machine learning that uses multi-layered neural networks (often with many \"hidden\" layers) to learn representations of data with multiple levels of abstraction.\n",
            "    *   **Impact:** Responsible for many breakthroughs in AI, particularly in perception tasks.\n",
            "*   **Natural Language Processing (NLP):**\n",
            "    *   **Definition:** Enables computers to understand, interpret, generate, and manipulate human language.\n",
            "    *   **Applications:** Voice assistants (Siri, Alexa), machine translation (Google Translate), sentiment analysis, chatbots, text summarization.\n",
            "*   **Computer Vision (CV):**\n",
            "    *   **Definition:** Gives computers the ability to \"see\" and interpret visual information from images and videos.\n",
            "    *   **Applications:** Facial recognition, object detection (self-driving cars), medical image analysis, augmented reality.\n",
            "*   **Robotics:**\n",
            "    *   **Definition:** The design, construction, operation, and use of robots. AI often provides the \"brain\" for robots, enabling them to perceive, navigate, and interact with their environment intelligently.\n",
            "*   **Expert Systems:**\n",
            "    *   **Definition:** Early AI systems designed to mimic the decision-making ability of a human expert within a specific domain, using rule-based knowledge. Less common now but foundational.\n",
            "*   **Generative AI:**\n",
            "    *   **Definition:** A type of AI that can create new, original content (text, images, audio, video) rather than just analyzing existing data.\n",
            "    *   **Examples:** Large Language Models (LLMs) like GPT-4, image generators like DALL-E and Midjourney.\n",
            "\n",
            "### 4. Types of AI (by Capability)\n",
            "\n",
            "*   **Narrow AI (or Weak AI):**\n",
            "    *   **Definition:** AI designed and trained for a specific, narrow task. Most AI existing today falls into this category.\n",
            "    *   **Examples:** Chess-playing programs, voice assistants, recommendation systems, self-driving cars.\n",
            "*   **General AI (AGI or Strong AI):**\n",
            "    *   **Definition:** Hypothetical AI that possesses human-level cognitive abilities across a wide range of tasks, capable of understanding, learning, and applying intelligence to any intellectual task that a human being can.\n",
            "    *   **Status:** Does not yet exist.\n",
            "*   **Superintelligence:**\n",
            "    *   **Definition:** Hypothetical AI that surpasses human intelligence across virtually all metrics, including creativity, general knowledge, and problem-solving skills.\n",
            "    *   **Status:** Purely theoretical at this stage.\n",
            "\n",
            "### 5. Foundational Principles of How AI Works\n",
            "\n",
            "At its core, AI's functionality relies on three main components:\n",
            "\n",
            "1.  **Data:** AI systems learn from vast amounts of data (text, images, audio, numerical data). The quality, quantity, and diversity of data are crucial.\n",
            "2.  **Algorithms:** These are the sets of rules and statistical models that AI systems use to process data, identify patterns, make predictions, or generate outputs.\n",
            "3.  **Computation:** Powerful processing capabilities (CPUs, GPUs, TPUs) are required to train and run complex AI models, especially for deep learning.\n",
            "\n",
            "### 6. Applications and Impact\n",
            "\n",
            "AI is being applied across nearly every sector:\n",
            "\n",
            "*   **Healthcare:** Disease diagnosis, drug discovery, personalized medicine, surgical robots.\n",
            "*   **Finance:** Fraud detection, algorithmic trading, credit scoring, personalized financial advice.\n",
            "*   **Automotive:** Self-driving cars, predictive maintenance.\n",
            "*   **Customer Service:** Chatbots, virtual assistants.\n",
            "*   **Retail:** Recommendation engines, inventory management, personalized marketing.\n",
            "*   **Education:** Personalized learning platforms, intelligent tutoring systems.\n",
            "*   **Manufacturing:** Predictive maintenance, quality control, robotic automation.\n",
            "*   **Entertainment:** Content creation (music, art), personalized content recommendations.\n",
            "\n",
            "### 7. Challenges, Risks, and Ethical Considerations\n",
            "\n",
            "While AI offers immense benefits, it also presents significant challenges:\n",
            "\n",
            "*   **Bias:** AI models can perpetuate or amplify existing societal biases present in their training data, leading to unfair or discriminatory outcomes.\n",
            "*   **Privacy and Data Security:** AI systems often require large datasets, raising concerns about data privacy, ownership, and potential misuse.\n",
            "*   **Job Displacement:** Automation driven by AI could lead to significant job losses in certain sectors, requiring societal adjustments and new skill development.\n",
            "*   **Accountability and Transparency (\"Black Box Problem\"):** Complex AI models, especially deep neural networks, can be opaque, making it difficult to understand how they arrive at specific decisions, leading to issues of accountability.\n",
            "*   **Misinformation and Malicious Use:** Generative AI can be used to create realistic fake content (deepfakes), spreading misinformation or for malicious purposes.\n",
            "*   **Control and Safety:** Ensuring that advanced AI systems remain aligned with human values and objectives, particularly concerning future AGI.\n",
            "*   **Economic Inequality:** The benefits of AI could be unevenly distributed, exacerbating existing wealth disparities.\n",
            "\n",
            "### 8. The Future of AI\n",
            "\n",
            "The future of AI is expected to involve:\n",
            "\n",
            "*   **Continued Integration:** AI will become increasingly embedded in everyday objects and services.\n",
            "*   **Human-AI Collaboration:** Rather than full replacement, AI is likely to augment human capabilities, leading to more efficient and innovative workflows.\n",
            "*   **Advancements in AGI Research:** While AGI is still distant, research continues to explore pathways to more general and adaptable AI systems.\n",
            "*   **Ethical AI Development:** A growing emphasis on developing AI responsibly, addressing bias, transparency, and safety concerns.\n",
            "*   **New Paradigms:** Exploration of new AI paradigms beyond current deep learning methods.\n",
            "\n",
            "---\n",
            "\n",
            "In conclusion, Artificial Intelligence is a dynamic and rapidly evolving field focused on replicating and extending human intelligence in machines. From its foundational concepts like machine learning and neural networks to its diverse applications and profound ethical implications, AI is set to continue as a primary driver of technological and societal change for decades to come.'\n",
            "Orchestrator: Delegating summarization to SummarizationAgent.\n",
            "  SummarizationAgent: Summarizing text...\n",
            "Orchestrator: Summarization complete. Result: 'Artificial Intelligence (AI) is a transformative field of computer science dedicated to creating machines that perform tasks typically requiring human intelligence, such as reasoning, learning, and problem-solving.\n",
            "\n",
            "Key concepts include:\n",
            "*   **Machine Learning (ML)**, where systems learn from data.\n",
            "*   **Neural Networks (NNs)** and **Deep Learning (DL)**, computational models inspired by the brain.\n",
            "*   **Natural Language Processing (NLP)** for human language understanding.\n",
            "*   **Computer Vision (CV)** for interpreting visual information.\n",
            "*   **Generative AI**, capable of creating new content.\n",
            "\n",
            "Most current AI is **Narrow AI** (task-specific), with **General AI (AGI)** and **Superintelligence** being hypothetical. AI functions through **data**, **algorithms**, and **computation**. It is rapidly transforming sectors like healthcare, finance, and automotive.\n",
            "\n",
            "Despite its benefits, AI faces challenges such as **bias**, **privacy concerns**, **job displacement**, **transparency issues**, and the potential for **misinformation**. The future of AI involves continued integration, human-AI collaboration, and a strong emphasis on ethical development.'\n",
            "Orchestrator: Delegating formatting to PresentationAgent.\n",
            "  PresentationAgent: Formatting output for presentation...\n",
            "Orchestrator: Formatting complete.\n",
            "Orchestrator: Complex task execution finished.\n",
            "\n",
            "Final Output (Scenario 1):\n",
            "--- Final Report ---\n",
            "## Artificial Intelligence (AI) Concise Report\n",
            "\n",
            "**1. Definition:**\n",
            "Artificial Intelligence (AI) is a transformative field of computer science dedicated to creating machines that perform tasks typically requiring human intelligence, such as reasoning, learning, and problem-solving.\n",
            "\n",
            "**2. Key AI Concepts:**\n",
            "*   **Machine Learning (ML):** Systems learn from data.\n",
            "*   **Neural Networks (NNs) & Deep Learning (DL):** Brain-inspired computational models.\n",
            "*   **Natural Language Processing (NLP):** Understanding human language.\n",
            "*   **Computer Vision (CV):** Interpreting visual information.\n",
            "*   **Generative AI:** Capable of creating new content.\n",
            "\n",
            "**3. AI Classification & Functionality:**\n",
            "*   Most current AI is **Narrow AI** (task-specific); **General AI (AGI)** and **Superintelligence** are hypothetical.\n",
            "*   AI functions through **data, algorithms, and computation**.\n",
            "\n",
            "**4. Sectoral Impact:**\n",
            "AI is rapidly transforming sectors such as healthcare, finance, and automotive.\n",
            "\n",
            "**5. Challenges:**\n",
            "Key challenges include **bias, privacy concerns, job displacement, transparency issues,** and the potential for **misinformation**.\n",
            "\n",
            "**6. Future Outlook:**\n",
            "The future involves continued AI integration, human-AI collaboration, and a strong emphasis on ethical development.\n",
            "--- End Report ---\n",
            "\n",
            "--- Scenario 2: Task - 'Research and summarize interesting facts about space' ---\n",
            "\n",
            "Orchestrator: Received complex task: 'space'\n",
            "Orchestrator: Delegating research to ResearchAgent.\n",
            "  ResearchAgent: Starting research on 'space'...\n",
            "Orchestrator: Research complete. Result: 'Researching the concept of \"space\" reveals it to be a multifaceted term with definitions spanning physics, mathematics, philosophy, and even common language. Here's a comprehensive overview focusing on its key concepts:\n",
            "\n",
            "---\n",
            "\n",
            "## Research on \"Space\": Key Concepts\n",
            "\n",
            "**Definition:**\n",
            "At its broadest, \"space\" refers to the three-dimensional extent in which objects and events have relative position and direction. However, its meaning varies significantly based on context.\n",
            "\n",
            "---\n",
            "\n",
            "### 1. Outer Space (Cosmological/Astrophysical Space)\n",
            "\n",
            "This is the most common understanding of \"space\" – the vast, nearly empty vacuum that exists beyond the Earth's atmosphere and between celestial bodies.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "*   **Vacuum (Near-Perfect):** Outer space is not entirely empty but contains an extremely low density of particles (mostly hydrogen and helium atoms), electromagnetic radiation, magnetic fields, and neutrinos. It's often referred to as a \"hard vacuum.\"\n",
            "*   **Vastness:** Space is incomprehensibly large, measured in astronomical units (AU), light-years (ly), and parsecs (pc). The observable universe alone stretches billions of light-years in every direction.\n",
            "*   **Low Temperature:** While direct sunlight can heat objects, the background temperature of space is extremely cold, dictated by the Cosmic Microwave Background (CMB) radiation at about 2.7 Kelvin (-270.45 °C or -454.81 °F).\n",
            "*   **Radiation:** Space is permeated by various forms of radiation, including cosmic rays (high-energy particles), solar flares, and electromagnetic radiation across the spectrum (radio waves, visible light, X-rays, gamma rays).\n",
            "*   **Microgravity:** Objects in orbit are not \"out of gravity\" but are in a continuous state of freefall around a larger body, leading to the sensation of weightlessness (microgravity). Gravity acts throughout space.\n",
            "*   **Celestial Bodies:**\n",
            "    *   **Stars:** Massive, luminous balls of plasma held together by their own gravity, producing light and heat through nuclear fusion.\n",
            "    *   **Planets:** Celestial bodies orbiting a star, large enough to be rounded by their own gravity and to have cleared their orbit of other debris. (Includes exoplanets).\n",
            "    *   **Moons/Natural Satellites:** Celestial bodies orbiting planets.\n",
            "    *   **Asteroids & Comets:** Smaller rocky or icy bodies orbiting stars.\n",
            "    *   **Galaxies:** Vast systems of stars, stellar remnants, interstellar gas, dust, and dark matter, all bound together by gravity (e.g., Milky Way, Andromeda). Galaxies cluster into larger structures called galaxy clusters and superclusters.\n",
            "    *   **Nebulae:** Interstellar clouds of dust, hydrogen, helium, and other ionized gases; often stellar nurseries or remnants of dead stars.\n",
            "*   **Dark Matter & Dark Energy:**\n",
            "    *   **Dark Matter:** A hypothetical form of matter that does not emit or reflect light or other electromagnetic radiation, making it undetectable by conventional means. Its presence is inferred from its gravitational effects on visible matter. It's thought to make up about 27% of the universe's mass-energy content.\n",
            "    *   **Dark Energy:** A hypothetical form of energy that permeates all of space and is responsible for the accelerating expansion of the universe. It's thought to comprise about 68% of the universe's mass-energy content.\n",
            "*   **Cosmic Microwave Background (CMB):** Residual radiation from the Big Bang, providing crucial evidence for the universe's origin and evolution. It's essentially the \"afterglow\" of the early universe.\n",
            "*   **Expansion of the Universe:** Observationally, space itself is expanding, carrying galaxies further apart from each other. This expansion is accelerating, driven by dark energy.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. Physical Space (Spacetime)\n",
            "\n",
            "In modern physics, particularly under Einstein's theories of relativity, space is not an empty, static container but is interwoven with time, forming a four-dimensional continuum called **spacetime**.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "*   **Four Dimensions:** Spacetime combines the three spatial dimensions (length, width, height) with the dimension of time. Events occur at specific points in spacetime.\n",
            "*   **Relativity (Special & General):**\n",
            "    *   **Special Relativity:** Demonstrates that space and time are relative to the observer's motion. Velocity through space affects the passage of time (time dilation) and the perceived length of objects (length contraction).\n",
            "    *   **General Relativity:** Describes gravity not as a force, but as a curvature of spacetime caused by the presence of mass and energy. Massive objects \"warp\" the fabric of spacetime, and other objects follow these curves, which we perceive as gravity.\n",
            "*   **Curvature of Spacetime:** The presence of mass/energy bends spacetime. This bending is what dictates the paths of objects, including light (gravitational lensing).\n",
            "*   **Gravitational Waves:** Ripples in the fabric of spacetime, caused by extremely energetic cosmic events (like the merger of black holes or neutron stars). Predicted by Einstein, first directly detected in 2015.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. Mathematical Space (Abstract Space)\n",
            "\n",
            "In mathematics, \"space\" refers to a set of points endowed with some additional structure. This allows for abstract conceptualization of relationships and properties that extend beyond physical intuition.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "*   **Euclidean Space (Rn):** The most intuitive mathematical space, corresponding to our everyday experience of 2D planes (R2) and 3D volumes (R3). It's defined by distance, angles, and straight lines following Euclid's axioms.\n",
            "*   **Vector Space:** A set of objects (vectors) that can be added together and multiplied by scalars (numbers), satisfying certain axioms. Crucial in linear algebra and physics.\n",
            "*   **Metric Space:** A set where a \"distance\" function (metric) is defined between any two points. This allows for concepts like convergence and continuity.\n",
            "*   **Topological Space:** A set equipped with a \"topology\" – a collection of subsets defined as \"open sets.\" This allows for the study of continuity, connectedness, and compactness without needing a precise concept of distance. It focuses on the fundamental shape and structure.\n",
            "*   **Hilbert Space:** A complete vector space with an inner product, allowing for the definition of distance and angle. Essential for quantum mechanics, where the \"state\" of a system is represented as a point in a Hilbert space.\n",
            "*   **Phase Space:** In physics and mathematics, a space whose axes represent the positions and momenta of all particles in a system. Each point in phase space represents a unique state of the system.\n",
            "\n",
            "---\n",
            "\n",
            "### 4. Philosophical Space\n",
            "\n",
            "Philosophers have debated the nature of space for millennia, often contrasting its absolute vs. relational properties.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "*   **Absolute Space (Newton):** Isaac Newton posited that space is a real, independent entity – an infinite, immutable, and unseen container within which all matter exists and moves. It exists whether anything is in it or not.\n",
            "*   **Relational Space (Leibniz):** Gottfried Wilhelm Leibniz argued that space is not an independent entity but merely a system of relations between objects. Space is simply the order of coexistence of things; if nothing existed, there would be no space.\n",
            "*   **Transcendental Idealism (Kant):** Immanuel Kant argued that space (and time) are not objective realities in themselves but are \"a priori intuitions\" – fundamental structures of the human mind through which we perceive and organize sensory experience. We cannot conceive of objects outside of space.\n",
            "\n",
            "---\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "The concept of \"space\" is profoundly complex and fundamental to our understanding of the universe, reality, and even abstract thought. From the cold, vast expanse of the cosmos filled with galaxies and dark matter, to the curved four-dimensional fabric of spacetime, to the abstract structures of mathematical thought, and the deep philosophical debates about its very nature, \"space\" remains a central pillar of scientific inquiry and human contemplation.'\n",
            "Orchestrator: Delegating summarization to SummarizationAgent.\n",
            "  SummarizationAgent: Summarizing text...\n",
            "Orchestrator: Summarization complete. Result: 'The text comprehensively defines \"space\" as a multifaceted concept spanning physics, mathematics, philosophy, and common language.\n",
            "\n",
            "It details **Outer Space** as the vast, near-vacuum beyond Earth, characterized by its extreme conditions, celestial bodies (stars, galaxies), and the pervasive influence of Dark Matter (27%) and Dark Energy (68%) driving its accelerating expansion, with the Cosmic Microwave Background as a Big Bang remnant.\n",
            "\n",
            "**Physical Space** is explained as four-dimensional \"spacetime\" in modern physics (Einstein's relativity), where mass and energy curve its fabric, manifesting as gravity, and its properties are relative to the observer.\n",
            "\n",
            "**Mathematical Space** refers to abstract sets with defined structures, including Euclidean (intuitive 2D/3D), vector, metric, topological, Hilbert, and phase spaces, each serving distinct analytical purposes.\n",
            "\n",
            "**Philosophical Space** explores historical debates: Newton's \"absolute space\" (an independent container), Leibniz's \"relational space\" (defined by object relationships), and Kant's \"a priori intuition\" (a fundamental structure of human perception).\n",
            "\n",
            "Ultimately, \"space\" is a profoundly complex and fundamental concept central to scientific inquiry and human understanding of reality.'\n",
            "Orchestrator: Delegating formatting to PresentationAgent.\n",
            "  PresentationAgent: Formatting output for presentation...\n",
            "Orchestrator: Formatting complete.\n",
            "Orchestrator: Complex task execution finished.\n",
            "\n",
            "Final Output (Scenario 2):\n",
            "--- Final Report ---\n",
            "## Report: The Multifaceted Concept of \"Space\"\n",
            "\n",
            "This report defines \"space\" as a multifaceted concept encompassing physics, mathematics, philosophy, and common language, exploring its various interpretations.\n",
            "\n",
            "**Key Interpretations:**\n",
            "\n",
            "*   **Outer Space:** The vast, near-vacuum beyond Earth, characterized by extreme conditions, celestial bodies, and the dominant influence of Dark Matter (27%) and Dark Energy (68%) driving its accelerating expansion. It contains the Cosmic Microwave Background as a Big Bang remnant.\n",
            "*   **Physical Space:** In modern physics (Einstein's relativity), understood as four-dimensional \"spacetime\" where mass and energy curve its fabric, creating gravity. Its properties are relative to the observer.\n",
            "*   **Mathematical Space:** Abstract sets with defined structures (e.g., Euclidean, vector, metric, topological, Hilbert, phase spaces) serving distinct analytical purposes.\n",
            "*   **Philosophical Space:** Explores historical debates, including Newton's \"absolute space\" (an independent container), Leibniz's \"relational space\" (defined by object relationships), and Kant's \"a priori intuition\" (a fundamental structure of human perception).\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Ultimately, \"space\" is a profoundly complex and fundamental concept vital to scientific inquiry and human understanding of reality.\n",
            "--- End Report ---\n",
            "\n",
            "--- Scenario 3: Simulating Error Propagation ---\n",
            "  ResearchAgent: Initialized with Gemini model.\n",
            "  SummarizationAgent: Initialized with Gemini model.\n",
            "  PresentationAgent: Initialized with Gemini model.\n",
            "Orchestrator initialized with Research, Summarization, and Presentation Agents.\n",
            "Orchestrator: Attempting task with potential fault...\n",
            "  FaultyAgent: Attempting task...\n",
            "Orchestrator: Caught an error from a sub-agent: Simulated agent failure!\n",
            "This illustrates how errors can propagate in Agentic AI if not handled.\n",
            "\n",
            "--- Agentic AI POC Tutorial End ---\n",
            "This POC demonstrates how an Orchestrator coordinates multiple specialized Agents\n",
            "to perform complex tasks. It highlights the modularity but also hints at challenges\n",
            "like complex coordination and potential error propagation.\n",
            "\n",
            "Note: For this script to work with the real Gemini LLM, ensure you have set\n",
            "your 'GEMINI' API key as an environment variable or in Google Colab secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "0YDZRr-LtCTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rag_poc.py\n",
        "import time\n",
        "import os\n",
        "import hashlib # Used for a simple \"embedding\" simulation\n",
        "import google.generativeai as genai\n",
        "\n",
        "# 2. Configuration for Agent\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"gemini-2.5-flash\"\n",
        "    MAX_AGENT_RETRIES: int = 2\n",
        "\n",
        "# 3. Google Colab / Gemini API Imports and Configuration\n",
        "GOOGLE_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "    print(\"Google Generative AI configured successfully using Colab Secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Not running in Google Colab or 'GEMINI' secret not found. Attempting to get 'GEMINI' environment variable.\")\n",
        "    GOOGLE_API_KEY = os.getenv('GEMINI')\n",
        "\n",
        "# Initialize Gemini API\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(f\"Gemini API configured with model: {AgentConfig.LLM_MODEL_NAME}\")\n",
        "else:\n",
        "    print(\"Warning: GOOGLE_API_KEY not found. LLM calls will not work.\")\n",
        "\n",
        "\n",
        "# --- Simulate Vector DB / Knowledge Base ---\n",
        "# In a real RAG system, this would be a vector database with actual embeddings.\n",
        "# Here, we use a dictionary where keys are \"simulated embeddings\" (hashes)\n",
        "# and values are text chunks.\n",
        "KNOWLEDGE_BASE = {\n",
        "    \"doc1_ai_history\": \"Artificial intelligence (AI) has a long history, dating back to the 1950s with early work on symbolic AI.\",\n",
        "    \"doc2_llm_basics\": \"Large Language Models (LLMs) are a type of AI model trained on vast amounts of text data to understand and generate human-like language.\",\n",
        "    \"doc3_rag_concept\": \"Retrieval Augmented Generation (RAG) combines the power of LLMs with external knowledge retrieval to provide more accurate and up-to-date information.\",\n",
        "    \"doc4_newton_physics\": \"Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.\",\n",
        "    \"doc5_einstein_relativity\": \"Albert Einstein developed the theory of relativity, revolutionizing our understanding of space, time, gravity, and the universe.\",\n",
        "    \"doc6_galileo_astronomy\": \"Galileo Galilei made significant contributions to astronomy, including observations of Jupiter's moons and phases of Venus.\",\n",
        "    \"doc7_hinton_deep_learning\": \"Geoffrey Hinton is a pioneer in deep learning, known for his work on neural networks and backpropagation.\",\n",
        "    \"doc8_ai_flight_planning\": \"AI agents can optimize flight paths considering weather, fuel efficiency, and air traffic control constraints, leading to safer and more economical travel.\"\n",
        "}\n",
        "\n",
        "# --- Simulate Embedding Model ---\n",
        "# A very simple hash function to represent an \"embedding\".\n",
        "# In reality, this would be a complex neural network producing dense vectors.\n",
        "def simulate_embedding(text):\n",
        "    \"\"\"\n",
        "    Simulates an embedding by creating a simple hash of the text.\n",
        "    This is NOT a real embedding but serves for POC matching.\n",
        "    \"\"\"\n",
        "    return hashlib.sha256(text.lower().encode()).hexdigest()[:10] # Use first 10 chars for simplicity\n",
        "\n",
        "# --- Simulate Retriever ---\n",
        "# Finds the most \"relevant\" chunk based on a simple keyword match or simulated embedding similarity.\n",
        "class Retriever:\n",
        "    def __init__(self, knowledge_base):\n",
        "        self.knowledge_base = knowledge_base\n",
        "        # Pre-compute \"embeddings\" for the knowledge base for simple lookup\n",
        "        self.indexed_knowledge = {simulate_embedding(v): v for k, v in knowledge_base.items()}\n",
        "        print(\"Retriever initialized with knowledge base.\")\n",
        "\n",
        "    def retrieve_chunks(self, query, top_k=1):\n",
        "        \"\"\"\n",
        "        Simulates retrieving relevant chunks based on the query.\n",
        "        In a real RAG, this would involve vector similarity search.\n",
        "        Here, we do a simple keyword matching for demonstration.\n",
        "        \"\"\"\n",
        "        print(f\"  Retriever: Searching for relevant chunks for query: '{query}'...\")\n",
        "        time.sleep(0.1) # Simulate retrieval time\n",
        "\n",
        "        relevant_chunks = []\n",
        "        query_words = query.lower().split()\n",
        "\n",
        "        # Simple keyword matching for demonstration\n",
        "        for chunk_id, chunk_text in self.knowledge_base.items():\n",
        "            if any(word in chunk_text.lower() for word in query_words):\n",
        "                relevant_chunks.append(chunk_text)\n",
        "                if len(relevant_chunks) >= top_k:\n",
        "                    break # Get top_k matching chunks\n",
        "\n",
        "        if not relevant_chunks:\n",
        "            # Fallback: if no keyword match, try a direct \"embedding\" match (very basic)\n",
        "            query_embedding = simulate_embedding(query)\n",
        "            if query_embedding in self.indexed_knowledge:\n",
        "                relevant_chunks.append(self.indexed_knowledge[query_embedding])\n",
        "\n",
        "        if not relevant_chunks:\n",
        "            print(\"  Retriever: No relevant chunks found.\")\n",
        "            return []\n",
        "        print(f\"  Retriever: Found {len(relevant_chunks)} relevant chunk(s).\")\n",
        "        return relevant_chunks\n",
        "\n",
        "# --- Simulate Generator (LLM) ---\n",
        "# Combines the retrieved information with the original query to generate a response.\n",
        "class Generator:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        if GOOGLE_API_KEY:\n",
        "            try:\n",
        "                self.model = genai.GenerativeModel(AgentConfig.LLM_MODEL_NAME)\n",
        "                print(f\"Generator (Gemini LLM) initialized with model: {AgentConfig.LLM_MODEL_NAME}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Generator: Error initializing Gemini model: {e}. LLM generation will be simulated.\")\n",
        "        else:\n",
        "            print(\"Generator (Simulated LLM) initialized without Gemini API key. LLM generation will be simulated.\")\n",
        "\n",
        "\n",
        "    def generate_response(self, original_query, retrieved_chunks):\n",
        "        \"\"\"\n",
        "        Generates a response using the real Gemini LLM, augmented with retrieved context.\n",
        "        \"\"\"\n",
        "        print(f\"  Generator: Combining query and retrieved chunks to generate response...\")\n",
        "\n",
        "        context_str = \"\\n\".join([f\"- {chunk}\" for chunk in retrieved_chunks]) if retrieved_chunks else \"No specific context retrieved.\"\n",
        "\n",
        "        full_prompt = (\n",
        "            f\"Original user query: '{original_query}'\\n\"\n",
        "            f\"Retrieved relevant information:\\n{context_str}\\n\\n\"\n",
        "            f\"Based on the original query and the retrieved information, provide a comprehensive answer.\"\n",
        "        )\n",
        "        print(\"\\n--- LLM Generation Prompt ---\")\n",
        "        print(f\"Prompt to LLM:\\n{full_prompt}\")\n",
        "\n",
        "        if self.model:\n",
        "            try:\n",
        "                response = self.model.generate_content([full_prompt])\n",
        "                if response.candidates:\n",
        "                    generated_text = response.candidates[0].content.parts[0].text\n",
        "                    return f\"Gemini LLM Response: {generated_text}\"\n",
        "                else:\n",
        "                    return f\"Gemini LLM Response: No content generated. {response.prompt_feedback}\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error calling Gemini LLM for generation: {e}. Falling back to simulated response.\")\n",
        "                # Fallback to simple simulated LLM response\n",
        "                if retrieved_chunks:\n",
        "                    return (\n",
        "                        f\"Simulated LLM Response (API Error): Based on your query '{original_query}' and the retrieved information:\\n\"\n",
        "                        f\"{context_str}\\n\\n\"\n",
        "                        f\"I can provide a simulated answer. How can I help you further?\"\n",
        "                    )\n",
        "                else:\n",
        "                    return f\"Simulated LLM Response (API Error): I couldn't find specific information for '{original_query}' in my knowledge base. Can you rephrase or provide more details?\"\n",
        "        else:\n",
        "            print(\"Warning: Gemini API not configured. Using simulated LLM response.\")\n",
        "            # Simple simulated LLM response if API is not configured\n",
        "            if retrieved_chunks:\n",
        "                return (\n",
        "                    f\"Simulated LLM Response: Based on your query '{original_query}' and the retrieved information:\\n\"\n",
        "                    f\"{context_str}\\n\\n\"\n",
        "                    f\"I can provide a simulated answer. How can I help you further?\"\n",
        "                )\n",
        "            else:\n",
        "                return f\"Simulated LLM Response: I couldn't find specific information for '{original_query}' in my knowledge base. Can you rephrase or provide more details?\"\n",
        "\n",
        "\n",
        "print(\"--- RAG POC Tutorial ---\")\n",
        "print(\"This POC demonstrates how RAG works by retrieving relevant information from a knowledge base\")\n",
        "print(\"and then using that information to augment the LLM's generation, leading to more informed responses.\")\n",
        "print(\"It highlights the use of external knowledge but also hints at challenges like context staleness\")\n",
        "print(\"if the knowledge base is not updated.\")\n",
        "print(\"\\nNote: For this script to work with the real Gemini LLM, ensure you have set\")\n",
        "print(\"your 'GEMINI' API key as an environment variable or in Google Colab secrets.\")\n",
        "print('\\n')\n",
        "\n",
        "# --- POC Demonstration ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Retrieval Augmented Generation (RAG) POC Tutorial ---\")\n",
        "    print(\"\\n--- POC Demonstration ---\")\n",
        "    print('\\n')\n",
        "\n",
        "    # 1. Initialize Retriever with Knowledge Base\n",
        "    retriever = Retriever(KNOWLEDGE_BASE)\n",
        "\n",
        "    # 2. Initialize Generator (Simulated LLM)\n",
        "    generator = Generator()\n",
        "\n",
        "    # --- Scenario 1: Query about RAG ---\n",
        "    print(\"\\n--- Scenario 1: User asks about RAG ---\")\n",
        "    user_query_1 = \"What is RAG?\"\n",
        "    print(f\"User Query: '{user_query_1}'\")\n",
        "    retrieved_chunks_1 = retriever.retrieve_chunks(user_query_1)\n",
        "    final_response_1 = generator.generate_response(user_query_1, retrieved_chunks_1)\n",
        "    print(f\"\\nFinal Response to User (Scenario 1):\\n{final_response_1}\")\n",
        "\n",
        "    # --- Scenario 2: Query about Newton ---\n",
        "    print(\"\\n--- Scenario 2: User asks about Newton ---\")\n",
        "    user_query_2 = \"Tell me about Isaac Newton.\"\n",
        "    print(f\"User Query: '{user_query_2}'\")\n",
        "    retrieved_chunks_2 = retriever.retrieve_chunks(user_query_2)\n",
        "    final_response_2 = generator.generate_response(user_query_2, retrieved_chunks_2)\n",
        "    print(f\"\\nFinal Response to User (Scenario 2):\\n{final_response_2}\")\n",
        "\n",
        "    # --- Scenario 3: Query about AI for flight planning (from user's context) ---\n",
        "    print(\"\\n--- Scenario 3: User asks about AI for flight planning ---\")\n",
        "    user_query_3 = \"How can AI help with flight planning?\"\n",
        "    print(f\"User Query: '{user_query_3}'\")\n",
        "    retrieved_chunks_3 = retriever.retrieve_chunks(user_query_3)\n",
        "    final_response_3 = generator.generate_response(user_query_3, retrieved_chunks_3)\n",
        "    print(f\"\\nFinal Response to User (Scenario 3):\\n{final_response_3}\")\n",
        "\n",
        "    # --- Scenario 4: Query with no direct match ---\n",
        "    print(\"\\n--- Scenario 4: User asks about something not in KB ---\")\n",
        "    user_query_4 = \"What is the capital of France?\"\n",
        "    print(f\"User Query: '{user_query_4}'\")\n",
        "    retrieved_chunks_4 = retriever.retrieve_chunks(user_query_4)\n",
        "    final_response_4 = generator.generate_response(user_query_4, retrieved_chunks_4)\n",
        "    print(f\"\\nFinal Response to User (Scenario 4):\\n{final_response_4}\")\n",
        "\n",
        "    print(\"\\n--- RAG POC Tutorial End ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6k_-ba1ltE-B",
        "outputId": "a64db08d-8dff-4803-f881-5886ebfd9a37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully using Colab Secrets.\n",
            "Gemini API configured with model: gemini-2.5-flash\n",
            "--- RAG POC Tutorial ---\n",
            "This POC demonstrates how RAG works by retrieving relevant information from a knowledge base\n",
            "and then using that information to augment the LLM's generation, leading to more informed responses.\n",
            "It highlights the use of external knowledge but also hints at challenges like context staleness\n",
            "if the knowledge base is not updated.\n",
            "\n",
            "Note: For this script to work with the real Gemini LLM, ensure you have set\n",
            "your 'GEMINI' API key as an environment variable or in Google Colab secrets.\n",
            "\n",
            "\n",
            "--- Retrieval Augmented Generation (RAG) POC Tutorial ---\n",
            "\n",
            "--- POC Demonstration ---\n",
            "\n",
            "\n",
            "Retriever initialized with knowledge base.\n",
            "Generator (Gemini LLM) initialized with model: gemini-2.5-flash\n",
            "\n",
            "--- Scenario 1: User asks about RAG ---\n",
            "User Query: 'What is RAG?'\n",
            "  Retriever: Searching for relevant chunks for query: 'What is RAG?'...\n",
            "  Retriever: Found 1 relevant chunk(s).\n",
            "  Generator: Combining query and retrieved chunks to generate response...\n",
            "\n",
            "--- LLM Generation Prompt ---\n",
            "Prompt to LLM:\n",
            "Original user query: 'What is RAG?'\n",
            "Retrieved relevant information:\n",
            "- Artificial intelligence (AI) has a long history, dating back to the 1950s with early work on symbolic AI.\n",
            "\n",
            "Based on the original query and the retrieved information, provide a comprehensive answer.\n",
            "\n",
            "Final Response to User (Scenario 1):\n",
            "Gemini LLM Response: RAG stands for **Retrieval Augmented Generation**. It's an advanced artificial intelligence (AI) technique designed to enhance the capabilities of large language models (LLMs) by giving them access to up-to-date, external information, thereby addressing common limitations like generating outdated or incorrect information (often called 'hallucinations').\n",
            "\n",
            "Here's a breakdown of what RAG is and how it works:\n",
            "\n",
            "**What Problem Does RAG Solve?**\n",
            "Large Language Models (LLMs) are trained on vast datasets of text and code, but their knowledge is limited to their training cut-off date. This means they can't access real-time information or specific, private, or niche data (like a company's internal documents). When asked questions about information outside their training data, LLMs might:\n",
            "1.  State they don't know.\n",
            "2.  \"Hallucinate\" or make up plausible-sounding but incorrect information.\n",
            "\n",
            "RAG aims to solve this by providing the LLM with relevant, external information *at the time of the query*.\n",
            "\n",
            "**How RAG Works:**\n",
            "\n",
            "1.  **Retrieval:** When a user poses a query, the RAG system first searches a designated knowledge base (which can be anything from a database of company documents, web pages, or specialized articles) for information relevant to the query. This often involves converting text into numerical representations (embeddings) and performing a vector similarity search to find the most related pieces of information.\n",
            "2.  **Augmentation:** The retrieved relevant information (e.g., specific paragraphs, articles, or data points) is then added to the original user query. This combined package forms a much richer and more context-specific prompt.\n",
            "3.  **Generation:** This augmented prompt is then fed into a large language model. Instead of relying solely on its pre-trained knowledge (which might be limited or out-of-date), the LLM uses the *provided context* from the retrieval step to formulate its answer.\n",
            "\n",
            "**Benefits of RAG:**\n",
            "\n",
            "*   **Accuracy:** Grounds the LLM's responses in factual, up-to-date information.\n",
            "*   **Reduced Hallucinations:** Minimizes the LLM's tendency to invent information.\n",
            "*   **Source Attribution:** Enables the system to cite the sources from which it retrieved information, increasing trustworthiness.\n",
            "*   **Up-to-Date Information:** Allows LLMs to access and utilize information that was not part of their original training data, keeping responses current without retraining the entire model.\n",
            "*   **Cost-Effective:** It's often much cheaper and faster to implement RAG than to continually fine-tune or retrain a large LLM on new data.\n",
            "\n",
            "**Connection to AI History:**\n",
            "While early work on artificial intelligence (AI) in the 1950s focused on symbolic AI and rule-based systems, RAG represents a significant evolution in the field. It combines the power of modern neural networks (LLMs) with traditional information retrieval methods, showcasing how AI techniques continue to advance to solve complex real-world problems that demand both vast general knowledge and specific, timely information. It's a hybrid approach that leverages the strengths of different AI paradigms.\n",
            "\n",
            "**Applications:**\n",
            "RAG is widely used in applications requiring highly accurate and context-specific responses, such as:\n",
            "*   Intelligent chatbots for customer service or technical support.\n",
            "*   Enterprise knowledge management systems, allowing employees to query internal documents.\n",
            "*   Research tools that synthesize information from vast datasets.\n",
            "*   Personalized content generation based on specific user profiles or real-time data.\n",
            "\n",
            "--- Scenario 2: User asks about Newton ---\n",
            "User Query: 'Tell me about Isaac Newton.'\n",
            "  Retriever: Searching for relevant chunks for query: 'Tell me about Isaac Newton.'...\n",
            "  Retriever: Found 1 relevant chunk(s).\n",
            "  Generator: Combining query and retrieved chunks to generate response...\n",
            "\n",
            "--- LLM Generation Prompt ---\n",
            "Prompt to LLM:\n",
            "Original user query: 'Tell me about Isaac Newton.'\n",
            "Retrieved relevant information:\n",
            "- Artificial intelligence (AI) has a long history, dating back to the 1950s with early work on symbolic AI.\n",
            "\n",
            "Based on the original query and the retrieved information, provide a comprehensive answer.\n",
            "\n",
            "Final Response to User (Scenario 2):\n",
            "Gemini LLM Response: The information provided about the history of Artificial Intelligence is not relevant to your query about Isaac Newton.\n",
            "\n",
            "However, I can tell you about Isaac Newton based on general knowledge:\n",
            "\n",
            "**Sir Isaac Newton (1642–1727)** was an English mathematician, physicist, astronomer, alchemist, theologian, and author widely recognised as one of the most influential scientists of all time. He was a key figure in the scientific revolution.\n",
            "\n",
            "Here are some of his most significant contributions and aspects of his life:\n",
            "\n",
            "1.  **Laws of Motion and Universal Gravitation:** His most famous work, *Philosophiæ Naturalis Principia Mathematica* (Mathematical Principles of Natural Philosophy), published in 1687, laid the foundations for classical mechanics. In it, he formulated the three laws of motion and the law of universal gravitation, which described the motion of celestial bodies and objects on Earth with unprecedented accuracy.\n",
            "2.  **Calculus:** Newton independently developed differential and integral calculus (a branch of mathematics dealing with rates of change and accumulation of quantities) around the same time as Gottfried Wilhelm Leibniz, leading to a long-standing controversy over who invented it first. Calculus is fundamental to much of modern science and engineering.\n",
            "3.  **Optics:** Newton made groundbreaking discoveries about the nature of light. He demonstrated that white light is composed of a spectrum of colors and designed and built the first reflecting telescope. His work on light and color was seminal.\n",
            "4.  **Beyond Science:** Besides his scientific work, Newton was also a prominent figure in public life. He served as the Master of the Royal Mint, where he played a crucial role in reforming England's currency. He was also a Member of Parliament and was knighted by Queen Anne in 1705.\n",
            "5.  **Alchemy and Theology:** Less known, but significant parts of his life, were his extensive studies in alchemy and theology. He wrote more on these subjects than on science, though these writings were largely kept private during his lifetime.\n",
            "\n",
            "Newton's work fundamentally shaped our understanding of the universe and remains central to physics and mathematics. He is widely considered one of the greatest minds in human history.\n",
            "\n",
            "--- Scenario 3: User asks about AI for flight planning ---\n",
            "User Query: 'How can AI help with flight planning?'\n",
            "  Retriever: Searching for relevant chunks for query: 'How can AI help with flight planning?'...\n",
            "  Retriever: Found 1 relevant chunk(s).\n",
            "  Generator: Combining query and retrieved chunks to generate response...\n",
            "\n",
            "--- LLM Generation Prompt ---\n",
            "Prompt to LLM:\n",
            "Original user query: 'How can AI help with flight planning?'\n",
            "Retrieved relevant information:\n",
            "- Artificial intelligence (AI) has a long history, dating back to the 1950s with early work on symbolic AI.\n",
            "\n",
            "Based on the original query and the retrieved information, provide a comprehensive answer.\n",
            "\n",
            "Final Response to User (Scenario 3):\n",
            "Gemini LLM Response: While the provided information highlights AI's long history dating back to the 1950s, its modern applications offer significant advancements in practical fields like flight planning. AI does not just have a history; it has a dynamic present and future in aviation.\n",
            "\n",
            "Here's how AI can comprehensively help with flight planning:\n",
            "\n",
            "1.  **Optimized Route Selection:**\n",
            "    *   **Efficiency:** AI algorithms can analyze vast amounts of data (wind patterns, terrain, restricted airspace, NOTAMs, aircraft performance characteristics, air traffic flow) to calculate the most efficient flight paths. This optimizes for factors like shortest distance, least fuel consumption, or fastest arrival time, considering real-time variables.\n",
            "    *   **Dynamic Adjustments:** Unlike static planning, AI can continuously monitor conditions and suggest real-time route adjustments in response to unexpected events like sudden weather changes, air traffic congestion, or runway closures.\n",
            "\n",
            "2.  **Advanced Weather Forecasting and Avoidance:**\n",
            "    *   **High Accuracy:** AI models can process complex meteorological data from various sources (satellites, radar, ground sensors) to provide highly accurate, localized, and predictive weather forecasts.\n",
            "    *   **Hazard Avoidance:** This allows for proactive route adjustments to avoid turbulence, thunderstorms, icing conditions, strong head/crosswinds, or volcanic ash, significantly enhancing safety and passenger comfort.\n",
            "\n",
            "3.  **Fuel Efficiency and Environmental Impact:**\n",
            "    *   **Precise Calculation:** By precisely calculating optimal altitudes, speeds, and routes based on current conditions and specific aircraft performance models, AI helps minimize fuel burn.\n",
            "    *   **Reduced Emissions:** This not only leads to significant operational cost savings for airlines but also contributes to a reduction in carbon emissions, supporting environmental goals.\n",
            "\n",
            "4.  **Risk Assessment and Safety Enhancement:**\n",
            "    *   **Conflict Prediction:** AI can identify potential hazards by cross-referencing flight plans with historical data, real-time sensor information, and regulatory requirements. It can flag potential conflicts with other aircraft, unsafe altitudes, or areas of high congestion.\n",
            "    *   **Alerting Systems:** It can provide immediate alerts to planners and pilots about potential risks, enabling them to make informed decisions.\n",
            "\n",
            "5.  **Air Traffic Management (ATM) Integration:**\n",
            "    *   **Congestion Prediction:** AI can interact with ATM systems to predict congestion hotspots at airports or in specific airspace sectors.\n",
            "    *   **Flow Optimization:** It can help optimize arrival and departure sequences, manage slot times more effectively, and reduce holding patterns, leading to smoother air traffic flow and less delays.\n",
            "\n",
            "6.  **Crew and Resource Optimization:**\n",
            "    *   **Scheduling:** While not directly \"flight planning,\" AI can assist in optimizing crew scheduling to ensure compliance with rest regulations and maximize efficiency, which indirectly impacts the feasibility and cost of a flight plan.\n",
            "    *   **Aircraft Maintenance:** AI can predict maintenance needs, ensuring that aircraft are available and safe for their planned flights.\n",
            "\n",
            "7.  **Data Analysis and Predictive Learning:**\n",
            "    *   **Continuous Improvement:** AI systems can continuously learn from past flight data, identifying patterns, inefficiencies, and successful strategies. This iterative learning process allows the system to refine its planning capabilities over time, leading to even more optimized and safer flights in the future.\n",
            "\n",
            "In essence, AI transforms flight planning from a complex, manually intensive process into a highly automated, dynamic, and optimized system, leading to enhanced safety, significant cost savings, and improved operational efficiency for airlines and air traffic controllers.\n",
            "\n",
            "--- Scenario 4: User asks about something not in KB ---\n",
            "User Query: 'What is the capital of France?'\n",
            "  Retriever: Searching for relevant chunks for query: 'What is the capital of France?'...\n",
            "  Retriever: Found 1 relevant chunk(s).\n",
            "  Generator: Combining query and retrieved chunks to generate response...\n",
            "\n",
            "--- LLM Generation Prompt ---\n",
            "Prompt to LLM:\n",
            "Original user query: 'What is the capital of France?'\n",
            "Retrieved relevant information:\n",
            "- Artificial intelligence (AI) has a long history, dating back to the 1950s with early work on symbolic AI.\n",
            "\n",
            "Based on the original query and the retrieved information, provide a comprehensive answer.\n",
            "\n",
            "Final Response to User (Scenario 4):\n",
            "Gemini LLM Response: The capital of France is Paris.\n",
            "\n",
            "The retrieved information about the history of artificial intelligence (AI) is not relevant to your question about the capital of France.\n",
            "\n",
            "--- RAG POC Tutorial End ---\n"
          ]
        }
      ]
    }
  ]
}
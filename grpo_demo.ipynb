{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/grpo_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3mEI_G6Bvru"
      },
      "outputs": [],
      "source": [
        "# Install condacolab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# Restart runtime here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda --version"
      ],
      "metadata": {
        "id": "lqOVstC9HNQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d0de57-dc25-494f-a09e-79f3ea296a93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conda 24.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda env list"
      ],
      "metadata": {
        "id": "neBiBsbIHQfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7668774c-0b84-4fb7-adbb-86b559b112d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# conda environments:\n",
            "#\n",
            "base                   /usr/local\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "jzb7vLhWHT9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d823233a-85b6-436f-90be-579e801d9da1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 27 16:51:25 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0              47W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQvZczfAB7ia"
      },
      "outputs": [],
      "source": [
        "# Update Conda (optional but recommended)\n",
        "!conda update -n base -c defaults conda\n",
        "\n",
        "# Create and activate conda environment\n",
        "!conda create -n openr1 python=3.11 -y\n",
        "!conda activate openr1\n",
        "\n",
        "# Clone the Open-R1 repository:\n",
        "!git clone https://github.com/huggingface/open-r1.git\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/open-r1\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install -e \".[dev]\"\n",
        "!pip install vllm==0.6.6.post1 -q\n",
        "!pip install vllm==0.6.6.post1 --extra-index-url https://download.pytorch.org/whl/cu121 -q\n",
        "\n",
        "\n",
        "# Unset WANDB_DISABLED if it exists\n",
        "import os\n",
        "if 'WANDB_DISABLED' in os.environ:\n",
        "    del os.environ['WANDB_DISABLED']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hzl3qEJ-dh0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GOKSBsoiB_Px"
      },
      "outputs": [],
      "source": [
        "!accelerate launch --config_file /content/open-r1/configs/zero3.yaml /content/open-r1/src/open_r1/grpo.py \\\n",
        "    --output_dir DeepSeek-R1-Distill-Qwen-7B-GRPO \\\n",
        "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
        "    --dataset_name AI-MO/NuminaMath-TIR \\\n",
        "    --max_prompt_length 256 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --logging_steps 10 \\\n",
        "    --bf16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes -q\n",
        "!pip install datasets -q\n",
        "!pip install transformers -q\n",
        "!pip install torch -q\n",
        "!pip install accelerate -q\n",
        "!pip install tqdm -q"
      ],
      "metadata": {
        "id": "b-qrC6xxLvG4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "0xuwVtXNsD5E"
      }
    },
    {
      "source": [
        "# Unset WANDB_DISABLED if it exists\n",
        "import os\n",
        "if 'WANDB_DISABLED' in os.environ:\n",
        "    del os.environ['WANDB_DISABLED']\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import bitsandbytes as bnb\n",
        "import os\n",
        "from accelerate import Accelerator  # Import Accelerator\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "\n",
        "# Parameters\n",
        "output_dir = \"DeepSeek-R1-Distill-Qwen-7B-GRPO\"\n",
        "max_prompt_length = 256\n",
        "per_device_train_batch_size = 1  # Not explicitly used in this simplified example\n",
        "gradient_accumulation_steps = 16  # Not explicitly used in this simplified example\n",
        "logging_steps = 10\n",
        "\n",
        "# Function to run training within a subprocess (or directly)\n",
        "def train_func(args):\n",
        "    # Initialize Accelerator\n",
        "    accelerator = Accelerator(mixed_precision=\"bf16\", device_placement=False, split_batches=False)\n",
        "\n",
        "    # 1. Load the Dataset\n",
        "    dataset = load_dataset(\"AI-MO/NuminaMath-TIR\")\n",
        "\n",
        "    # 2. Load the DeepSeek-R1-Distill-Qwen-7B model with 4-bit quantization\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading\n",
        "    )\n",
        "\n",
        "    # Load the model without specifying a device_map\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
        "        quantization_config=quantization_config,\n",
        "    )\n",
        "\n",
        "    # Explicitly move the entire model to GPU 0\n",
        "    model.to(accelerator.device)\n",
        "\n",
        "    output_dim = 10  # Or the appropriate number of actions for your environment\n",
        "\n",
        "\n",
        "    # This should be done once, before you initialize the PolicyNetwork\n",
        "    env = SimpleEnvironment(dataset, tokenizer, model, accelerator, output_dim)\n",
        "    state_embedding = env.get_state()  # Get a sample state embedding\n",
        "    input_dim = state_embedding.shape[-1]  # Now get the input size\n",
        "    output_dim = 10  # Assume 10 possible actions (adapt to your task)\n",
        "\n",
        "\n",
        "    # Create models, optimizer\n",
        "    #policy_network = PolicyNetwork(input_dim, output_dim, accelerator.device)  # Pass device\n",
        "\n",
        "    policy_network = PolicyNetwork(state_embedding.shape[-1], output_dim, accelerator.device) # Using state_embedding.shape[-1] to get actual input size\n",
        "    value_network = ValueNetwork(input_dim, accelerator.device)  # Pass device\n",
        "\n",
        "    # In your GRPOTrainer class or where you define the optimizer\n",
        "    optimizer = optim.Adam(policy_network.parameters(), lr=0.001)  # Try a smaller learning rate\n",
        "\n",
        "\n",
        "    # Prepare with accelerate (moves models and optimizer to device)\n",
        "    policy_network, value_network, optimizer, env = accelerator.prepare(\n",
        "       policy_network, value_network, optimizer, env\n",
        "   )\n",
        "\n",
        "\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Pass the model to GRPOTrainer\n",
        "    trainer = GRPOTrainer(env, policy_network, value_network, optimizer, learning_rate=0.01, accelerator=accelerator, tokenizer=tokenizer, model=model)\n",
        "\n",
        "    trainer.train(num_epochs=1, num_trajectories_per_epoch=10)\n",
        "\n",
        "    # Save the model using the standard PyTorch save method\n",
        "    torch.save(accelerator.unwrap_model(policy_network).state_dict(), os.path.join(output_dir, 'policy_network.pth'))\n",
        "\n",
        "# 3. Define Policy Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, device):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        # Update the input dimension for fc1 to match the state embedding size\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "\n",
        "        self.relu = nn.ReLU()  # Using ReLU activation\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.device = device\n",
        "\n",
        "        # Apply weight initialization (example using Kaiming/He)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_uniform_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device).type(torch.float32)\n",
        "        x = self.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=-1)  # Softmax for probability distribution\n",
        "\n",
        "# 4. Define Value Network\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, device):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.device = device  # Store the device\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 5. Environment (simplified example - assumes 'text' and 'label' fields in dataset)\n",
        "class SimpleEnvironment:\n",
        "    def __init__(self, dataset, tokenizer, model, accelerator, output_dim):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.accelerator = accelerator  # Store accelerator\n",
        "        self.current_index = 0\n",
        "        self.episode_length = 20  # Set the desired episode length\n",
        "        self.output_dim = output_dim  # Store output_dim as an attribute\n",
        "        # Get initial state embedding and initialize PolicyNetwork with the correct input_dim\n",
        "        initial_state_embedding = self.get_state()  # Call get_state() to get the initial embedding\n",
        "        self.input_dim = initial_state_embedding.shape[-1]  # Store the initial input dimension\n",
        "        self.policy_network = PolicyNetwork(self.input_dim, self.output_dim, self.accelerator.device)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_index = 0\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        text_description = self.dataset[\"train\"][self.current_index].get(\"text\", \"\")\n",
        "        text_description = text_description[:max_prompt_length]\n",
        "\n",
        "        inputs = self.tokenizer(text_description, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, output_hidden_states=True)\n",
        "\n",
        "        # In the SimpleEnvironment.get_state() function:\n",
        "        state_embedding = outputs.hidden_states[-1][:, 0, :].to(self.accelerator.device)\n",
        "\n",
        "        # Define extra_features before using it\n",
        "        extra_features = torch.tensor([self.current_index / self.episode_length], device=self.accelerator.device)  # Example extra feature\n",
        "\n",
        "        # Add an extra dimension to extra_features using unsqueeze() to match state_embedding shape\n",
        "        extra_features = extra_features.unsqueeze(0).repeat(state_embedding.shape[0], 1)  # Repeat for all elements in batch\n",
        "\n",
        "        # **Change is here:**\n",
        "        state_embedding = torch.cat([state_embedding, extra_features], dim=-1)\n",
        "\n",
        "        # Update input_dim based on the final shape of state_embedding\n",
        "        input_dim = state_embedding.shape[-1]\n",
        "\n",
        "        # Re-initialize the PolicyNetwork if the input dimension has changed\n",
        "        if hasattr(self, 'policy_network') and self.policy_network.fc1.in_features != input_dim:\n",
        "            self.policy_network = PolicyNetwork(input_dim, self.output_dim, self.accelerator.device)\n",
        "\n",
        "        return state_embedding.to(torch.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        #print(\"Entering env.step()\")  # Print at the beginning\n",
        "        target = self.dataset[\"train\"][self.current_index].get(\"label\", 0)  # Assume 0 if 'label' missing\n",
        "        #print(\"Target obtained:\", target)  # Print target value\n",
        "\n",
        "        #reward = 1.0 if action == target else -1.0\n",
        "        #print(\"Reward calculated:\", reward)  # Print reward value\n",
        "\n",
        "        # Reward calculation using self.output_dim\n",
        "        if action == target:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = -0.1 + 0.9 * (1 - abs(action - target) / (self.output_dim - 1))\n",
        "\n",
        "        self.current_index += 1\n",
        "        done = self.current_index >= self.episode_length  # Check if episode length is reached\n",
        "        next_state = self.get_state() if not done else None\n",
        "\n",
        "        #print(\"Exiting env.step()\")  # Print before returning\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "# 6. Trajectory Collection\n",
        "import torch\n",
        "\n",
        "def collect_trajectories(env, policy_network, num_trajectories, accelerator):\n",
        "    trajectories = []\n",
        "    for _ in range(num_trajectories):\n",
        "        state = env.reset()\n",
        "        trajectory = []\n",
        "        while True:\n",
        "            # Move policy_network and state to the correct device\n",
        "            policy_network = policy_network.to(accelerator.device)\n",
        "            state = state.to(accelerator.device)\n",
        "\n",
        "\n",
        "            # Inside the collect_trajectories() function or train() function\n",
        "            action_probs = policy_network(state)\n",
        "            #print(f\"Action probabilities: {action_probs}\")\n",
        "\n",
        "\n",
        "\n",
        "            # Check for invalid probabilities (e.g., NaN or probabilities not summing to 1)\n",
        "            if not torch.isfinite(action_probs).all() or not torch.allclose(action_probs.sum(), torch.tensor(1.0, device=action_probs.device)):\n",
        "                # If invalid, assign uniform probabilities to all actions\n",
        "                action_probs = torch.ones_like(action_probs) / action_probs.shape[-1]\n",
        "\n",
        "            # Sample an action from the probability distribution\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "\n",
        "            # Take a step in the environment\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Move next_state to the correct device if it's not None\n",
        "            if next_state is not None:\n",
        "                next_state = next_state.to(accelerator.device)\n",
        "\n",
        "            # Append the current state, action, and reward to the trajectory\n",
        "            trajectory.append((state, action, reward))\n",
        "\n",
        "            # Check if the episode is done\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            # Update the current state for the next step\n",
        "            state = next_state\n",
        "\n",
        "        # Append the completed trajectory to the list of trajectories\n",
        "        trajectories.append(trajectory)\n",
        "\n",
        "    # Return the collected trajectories\n",
        "    return trajectories\n",
        "\n",
        "# 7. GRPO Update (simplified - needs customization)\n",
        "import torch\n",
        "\n",
        "def update_policy(policy_network, value_network, trajectories, optimizer, accelerator):\n",
        "    policy_loss_total = 0\n",
        "\n",
        "    # Move both networks to the accelerator device before processing trajectories\n",
        "    policy_network = policy_network.to(accelerator.device)\n",
        "    value_network = value_network.to(accelerator.device)\n",
        "\n",
        "    for trajectory in trajectories:\n",
        "        # Calculate total rewards for each step in the trajectory (using a simple Monte Carlo estimate)\n",
        "        rewards = [traj[2] for traj in trajectory]  # List of rewards\n",
        "        total_rewards = [sum(rewards[i:]) for i in range(len(rewards))]  # Calculate cumulative rewards\n",
        "\n",
        "        for i in range(len(trajectory)):\n",
        "            state, action, reward = trajectory[i]\n",
        "\n",
        "            # 1. Estimate advantage (using a simple Monte Carlo estimate)\n",
        "            advantage = total_rewards[i] - value_network(state).item()\n",
        "\n",
        "            # 2. Calculate policy gradient\n",
        "            action_probs = policy_network(state)\n",
        "            log_prob = torch.log(action_probs[0][action])  # Assuming action_probs is a batch of size 1\n",
        "            policy_loss = -advantage * log_prob\n",
        "\n",
        "            # Accumulate policy loss\n",
        "            policy_loss_total += policy_loss\n",
        "\n",
        "            # Debugging print statements (placed within the loop)\n",
        "            print(f\"State: {state}\")  # Check if state is changing\n",
        "            print(f\"Action Probabilities: {action_probs}\")  # Check if probabilities are always the same\n",
        "            print(f\"Advantage: {advantage}\")  # Check if advantage is meaningful\n",
        "            print(f\"Log Probability: {log_prob}\")  # Check if log probability is valid\n",
        "            print(f\"Policy Loss (before accumulation): {policy_loss}\")  # Check if policy loss is non-zero\n",
        "\n",
        "    # Debugging Prints: Total Policy Loss\n",
        "    print(f\"Total policy loss: {policy_loss_total}\")\n",
        "\n",
        "    # Perform the optimization step\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss_total.backward(retain_graph=True)  # Backpropagate the total loss, retain_graph=True\n",
        "\n",
        "    # Print gradients of policy network parameters\n",
        "    for name, param in policy_network.named_parameters():\n",
        "        print(f\"Gradient of {name}: {param.grad}\")\n",
        "\n",
        "    optimizer.step()  # Update the policy network parameters\n",
        "\n",
        "    return policy_loss_total\n",
        "\n",
        "\n",
        "\n",
        "# 8. GRPO Trainer\n",
        "class GRPOTrainer:\n",
        "    def __init__(self, env, policy_network, value_network, optimizer, learning_rate, accelerator, tokenizer, model):\n",
        "        self.env = env\n",
        "        self.policy_network = policy_network\n",
        "        self.value_network = value_network\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.global_step = 0\n",
        "        self.policy_loss = 0\n",
        "        self.accelerator = accelerator\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "        # Prepare with accelerate\n",
        "        self.policy_network, self.value_network, self.optimizer, self.env = accelerator.prepare(\n",
        "            policy_network, value_network, optimizer, env\n",
        "        )\n",
        "\n",
        "        # Create evaluation dataset and dataloader\n",
        "        eval_dataset = load_dataset(\"AI-MO/NuminaMath-TIR\", split=\"test\")\n",
        "        self.eval_dataloader = torch.utils.data.DataLoader(\n",
        "            eval_dataset, batch_size=1,  # Reduced batch size\n",
        "            collate_fn=lambda examples: self.process_batch(examples)\n",
        "        )\n",
        "\n",
        "    def process_batch(self, examples):\n",
        "        # Extract the relevant fields for tokenization, joining list elements if necessary\n",
        "        texts = [\n",
        "            example['problem'] +\n",
        "            (example['solution'] if isinstance(example['solution'], str) else ' '.join(example['solution'])) +\n",
        "            (example['messages'] if isinstance(example['messages'], str) else ' '.join(str(item) for item in example['messages']))\n",
        "            for example in examples\n",
        "        ]\n",
        "\n",
        "        # Tokenize the extracted texts\n",
        "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=self.model.config.max_position_embeddings)\n",
        "\n",
        "        # Other necessary batching processes\n",
        "        labels = [example.get('label', 0) for example in examples]\n",
        "\n",
        "        # Correct the input labels to match input_ids shape\n",
        "        inputs['labels'] = torch.tensor(labels).unsqueeze(1).repeat(1, inputs['input_ids'].shape[1]) # shape (batch_size, seq_len)\n",
        "        inputs['labels'] = torch.where(inputs['attention_mask'] == 1, inputs['labels'], -100) # shape (batch_size, seq_len)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def calculate_accuracy(self, predictions, ground_truth):\n",
        "        # Calculate accuracy\n",
        "        accuracy = np.mean(predictions == ground_truth)  # Removed argmax since predictions are already single values\n",
        "        return accuracy\n",
        "\n",
        "    def collect_trajectories(self, num_trajectories):\n",
        "        return collect_trajectories(self.env, self.policy_network, num_trajectories, self.accelerator)\n",
        "\n",
        "    def update_policy(self, trajectories):\n",
        "        self.policy_loss = update_policy(self.policy_network, self.value_network, trajectories, self.optimizer, self.accelerator)\n",
        "\n",
        "    def train(self, num_epochs, num_trajectories_per_epoch):\n",
        "        total_trajectories = num_epochs * num_trajectories_per_epoch\n",
        "        for epoch in range(num_epochs):\n",
        "            for trajectory_index in tqdm(range(num_trajectories_per_epoch), desc=f\"Epoch {epoch + 1}/{num_epochs}\", total=total_trajectories, position=0, leave=True):\n",
        "                # Collect trajectories and update policy within the trajectory loop\n",
        "                trajectories = self.collect_trajectories(1)\n",
        "\n",
        "                with self.accelerator.accumulate(self.policy_network):\n",
        "                    self.policy_loss = update_policy(self.policy_network, self.value_network, trajectories, self.optimizer, self.accelerator)\n",
        "\n",
        "                    # Add print statement to check if policy_loss is detached\n",
        "                    print(f\"policy_loss.is_leaf: {self.policy_loss.is_leaf}, policy_loss.requires_grad: {self.policy_loss.requires_grad}\")\n",
        "\n",
        "                    self.accelerator.backward(self.policy_loss, retain_graph=True)  # retain_graph=True added\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    print(f\"Policy Loss: {self.policy_loss.item()}\")\n",
        "\n",
        "            # Accuracy calculation and logging (POC) - performed after each epoch\n",
        "            all_predictions = []\n",
        "            all_ground_truth_labels = []\n",
        "\n",
        "            for batch in self.eval_dataloader:\n",
        "                with torch.no_grad():\n",
        "                    inputs = {k: v.to(self.accelerator.device) for k, v in batch.items()}\n",
        "                    outputs = self.model(**inputs, output_hidden_states=True)\n",
        "                    states = outputs.hidden_states[-1][:, 0, :].to(torch.float32).to(self.accelerator.device)\n",
        "\n",
        "                ground_truth_labels = batch['labels'].to(self.accelerator.device)\n",
        "                self.policy_network = self.policy_network.to(self.accelerator.device)\n",
        "\n",
        "                for state, label_sequence in zip(states, ground_truth_labels):\n",
        "                    with torch.no_grad():\n",
        "                        #prediction = self.policy_network(state.squeeze(0))\n",
        "                        prediction = self.policy_network(state.unsqueeze(0)) # Add unsqueeze(0) here\n",
        "\n",
        "\n",
        "\n",
        "                    batch_prediction = prediction.argmax(dim=-1).item()\n",
        "                    ground_truth_label = label_sequence[0].item()\n",
        "\n",
        "                    all_predictions.append(batch_prediction)\n",
        "                    all_ground_truth_labels.append(ground_truth_label)\n",
        "\n",
        "            accuracy = self.calculate_accuracy(np.array(all_predictions), np.array(all_ground_truth_labels))\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}, Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "# Entry point for your script\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a namespace with the desired values\n",
        "    import argparse\n",
        "    args = argparse.Namespace(mixed_precision=\"bf16\")  # Add other arguments as needed\n",
        "\n",
        "    train_func(args)  # Call the training function"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vPb7LlCGSScy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION"
      ],
      "metadata": {
        "id": "e3jyKT-fqoAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!export MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
        "#!export MODEL_ARGS=\"pretrained=$MODEL,dtype=float16,max_model_length=32768,gpu_memory_utilisation=0.8\"\n",
        "#!export TASK=aime24\n",
        "#!export OUTPUT_DIR=data/evals/$MODEL\n",
        "\n",
        "#!lighteval vllm $MODEL_ARGS \"custom|$TASK|0|0\" \\\n",
        "#    --custom-tasks src/open_r1/evaluate.py \\\n",
        "#    --use-chat-template \\\n",
        "#    --system-prompt=\"Please reason step by step, and put your final answer within \\boxed{}.\" \\\n",
        "#    --output-dir $OUTPUT_DIR"
      ],
      "metadata": {
        "id": "Vqzely8GqmkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION - python"
      ],
      "metadata": {
        "id": "M_E9fg_ofHMc"
      }
    },
    {
      "source": [
        "accelerator = Accelerator(mixed_precision=\"bf16\", device_placement=False, split_batches=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
        "\n",
        "# Load the model with quantization config (as in training)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "model.to(accelerator.device)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"AI-MO/NuminaMath-TIR\")\n",
        "\n",
        "# Create the environment\n",
        "env = SimpleEnvironment(dataset, tokenizer, model, accelerator)  # Pass 'model' here\n",
        "\n",
        "# Define input and output dimensions for networks\n",
        "input_dim = model.config.hidden_size  # Use 'model' here\n",
        "output_dim = 10  # Or the appropriate number of actions for your environment\n",
        "\n",
        "# Create policy and value networks (as before)\n",
        "policy_network = PolicyNetwork(input_dim, output_dim, accelerator.device)\n",
        "value_network = ValueNetwork(input_dim, accelerator.device)\n",
        "\n",
        "# *** Load the fine-tuned policy network ***\n",
        "policy_network.load_state_dict(torch.load(\"/content/DeepSeek-R1-Distill-Qwen-7B-GRPO/policy_network.pth\"))\n",
        "\n",
        "# Create optimizer (as before)\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create the GRPOTrainer instance\n",
        "trainer = GRPOTrainer(env, policy_network, value_network, optimizer, learning_rate, accelerator, tokenizer, model)  # Pass 'model' here\n",
        "\n",
        "# Perform evaluation\n",
        "trainer.evaluate()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "072edb5907a44ee1bed9632633e99c9c",
            "be3e114ecc464bbcb4fcfcf9816fd684",
            "26f37d522fe74a18bc7d10b36cdcd82b",
            "785c6a9af2a541389bdf4462893eb865",
            "76de5f5c83324213947f12adcd65d12e",
            "852102e37b21432bb4b4cbedfed68f9f",
            "38c97af39ded4dbe999d12e1e069d5df",
            "f473770ca8f94f6abf6eb8844a8cae4b",
            "a0705f240be64b8fa414b616256779ac",
            "fc8e0a0a14994887b9e78430b6ca89ee",
            "1bddd234683f423bbdd8d7ca70649334"
          ]
        },
        "id": "5iZOoNdZe535",
        "outputId": "f26c3b01-5ebd-4aaf-e07f-b66f641e9956"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "072edb5907a44ee1bed9632633e99c9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-10c24dfe90a7>:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  policy_network.load_state_dict(torch.load(\"/content/DeepSeek-R1-Distill-Qwen-7B-GRPO/policy_network.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPdcuTKGWHNZnLU15R5KYuZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "072edb5907a44ee1bed9632633e99c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be3e114ecc464bbcb4fcfcf9816fd684",
              "IPY_MODEL_26f37d522fe74a18bc7d10b36cdcd82b",
              "IPY_MODEL_785c6a9af2a541389bdf4462893eb865"
            ],
            "layout": "IPY_MODEL_76de5f5c83324213947f12adcd65d12e"
          }
        },
        "be3e114ecc464bbcb4fcfcf9816fd684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_852102e37b21432bb4b4cbedfed68f9f",
            "placeholder": "​",
            "style": "IPY_MODEL_38c97af39ded4dbe999d12e1e069d5df",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "26f37d522fe74a18bc7d10b36cdcd82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f473770ca8f94f6abf6eb8844a8cae4b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0705f240be64b8fa414b616256779ac",
            "value": 2
          }
        },
        "785c6a9af2a541389bdf4462893eb865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc8e0a0a14994887b9e78430b6ca89ee",
            "placeholder": "​",
            "style": "IPY_MODEL_1bddd234683f423bbdd8d7ca70649334",
            "value": " 2/2 [00:08&lt;00:00,  4.32s/it]"
          }
        },
        "76de5f5c83324213947f12adcd65d12e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852102e37b21432bb4b4cbedfed68f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38c97af39ded4dbe999d12e1e069d5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f473770ca8f94f6abf6eb8844a8cae4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0705f240be64b8fa414b616256779ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc8e0a0a14994887b9e78430b6ca89ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bddd234683f423bbdd8d7ca70649334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
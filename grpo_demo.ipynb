{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/grpo_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3mEI_G6Bvru"
      },
      "outputs": [],
      "source": [
        "# Install condacolab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# Restart runtime here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda --version"
      ],
      "metadata": {
        "id": "lqOVstC9HNQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d0de57-dc25-494f-a09e-79f3ea296a93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conda 24.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda env list"
      ],
      "metadata": {
        "id": "neBiBsbIHQfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7668774c-0b84-4fb7-adbb-86b559b112d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# conda environments:\n",
            "#\n",
            "base                   /usr/local\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "jzb7vLhWHT9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d823233a-85b6-436f-90be-579e801d9da1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 27 16:51:25 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0              47W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQvZczfAB7ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6c7c3e-8f69-4b1f-aa99-515bc6aff012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - defaults\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.2\n",
            "    latest version: 25.1.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - conda\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2024.12.31 |       h06a4308_0         128 KB\n",
            "    certifi-2024.12.14         |  py311h06a4308_0         161 KB\n",
            "    conda-24.11.3              |  py311h06a4308_0         1.2 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         1.5 MB\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2024.12.~ --> pkgs/main::ca-certificates-2024.12.31-h06a4308_0 \n",
            "  conda              conda-forge::conda-24.11.2-py311h38be~ --> pkgs/main::conda-24.11.3-py311h06a4308_0 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge/noarch::certifi-2024.12.1~ --> pkgs/main/linux-64::certifi-2024.12.14-py311h06a4308_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "conda-24.11.3        | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\n",
            "certifi-2024.12.14   | 161 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "ca-certificates-2024 | 128 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "ca-certificates-2024 | 128 KB    | : 100% 1.0/1 [00:00<00:00, 28.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "ca-certificates-2024 | 128 KB    | : 100% 1.0/1 [00:00<00:00, 22.97it/s]\u001b[A\u001b[A\n",
            "certifi-2024.12.14   | 161 KB    | : 100% 1.0/1 [00:00<00:00, 19.14it/s]\u001b[A\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\bdone\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/openr1\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.11\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    libsqlite-3.48.0           |       hee588c1_1         858 KB  conda-forge\n",
            "    pip-25.0                   |     pyh8b19718_0         1.2 MB  conda-forge\n",
            "    setuptools-75.8.0          |     pyhff2d567_0         757 KB  conda-forge\n",
            "    tzdata-2025a               |       h78e105d_0         120 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         2.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h4bc722e_7 \n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2024.12.14-hbcca054_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.43-h712a8e2_2 \n",
            "  libexpat           conda-forge/linux-64::libexpat-2.6.4-h5888daf_0 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-14.2.0-h69a702a_1 \n",
            "  libgomp            conda-forge/linux-64::libgomp-14.2.0-h77fa898_1 \n",
            "  liblzma            conda-forge/linux-64::liblzma-5.6.3-hb9d3cd8_1 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.48.0-hee588c1_1 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_2 \n",
            "  openssl            conda-forge/linux-64::openssl-3.4.0-h7b32b05_1 \n",
            "  pip                conda-forge/noarch::pip-25.0-pyh8b19718_0 \n",
            "  python             conda-forge/linux-64::python-3.11.11-h9e4cc4f_1_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n",
            "  setuptools         conda-forge/noarch::setuptools-75.8.0-pyhff2d567_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_h4845f30_101 \n",
            "  tzdata             conda-forge/noarch::tzdata-2025a-h78e105d_0 \n",
            "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "pip-25.0             | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\n",
            "libsqlite-3.48.0     | 858 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "setuptools-75.8.0    | 757 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tzdata-2025a         | 120 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "libsqlite-3.48.0     | 858 KB    | : 100% 1.0/1 [00:00<00:00, 15.07it/s]\u001b[A\n",
            "\n",
            "\n",
            "tzdata-2025a         | 120 KB    | : 100% 1.0/1 [00:00<00:00, 15.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pip-25.0             | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  9.18it/s]\n",
            "libsqlite-3.48.0     | 858 KB    | : 100% 1.0/1 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "libsqlite-3.48.0     | 858 KB    | : 100% 1.0/1 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "\n",
            "\n",
            "tzdata-2025a         | 120 KB    | : 100% 1.0/1 [00:00<00:00,  2.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pip-25.0             | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  9.18it/s]\n",
            "\n",
            "setuptools-75.8.0    | 757 KB    | : 100% 1.0/1 [00:00<00:00,  1.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate openr1\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "\n",
            "CondaError: Run 'conda init' before 'conda activate'\n",
            "\n",
            "Cloning into 'open-r1'...\n",
            "remote: Enumerating objects: 308, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 308 (delta 59), reused 39 (delta 38), pack-reused 234 (from 2)\u001b[K\n",
            "Receiving objects: 100% (308/308), 450.69 KiB | 21.46 MiB/s, done.\n",
            "Resolving deltas: 100% (144/144), done.\n",
            "/content/open-r1\n",
            "Obtaining file:///content/open-r1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers.git@main (from open-r1==0.1.0.dev0)\n",
            "  Cloning https://github.com/huggingface/transformers.git (to revision main) to /tmp/pip-install-g0dqo90p/transformers_f93295f8358d4ba69547ff6fe7eb2c91\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-g0dqo90p/transformers_f93295f8358d4ba69547ff6fe7eb2c91\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit c550a1c6404664734f4607eacf44b1a3b100bca2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trl@ git+https://github.com/huggingface/trl.git@main (from open-r1==0.1.0.dev0)\n",
            "  Cloning https://github.com/huggingface/trl.git (to revision main) to /tmp/pip-install-g0dqo90p/trl_bc2256c9caed4f20905fa31020a19df0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-install-g0dqo90p/trl_bc2256c9caed4f20905fa31020a19df0\n",
            "  Resolved https://github.com/huggingface/trl.git to commit 1123bd0f514164eb297e7d6d48d8d8057c6e7334\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate>=1.2.1 (from open-r1==0.1.0.dev0)\n",
            "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes>=0.43.0 (from open-r1==0.1.0.dev0)\n",
            "  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting einops>=0.8.0 (from open-r1==0.1.0.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting datasets>=3.2.0 (from open-r1==0.1.0.dev0)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting deepspeed==0.15.4 (from open-r1==0.1.0.dev0)\n",
            "  Downloading deepspeed-0.15.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hf_transfer>=0.1.4 (from open-r1==0.1.0.dev0)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.2 (from huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n",
            "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting liger_kernel==0.5.2 (from open-r1==0.1.0.dev0)\n",
            "  Downloading liger_kernel-0.5.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/site-packages (from open-r1==0.1.0.dev0) (24.2)\n",
            "Collecting safetensors>=0.3.3 (from open-r1==0.1.0.dev0)\n",
            "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting sentencepiece>=0.1.99 (from open-r1==0.1.0.dev0)\n",
            "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting hjson (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting msgpack (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting ninja (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting numpy (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting psutil (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting py-cpuinfo (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting pydantic>=2.0.0 (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting torch (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from deepspeed==0.15.4->open-r1==0.1.0.dev0) (4.67.1)\n",
            "Collecting triton>=2.3.1 (from liger_kernel==0.5.2->open-r1==0.1.0.dev0)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "\u001b[33mDEPRECATION: git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math] (from open-r1==0.1.0.dev0)\n",
            "  Cloning https://github.com/huggingface/lighteval.git (to revision 4f381b352c0e467b5870a97d41cb66b487a2c503) to /tmp/pip-install-g0dqo90p/lighteval_057a6cc87bb946e183a91907ed8e859a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/lighteval.git /tmp/pip-install-g0dqo90p/lighteval_057a6cc87bb946e183a91907ed8e859a\n",
            "  Running command git rev-parse -q --verify 'sha^4f381b352c0e467b5870a97d41cb66b487a2c503'\n",
            "  Running command git fetch -q https://github.com/huggingface/lighteval.git 4f381b352c0e467b5870a97d41cb66b487a2c503\n",
            "  Running command git checkout -q 4f381b352c0e467b5870a97d41cb66b487a2c503\n",
            "  Resolved https://github.com/huggingface/lighteval.git to commit 4f381b352c0e467b5870a97d41cb66b487a2c503\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting black>=24.4.2 (from open-r1==0.1.0.dev0)\n",
            "  Downloading black-24.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "Collecting isort>=5.12.0 (from open-r1==0.1.0.dev0)\n",
            "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8>=6.0.0 (from open-r1==0.1.0.dev0)\n",
            "  Downloading flake8-7.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pytest (from open-r1==0.1.0.dev0)\n",
            "  Downloading pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting parameterized>=0.9.0 (from open-r1==0.1.0.dev0)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting math-verify>=0.3.2 (from open-r1==0.1.0.dev0)\n",
            "  Downloading math_verify-0.3.2-py3-none-any.whl.metadata (325 bytes)\n",
            "Collecting pyyaml (from accelerate>=1.2.1->open-r1==0.1.0.dev0)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting click>=8.0.0 (from black>=24.4.2->open-r1==0.1.0.dev0)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black>=24.4.2->open-r1==0.1.0.dev0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black>=24.4.2->open-r1==0.1.0.dev0)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/site-packages (from black>=24.4.2->open-r1==0.1.0.dev0) (4.3.6)\n",
            "Collecting filelock (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading pyarrow-19.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets>=3.2.0->open-r1==0.1.0.dev0) (2.32.3)\n",
            "Collecting xxhash (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=6.0.0->open-r1==0.1.0.dev0)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.13.0,>=2.12.0 (from flake8>=6.0.0->open-r1==0.1.0.dev0)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8>=6.0.0->open-r1==0.1.0.dev0)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.19.2->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting prompt-toolkit<4.0.0,>=3.0.1 (from InquirerPy==0.3.4->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n",
            "  Downloading prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting latex2sympy2_extended>=0.9.2 (from math-verify>=0.3.2->open-r1==0.1.0.dev0)\n",
            "  Downloading latex2sympy2_extended-0.9.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting GitPython>=3.1.41 (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting typer (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting termcolor==2.3.0 (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pytablewriter (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rich (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting colorlog (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aenum==3.1.15 (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting nltk==3.9.1 (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting scikit-learn (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting spacy==3.7.2 (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading spacy-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting sacrebleu (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Collecting rouge_score==0.1.2 (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting protobuf==3.20.* (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting pycountry (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting joblib (from nltk==3.9.1->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk==3.9.1->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting absl-py (from rouge_score==0.1.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting six>=1.14.0 (from rouge_score==0.1.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.3.0,>=8.1.8 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer (from lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting jinja2 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0) (65.6.3)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers@ git+https://github.com/huggingface/transformers.git@main->open-r1==0.1.0.dev0)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting iniconfig (from pytest->open-r1==0.1.0.dev0)\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/site-packages (from pytest->open-r1==0.1.0.dev0) (1.5.0)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.1.41->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended>=0.9.2->math-verify>=0.3.2->open-r1==0.1.0.dev0)\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Collecting sympy (from latex2sympy2_extended>=0.9.2->math-verify>=0.3.2->open-r1==0.1.0.dev0)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic>=2.0.0->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.2.0->open-r1==0.1.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.2.0->open-r1==0.1.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.2.0->open-r1==0.1.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.2.0->open-r1==0.1.0.dev0) (2024.12.14)\n",
            "Collecting networkx (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->deepspeed==0.15.4->open-r1==0.1.0.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton>=2.3.1 (from liger_kernel==0.5.2->open-r1==0.1.0.dev0)\n",
            "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy (from latex2sympy2_extended>=0.9.2->math-verify>=0.3.2->open-r1==0.1.0.dev0)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->latex2sympy2_extended>=0.9.2->math-verify>=0.3.2->open-r1==0.1.0.dev0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets>=3.2.0->open-r1==0.1.0.dev0)\n",
            "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting portalocker (from sacrebleu->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/site-packages (from sacrebleu->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0) (0.4.6)\n",
            "Collecting lxml (from sacrebleu->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading lxml-5.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=3.1.41->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2->lighteval@ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]->open-r1==0.1.0.dev0)\n",
            "  Downloading marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Downloading liger_kernel-0.5.2-py3-none-any.whl (104 kB)\n",
            "Downloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
            "Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m141.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading black-24.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "Downloading flake8-7.1.1-py2.py3-none-any.whl (57 kB)\n",
            "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "Downloading math_verify-0.3.2-py3-none-any.whl (18 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
            "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Downloading spacy-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Downloading pytest-8.3.4-py3-none-any.whl (343 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "Downloading latex2sympy2_extended-0.9.2-py3-none-any.whl (77 kB)\n",
            "Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pyarrow-19.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m188.5/188.7 MB\u001b[0m \u001b[31m234.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Update Conda (optional but recommended)\n",
        "!conda update -n base -c defaults conda\n",
        "\n",
        "# Create and activate conda environment\n",
        "!conda create -n openr1 python=3.11 -y\n",
        "!conda activate openr1\n",
        "\n",
        "# Clone the Open-R1 repository:\n",
        "!git clone https://github.com/huggingface/open-r1.git\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/open-r1\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install -e \".[dev]\"\n",
        "!pip install vllm==0.6.6.post1 -q\n",
        "!pip install vllm==0.6.6.post1 --extra-index-url https://download.pytorch.org/whl/cu121 -q\n",
        "\n",
        "\n",
        "# Unset WANDB_DISABLED if it exists\n",
        "import os\n",
        "if 'WANDB_DISABLED' in os.environ:\n",
        "    del os.environ['WANDB_DISABLED']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hzl3qEJ-dh0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GOKSBsoiB_Px"
      },
      "outputs": [],
      "source": [
        "!accelerate launch --config_file /content/open-r1/configs/zero3.yaml /content/open-r1/src/open_r1/grpo.py \\\n",
        "    --output_dir DeepSeek-R1-Distill-Qwen-7B-GRPO \\\n",
        "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
        "    --dataset_name AI-MO/NuminaMath-TIR \\\n",
        "    --max_prompt_length 256 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --logging_steps 10 \\\n",
        "    --bf16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes -q\n",
        "!pip install datasets -q\n",
        "!pip install transformers -q\n",
        "!pip install torch -q\n",
        "!pip install accelerate -q\n",
        "!pip install tqdm -q"
      ],
      "metadata": {
        "id": "b-qrC6xxLvG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Unset WANDB_DISABLED if it exists\n",
        "import os\n",
        "if 'WANDB_DISABLED' in os.environ:\n",
        "    del os.environ['WANDB_DISABLED']\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import bitsandbytes as bnb\n",
        "import os\n",
        "from accelerate import Accelerator  # Import Accelerator\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "\n",
        "# Parameters\n",
        "output_dir = \"DeepSeek-R1-Distill-Qwen-7B-GRPO\"\n",
        "max_prompt_length = 256\n",
        "per_device_train_batch_size = 1  # Not explicitly used in this simplified example\n",
        "gradient_accumulation_steps = 16  # Not explicitly used in this simplified example\n",
        "logging_steps = 10\n",
        "\n",
        "# Function to run training within a subprocess (or directly)\n",
        "def train_func(args):\n",
        "    # Initialize Accelerator\n",
        "    accelerator = Accelerator(mixed_precision=\"bf16\", device_placement=False, split_batches=False)\n",
        "\n",
        "    # 1. Load the Dataset\n",
        "    dataset = load_dataset(\"AI-MO/NuminaMath-TIR\")\n",
        "\n",
        "    # 2. Load the DeepSeek-R1-Distill-Qwen-7B model with 4-bit quantization\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading\n",
        "    )\n",
        "\n",
        "    # Load the model without specifying a device_map\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
        "        quantization_config=quantization_config,\n",
        "    )\n",
        "\n",
        "    # Explicitly move the entire model to GPU 0\n",
        "    model.to(accelerator.device)\n",
        "\n",
        "\n",
        "\n",
        "    # Main Execution\n",
        "    env = SimpleEnvironment(dataset, tokenizer, model, accelerator)  # Pass accelerator here\n",
        "    input_dim = model.config.hidden_size\n",
        "    output_dim = 10  # Assume 10 possible actions (adapt to your task)\n",
        "\n",
        "\n",
        "    # Create models, optimizer\n",
        "    policy_network = PolicyNetwork(input_dim, output_dim, accelerator.device)  # Pass device\n",
        "    value_network = ValueNetwork(input_dim, accelerator.device)  # Pass device\n",
        "    optimizer = optim.Adam(policy_network.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "    # Prepare with accelerate (moves models and optimizer to device)\n",
        "    policy_network, value_network, optimizer, env = accelerator.prepare(\n",
        "       policy_network, value_network, optimizer, env\n",
        "   )\n",
        "\n",
        "\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Pass the model to GRPOTrainer\n",
        "    trainer = GRPOTrainer(env, policy_network, value_network, optimizer, learning_rate=0.01, accelerator=accelerator, tokenizer=tokenizer, model=model)\n",
        "\n",
        "    trainer.train(num_epochs=1, num_trajectories_per_epoch=10)\n",
        "\n",
        "    # Save the model using the standard PyTorch save method\n",
        "    torch.save(accelerator.unwrap_model(policy_network).state_dict(), os.path.join(output_dir, 'policy_network.pth'))\n",
        "\n",
        "# 3. Define Policy Network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, device):  # Add device argument\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.device = device  # Store device as attribute\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Change to cast the input to float32 BEFORE it's passed to the linear layer\n",
        "        x = x.to(self.device).type(torch.float32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "# 4. Define Value Network\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, device):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.device = device  # Store the device\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 5. Environment (simplified example - assumes 'text' and 'label' fields in dataset)\n",
        "class SimpleEnvironment:\n",
        "    def __init__(self, dataset, tokenizer, model, accelerator):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.accelerator = accelerator  # Store accelerator\n",
        "        self.current_index = 0\n",
        "        self.episode_length = 20  # Set the desired episode length\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_index = 0\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        text_description = self.dataset[\"train\"][self.current_index].get(\"text\", \"\")\n",
        "        text_description = text_description[:max_prompt_length]\n",
        "\n",
        "        # Move tokenizer outputs AND position_ids to the correct device\n",
        "        inputs = self.tokenizer(text_description, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, output_hidden_states=True)\n",
        "\n",
        "        # Move state_embedding to the correct device\n",
        "        state_embedding = outputs.hidden_states[-1][:, 0, :].to(self.accelerator.device)\n",
        "\n",
        "        return state_embedding.to(torch.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        #print(\"Entering env.step()\")  # Print at the beginning\n",
        "        target = self.dataset[\"train\"][self.current_index].get(\"label\", 0)  # Assume 0 if 'label' missing\n",
        "        #print(\"Target obtained:\", target)  # Print target value\n",
        "        reward = 1.0 if action == target else -1.0\n",
        "        #print(\"Reward calculated:\", reward)  # Print reward value\n",
        "\n",
        "        self.current_index += 1\n",
        "        done = self.current_index >= self.episode_length  # Check if episode length is reached\n",
        "        next_state = self.get_state() if not done else None\n",
        "\n",
        "        #print(\"Exiting env.step()\")  # Print before returning\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "# 6. Trajectory Collection\n",
        "def collect_trajectories(env, policy_network, num_trajectories, accelerator):\n",
        "    trajectories = []\n",
        "    for _ in range(num_trajectories):\n",
        "        state = env.reset()\n",
        "        trajectory = []\n",
        "        while True:\n",
        "            policy_network = policy_network.to(accelerator.device)\n",
        "            state = state.to(accelerator.device)\n",
        "\n",
        "            action_probs = policy_network(state)\n",
        "\n",
        "            # Check for invalid probabilities (keep this part)\n",
        "            if not torch.isfinite(action_probs).all() or not torch.allclose(action_probs.sum(), torch.tensor(1.0, device=action_probs.device)):\n",
        "                action_probs = torch.ones_like(action_probs) / action_probs.shape[-1]\n",
        "\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            if next_state is not None:\n",
        "                next_state = next_state.to(accelerator.device)\n",
        "\n",
        "            trajectory.append((state, action, reward))\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "        trajectories.append(trajectory)\n",
        "    return trajectories\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 7. GRPO Update (simplified - needs customization)\n",
        "def update_policy(policy_network, value_network, trajectories, optimizer, accelerator):  # Add accelerator\n",
        "    # Accumulate policy loss across trajectories for a single backward pass\n",
        "    policy_loss_total = 0\n",
        "\n",
        "    # Debugging Prints: Input Information\n",
        "    #print(f\"Policy network device: {policy_network.device}\")\n",
        "    #print(f\"Value network device: {value_network.device}\")\n",
        "    #print(f\"Trajectories length: {len(trajectories)}\")\n",
        "    #print(f\"Optimizer: {optimizer}\")\n",
        "    #print(f\"Accelerator: {accelerator}\")\n",
        "\n",
        "    # Move both networks to the accelerator device before processing trajectories\n",
        "    policy_network = policy_network.to(accelerator.device)\n",
        "    value_network = value_network.to(accelerator.device)\n",
        "\n",
        "    for trajectory in trajectories:\n",
        "        for i in range(len(trajectory)):  # Iterate using index\n",
        "            state, action, reward = trajectory[i]\n",
        "\n",
        "            # 1. Estimate advantage (using a simple Monte Carlo estimate)\n",
        "            # In a real GRPO implementation, you would likely use a more advanced advantage estimation method\n",
        "            # like Generalized Advantage Estimation (GAE).\n",
        "            returns = sum(r for _, _, r in trajectory[i:])  # Calculate returns from current index\n",
        "\n",
        "            # Ensure state is on the correct device\n",
        "            state = state.to(accelerator.device)\n",
        "\n",
        "            # Calculate advantage (value_network is already on the correct device)\n",
        "            advantage = returns - value_network(state).item()\n",
        "\n",
        "            # Debugging Prints: Intermediate Values\n",
        "            #print(f\"State shape: {state.shape}, device: {state.device}\")\n",
        "            #print(f\"Action: {action}\")\n",
        "            #print(f\"Reward: {reward}\")\n",
        "            #print(f\"Returns: {returns}\")\n",
        "            #print(f\"Advantage: {advantage}\")\n",
        "\n",
        "            # 2. Calculate policy gradient (policy_network is already on the correct device)\n",
        "            action_probs = policy_network(state)\n",
        "            log_prob = torch.log(action_probs[0][action])  # Assuming action_probs is a batch of size 1\n",
        "            policy_loss = -advantage * log_prob\n",
        "\n",
        "            # Debugging Prints: Policy Loss\n",
        "            #print(f\"Action probabilities: {action_probs}\")\n",
        "            #print(f\"Log probability: {log_prob}\")\n",
        "            #print(f\"Policy loss: {policy_loss}\")\n",
        "\n",
        "            # Accumulate policy loss\n",
        "            policy_loss_total += policy_loss\n",
        "\n",
        "    # Debugging Prints: Total Policy Loss\n",
        "    print(f\"Total policy loss: {policy_loss_total}\")\n",
        "\n",
        "    # Return the total policy loss for the backward pass\n",
        "    return policy_loss_total\n",
        "\n",
        "\n",
        "# 8. GRPO Trainer\n",
        "class GRPOTrainer:\n",
        "    def __init__(self, env, policy_network, value_network, optimizer, learning_rate, accelerator, tokenizer, model):\n",
        "        self.env = env\n",
        "        self.policy_network = policy_network\n",
        "        self.value_network = value_network\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.global_step = 0\n",
        "        self.policy_loss = 0\n",
        "        self.accelerator = accelerator\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "        # Prepare with accelerate\n",
        "        self.policy_network, self.value_network, self.optimizer, self.env = accelerator.prepare(\n",
        "            policy_network, value_network, optimizer, env\n",
        "        )\n",
        "\n",
        "        # Create evaluation dataset and dataloader\n",
        "        eval_dataset = load_dataset(\"AI-MO/NuminaMath-TIR\", split=\"test\")\n",
        "        self.eval_dataloader = torch.utils.data.DataLoader(\n",
        "            eval_dataset, batch_size=1,  # Reduced batch size\n",
        "            collate_fn=lambda examples: self.process_batch(examples)\n",
        "        )\n",
        "\n",
        "    def process_batch(self, examples):\n",
        "        # Extract the relevant fields for tokenization, joining list elements if necessary\n",
        "        texts = [\n",
        "            example['problem'] +\n",
        "            (example['solution'] if isinstance(example['solution'], str) else ' '.join(example['solution'])) +\n",
        "            (example['messages'] if isinstance(example['messages'], str) else ' '.join(str(item) for item in example['messages']))\n",
        "            for example in examples\n",
        "        ]\n",
        "\n",
        "        # Tokenize the extracted texts\n",
        "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=self.model.config.max_position_embeddings)\n",
        "\n",
        "        # Other necessary batching processes\n",
        "        labels = [example.get('label', 0) for example in examples]\n",
        "\n",
        "        # Correct the input labels to match input_ids shape\n",
        "        inputs['labels'] = torch.tensor(labels).unsqueeze(1).repeat(1, inputs['input_ids'].shape[1]) # shape (batch_size, seq_len)\n",
        "        inputs['labels'] = torch.where(inputs['attention_mask'] == 1, inputs['labels'], -100) # shape (batch_size, seq_len)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def calculate_accuracy(self, predictions, ground_truth):\n",
        "        # Calculate accuracy\n",
        "        accuracy = np.mean(predictions == ground_truth)  # Removed argmax since predictions are already single values\n",
        "        return accuracy\n",
        "\n",
        "    def collect_trajectories(self, num_trajectories):\n",
        "        return collect_trajectories(self.env, self.policy_network, num_trajectories, self.accelerator)\n",
        "\n",
        "    def update_policy(self, trajectories):\n",
        "        self.policy_loss = update_policy(self.policy_network, self.value_network, trajectories, self.optimizer, self.accelerator)\n",
        "\n",
        "    def train(self, num_epochs, num_trajectories_per_epoch):\n",
        "        total_trajectories = num_epochs * num_trajectories_per_epoch\n",
        "        for epoch in range(num_epochs):\n",
        "            for trajectory_index in tqdm(range(num_trajectories_per_epoch), desc=f\"Epoch {epoch + 1}/{num_epochs}\", total=total_trajectories, position=0, leave=True):\n",
        "                # Collect trajectories and update policy within the trajectory loop\n",
        "                trajectories = self.collect_trajectories(1)\n",
        "\n",
        "                with self.accelerator.accumulate(self.policy_network):\n",
        "                    self.policy_loss = update_policy(self.policy_network, self.value_network, trajectories, self.optimizer, self.accelerator)\n",
        "                    self.accelerator.backward(self.policy_loss)\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    print(f\"Policy Loss: {self.policy_loss.item()}\")\n",
        "\n",
        "            # Accuracy calculation and logging (POC) - performed after each epoch\n",
        "            all_predictions = []\n",
        "            all_ground_truth_labels = []\n",
        "\n",
        "            for batch in self.eval_dataloader:\n",
        "                with torch.no_grad():\n",
        "                    inputs = {k: v.to(self.accelerator.device) for k, v in batch.items()}\n",
        "\n",
        "                    # Call the model to get the hidden states (using the 4-bit quantized model)\n",
        "                    # Remove output_hidden_states=True to reduce memory usage\n",
        "                    outputs = self.model(**inputs, output_hidden_states=True)\n",
        "                    states = outputs.hidden_states[-1][:, 0, :].to(torch.float32).to(self.accelerator.device)\n",
        "\n",
        "                ground_truth_labels = batch['labels'].to(self.accelerator.device)  # Move labels to device\n",
        "\n",
        "                # Ensure policy_network is on the correct device\n",
        "                self.policy_network = self.policy_network.to(self.accelerator.device)\n",
        "\n",
        "                # *** Iterate over each item in the batch ***\n",
        "                for state, label_sequence in zip(states, ground_truth_labels):  # label_sequence now represents the sequence of labels\n",
        "                    with torch.no_grad():\n",
        "                        prediction = self.policy_network(state.unsqueeze(0))  # Add batch dimension\n",
        "\n",
        "                    # Get prediction for the current item\n",
        "                    batch_prediction = prediction.argmax(dim=-1).item()  # Get the predicted class as a single value\n",
        "\n",
        "                    # Get the ground truth label for the current item (assuming the first label in the sequence is the target)\n",
        "                    ground_truth_label = label_sequence[0].item()  # Get the first label in the sequence as a scalar\n",
        "\n",
        "                    # Append prediction and label for the current item\n",
        "                    all_predictions.append(batch_prediction)\n",
        "                    all_ground_truth_labels.append(ground_truth_label)\n",
        "\n",
        "            # Calculate accuracy outside the loop using accumulated predictions and labels\n",
        "            accuracy = self.calculate_accuracy(np.array(all_predictions), np.array(all_ground_truth_labels))\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}, Accuracy: {accuracy}\")\n",
        "\n",
        "# Entry point for your script\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a namespace with the desired values\n",
        "    import argparse\n",
        "    args = argparse.Namespace(mixed_precision=\"bf16\")  # Add other arguments as needed\n",
        "\n",
        "    train_func(args)  # Call the training function"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9c576b2fd06e4b2b8877a010e2c69c89",
            "8977b6733b9b44a3ae361b29c15637e5",
            "e3c1fae4036446ae913f4d90bdea7600",
            "cc94416539134d8d9342ae1135ff2c64",
            "9eab6c0ee2dd47d89065ba66a4ebd88a",
            "fbcc9143f82640e6aceb044954e1953c",
            "5d6a553998994617866a542447f9e695",
            "da6c5b54affb49c8a733825f50bd9fb9",
            "72b17683d777407e91383313ef6c216a",
            "5143dc46838b4a6e878537d81c6ffcd0",
            "d1b821f53d91461d80eb5af49182cbe1"
          ]
        },
        "id": "vPb7LlCGSScy",
        "outputId": "acf0b2b0-0279-4a46-eb86-bc7252403395"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c576b2fd06e4b2b8877a010e2c69c89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 8\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 8\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 1\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 7\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 8\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 8\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 9\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 8\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 5\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 3\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 6\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 6\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 8\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 3\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 9\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  10%|█         | 1/10 [00:01<00:11,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 1\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 8\n",
            "Reward: -1.0\n",
            "Returns: -14.0\n",
            "Advantage: -14.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.8227126598358154\n",
            "Policy loss: -25.962974548339844\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 8\n",
            "Reward: -1.0\n",
            "Returns: -13.0\n",
            "Advantage: -13.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.8227126598358154\n",
            "Policy loss: -24.140262603759766\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: -12.0\n",
            "Advantage: -12.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.6957594156265259\n",
            "Policy loss: -20.76311683654785\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 1\n",
            "Reward: -1.0\n",
            "Returns: -13.0\n",
            "Advantage: -13.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.2023510932922363\n",
            "Policy loss: -29.16824722290039\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: -12.0\n",
            "Advantage: -12.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.6957594156265259\n",
            "Policy loss: -20.76311683654785\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 7\n",
            "Reward: -1.0\n",
            "Returns: -13.0\n",
            "Advantage: -13.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -3.0863842964172363\n",
            "Policy loss: -40.87650680541992\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 8\n",
            "Reward: -1.0\n",
            "Returns: -12.0\n",
            "Advantage: -12.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.8227126598358154\n",
            "Policy loss: -22.317550659179688\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 8\n",
            "Reward: -1.0\n",
            "Returns: -11.0\n",
            "Advantage: -11.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.8227126598358154\n",
            "Policy loss: -20.494836807250977\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 9\n",
            "Reward: -1.0\n",
            "Returns: -10.0\n",
            "Advantage: -10.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.3041579723358154\n",
            "Policy loss: -23.60411834716797\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 8\n",
            "Reward: -1.0\n",
            "Returns: -9.0\n",
            "Advantage: -9.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.8227126598358154\n",
            "Policy loss: -16.84941291809082\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 5\n",
            "Reward: -1.0\n",
            "Returns: -8.0\n",
            "Advantage: -8.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -3.3168532848358154\n",
            "Policy loss: -27.3446044921875\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 3\n",
            "Reward: -1.0\n",
            "Returns: -7.0\n",
            "Advantage: -7.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.6469311714172363\n",
            "Policy loss: -19.174741744995117\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 6\n",
            "Reward: -1.0\n",
            "Returns: -6.0\n",
            "Advantage: -6.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.6117749214172363\n",
            "Policy loss: -16.308290481567383\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: -5.0\n",
            "Advantage: -5.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.6957594156265259\n",
            "Policy loss: -8.892801284790039\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 6\n",
            "Reward: -1.0\n",
            "Returns: -6.0\n",
            "Advantage: -6.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.6117749214172363\n",
            "Policy loss: -16.308290481567383\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: -5.0\n",
            "Advantage: -5.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.8109936714172363\n",
            "Policy loss: -9.497105598449707\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 8\n",
            "Reward: -1.0\n",
            "Returns: -4.0\n",
            "Advantage: -4.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.8227126598358154\n",
            "Policy loss: -7.735848903656006\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 3\n",
            "Reward: -1.0\n",
            "Returns: -3.0\n",
            "Advantage: -3.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.6469311714172363\n",
            "Policy loss: -8.587017059326172\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 9\n",
            "Reward: -1.0\n",
            "Returns: -2.0\n",
            "Advantage: -2.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.3041579723358154\n",
            "Policy loss: -5.170854568481445\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 1\n",
            "Reward: -1.0\n",
            "Returns: -1.0\n",
            "Advantage: -1.244140625\n",
            "Action probabilities: tensor([[0.1835, 0.1105, 0.1635, 0.0709, 0.0549, 0.0363, 0.0734, 0.0457, 0.1616,\n",
            "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -2.2023510932922363\n",
            "Policy loss: -2.740034580230713\n",
            "Total policy loss: -366.6996765136719\n",
            "Policy Loss: -366.6996765136719\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 2\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: -1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: -2.0\n",
            "Advantage: -2.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: -2.5513498783111572\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: -3.0\n",
            "Advantage: -3.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -1.2551385164260864\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: -2.0\n",
            "Advantage: -2.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -0.8682445287704468\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: -1.0\n",
            "Advantage: -1.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: -1.4144560098648071\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: -2.0\n",
            "Advantage: -2.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -0.8682445287704468\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: -1.0\n",
            "Advantage: -1.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -0.48135054111480713\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 0.0\n",
            "Advantage: -0.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: -0.2775620222091675\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: -1.0\n",
            "Advantage: -1.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -0.48135054111480713\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: 0.0\n",
            "Advantage: -0.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -0.09445653855800629\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: 0.29243743419647217\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: 1.9962259531021118\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: 0.8593319654464722\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: 0.0\n",
            "Advantage: -0.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -0.09445653855800629\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: 0.29243743419647217\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: 1.9962259531021118\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: 0.29243743419647217\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: 1.9962259531021118\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: 0.8593319654464722\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 0.0\n",
            "Advantage: -0.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -1.1368939876556396\n",
            "Policy loss: -0.2775620222091675\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 2\n",
            "Reward: -1.0\n",
            "Returns: -1.0\n",
            "Advantage: -1.244140625\n",
            "Action probabilities: tensor([[3.2081e-01, 2.8397e-07, 6.7916e-01, 7.3237e-06, 1.4565e-05, 4.1157e-20,\n",
            "         3.6463e-07, 4.2527e-23, 2.4791e-27, 4.1318e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: -0.38689398765563965\n",
            "Policy loss: -0.48135054111480713\n",
            "Total policy loss: -0.5608682632446289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  20%|██        | 2/10 [00:02<00:10,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Loss: -0.5608682632446289\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  30%|███       | 3/10 [00:03<00:08,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.8824e-13, 4.4202e-14, 1.0262e-10, 2.2897e-11, 1.2317e-42,\n",
            "         1.2015e-13, 0.0000e+00, 0.0000e+00, 1.6587e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n",
            "Policy Loss: 0.0\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.5629e-18, 9.7276e-29, 4.6589e-15, 1.8064e-16, 0.0000e+00,\n",
            "         2.7159e-19, 0.0000e+00, 0.0000e+00, 3.1391e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  40%|████      | 4/10 [00:05<00:07,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Loss: 0.0\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  50%|█████     | 5/10 [00:06<00:06,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 3.3517e-23, 1.6675e-43, 5.7495e-19, 6.3871e-21, 0.0000e+00,\n",
            "         3.5326e-24, 0.0000e+00, 0.0000e+00, 2.3497e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n",
            "Policy Loss: 0.0\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 5.3111e-27, 0.0000e+00, 3.1800e-22, 1.2996e-24, 0.0000e+00,\n",
            "         4.3596e-28, 0.0000e+00, 0.0000e+00, 7.8824e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  60%|██████    | 6/10 [00:07<00:05,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Loss: 0.0\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  70%|███████   | 7/10 [00:08<00:03,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.7817e-30, 0.0000e+00, 4.7809e-25, 7.1878e-28, 0.0000e+00,\n",
            "         5.3802e-32, 0.0000e+00, 0.0000e+00, 4.3596e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n",
            "Policy Loss: 0.0\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 1.6247e-33, 0.0000e+00, 1.1851e-27, 6.5544e-31, 0.0000e+00,\n",
            "         4.9061e-35, 0.0000e+00, 0.0000e+00, 1.0806e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  80%|████████  | 8/10 [00:10<00:02,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Loss: 0.0\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  90%|█████████ | 9/10 [00:11<00:01,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 2.4426e-36, 0.0000e+00, 2.9375e-30, 9.8542e-34, 0.0000e+00,\n",
            "         4.4738e-38, 0.0000e+00, 0.0000e+00, 9.8542e-34]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n",
            "Policy Loss: 0.0\n",
            "Starting trajectory collection...\n",
            "Environment reset, state obtained.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Action probabilities calculated.\n",
            "Action selected: 0\n",
            "Entering env.step()\n",
            "Target obtained: 0\n",
            "Reward calculated: 1.0\n",
            "Exiting env.step()\n",
            "Environment step taken.\n",
            "Policy network device: cuda\n",
            "Value network device: cuda\n",
            "Trajectories length: 1\n",
            "Optimizer: AcceleratedOptimizer (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Accelerator: <accelerate.accelerator.Accelerator object at 0x7e4134398b90>\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 20.0\n",
            "Advantage: 19.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 19.0\n",
            "Advantage: 18.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 18.0\n",
            "Advantage: 17.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 17.0\n",
            "Advantage: 16.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 16.0\n",
            "Advantage: 15.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 15.0\n",
            "Advantage: 14.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 14.0\n",
            "Advantage: 13.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 13.0\n",
            "Advantage: 12.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 12.0\n",
            "Advantage: 11.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 11.0\n",
            "Advantage: 10.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 10.0\n",
            "Advantage: 9.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 9.0\n",
            "Advantage: 8.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 8.0\n",
            "Advantage: 7.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 7.0\n",
            "Advantage: 6.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 6.0\n",
            "Advantage: 5.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 10/10 [00:12<00:00,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 5.0\n",
            "Advantage: 4.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 4.0\n",
            "Advantage: 3.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 3.0\n",
            "Advantage: 2.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 2.0\n",
            "Advantage: 1.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "State shape: torch.Size([1, 3584]), device: cuda:0\n",
            "Action: 0\n",
            "Reward: 1.0\n",
            "Returns: 1.0\n",
            "Advantage: 0.755859375\n",
            "Action probabilities: tensor([[1.0000e+00, 6.0546e-39, 0.0000e+00, 1.9793e-32, 2.4426e-36, 0.0000e+00,\n",
            "         1.1089e-40, 0.0000e+00, 0.0000e+00, 6.6397e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Log probability: 0.0\n",
            "Policy loss: -0.0\n",
            "Total policy loss: 0.0\n",
            "Policy Loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION"
      ],
      "metadata": {
        "id": "e3jyKT-fqoAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!export MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
        "#!export MODEL_ARGS=\"pretrained=$MODEL,dtype=float16,max_model_length=32768,gpu_memory_utilisation=0.8\"\n",
        "#!export TASK=aime24\n",
        "#!export OUTPUT_DIR=data/evals/$MODEL\n",
        "\n",
        "#!lighteval vllm $MODEL_ARGS \"custom|$TASK|0|0\" \\\n",
        "#    --custom-tasks src/open_r1/evaluate.py \\\n",
        "#    --use-chat-template \\\n",
        "#    --system-prompt=\"Please reason step by step, and put your final answer within \\boxed{}.\" \\\n",
        "#    --output-dir $OUTPUT_DIR"
      ],
      "metadata": {
        "id": "Vqzely8GqmkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION - python"
      ],
      "metadata": {
        "id": "M_E9fg_ofHMc"
      }
    },
    {
      "source": [
        "accelerator = Accelerator(mixed_precision=\"bf16\", device_placement=False, split_batches=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
        "\n",
        "# Load the model with quantization config (as in training)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "model.to(accelerator.device)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"AI-MO/NuminaMath-TIR\")\n",
        "\n",
        "# Create the environment\n",
        "env = SimpleEnvironment(dataset, tokenizer, model, accelerator)  # Pass 'model' here\n",
        "\n",
        "# Define input and output dimensions for networks\n",
        "input_dim = model.config.hidden_size  # Use 'model' here\n",
        "output_dim = 10  # Or the appropriate number of actions for your environment\n",
        "\n",
        "# Create policy and value networks (as before)\n",
        "policy_network = PolicyNetwork(input_dim, output_dim, accelerator.device)\n",
        "value_network = ValueNetwork(input_dim, accelerator.device)\n",
        "\n",
        "# *** Load the fine-tuned policy network ***\n",
        "policy_network.load_state_dict(torch.load(\"/content/DeepSeek-R1-Distill-Qwen-7B-GRPO/policy_network.pth\"))\n",
        "\n",
        "# Create optimizer (as before)\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create the GRPOTrainer instance\n",
        "trainer = GRPOTrainer(env, policy_network, value_network, optimizer, learning_rate, accelerator, tokenizer, model)  # Pass 'model' here\n",
        "\n",
        "# Perform evaluation\n",
        "trainer.evaluate()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "072edb5907a44ee1bed9632633e99c9c",
            "be3e114ecc464bbcb4fcfcf9816fd684",
            "26f37d522fe74a18bc7d10b36cdcd82b",
            "785c6a9af2a541389bdf4462893eb865",
            "76de5f5c83324213947f12adcd65d12e",
            "852102e37b21432bb4b4cbedfed68f9f",
            "38c97af39ded4dbe999d12e1e069d5df",
            "f473770ca8f94f6abf6eb8844a8cae4b",
            "a0705f240be64b8fa414b616256779ac",
            "fc8e0a0a14994887b9e78430b6ca89ee",
            "1bddd234683f423bbdd8d7ca70649334"
          ]
        },
        "id": "5iZOoNdZe535",
        "outputId": "f26c3b01-5ebd-4aaf-e07f-b66f641e9956"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "072edb5907a44ee1bed9632633e99c9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-10c24dfe90a7>:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  policy_network.load_state_dict(torch.load(\"/content/DeepSeek-R1-Distill-Qwen-7B-GRPO/policy_network.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zkxQ_nz_gDEw"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOGizYICVLp7zIoSss6x/PL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c576b2fd06e4b2b8877a010e2c69c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8977b6733b9b44a3ae361b29c15637e5",
              "IPY_MODEL_e3c1fae4036446ae913f4d90bdea7600",
              "IPY_MODEL_cc94416539134d8d9342ae1135ff2c64"
            ],
            "layout": "IPY_MODEL_9eab6c0ee2dd47d89065ba66a4ebd88a"
          }
        },
        "8977b6733b9b44a3ae361b29c15637e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbcc9143f82640e6aceb044954e1953c",
            "placeholder": "​",
            "style": "IPY_MODEL_5d6a553998994617866a542447f9e695",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e3c1fae4036446ae913f4d90bdea7600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da6c5b54affb49c8a733825f50bd9fb9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72b17683d777407e91383313ef6c216a",
            "value": 2
          }
        },
        "cc94416539134d8d9342ae1135ff2c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5143dc46838b4a6e878537d81c6ffcd0",
            "placeholder": "​",
            "style": "IPY_MODEL_d1b821f53d91461d80eb5af49182cbe1",
            "value": " 2/2 [00:08&lt;00:00,  4.42s/it]"
          }
        },
        "9eab6c0ee2dd47d89065ba66a4ebd88a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbcc9143f82640e6aceb044954e1953c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d6a553998994617866a542447f9e695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da6c5b54affb49c8a733825f50bd9fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b17683d777407e91383313ef6c216a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5143dc46838b4a6e878537d81c6ffcd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b821f53d91461d80eb5af49182cbe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "072edb5907a44ee1bed9632633e99c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be3e114ecc464bbcb4fcfcf9816fd684",
              "IPY_MODEL_26f37d522fe74a18bc7d10b36cdcd82b",
              "IPY_MODEL_785c6a9af2a541389bdf4462893eb865"
            ],
            "layout": "IPY_MODEL_76de5f5c83324213947f12adcd65d12e"
          }
        },
        "be3e114ecc464bbcb4fcfcf9816fd684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_852102e37b21432bb4b4cbedfed68f9f",
            "placeholder": "​",
            "style": "IPY_MODEL_38c97af39ded4dbe999d12e1e069d5df",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "26f37d522fe74a18bc7d10b36cdcd82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f473770ca8f94f6abf6eb8844a8cae4b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0705f240be64b8fa414b616256779ac",
            "value": 2
          }
        },
        "785c6a9af2a541389bdf4462893eb865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc8e0a0a14994887b9e78430b6ca89ee",
            "placeholder": "​",
            "style": "IPY_MODEL_1bddd234683f423bbdd8d7ca70649334",
            "value": " 2/2 [00:08&lt;00:00,  4.32s/it]"
          }
        },
        "76de5f5c83324213947f12adcd65d12e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852102e37b21432bb4b4cbedfed68f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38c97af39ded4dbe999d12e1e069d5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f473770ca8f94f6abf6eb8844a8cae4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0705f240be64b8fa414b616256779ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc8e0a0a14994887b9e78430b6ca89ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bddd234683f423bbdd8d7ca70649334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
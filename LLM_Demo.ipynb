{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/LLM_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# --- 1. Tokenization and Embedding ---\n",
        "# A very simple tokenizer for demonstration purposes.\n",
        "# In a real-world scenario, this would be a more complex BPE or WordPiece tokenizer.\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        # Create a mapping from token to integer ID and vice versa.\n",
        "        self.vocab = vocab\n",
        "        self.token_to_id = {token: i for i, token in enumerate(vocab)}\n",
        "        self.id_to_token = {i: token for i, token in enumerate(vocab)}\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Convert a string of text into a list of token IDs.\n",
        "        tokens = text.split()\n",
        "        return [self.token_to_id.get(token, self.token_to_id['<unk>']) for token in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # Convert a list of token IDs back into a string.\n",
        "        return \" \".join([self.id_to_token.get(id, '<unk>') for id in ids])\n",
        "\n",
        "# --- 2. Token and Positional Embedding ---\n",
        "# This class combines token embeddings and positional embeddings.\n",
        "class TokenAndPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        # Token embedding layer: a lookup table for each token ID.\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional embedding layer: a lookup table for each position.\n",
        "        # This injects information about the order of tokens in the sequence.\n",
        "        self.positional_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        # Get the token embeddings.\n",
        "        token_embs = self.token_embedding(token_ids)\n",
        "\n",
        "        # Create position IDs from 0 to sequence length - 1.\n",
        "        position_ids = torch.arange(token_ids.size(1), device=token_ids.device)\n",
        "        position_embs = self.positional_embedding(position_ids)\n",
        "\n",
        "        # Add the two embeddings together. This is a crucial step in the Transformer architecture.\n",
        "        # The PDF explains why summation is preferred over concatenation.\n",
        "        return token_embs + position_embs\n",
        "\n",
        "# --- 3. Multi-Head Attention with Dropout ---\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        if d_model % num_heads != 0:\n",
        "            raise ValueError(\"d_model must be divisible by num_heads\")\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.Wq = nn.Linear(d_model, d_model)\n",
        "        self.Wk = nn.Linear(d_model, d_model)\n",
        "        self.Wv = nn.Linear(d_model, d_model)\n",
        "        self.W_out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "\n",
        "        q = self.Wq(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.Wk(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.Wv(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Causal mask for attention\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
        "        attention_scores.masked_fill_(mask, float('-inf'))\n",
        "\n",
        "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply dropout to attention weights as mentioned in the document\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        context_vector = torch.matmul(attention_weights, v)\n",
        "\n",
        "        context_vector = context_vector.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "        output = self.W_out(context_vector)\n",
        "\n",
        "        return output\n",
        "\n",
        "# --- 4. Feed-Forward Network with GELU and Dropout ---\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        # The document states that inputs are projected into a four-times larger space.\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Then shrunk back to the original dimension.\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# --- 5. The Transformer Block with Residual Connections ---\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
        "        # Document's diagram shows LayerNorm after Attention and FeedForward\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # A 4x expansion for the feed-forward network, as is common\n",
        "        self.feed_forward = FeedForward(d_model, d_model * 4, dropout_rate)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The residual connection from the input is added to the attention output\n",
        "        # followed by normalization, as per the document's diagram.\n",
        "        attention_output = self.attention(self.norm1(x))\n",
        "        x = x + self.dropout1(attention_output)\n",
        "\n",
        "        # A second residual connection for the feed-forward network.\n",
        "        ff_output = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout2(ff_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "# --- 6. Putting it all together: A more complete Simple LLM Model ---\n",
        "class SimpleLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_len=512):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
        "        # Stacking multiple Transformer blocks as is done in real LLMs.\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "        self.linear_output = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        x = self.embedding(token_ids)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.linear_output(x)\n",
        "        return logits\n",
        "\n",
        "def get_loss(logits, targets, pad_token_id):\n",
        "    pred_logits_flat = logits.reshape(-1, logits.size(-1))\n",
        "    targets_flat = targets.reshape(-1)\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "    loss = loss_function(pred_logits_flat, targets_flat)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# --- 7. Demonstration ---\n",
        "if __name__ == \"__main__\":\n",
        "    vocab = [\"<unk>\", \"<pad>\", \"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"llm\", \"demo\", \"training\", \"works\"]\n",
        "    pad_token_id = 1\n",
        "    vocab_size = len(vocab)\n",
        "    d_model = 16\n",
        "    num_heads = 4\n",
        "    num_layers = 2\n",
        "\n",
        "    tokenizer = SimpleTokenizer(vocab)\n",
        "    model = SimpleLLM(vocab_size, d_model, num_heads, num_layers)\n",
        "\n",
        "    # Expanded input text for a more meaningful training demo.\n",
        "    input_text = \"hello world this is a test llm demo training works\"\n",
        "    token_ids = tokenizer.encode(input_text)\n",
        "    input_tensor = torch.tensor([token_ids])\n",
        "\n",
        "    print(\"--- Model Structure ---\")\n",
        "    print(model)\n",
        "    print(\"\\n--- Input and Tokenization ---\")\n",
        "    print(f\"Input text: '{input_text}'\")\n",
        "    print(f\"Token IDs: {input_tensor}\")\n",
        "\n",
        "    # Get initial prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    print(\"\\n--- Model Output (Logits) ---\")\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "\n",
        "    print(\"\\n--- Next-Word Prediction Demo ---\")\n",
        "    targets = input_tensor.clone()\n",
        "    prediction_logits = logits[0, 0]\n",
        "    predicted_token_id = torch.argmax(prediction_logits).item()\n",
        "    predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "    print(f\"\\nPredicting next token after 'hello'...\")\n",
        "    print(f\"Predicted token ID: {predicted_token_id}\")\n",
        "    print(f\"Predicted token: '{predicted_token}'\")\n",
        "\n",
        "    print(\"\\n--- Training Loop Demonstration ---\")\n",
        "\n",
        "    # Create the training batch and targets.\n",
        "    input_batch = torch.tensor([tokenizer.encode(input_text)])\n",
        "    targets_batch = input_batch.clone()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Run the training loop with early stopping.\n",
        "    num_epochs = 100\n",
        "    patience = 5\n",
        "    min_delta = 1e-4\n",
        "    best_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # Get initial loss to show improvement.\n",
        "    with torch.no_grad():\n",
        "        initial_logits = model(input_batch)\n",
        "        initial_loss = get_loss(initial_logits[:, :-1, :], targets_batch[:, 1:], pad_token_id)\n",
        "    print(f\"Initial Loss (before training): {initial_loss.item():.4f}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set the model to training mode\n",
        "        logits = model(input_batch)\n",
        "        loss = get_loss(logits[:, :-1, :], targets_batch[:, 1:], pad_token_id)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Check for early stopping\n",
        "        if loss.item() < best_loss - min_delta:\n",
        "            best_loss = loss.item()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
        "                break\n",
        "\n",
        "    # Get final loss to show improvement.\n",
        "    with torch.no_grad():\n",
        "        final_logits = model(input_batch)\n",
        "        final_loss = get_loss(final_logits[:, :-1, :], targets_batch[:, 1:], pad_token_id)\n",
        "\n",
        "    print(f\"Final Loss (after {epoch + 1} epochs): {final_loss.item():.4f}\")\n",
        "\n",
        "    # --- Next-Word Prediction after training ---\n",
        "    print(\"\\n--- Next-Word Prediction Demo after Training ---\")\n",
        "    with torch.no_grad():\n",
        "        logits_after_training = model(input_batch)\n",
        "        # To get the prediction for 'hello', we need to look at the second token's logits\n",
        "        # The target for 'hello' is 'world'\n",
        "        prediction_logits_after_training = logits_after_training[0, 0]\n",
        "        predicted_token_id_after_training = torch.argmax(prediction_logits_after_training).item()\n",
        "        predicted_token_after_training = tokenizer.decode([predicted_token_id_after_training])\n",
        "\n",
        "    print(f\"Predicted token ID after training: {predicted_token_id_after_training}\")\n",
        "    print(f\"Predicted token after training: '{predicted_token_after_training}'\")\n",
        "\n",
        "    print(\"\\nThis concludes the training loop demonstration with loss improvement and a final prediction.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Structure ---\n",
            "SimpleLLM(\n",
            "  (embedding): TokenAndPositionalEmbedding(\n",
            "    (token_embedding): Embedding(12, 16)\n",
            "    (positional_embedding): Embedding(512, 16)\n",
            "  )\n",
            "  (transformer_blocks): ModuleList(\n",
            "    (0-1): 2 x TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (Wq): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (Wk): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (Wv): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (W_out): Linear(in_features=16, out_features=16, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (feed_forward): FeedForward(\n",
            "        (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
            "        (gelu): GELU(approximate='none')\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "  (linear_output): Linear(in_features=16, out_features=12, bias=True)\n",
            ")\n",
            "\n",
            "--- Input and Tokenization ---\n",
            "Input text: 'hello world this is a test llm demo training works'\n",
            "Token IDs: tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\n",
            "\n",
            "--- Model Output (Logits) ---\n",
            "Logits shape: torch.Size([1, 10, 12])\n",
            "\n",
            "--- Next-Word Prediction Demo ---\n",
            "\n",
            "Predicting next token after 'hello'...\n",
            "Predicted token ID: 0\n",
            "Predicted token: '<unk>'\n",
            "\n",
            "--- Training Loop Demonstration ---\n",
            "Initial Loss (before training): 2.7820\n",
            "Final Loss (after 100 epochs): 0.4135\n",
            "\n",
            "--- Next-Word Prediction Demo after Training ---\n",
            "Predicted token ID after training: 3\n",
            "Predicted token after training: 'world'\n",
            "\n",
            "This concludes the training loop demonstration with loss improvement and a final prediction.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Cg1uD4rSHK",
        "outputId": "da5ec885-1ce0-4d5f-9456-a319fc7503a4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_FINETUNE_TPU_COLAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Cloud TPU V6e Trillium - Economical Option for AI Models But Locked-In: https://www.youtube.com/watch?v=20Ysq8w_0g4"
      ],
      "metadata": {
        "id": "YNLZtTtopjvR"
      },
      "id": "YNLZtTtopjvR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided Jupyter notebook, the maximum number of steps for the training process is 5,834.\n",
        "\n",
        "This value is derived from the size of the training dataset and the specified batch size.\n",
        "\n",
        "* The training dataset, train_data, contains 46,670 samples after null values are dropped.\n",
        "\n",
        "* The BATCH_SIZE is set to 8.\n",
        "\n",
        "* The code performs training for a single epoch (epoch 2), with each step processing a batch of data.\n",
        "\n",
        "The total number of steps is calculated by dividing the total number of data samples by the batch size:\n",
        "\n",
        "Total Steps=⌈\n",
        "46,670/8⌉=⌈5833.75⌉=5834"
      ],
      "metadata": {
        "id": "P8xqONCoqw01"
      },
      "id": "P8xqONCoqw01"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages for LLM fine-tuning\n",
        "!pip install einops -q\n",
        "!pip install langsmith -q\n",
        "!pip install bitsandbytes -q\n",
        "!pip install peft --upgrade -q\n",
        "!pip install trl -q"
      ],
      "metadata": {
        "id": "V8S-VjaYnpDv"
      },
      "id": "V8S-VjaYnpDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed problematic import\n",
        "import torch_xla\n",
        "from torch_xla.distributed.spmd import XLAShardedTensor, XLAShard"
      ],
      "metadata": {
        "id": "rrvJ5T3gtLub"
      },
      "id": "rrvJ5T3gtLub",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.runtime as xr"
      ],
      "metadata": {
        "id": "iLfkpbssv1JU"
      },
      "id": "iLfkpbssv1JU",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.debug.profiler as xp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import torch.nn.functional as F\n",
        "import torch_xla.runtime as xr\n",
        "\n",
        "\n",
        "xr.use_spmd()"
      ],
      "metadata": {
        "id": "Y5yvZlZgodCB"
      },
      "id": "Y5yvZlZgodCB",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "iJHZbx9sv9K-"
      },
      "id": "iJHZbx9sv9K-",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft -q"
      ],
      "metadata": {
        "id": "p1BjRjw2wMLp"
      },
      "id": "p1BjRjw2wMLp",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoConfig,\n",
        "    GPTNeoXConfig,\n",
        "    T5Config,\n",
        "    LlamaConfig,\n",
        "    MistralConfig,\n",
        ")\n",
        "from transformers.tokenization_utils_base import (\n",
        "    PreTrainedTokenizerBase,\n",
        "    PaddingStrategy,\n",
        ")\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "from peft import (\n",
        "    # prepare_model_for_int8_training, # Removed this import\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel,\n",
        ")"
      ],
      "metadata": {
        "id": "W_B-2zQ4oiqL"
      },
      "id": "W_B-2zQ4oiqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    GPTNeoXConfig,\n",
        "    T5Config,\n",
        "    LlamaConfig,\n",
        "    MistralConfig,\n",
        ")\n",
        "import torch.nn as nn\n",
        "# import torch_xla.experimental.xla_sharding as xs\n",
        "import torch_xla.core.xla_model as xm\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# ends with $ to prevent sharding lora parameters\n",
        "GPTNEOX_RULES = (\n",
        "    # embeddings\n",
        "    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n",
        "    # atention\n",
        "    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n",
        "    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n",
        "    # mlp\n",
        "    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n",
        "    # output\n",
        "    (\"embed_out\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "\n",
        "T5_RULES = (\n",
        "    # embeddings\n",
        "    (\"shared$\", (\"mp\", \"fsdp\")),\n",
        "    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # attention\n",
        "    (\"q$\", (\"fsdp\", \"mp\")),\n",
        "    (\"k$\", (\"fsdp\", \"mp\")),\n",
        "    (\"v$\", (\"fsdp\", \"mp\")),\n",
        "    (\"o$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # mlp\n",
        "    (\"w$\", (\"fsdp\", \"mp\")),\n",
        "    (\"wi_0$\", (\"fsdp\", \"mp\")),\n",
        "    (\"wi_1$\", (\"fsdp\", \"mp\")),\n",
        "    (\"wo$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # seq2seq lm head\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "\n",
        "LLAMA_RULES = (\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"fsdp\")),\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        "    )\n",
        "\n",
        "MISTRAL_RULES = (\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        "    )\n",
        "\n",
        "ALL_RULES = [\n",
        "    (GPTNeoXConfig, GPTNEOX_RULES),\n",
        "    (T5Config, T5_RULES),\n",
        "    (LlamaConfig, LLAMA_RULES),\n",
        "    (MistralConfig, MISTRAL_RULES)\n",
        "]\n",
        "\n",
        "strkey2id = {\n",
        "    \"dp\": 0,\n",
        "    \"fsdp\": 1,\n",
        "    \"mp\": 2\n",
        "}\n",
        "\n",
        "def find_rule(model):\n",
        "    for config, rule in ALL_RULES:\n",
        "        if model.config.__class__ == config:\n",
        "            return rule\n",
        "    raise Exception(\"unsupported model to partitioning\")\n",
        "\n",
        "def partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n",
        "    partition_specs = find_rule(model)\n",
        "    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        module.to(device)\n",
        "        # print(name, module.__class__.__name__)\n",
        "        if isinstance(module, (torch.nn.Embedding, torch.nn.Linear)):\n",
        "            for rule_pattern, spec in rule:\n",
        "                if re.findall(rule_pattern, name):\n",
        "                    if verbose:\n",
        "                        print(\"match\", rule_pattern, name)\n",
        "\n",
        "                    # xs.mark_sharding(module.weight, mesh, spec) # Commented out due to missing xs\n",
        "                    break\n",
        "\n",
        "def partition_module_dp(model, mesh, device=xm.xla_device(), verbose=False):\n",
        "    spec = (1, 2)\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        module.to(device)\n",
        "        if isinstance(module, (nn.Embedding, nn.nn.Linear)):\n",
        "            # xs.mark_sharding(module.weight, mesh, spec) # Commented out due to missing xs\n",
        "            pass # Added a pass statement as a placeholder"
      ],
      "metadata": {
        "id": "D3qGaEalotBu"
      },
      "id": "D3qGaEalotBu",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def apk(actual, predicted, label_weights=(), k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "    This function computes the average prescision at k between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "        A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "        A list of predicted elements (order does matter)\n",
        "    label_weights : list\n",
        "        A list of weights corresponding to each actual item\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(actual, (list, pd.core.series.Series, np.ndarray)):\n",
        "        raise Exception(\n",
        "            \"actual should be either list,pd.core.series.Series,np.ndarray\"\n",
        "        )\n",
        "\n",
        "    if len(actual) < 1:\n",
        "        return 0.0\n",
        "\n",
        "    # Normalize the weights in order not to get apk above 1\n",
        "    label_weights_count = len(label_weights)\n",
        "    label_weights_sum = sum(label_weights)\n",
        "\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def recall_at_k(actual, predicted, label_weights=(), k=10):\n",
        "    \"\"\"\n",
        "    Computes the percentage of actual items found in the top k predictions over\n",
        "    all actual items.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "        A list of the actually clicked items.\n",
        "    predicted : list\n",
        "        A list of the predicted items, ranked.\n",
        "    label_weights : list\n",
        "        A list of weights corresponding to each actual item\n",
        "    k : int\n",
        "        The number of the top predictions that will be taken into account for the\n",
        "        computation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (double): recall@k\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(actual, (list, pd.core.series.Series, np.ndarray)):\n",
        "        raise Exception(\n",
        "            \"actual should be either list,pd.core.series.Series,np.ndarray\"\n",
        "        )\n",
        "\n",
        "    if len(actual) < 1:\n",
        "        return 0.0\n",
        "\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    total_pred_weighted = sum(label_weights)\n",
        "    success_weighted = 0.0\n",
        "    for i, item in enumerate(actual):\n",
        "        if item in predicted:\n",
        "            success_weighted += label_weights[i]\n",
        "\n",
        "    if success_weighted == 0.0 and total_pred_weighted == 0.0:\n",
        "        return 0.0\n",
        "\n",
        "    return success_weighted / total_pred_weighted\n",
        "\n",
        "\n",
        "def mean_metric(actual, predicted, metric_name, k=10, weights=()):\n",
        "    \"\"\"\n",
        "    Computes the mean of the given metric after it calculates it for each sample.\n",
        "\n",
        "    Possible values for metric_name:\n",
        "        - map@k\n",
        "        - mr@k\n",
        "        - mean_rank_clicked\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    weights : list\n",
        "              A list of lists of weights, each one characterizing a pair of items.\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean of the metric with metric_name.\n",
        "    \"\"\"\n",
        "\n",
        "    if metric_name == \"map@k\":\n",
        "        return np.mean([apk(a, p, w, k) for a, p, w in zip(actual, predicted, weights)])\n",
        "    if metric_name == \"mr@k\":\n",
        "        return np.mean(\n",
        "            [recall_at_k(a, p, w, k) for a, p, w in zip(actual, predicted, weights)]\n",
        "        )\n",
        "    raise Exception(\"metric_name should one of the following: 'map@k', 'mr@k'\")"
      ],
      "metadata": {
        "id": "rDpdPyGewd7l"
      },
      "id": "rDpdPyGewd7l",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt_training(context, prompt, a, b, c, d, e, answer):\n",
        "    return f\"\"\"<human>: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction: You will be given question with 5 possible answers. Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n",
        "\n",
        "### Context: {context}\\n\n",
        "\n",
        "### Question: {prompt}\\n\n",
        "A) {a}\\n\n",
        "B) {b}\\n\n",
        "C) {c}\\n\n",
        "D) {d}\\n\n",
        "E) {e}\\n\n",
        "\n",
        "<assistant>: The correct answer is: {answer}\"\"\"\n",
        "\n",
        "\n",
        "def generate_prompt_inference(context, prompt, a, b, c, d, e):\n",
        "    return f\"\"\"<human>: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction: You will be given question with 5 possible answers. Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n",
        "\n",
        "### Context: {context}\\n\n",
        "\n",
        "### Question: {prompt}\\n\n",
        "A) {a}\\n\n",
        "B) {b}\\n\n",
        "C) {c}\\n\n",
        "D) {d}\\n\n",
        "E) {e}\\n\n",
        "\n",
        "<assistant>: The correct answer is:  \"\"\"\n",
        "\n",
        "\n",
        "def generate_prompt_inference(context, prompt, a, b, c, d, e):\n",
        "    return f\"\"\"<human>: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction: You will be given question with 5 possible answers. Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n",
        "\n",
        "### Context: {context}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def generate_question_inference(prompt, a, b, c, d, e):\n",
        "    return f\"\"\"\n",
        "### Question: {prompt}\\n\n",
        "A) {a}\\n\n",
        "B) {b}\\n\n",
        "C) {c}\\n\n",
        "D) {d}\\n\n",
        "E) {e}\\n\n",
        "\n",
        "<assistant>: The correct answer is:  \"\"\"\n",
        "\n",
        "\n",
        "def generate_answer_inference(prompt, a, b, c, d, e, answer):\n",
        "    return f\"\"\"\n",
        "### Question: {prompt}\\n\n",
        "A) {a}\\n\n",
        "B) {b}\\n\n",
        "C) {c}\\n\n",
        "D) {d}\\n\n",
        "E) {e}\\n\n",
        "\n",
        "<assistant>: The correct answer is: {answer}\"\"\""
      ],
      "metadata": {
        "id": "eA5ukNP9widJ"
      },
      "id": "eA5ukNP9widJ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "class TorchDataset(Dataset):\n",
        "    def __init__(self, df, inference_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df\n",
        "        self.inference_only = inference_only\n",
        "        self.prompt = df.prompt.tolist()\n",
        "        self.input_ids = df.input_ids.tolist()\n",
        "        self.attention_mask = df.attention_mask.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = torch.tensor(self.input_ids[index])\n",
        "        attention_mask = torch.tensor(self.attention_mask[index])\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "        }"
      ],
      "metadata": {
        "id": "DlV_55IqwnYi"
      },
      "id": "DlV_55IqwnYi",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt_text(data):\n",
        "    prompt_texts = []\n",
        "\n",
        "    for index in tqdm(range(len(data))):\n",
        "        sample_raw_prompt = generate_prompt_inference(\n",
        "            context=data.iloc[index][\"more_context\"],\n",
        "            prompt=data.iloc[index][\"prompt\"],\n",
        "            a=data.iloc[index][\"A\"],\n",
        "            b=data.iloc[index][\"B\"],\n",
        "            c=data.iloc[index][\"C\"],\n",
        "            d=data.iloc[index][\"D\"],\n",
        "            e=data.iloc[index][\"E\"],\n",
        "        )\n",
        "\n",
        "        prompt_texts.append(sample_raw_prompt)\n",
        "\n",
        "    data[\"prompt_text\"] = prompt_texts\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_answer_text(data):\n",
        "\n",
        "    prompt_texts = []\n",
        "\n",
        "    for index in tqdm(range(len(data))):\n",
        "        sample_raw_prompt = generate_answer_inference(\n",
        "            prompt=data.iloc[index][\"prompt\"],\n",
        "            a=data.iloc[index][\"A\"],\n",
        "            b=data.iloc[index][\"B\"],\n",
        "            c=data.iloc[index][\"C\"],\n",
        "            d=data.iloc[index][\"D\"],\n",
        "            e=data.iloc[index][\"E\"],\n",
        "            answer=data.iloc[index][\"answer\"],\n",
        "        )\n",
        "\n",
        "        prompt_texts.append(sample_raw_prompt)\n",
        "\n",
        "    data[\"answer_text\"] = prompt_texts\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_question_text(data):\n",
        "    prompt_texts = []\n",
        "\n",
        "    for index in tqdm(range(len(data))):\n",
        "        sample_raw_prompt = generate_question_inference(\n",
        "            prompt=data.iloc[index][\"prompt\"],\n",
        "            a=data.iloc[index][\"A\"],\n",
        "            b=data.iloc[index][\"B\"],\n",
        "            c=data.iloc[index][\"C\"],\n",
        "            d=data.iloc[index][\"D\"],\n",
        "            e=data.iloc[index][\"E\"],\n",
        "        )\n",
        "\n",
        "        prompt_texts.append(sample_raw_prompt)\n",
        "\n",
        "    data[\"question_text\"] = prompt_texts\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "KgxfIqgWwpGw"
      },
      "id": "KgxfIqgWwpGw",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_LENGTH = 1280\n",
        "LOGGING_STEPS = 100\n",
        "NUM_REPLICAS = 1\n",
        "\n",
        "MODEL_NAME= \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "TRAIN = True\n",
        "SAVE_MODEL = True\n",
        "LOAD_MODEL = True"
      ],
      "metadata": {
        "id": "E33MOQkZww4F"
      },
      "id": "E33MOQkZww4F",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pytorch/xla"
      ],
      "metadata": {
        "id": "sgMYO3qN2BEb"
      },
      "id": "sgMYO3qN2BEb"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.8.0 'torch_xla[tpu]==2.8.0' -f https://storage.googleapis.com/libtpu-releases/index.html -q\n",
        "\n",
        "# Optional: if you're using custom kernels, install pallas dependencies\n",
        "!pip install 'torch_xla[pallas]' -f https://storage.googleapis.com/libtpu-releases/index.html -q"
      ],
      "metadata": {
        "id": "tqz5DWAX1C57"
      },
      "id": "tqz5DWAX1C57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "metadata": {
        "id": "deuXvNs1w4Bq"
      },
      "id": "deuXvNs1w4Bq",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla\n",
        "device = torch_xla.device()\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjFb5x0gw8Gh",
        "outputId": "b1175753-aca1-4276-a79f-df86eda3c358"
      },
      "id": "mjFb5x0gw8Gh",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='xla', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"cdeotte/60k-data-with-context-v2\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTbs6JGc2WI6",
        "outputId": "52f32c81-121d-4b0a-e10d-bf9ddbd49322"
      },
      "id": "CTbs6JGc2WI6",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/60k-data-with-context-v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "5CzCgX2G3Hvl"
      },
      "id": "5CzCgX2G3Hvl",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ").to(device)\n",
        "\n",
        "# Freezing most of the model's layers\n",
        "cnt = 0\n",
        "for param in model.parameters():\n",
        "    cnt += 1\n",
        "    param.requires_grad = True\n",
        "    if cnt < 285:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "EDTWr7gyxB7f"
      },
      "id": "EDTWr7gyxB7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# tokenizer.truncation_side = \"left\"\n",
        "tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "_Fd1cEvAzANi"
      },
      "id": "_Fd1cEvAzANi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\n",
        "    \"/kaggle/input/60k-data-with-context-v2/all_12_with_context2.csv\"\n",
        ")\n",
        "\n",
        "train_data = train_data.dropna()\n",
        "train_data = train_data.sample(frac=1.0, random_state=42)\n",
        "print(train_data.shape)\n",
        "\n",
        "train_data[\"more_context\"] = train_data[\"context\"].copy()\n",
        "\n",
        "train_data.reset_index(drop=True, inplace=True)\n",
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "VSP0NTv33161",
        "outputId": "5f442b98-d4fa-4828-ed93-5d4c8b9382cf"
      },
      "id": "VSP0NTv33161",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(46670, 9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              prompt  \\\n",
              "0  How did the AS-15TT missile compare to the Bri...   \n",
              "1  What were the main objectives for the formatio...   \n",
              "2  What is the significance of the song \"Oh No! O...   \n",
              "3  In which event did Carol Lindroos compete at t...   \n",
              "4  What is the capital of Tarata District and Tar...   \n",
              "\n",
              "                                             context  \\\n",
              "0  The AS-15TT missile was relatively similar to ...   \n",
              "1  The 1st Colorado Infantry Regiment (officially...   \n",
              "2  Oh My!' is the debut album of indie rock band ...   \n",
              "3  Carol Lindroos (29 May 1930 - 9 December 2001)...   \n",
              "4  Tarata is a city in the Tacna Region in southe...   \n",
              "\n",
              "                                                   A  \\\n",
              "0  The AS-15TT missile was red in color, unlike t...   \n",
              "1  The 1st Colorado Cavalry Regiment was formed i...   \n",
              "2  The song \"Oh No! Oh My!\" was a bonus track add...   \n",
              "3                                 Men's discus throw   \n",
              "4                                               Lima   \n",
              "\n",
              "                                                   B  \\\n",
              "0  The AS-15TT missile was of the same size as th...   \n",
              "1  The 1st Colorado Cavalry Regiment was formed i...   \n",
              "2  The song \"Oh No! Oh My!\" was originally releas...   \n",
              "3                                   100-meter sprint   \n",
              "4                                          Peru City   \n",
              "\n",
              "                                                   C  \\\n",
              "0  The AS-15TT missile was identical in size, wei...   \n",
              "1  The 1st Colorado Cavalry Regiment was formed i...   \n",
              "2  The song \"Oh No! Oh My!\" was written by Ryland...   \n",
              "3                                           Shot put   \n",
              "4                                               Puno   \n",
              "\n",
              "                                                   D  \\\n",
              "0  The AS-15TT missile was smaller, slimmer, ligh...   \n",
              "1  The 1st Colorado Cavalry Regiment was formed i...   \n",
              "2  The song \"Oh No! Oh My!\" was the lead single f...   \n",
              "3                                          Long jump   \n",
              "4                                             Tarata   \n",
              "\n",
              "                                                   E answer  source  \\\n",
              "0  The AS-15TT missile was larger, wider, and hea...      D       3   \n",
              "1  The 1st Colorado Cavalry Regiment was formed i...      B       4   \n",
              "2  The song \"Oh No! Oh My!\" was the band Oh No! O...      B       4   \n",
              "3                                          High jump      A       4   \n",
              "4                                              Tacna      D       2   \n",
              "\n",
              "                                        more_context  \n",
              "0  The AS-15TT missile was relatively similar to ...  \n",
              "1  The 1st Colorado Infantry Regiment (officially...  \n",
              "2  Oh My!' is the debut album of indie rock band ...  \n",
              "3  Carol Lindroos (29 May 1930 - 9 December 2001)...  \n",
              "4  Tarata is a city in the Tacna Region in southe...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e944db9-aee9-40b2-8b48-b329929d3a51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>context</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>answer</th>\n",
              "      <th>source</th>\n",
              "      <th>more_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How did the AS-15TT missile compare to the Bri...</td>\n",
              "      <td>The AS-15TT missile was relatively similar to ...</td>\n",
              "      <td>The AS-15TT missile was red in color, unlike t...</td>\n",
              "      <td>The AS-15TT missile was of the same size as th...</td>\n",
              "      <td>The AS-15TT missile was identical in size, wei...</td>\n",
              "      <td>The AS-15TT missile was smaller, slimmer, ligh...</td>\n",
              "      <td>The AS-15TT missile was larger, wider, and hea...</td>\n",
              "      <td>D</td>\n",
              "      <td>3</td>\n",
              "      <td>The AS-15TT missile was relatively similar to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What were the main objectives for the formatio...</td>\n",
              "      <td>The 1st Colorado Infantry Regiment (officially...</td>\n",
              "      <td>The 1st Colorado Cavalry Regiment was formed i...</td>\n",
              "      <td>The 1st Colorado Cavalry Regiment was formed i...</td>\n",
              "      <td>The 1st Colorado Cavalry Regiment was formed i...</td>\n",
              "      <td>The 1st Colorado Cavalry Regiment was formed i...</td>\n",
              "      <td>The 1st Colorado Cavalry Regiment was formed i...</td>\n",
              "      <td>B</td>\n",
              "      <td>4</td>\n",
              "      <td>The 1st Colorado Infantry Regiment (officially...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the significance of the song \"Oh No! O...</td>\n",
              "      <td>Oh My!' is the debut album of indie rock band ...</td>\n",
              "      <td>The song \"Oh No! Oh My!\" was a bonus track add...</td>\n",
              "      <td>The song \"Oh No! Oh My!\" was originally releas...</td>\n",
              "      <td>The song \"Oh No! Oh My!\" was written by Ryland...</td>\n",
              "      <td>The song \"Oh No! Oh My!\" was the lead single f...</td>\n",
              "      <td>The song \"Oh No! Oh My!\" was the band Oh No! O...</td>\n",
              "      <td>B</td>\n",
              "      <td>4</td>\n",
              "      <td>Oh My!' is the debut album of indie rock band ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In which event did Carol Lindroos compete at t...</td>\n",
              "      <td>Carol Lindroos (29 May 1930 - 9 December 2001)...</td>\n",
              "      <td>Men's discus throw</td>\n",
              "      <td>100-meter sprint</td>\n",
              "      <td>Shot put</td>\n",
              "      <td>Long jump</td>\n",
              "      <td>High jump</td>\n",
              "      <td>A</td>\n",
              "      <td>4</td>\n",
              "      <td>Carol Lindroos (29 May 1930 - 9 December 2001)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the capital of Tarata District and Tar...</td>\n",
              "      <td>Tarata is a city in the Tacna Region in southe...</td>\n",
              "      <td>Lima</td>\n",
              "      <td>Peru City</td>\n",
              "      <td>Puno</td>\n",
              "      <td>Tarata</td>\n",
              "      <td>Tacna</td>\n",
              "      <td>D</td>\n",
              "      <td>2</td>\n",
              "      <td>Tarata is a city in the Tacna Region in southe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e944db9-aee9-40b2-8b48-b329929d3a51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e944db9-aee9-40b2-8b48-b329929d3a51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e944db9-aee9-40b2-8b48-b329929d3a51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3c348dea-e80a-4c69-9b21-2584655e5f43\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c348dea-e80a-4c69-9b21-2584655e5f43')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3c348dea-e80a-4c69-9b21-2584655e5f43 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data",
              "summary": "{\n  \"name\": \"train_data\",\n  \"rows\": 46670,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40605,\n        \"samples\": [\n          \"What is the significance of the Sherkin Regatta festival on Sherkin Island?\",\n          \"Which period was the Jink\\u014dki considered to be one of the most widely read books?\",\n          \"Which of the following statements accurately describes a conchospiral in mathematics?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41299,\n        \"samples\": [\n          \"A second non-medal athletics competition was held as a series of demonstration events on the streets of Baku in a Great City Games format. Athletics at the 2015 European Games took place at the Baku National Stadium and on the streets of Baku, Azerbaijan. ==Program== The main athletics programme in the National Stadium was a combined European Games and 2015 European Team Championships Third League competition. The swimming events at the 2015 European Games took place at the Baku Aquatics Centre, Baku from 23 to 27 June 2015. 42 events were contested in long course conditions. Karate competitions at the 2015 European Games in Baku were held from 13 to 14 June 2015 at the Crystal Hall complex in Baku. The 2015 European Games was a multi-sport event held in Baku, Azerbaijan from 12 to 28 June 2015. The Diving events at the 2015 European Games took place at the Baku Aquatics Centre, Baku from 18 to 21 June 2015. Events include the high jump and pole vault, and 24 athletes were scheduled to compete. We lost gold medal from EH in Baku . Athletics at the European Games offers possible qualification standards for the Rio 2016 Summer Olympics. European Athletics. Baku 2015 Minsk 2019 == Medal table == The medal table is based on information provided by the International Olympic Committee (IOC) and is consistent with IOC convention in its published medal tables. However, following negotiations with the organising authorities, a compromise was reached whereby, in 2015, these events were for junior divers only - in effect, athletes between the ages of 14 and 18.Baku 2015 Newsletter 1 ==Qualification== 160 divers are expected to take part. Eight events were contested, six from the Olympic program, 3 metre and 3 metre synchronised springboard, and platform for both men and women. In effect, therefore, the swimming portion of the 2015 European Games doubled as the 2015 European Junior Swimming Championships. ==Programme== A number of non-Olympic distances will be raced, in addition to a full Olympic programme. Retrieved on 2016-08-10. ==Doping== Azerbaijan's Chaltu Beji \\u2013 the winner of the women's steeplechase event \\u2013 was disqualified after her in-competition drug test came back positive for the banned substance ostarine.Hayward, Tom (2015-07-27). It is the lowest league of the Team Championships, with 17 teams from smaller European athletics nations, including Azerbaijan. 600 athletes in total \\u2013 competed over two days for points in 20 men's and women's track and field events. Athletics was not included in the earliest list of sports confirmed for the 2015 Games, as the European Athletics authorities at that stage were minded not to take part. The event was held for the first time and saw 5,898 athletes from 50 National Olympic Committees (NOCs) competing in 253 events in 20 sports. Each event will consist of eight competitors Qualification will be based on the 2015 European Karate Championships between 19 and 22 March 2015 in Istanbul, Turkey; the first six finishers in each event qualify for the European Games. In 2022 Elisabeth Niedereder of Austria had all her results since May 2015 annulled by World Athletics and Austria were docked 11 points meaning that the gold medal reverted back to Slovakia after 7 years == References == 2015 Athletics European Games European Games \",\n          \"The team also moved from Montverde Academy where they spent the 2019 season, to the newly refurbished Osceola County Stadium at Orlando City's new training complex in Kissimmee, Florida. In 2020, as part of a club-wide move, the team relocated again, this time to the new Orlando City training facility at Osceola Heritage Park alongside both the senior MLS team and Development Academy. The team also moved from Orlando City Stadium where they spent the 2017 season, to Montverde Academy which already housed Orlando City's Development Academy. For the 2017 season in the USL, Orlando City B moved to Orlando City's newly built home stadium, Orlando City Stadium. For the 2019 season in USL League One, the team moved to Montverde Academy. The stadium will be redeveloped to be part of a larger training complex at Osceola Heritage Park for Orlando City SC of Major League Soccer to house its senior MLS team, USL League One reserve team and Development Academy. Originally a baseball park, it was converted into a soccer-specific stadium by Orlando City SC in 2019 to house the club's MLS Next Pro reserve team Orlando City B ahead of the 2020 season. Owned by Orlando City SC and based at the Orlando City training facility in Kissimmee, the club plays its home games at Osceola County Stadium. On October 15, 2015, the club was officially branded Orlando City B, with home games to be played in Melbourne at the Titan Soccer Complex on the campus of Eastern Florida State College. All-time Orlando City B coaching stats Name Nationality From To P W D L GF GA Win% Anthony Pulis June 30, 2015 November 20, 2017 Fernando De Argila October 3, 2018 July 25, 2019 Roberto Sibaja (interim) July 25, 2019 December 20, 2019 Marcelo Neveleff December 20, 2019 October 16, 2020 Mart\\u00edn Perelman March 9, 2022 present Total ==References== ==External links== * Category:Association football clubs established in 2015 Category:Soccer clubs in Florida Category:Former USL Championship teams Category:Orlando City SC Category:Sports in Brevard County, Florida Category:2015 establishments in Florida Category:Reserve soccer teams in the United States Category:Lake County, Florida Category:Former USL League One teams Category:MLS Next Pro teams Category:Sports in Kissimmee, Florida In May 2019, the team announced plans to relocate OCB as part of a wider vision to house all of Orlando City's development pyramid at the same location for the first time, creating a 20-acre training complex at Osceola Heritage Park to house the senior MLS team, OCB and Development Academy. After another season on hiatus in 2021, the team became an inaugural member of MLS Next Pro. ==History== ===United Soccer League=== On June 30, 2015, Orlando City SC announced that they would operate a USL club starting in 2016. in the Central Florida area. Orlando City B (or OCB for short) is an American soccer club that began play in 2016 and currently plays in MLS Next Pro. It was hoped the move would lead OCB to better act as an upward transitional stepping-stone between Orlando City's Development Academy and the senior MLS team. The team would be their direct USL affiliate, and Orlando City and Louisville City (their USL affiliate club for 2015) had negotiated a \\\"long-term formal partnership\\\" to replace their affiliation arrangement.. Osceola County Stadium is an outdoor sports venue located in Kissimmee, Florida, part of the wider Orlando City SC Training Ground at Osceola Heritage Park. In the mid-1990s Osceola County Stadium was planned to be the home of the yet-to-be named Central Florida team, a charter franchise of the United League (UL) which was a planned third league of Major League Baseball (MLB). In October 2020, the team announced it was withdrawing from USL1 at the end of the season with a possibility of an MLS reserve league launching in 2021. ===MLS Next Pro=== After spending the 2021 season on hiatus, the club announced on December 6, 2021, that it was joining the inaugural 21-team MLS Next Pro season starting in 2022. == Location == Tim Holt, Orlando's vice president of development, said that they would be looking for a stadium in Central Florida in order to facilitate the training of emerging players with the MLS team. Orlando City B will play their matches at the new site starting in the 2020 season following the departure of minor league baseball team Florida Fire Frogs. In November 2016 it was announced that OCB would be moving to the newly opened Orlando City Stadium in time for the 2017 season. \",\n          \"Angiopoietins are proteins with important roles in vascular development and angiogenesis. Angiopoietin is part of a family of vascular growth factors that play a role in embryonic and postnatal angiogenesis. Angiopoietin signaling most directly corresponds with angiogenesis, the process by which new arteries and veins form from preexisting blood vessels. The mechanism by which they contribute to angiogenesis is thought to involve regulation of endothelial cell interactions with supporting perivascular cells. Angiopoietin-4 is a protein that in humans is encoded by the ANGPT4 gene. Angiopoietin-1 and tyrosine kinase signaling are essential for regulating blood vessel development and the stability of mature vessels. Angiopoietin-like 4 is a protein that in human is encoded by the ANGPTL4 gene. All angiopoietins bind with similar affinity to an endothelial cell-specific tyrosine-protein kinase receptor. The expression of Angiopoietin-2 in the absence of vascular endothelial growth factor (VEGF) leads to endothelial cell death and vascular regression. Research has shown angiopoietin signaling to be relevant in treating cancer as well. Angiopoietins have been known to be recruited as well as VEGFs and platelet- derived growth factors (PDGFs). Angiogenesis as a therapeutic target. Angiopoietin-2, on the other hand, promotes cell death and disrupts vascularization. Its role during angiogenesis depends on the presence of Vegf-a. Therapeutic angiogenesis: a new frontier for vascular therapy. Yet, when it is in conjunction with vascular endothelial growth factors, or VEGF, it can promote neo-vascularization. ==Structure== Structurally, angiopoietins have an N-terminal super clustering domain, a central coiled domain, a linker region, and a C-terminal fibrinogen-related domain responsible for the binding between the ligand and receptor. Angiopoeitin, more specifically Ang-1 and Ang-2, work hand in hand with VEGF to mediate angiogenesis. Angiopoietin-1 is a growth factor produced by vascular support cells, specialized pericytes in the kidney, and hepatic stellate cells (ITO) cells in the liver. The protein encoded by this gene functions as an agonist and is an angiopoietin. ==References== ==External links== * ==Further reading== * * * * * * * * * * * VEGF and ang-1 are involved in endothelial tube formation. ===Vascular permeability signaling=== Angiopoietin-1 and angiopoietin-2 are modulators of endothelial permeability and barrier function. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38583,\n        \"samples\": [\n          \"inferior right\",\n          \"To create separate environments for different academic disciplines and facilitate collaborative learning.\",\n          \"Extracting fees from the victim for the work permit and other fees.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38584,\n        \"samples\": [\n          \"Zhou Enlai wanted to negotiate a peace agreement with the nationalists.\",\n          \"Twisty Fries\",\n          \"The structure is unstable in pure form \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38620,\n        \"samples\": [\n          \"It extends from Nantucket and Martha's Vineyard to Long Island and Staten Island.\",\n          \"A HAWK beacon is a device used to notify pedestrians of oncoming traffic and ensure their safety while crossing the road.\",\n          \"Pieter Jacobsz Olycan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"D\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38435,\n        \"samples\": [\n          \"Calcium chloride is commonly encountered as a yellow solid at room temperature, and it is insoluble in water.\",\n          \"Fossil fuels\",\n          \"To introduce GNM and GNMX and the PlayStation Shader Language (PSSL) for the PlayStation 4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"E\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38291,\n        \"samples\": [\n          \"Caldwell died in Jamaica, New York.\",\n          \"hydrogen atom\",\n          \"Glenorie Bus Company operated express routes from West Pennant Hills to the City via the M2.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"B\",\n          \"C\",\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          7,\n          4,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"more_context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41299,\n        \"samples\": [\n          \"A second non-medal athletics competition was held as a series of demonstration events on the streets of Baku in a Great City Games format. Athletics at the 2015 European Games took place at the Baku National Stadium and on the streets of Baku, Azerbaijan. ==Program== The main athletics programme in the National Stadium was a combined European Games and 2015 European Team Championships Third League competition. The swimming events at the 2015 European Games took place at the Baku Aquatics Centre, Baku from 23 to 27 June 2015. 42 events were contested in long course conditions. Karate competitions at the 2015 European Games in Baku were held from 13 to 14 June 2015 at the Crystal Hall complex in Baku. The 2015 European Games was a multi-sport event held in Baku, Azerbaijan from 12 to 28 June 2015. The Diving events at the 2015 European Games took place at the Baku Aquatics Centre, Baku from 18 to 21 June 2015. Events include the high jump and pole vault, and 24 athletes were scheduled to compete. We lost gold medal from EH in Baku . Athletics at the European Games offers possible qualification standards for the Rio 2016 Summer Olympics. European Athletics. Baku 2015 Minsk 2019 == Medal table == The medal table is based on information provided by the International Olympic Committee (IOC) and is consistent with IOC convention in its published medal tables. However, following negotiations with the organising authorities, a compromise was reached whereby, in 2015, these events were for junior divers only - in effect, athletes between the ages of 14 and 18.Baku 2015 Newsletter 1 ==Qualification== 160 divers are expected to take part. Eight events were contested, six from the Olympic program, 3 metre and 3 metre synchronised springboard, and platform for both men and women. In effect, therefore, the swimming portion of the 2015 European Games doubled as the 2015 European Junior Swimming Championships. ==Programme== A number of non-Olympic distances will be raced, in addition to a full Olympic programme. Retrieved on 2016-08-10. ==Doping== Azerbaijan's Chaltu Beji \\u2013 the winner of the women's steeplechase event \\u2013 was disqualified after her in-competition drug test came back positive for the banned substance ostarine.Hayward, Tom (2015-07-27). It is the lowest league of the Team Championships, with 17 teams from smaller European athletics nations, including Azerbaijan. 600 athletes in total \\u2013 competed over two days for points in 20 men's and women's track and field events. Athletics was not included in the earliest list of sports confirmed for the 2015 Games, as the European Athletics authorities at that stage were minded not to take part. The event was held for the first time and saw 5,898 athletes from 50 National Olympic Committees (NOCs) competing in 253 events in 20 sports. Each event will consist of eight competitors Qualification will be based on the 2015 European Karate Championships between 19 and 22 March 2015 in Istanbul, Turkey; the first six finishers in each event qualify for the European Games. In 2022 Elisabeth Niedereder of Austria had all her results since May 2015 annulled by World Athletics and Austria were docked 11 points meaning that the gold medal reverted back to Slovakia after 7 years == References == 2015 Athletics European Games European Games \",\n          \"The team also moved from Montverde Academy where they spent the 2019 season, to the newly refurbished Osceola County Stadium at Orlando City's new training complex in Kissimmee, Florida. In 2020, as part of a club-wide move, the team relocated again, this time to the new Orlando City training facility at Osceola Heritage Park alongside both the senior MLS team and Development Academy. The team also moved from Orlando City Stadium where they spent the 2017 season, to Montverde Academy which already housed Orlando City's Development Academy. For the 2017 season in the USL, Orlando City B moved to Orlando City's newly built home stadium, Orlando City Stadium. For the 2019 season in USL League One, the team moved to Montverde Academy. The stadium will be redeveloped to be part of a larger training complex at Osceola Heritage Park for Orlando City SC of Major League Soccer to house its senior MLS team, USL League One reserve team and Development Academy. Originally a baseball park, it was converted into a soccer-specific stadium by Orlando City SC in 2019 to house the club's MLS Next Pro reserve team Orlando City B ahead of the 2020 season. Owned by Orlando City SC and based at the Orlando City training facility in Kissimmee, the club plays its home games at Osceola County Stadium. On October 15, 2015, the club was officially branded Orlando City B, with home games to be played in Melbourne at the Titan Soccer Complex on the campus of Eastern Florida State College. All-time Orlando City B coaching stats Name Nationality From To P W D L GF GA Win% Anthony Pulis June 30, 2015 November 20, 2017 Fernando De Argila October 3, 2018 July 25, 2019 Roberto Sibaja (interim) July 25, 2019 December 20, 2019 Marcelo Neveleff December 20, 2019 October 16, 2020 Mart\\u00edn Perelman March 9, 2022 present Total ==References== ==External links== * Category:Association football clubs established in 2015 Category:Soccer clubs in Florida Category:Former USL Championship teams Category:Orlando City SC Category:Sports in Brevard County, Florida Category:2015 establishments in Florida Category:Reserve soccer teams in the United States Category:Lake County, Florida Category:Former USL League One teams Category:MLS Next Pro teams Category:Sports in Kissimmee, Florida In May 2019, the team announced plans to relocate OCB as part of a wider vision to house all of Orlando City's development pyramid at the same location for the first time, creating a 20-acre training complex at Osceola Heritage Park to house the senior MLS team, OCB and Development Academy. After another season on hiatus in 2021, the team became an inaugural member of MLS Next Pro. ==History== ===United Soccer League=== On June 30, 2015, Orlando City SC announced that they would operate a USL club starting in 2016. in the Central Florida area. Orlando City B (or OCB for short) is an American soccer club that began play in 2016 and currently plays in MLS Next Pro. It was hoped the move would lead OCB to better act as an upward transitional stepping-stone between Orlando City's Development Academy and the senior MLS team. The team would be their direct USL affiliate, and Orlando City and Louisville City (their USL affiliate club for 2015) had negotiated a \\\"long-term formal partnership\\\" to replace their affiliation arrangement.. Osceola County Stadium is an outdoor sports venue located in Kissimmee, Florida, part of the wider Orlando City SC Training Ground at Osceola Heritage Park. In the mid-1990s Osceola County Stadium was planned to be the home of the yet-to-be named Central Florida team, a charter franchise of the United League (UL) which was a planned third league of Major League Baseball (MLB). In October 2020, the team announced it was withdrawing from USL1 at the end of the season with a possibility of an MLS reserve league launching in 2021. ===MLS Next Pro=== After spending the 2021 season on hiatus, the club announced on December 6, 2021, that it was joining the inaugural 21-team MLS Next Pro season starting in 2022. == Location == Tim Holt, Orlando's vice president of development, said that they would be looking for a stadium in Central Florida in order to facilitate the training of emerging players with the MLS team. Orlando City B will play their matches at the new site starting in the 2020 season following the departure of minor league baseball team Florida Fire Frogs. In November 2016 it was announced that OCB would be moving to the newly opened Orlando City Stadium in time for the 2017 season. \",\n          \"Angiopoietins are proteins with important roles in vascular development and angiogenesis. Angiopoietin is part of a family of vascular growth factors that play a role in embryonic and postnatal angiogenesis. Angiopoietin signaling most directly corresponds with angiogenesis, the process by which new arteries and veins form from preexisting blood vessels. The mechanism by which they contribute to angiogenesis is thought to involve regulation of endothelial cell interactions with supporting perivascular cells. Angiopoietin-4 is a protein that in humans is encoded by the ANGPT4 gene. Angiopoietin-1 and tyrosine kinase signaling are essential for regulating blood vessel development and the stability of mature vessels. Angiopoietin-like 4 is a protein that in human is encoded by the ANGPTL4 gene. All angiopoietins bind with similar affinity to an endothelial cell-specific tyrosine-protein kinase receptor. The expression of Angiopoietin-2 in the absence of vascular endothelial growth factor (VEGF) leads to endothelial cell death and vascular regression. Research has shown angiopoietin signaling to be relevant in treating cancer as well. Angiopoietins have been known to be recruited as well as VEGFs and platelet- derived growth factors (PDGFs). Angiogenesis as a therapeutic target. Angiopoietin-2, on the other hand, promotes cell death and disrupts vascularization. Its role during angiogenesis depends on the presence of Vegf-a. Therapeutic angiogenesis: a new frontier for vascular therapy. Yet, when it is in conjunction with vascular endothelial growth factors, or VEGF, it can promote neo-vascularization. ==Structure== Structurally, angiopoietins have an N-terminal super clustering domain, a central coiled domain, a linker region, and a C-terminal fibrinogen-related domain responsible for the binding between the ligand and receptor. Angiopoeitin, more specifically Ang-1 and Ang-2, work hand in hand with VEGF to mediate angiogenesis. Angiopoietin-1 is a growth factor produced by vascular support cells, specialized pericytes in the kidney, and hepatic stellate cells (ITO) cells in the liver. The protein encoded by this gene functions as an agonist and is an angiopoietin. ==References== ==External links== * ==Further reading== * * * * * * * * * * * VEGF and ang-1 are involved in endothelial tube formation. ===Vascular permeability signaling=== Angiopoietin-1 and angiopoietin-2 are modulators of endothelial permeability and barrier function. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "X97uzYU55cbB"
      },
      "id": "X97uzYU55cbB",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = get_prompt_text(train_data)\n",
        "train_data = get_answer_text(train_data)\n",
        "train_data = get_question_text(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sx_UeIr38OU",
        "outputId": "0aee633c-4a4b-4879-bb43-06506943d6cf"
      },
      "id": "8sx_UeIr38OU",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46670/46670 [00:05<00:00, 8522.16it/s]\n",
            "100%|██████████| 46670/46670 [00:05<00:00, 8511.71it/s]\n",
            "100%|██████████| 46670/46670 [00:04<00:00, 9534.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(example):\n",
        "    text_tokens = tokenizer(\n",
        "        example[\"prompt_text\"],\n",
        "        \" \\n \" + example[\"question_text\"],\n",
        "        truncation='only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    answer_tokens = tokenizer(\n",
        "        example[\"prompt_text\"],\n",
        "        \" \\n \" + example[\"answer_text\"],\n",
        "          truncation='only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    answer_tokens = [-100 for i in range(len(answer_tokens) - 1)] + [answer_tokens[-1]]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": text_tokens,\n",
        "        \"label\": answer_tokens,\n",
        "    }\n",
        "\n",
        "\n",
        "def preprocess_function_inference(example):\n",
        "    text_tokens = tokenizer(\n",
        "        example[\"prompt_text\"],\n",
        "        \" \\n \" + example[\"question_text\"],\n",
        "        truncation='only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    answer_tokens = tokenizer(\n",
        "        example[\"prompt_text\"],\n",
        "        \" \\n \" + example[\"answer_text\"],\n",
        "        truncation='only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    answer_tokens = [-100 for i in range(len(answer_tokens) - 1)] + [answer_tokens[-1]]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": text_tokens,\n",
        "        \"label\": answer_tokens,\n",
        "    }"
      ],
      "metadata": {
        "id": "hMo9kubO5pd7"
      },
      "id": "hMo9kubO5pd7",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = Dataset.from_pandas(train_data)\n",
        "\n",
        "data_train = data_train.map(\n",
        "    preprocess_function,\n",
        "    batched=False,\n",
        "    num_proc=56\n",
        ").remove_columns(list(train_data.columns))\n",
        "\n",
        "data_train"
      ],
      "metadata": {
        "id": "t2XbWe9f5vV-"
      },
      "id": "t2XbWe9f5vV-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(data_train[\"input_ids\"][44]))"
      ],
      "metadata": {
        "id": "1m17nhEJ6EFc"
      },
      "id": "1m17nhEJ6EFc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[\"label\"][44][-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOB-cw456RZ0",
        "outputId": "19e34054-9b84-4d86-a5ff-dcfec4e215cf"
      },
      "id": "OOB-cw456RZ0",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-100, -100, -100, -100, -100, -100, -100, -100, -100, 384]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FLAGS = {\n",
        "    'MAX_INPUT': MAX_LENGTH,\n",
        "    'LOGGING_STEPS': LOGGING_STEPS,\n",
        "    'NUM_EPOCHS': EPOCHS,\n",
        "    'BATCH_SIZE': BATCH_SIZE,\n",
        "    'NUM_STEPS': len(data_train)\n",
        "}"
      ],
      "metadata": {
        "id": "LyzN752t6WGA"
      },
      "id": "LyzN752t6WGA",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "from torch_xla.distributed.parallel_loader import ParallelLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "#this guy is responsible for distributing data across 8 cores\n",
        "# train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "#     data_train, num_replicas=NUM_REPLICAS, rank=xmp.get_ordinal(), shuffle=True)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    data_train, batch_size=FLAGS['BATCH_SIZE'],\n",
        "    collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    shuffle=True # Use default RandomSampler with shuffling\n",
        "    )\n",
        "\n",
        "# xla_train_loader = pl.MpDeviceLoader(training_loader, device)"
      ],
      "metadata": {
        "id": "f-9BElJoDRWN"
      },
      "id": "f-9BElJoDRWN",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "from torch_xla.distributed.parallel_loader import ParallelLoader as pl, MpDeviceLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "\n",
        "test_data = pd.read_csv(\n",
        "    \"/kaggle/input/60k-data-with-context-v2/train_with_context2.csv\"\n",
        ")\n",
        "\n",
        "# Remove or comment out the line filtering by 'dataset' column\n",
        "# test_data = test_data.loc[(test_data[\"dataset\"]==\"kaggle200\")]\n",
        "\n",
        "test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Add the 'more_context' column to test_data\n",
        "test_data[\"more_context\"] = test_data[\"context\"].copy()\n",
        "\n",
        "test_data = get_prompt_text(test_data)\n",
        "test_data = get_answer_text(test_data)\n",
        "test_data = get_question_text(test_data)\n",
        "\n",
        "data_test = Dataset.from_pandas(test_data)\n",
        "\n",
        "#remove everything except for input_ids and labels\n",
        "data_test = data_test.map(\n",
        "    preprocess_function_inference,\n",
        "    batched=False,\n",
        "    num_proc=56\n",
        ").remove_columns(list(test_data.columns))\n",
        "\n",
        "#this guy is responsible for distributing data across 8 cores\n",
        "# test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "#     data_test, num_replicas=1, rank=xmp.get_ordinal(), shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    data_test, batch_size=1,\n",
        "    collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    shuffle=False # No need to shuffle test data\n",
        "    )\n",
        "\n",
        "xla_test_loader = MpDeviceLoader(test_loader, device)"
      ],
      "metadata": {
        "id": "WNvcyHI96frg"
      },
      "id": "WNvcyHI96frg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77979d36",
        "outputId": "06926b38-5d8d-4375-e6f5-e4a07acd3cdb"
      },
      "source": [
        "import os\n",
        "\n",
        "# List files in the downloaded dataset directory\n",
        "downloaded_dataset_path = \"/kaggle/input/60k-data-with-context-v2\"\n",
        "if os.path.exists(downloaded_dataset_path):\n",
        "    print(f\"Files in {downloaded_dataset_path}:\")\n",
        "    for root, dirs, files in os.walk(downloaded_dataset_path):\n",
        "        for file in files:\n",
        "            print(os.path.join(root, file))\n",
        "else:\n",
        "    print(f\"Directory not found: {downloaded_dataset_path}\")"
      ],
      "id": "77979d36",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in /kaggle/input/60k-data-with-context-v2:\n",
            "/kaggle/input/60k-data-with-context-v2/all_12_with_context2.csv\n",
            "/kaggle/input/60k-data-with-context-v2/train_with_context2.csv\n",
            "/kaggle/input/60k-data-with-context-v2/sources.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export XLA_USE_BF16=1"
      ],
      "metadata": {
        "id": "BRMRl2IcJOkY"
      },
      "id": "BRMRl2IcJOkY",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "def train(FLAGS):\n",
        "    num_replicas = NUM_REPLICAS\n",
        "\n",
        "    num_iterations = int(FLAGS['NUM_STEPS'] / FLAGS['BATCH_SIZE'] / num_replicas)\n",
        "    print(f\"num_iterations: {num_iterations}\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-7,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_iterations*FLAGS[\"NUM_EPOCHS\"],\n",
        "        eta_min=1e-7,\n",
        "        last_epoch=-1,\n",
        "        # verbose=False # Removed verbose argument\n",
        "    )\n",
        "\n",
        "    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n",
        "        model.train()\n",
        "        falcon_7b_responses = []\n",
        "\n",
        "        if epoch > 1:\n",
        "            for step, batch in enumerate(training_loader):\n",
        "                if step % 100 == 0:\n",
        "                    xm.master_print('Epoch {} step {} train begin {}'.format(\n",
        "                        epoch, step, test_utils.now()))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                input_ids, attention_mask, labels = batch.input_ids.to(device), batch.attention_mask.to(device), batch.labels.to(device)\n",
        "\n",
        "                attention_mask = torch.where(input_ids==2, 0, 1).to(device)\n",
        "\n",
        "                # xs.mark_sharding(input_ids, mesh, (0, 1)) # Commented out due to missing xs\n",
        "                # xs.mark_sharding(attention_mask, mesh, (0, 1)) # Commented out due to missing xs\n",
        "                # xs.mark_sharding(labels, mesh, (0, 1)) # Commented out due to missing xs\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                logits = outputs.logits[:, -1, [330, 365, 334, 384, 413]]\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                xm.mark_step()\n",
        "\n",
        "                if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n",
        "                    print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step}')\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        total_steps = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(xla_test_loader):\n",
        "                input_ids, attention_mask, labels = batch.input_ids.to(device), batch.attention_mask.to(device), batch.labels.to(device)\n",
        "\n",
        "                attention_mask = torch.where(input_ids==2, 0, 1).to(device)\n",
        "\n",
        "                # xs.mark_sharding(input_ids, mesh, (0, 1)) # Commented out due to missing xs\n",
        "                # xs.mark_sharding(attention_mask, mesh, (0, 1)) # Commented out due to missing xs\n",
        "                # xs.mark_sharding(labels, mesh, (0, 1)) # Commented out due to missing xs\n",
        "                sample_prediction = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                logits = sample_prediction.logits[:, -1, [330, 365, 334, 384, 413]]\n",
        "\n",
        "                if step in [0, 100]:\n",
        "                    print(f\"Step: {step}\")\n",
        "                    print(f\"Logits: {logits}\")\n",
        "\n",
        "                sorted_logits = torch.argsort(-logits[0])\n",
        "\n",
        "                falcon_7b_responses.append([\"ABCDE\"[x] for x in sorted_logits][:3])\n",
        "\n",
        "                loss = sample_prediction.loss\n",
        "                total_loss += loss.item()\n",
        "                total_steps += 1\n",
        "\n",
        "        test_data[\"clean_answer\"] = falcon_7b_responses\n",
        "\n",
        "        apks = [apk([actual], predicted, k=3) for actual, predicted in zip(\n",
        "            test_data[\"answer\"].values, test_data[\"clean_answer\"].values\n",
        "        )]\n",
        "\n",
        "        average_loss = total_loss / total_steps\n",
        "        xm.master_print('Epoch {} test end {}, test loss={:.6f}'.format(\n",
        "            epoch, test_utils.now(), average_loss))\n",
        "        xm.master_print('Epoch {} test end {}, test MAP@3={:.6f}'.format(\n",
        "            epoch, test_utils.now(), np.mean(apks)))\n",
        "        xm.master_print('Epoch {} train end {}'.format(\n",
        "            epoch, test_utils.now()))\n",
        "\n",
        "    xm.master_print(\"Saving the model\")\n",
        "    xm.save(model.state_dict(), \"tpu-llama.bin\")\n",
        "\n",
        "# xmp.spawn(train, args=(FLAGS,))"
      ],
      "metadata": {
        "id": "5bmOK1ZdJTpY"
      },
      "id": "5bmOK1ZdJTpY",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "    train(FLAGS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB3MnC8SJYvt",
        "outputId": "d1fc9bed-b9d4-47f5-f6c5-9cb7366d06aa"
      },
      "id": "PB3MnC8SJYvt",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_iterations: 5833\n",
            "Step: 0\n",
            "Logits: tensor([[11.1875, 11.0000,  9.8125, 12.6875, 12.5625]], device='xla:0',\n",
            "       dtype=torch.bfloat16)\n",
            "Step: 100\n",
            "Logits: tensor([[10.0000,  9.8750,  8.5000, 14.3750, 11.7500]], device='xla:0',\n",
            "       dtype=torch.bfloat16)\n",
            "Epoch 1 test end 09:46:56, test loss=0.978216\n",
            "Epoch 1 test end 09:46:56, test MAP@3=0.818333\n",
            "Epoch 1 train end 09:46:56\n",
            "Epoch 2 step 0 train begin 09:46:56\n",
            "loss: 0.7403604388237, time: 09:48:43, step: 99\n",
            "Epoch 2 step 100 train begin 09:48:43\n",
            "loss: 1.446129560470581, time: 09:49:54, step: 199\n",
            "Epoch 2 step 200 train begin 09:49:54\n",
            "loss: 1.5683820247650146, time: 09:51:05, step: 299\n",
            "Epoch 2 step 300 train begin 09:51:05\n",
            "loss: 1.0947434902191162, time: 09:52:16, step: 399\n",
            "Epoch 2 step 400 train begin 09:52:16\n",
            "loss: 1.2663393020629883, time: 09:53:27, step: 499\n",
            "Epoch 2 step 500 train begin 09:53:27\n",
            "loss: 0.3549329340457916, time: 09:54:38, step: 599\n",
            "Epoch 2 step 600 train begin 09:54:38\n",
            "loss: 0.9130598306655884, time: 09:55:49, step: 699\n",
            "Epoch 2 step 700 train begin 09:55:49\n",
            "loss: 0.6817241907119751, time: 09:57:00, step: 799\n",
            "Epoch 2 step 800 train begin 09:57:00\n",
            "loss: 1.2688870429992676, time: 09:58:12, step: 899\n",
            "Epoch 2 step 900 train begin 09:58:12\n",
            "loss: 0.891920268535614, time: 09:59:23, step: 999\n",
            "Epoch 2 step 1000 train begin 09:59:23\n",
            "loss: 0.18732291460037231, time: 10:00:34, step: 1099\n",
            "Epoch 2 step 1100 train begin 10:00:34\n",
            "loss: 0.8657701015472412, time: 10:01:45, step: 1199\n",
            "Epoch 2 step 1200 train begin 10:01:45\n",
            "loss: 0.824934720993042, time: 10:02:56, step: 1299\n",
            "Epoch 2 step 1300 train begin 10:02:56\n",
            "loss: 0.9544516205787659, time: 10:04:07, step: 1399\n",
            "Epoch 2 step 1400 train begin 10:04:07\n",
            "loss: 1.6218782663345337, time: 10:05:19, step: 1499\n",
            "Epoch 2 step 1500 train begin 10:05:19\n",
            "loss: 0.9295260906219482, time: 10:06:30, step: 1599\n",
            "Epoch 2 step 1600 train begin 10:06:30\n",
            "loss: 1.121783971786499, time: 10:07:41, step: 1699\n",
            "Epoch 2 step 1700 train begin 10:07:41\n",
            "loss: 0.6483761072158813, time: 10:08:52, step: 1799\n",
            "Epoch 2 step 1800 train begin 10:08:52\n",
            "loss: 1.0503358840942383, time: 10:10:03, step: 1899\n",
            "Epoch 2 step 1900 train begin 10:10:03\n",
            "loss: 0.9475814700126648, time: 10:11:14, step: 1999\n",
            "Epoch 2 step 2000 train begin 10:11:14\n",
            "loss: 0.8045895099639893, time: 10:12:25, step: 2099\n",
            "Epoch 2 step 2100 train begin 10:12:25\n",
            "loss: 0.9884810447692871, time: 10:13:37, step: 2199\n",
            "Epoch 2 step 2200 train begin 10:13:37\n",
            "loss: 0.4583134353160858, time: 10:14:48, step: 2299\n",
            "Epoch 2 step 2300 train begin 10:14:48\n",
            "loss: 1.0300320386886597, time: 10:15:59, step: 2399\n",
            "Epoch 2 step 2400 train begin 10:15:59\n",
            "loss: 0.9776401519775391, time: 10:17:10, step: 2499\n",
            "Epoch 2 step 2500 train begin 10:17:10\n",
            "loss: 0.8510475158691406, time: 10:18:21, step: 2599\n",
            "Epoch 2 step 2600 train begin 10:18:21\n",
            "loss: 0.6109014749526978, time: 10:19:33, step: 2699\n",
            "Epoch 2 step 2700 train begin 10:19:33\n",
            "loss: 0.7344649434089661, time: 10:20:44, step: 2799\n",
            "Epoch 2 step 2800 train begin 10:20:44\n",
            "loss: 0.6898784637451172, time: 10:21:55, step: 2899\n",
            "Epoch 2 step 2900 train begin 10:21:55\n",
            "loss: 1.3478827476501465, time: 10:23:06, step: 2999\n",
            "Epoch 2 step 3000 train begin 10:23:06\n",
            "loss: 0.8805762529373169, time: 10:24:17, step: 3099\n",
            "Epoch 2 step 3100 train begin 10:24:17\n",
            "loss: 0.9379779100418091, time: 10:25:29, step: 3199\n",
            "Epoch 2 step 3200 train begin 10:25:29\n",
            "loss: 0.9177252054214478, time: 10:26:40, step: 3299\n",
            "Epoch 2 step 3300 train begin 10:26:40\n",
            "loss: 1.276578664779663, time: 10:27:51, step: 3399\n",
            "Epoch 2 step 3400 train begin 10:27:51\n",
            "loss: 0.8572304248809814, time: 10:29:02, step: 3499\n",
            "Epoch 2 step 3500 train begin 10:29:02\n",
            "loss: 0.8391048908233643, time: 10:30:13, step: 3599\n",
            "Epoch 2 step 3600 train begin 10:30:13\n",
            "loss: 0.9620888233184814, time: 10:31:25, step: 3699\n",
            "Epoch 2 step 3700 train begin 10:31:25\n",
            "loss: 0.6624276638031006, time: 10:32:36, step: 3799\n",
            "Epoch 2 step 3800 train begin 10:32:36\n",
            "loss: 0.7669235467910767, time: 10:33:47, step: 3899\n",
            "Epoch 2 step 3900 train begin 10:33:47\n",
            "loss: 0.7165130972862244, time: 10:34:58, step: 3999\n",
            "Epoch 2 step 4000 train begin 10:34:58\n",
            "loss: 1.163172960281372, time: 10:36:09, step: 4099\n",
            "Epoch 2 step 4100 train begin 10:36:09\n",
            "loss: 0.6763284206390381, time: 10:37:20, step: 4199\n",
            "Epoch 2 step 4200 train begin 10:37:21\n",
            "loss: 0.571314811706543, time: 10:38:32, step: 4299\n",
            "Epoch 2 step 4300 train begin 10:38:32\n",
            "loss: 0.7989970445632935, time: 10:39:43, step: 4399\n",
            "Epoch 2 step 4400 train begin 10:39:43\n",
            "loss: 1.2050296068191528, time: 10:40:54, step: 4499\n",
            "Epoch 2 step 4500 train begin 10:40:54\n",
            "loss: 1.0413157939910889, time: 10:42:06, step: 4599\n",
            "Epoch 2 step 4600 train begin 10:42:06\n",
            "loss: 0.7163159251213074, time: 10:43:17, step: 4699\n",
            "Epoch 2 step 4700 train begin 10:43:17\n",
            "loss: 0.6907926797866821, time: 10:44:28, step: 4799\n",
            "Epoch 2 step 4800 train begin 10:44:28\n",
            "loss: 1.4542831182479858, time: 10:45:39, step: 4899\n",
            "Epoch 2 step 4900 train begin 10:45:39\n",
            "loss: 1.1043777465820312, time: 10:46:50, step: 4999\n",
            "Epoch 2 step 5000 train begin 10:46:50\n",
            "loss: 0.562116265296936, time: 10:48:02, step: 5099\n",
            "Epoch 2 step 5100 train begin 10:48:02\n",
            "loss: 1.237805724143982, time: 10:49:13, step: 5199\n",
            "Epoch 2 step 5200 train begin 10:49:13\n",
            "loss: 0.9789973497390747, time: 10:50:24, step: 5299\n",
            "Epoch 2 step 5300 train begin 10:50:24\n",
            "loss: 0.5799493789672852, time: 10:51:35, step: 5399\n",
            "Epoch 2 step 5400 train begin 10:51:35\n",
            "loss: 0.6054227352142334, time: 10:52:46, step: 5499\n",
            "Epoch 2 step 5500 train begin 10:52:46\n",
            "loss: 1.559044599533081, time: 10:54:24, step: 5599\n",
            "Epoch 2 step 5600 train begin 10:54:24\n",
            "loss: 1.2535853385925293, time: 10:55:35, step: 5699\n",
            "Epoch 2 step 5700 train begin 10:55:35\n",
            "loss: 0.7762961983680725, time: 10:56:46, step: 5799\n",
            "Epoch 2 step 5800 train begin 10:56:46\n",
            "Step: 0\n",
            "Logits: tensor([[14.1875, 13.3750, 12.5625, 14.3125, 13.5000]], device='xla:0',\n",
            "       dtype=torch.bfloat16)\n",
            "Step: 100\n",
            "Logits: tensor([[13.8750, 13.6875, 12.0000, 16.8750, 12.8750]], device='xla:0',\n",
            "       dtype=torch.bfloat16)\n",
            "Epoch 2 test end 10:58:51, test loss=0.805856\n",
            "Epoch 2 test end 10:58:51, test MAP@3=0.795000\n",
            "Epoch 2 train end 10:58:51\n",
            "Saving the model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL EVAL"
      ],
      "metadata": {
        "id": "t5MWWYFjLUWH"
      },
      "id": "t5MWWYFjLUWH"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt_text(data):\n",
        "    prompt_texts = []\n",
        "\n",
        "    for index in tqdm(range(len(data))):\n",
        "        sample_raw_prompt = generate_prompt_inference(\n",
        "            context=data.iloc[index][\"more_context\"],\n",
        "            prompt=data.iloc[index][\"prompt\"],\n",
        "            a=data.iloc[index][\"A\"],\n",
        "            b=data.iloc[index][\"B\"],\n",
        "            c=data.iloc[index][\"C\"],\n",
        "            d=data.iloc[index][\"D\"],\n",
        "            e=data.iloc[index][\"E\"],\n",
        "        )\n",
        "\n",
        "        prompt_texts.append(sample_raw_prompt)\n",
        "\n",
        "    data[\"prompt_text\"] = prompt_texts\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_answer_text(data):\n",
        "\n",
        "    prompt_texts = []\n",
        "\n",
        "    for index in tqdm(range(len(data))):\n",
        "        sample_raw_prompt = generate_answer_inference(\n",
        "            prompt=data.iloc[index][\"prompt\"],\n",
        "            a=data.iloc[index][\"A\"],\n",
        "            b=data.iloc[index][\"B\"],\n",
        "            c=data.iloc[index][\"C\"],\n",
        "            d=data.iloc[index][\"D\"],\n",
        "            e=data.iloc[index][\"E\"],\n",
        "            answer=data.iloc[index][\"answer\"],\n",
        "        )\n",
        "\n",
        "        prompt_texts.append(sample_raw_prompt)\n",
        "\n",
        "    data[\"answer_text\"] = prompt_texts\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_question_text(data):\n",
        "    prompt_texts = []\n",
        "\n",
        "    for index in tqdm(range(len(data))):\n",
        "        sample_raw_prompt = generate_question_inference(\n",
        "            prompt=data.iloc[index][\"prompt\"],\n",
        "            a=data.iloc[index][\"A\"],\n",
        "            b=data.iloc[index][\"B\"],\n",
        "            c=data.iloc[index][\"C\"],\n",
        "            d=data.iloc[index][\"D\"],\n",
        "            e=data.iloc[index][\"E\"],\n",
        "        )\n",
        "\n",
        "        prompt_texts.append(sample_raw_prompt)\n",
        "\n",
        "    data[\"question_text\"] = prompt_texts\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "2TlyTPgDgJd5"
      },
      "id": "2TlyTPgDgJd5",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "from torch_xla.distributed.parallel_loader import ParallelLoader as pl, MpDeviceLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "test_data = pd.read_csv(\n",
        "    \"/kaggle/input/60k-data-with-context-v2/train_with_context2.csv\"\n",
        ")\n",
        "\n",
        "# Remove or comment out the line filtering by 'dataset' column\n",
        "# test_data = test_data.loc[(test_data[\"dataset\"]==\"kaggle200\")]\n",
        "\n",
        "test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Add the 'more_context' column to test_data\n",
        "test_data[\"more_context\"] = test_data[\"context\"].copy()\n",
        "\n",
        "test_data = get_prompt_text(test_data)\n",
        "test_data = get_answer_text(test_data)\n",
        "test_data = get_question_text(test_data)\n",
        "\n",
        "data_test = Dataset.from_pandas(test_data)\n",
        "\n",
        "#remove everything except for input_ids and labels\n",
        "data_test = data_test.map(\n",
        "    preprocess_function_inference,\n",
        "    batched=False,\n",
        "    num_proc=56\n",
        ").remove_columns(list(test_data.columns))\n",
        "\n",
        "#this guy is responsible for distributing data across 8 cores\n",
        "# test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "#     data_test, num_replicas=1, rank=xmp.get_ordinal(), shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    data_test, batch_size=1,\n",
        "    collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    shuffle=False # No need to shuffle test data\n",
        "    )\n",
        "\n",
        "xla_test_loader = MpDeviceLoader(test_loader, device)"
      ],
      "metadata": {
        "id": "6XvJ8pi9LSRO"
      },
      "id": "6XvJ8pi9LSRO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import gc\n",
        "\n",
        "mistral_7b_responses = []\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for step, data in enumerate(xla_test_loader):\n",
        "    with torch.no_grad():\n",
        "        if (step + 1) % 20 == 0:\n",
        "            print(step + 1)\n",
        "\n",
        "        # data[\"attention_mask\"] = torch.where(data[\"input_ids\"]==2, 0, 1).to(device) # Data is already on device with xla_test_loader\n",
        "\n",
        "        sample_prediction = model(**data)\n",
        "        sorted_logits = torch.argsort(\n",
        "            -sample_prediction.logits[0][-1, [330, 365, 334, 384, 413]])\n",
        "\n",
        "        mistral_7b_responses.append([\"ABCDE\"[x] for x in sorted_logits][:3])\n",
        "\n",
        "        del sample_prediction\n",
        "        del sorted_logits\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "print(len(mistral_7b_responses), mistral_7b_responses[:10])\n",
        "\n",
        "test_data[\"clean_answer\"] = mistral_7b_responses\n",
        "\n",
        "apks = [apk([actual], predicted, k=3) for actual, predicted in zip(\n",
        "    test_data[\"answer\"].values,\n",
        "    test_data[\"clean_answer\"].values\n",
        ")]\n",
        "\n",
        "test_data[\"apk\"] = apks\n",
        "print(test_data[\"apk\"].value_counts())\n",
        "print(\"\\n\")\n",
        "map3_score = np.mean(apks)\n",
        "print(f\"MAP@3: {map3_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "404TrLAPL_rG",
        "outputId": "35ae0882-f3fe-4066-a30e-a2d5c0de2a59"
      },
      "id": "404TrLAPL_rG",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "160\n",
            "180\n",
            "200\n",
            "200 [['D', 'A', 'E'], ['A', 'B', 'D'], ['A', 'C', 'B'], ['B', 'A', 'C'], ['B', 'A', 'D'], ['B', 'A', 'C'], ['A', 'C', 'B'], ['B', 'D', 'A'], ['B', 'C', 'A'], ['A', 'C', 'B']]\n",
            "apk\n",
            "1.000000    130\n",
            "0.500000     32\n",
            "0.000000     23\n",
            "0.333333     15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "MAP@3: 0.755\n",
            "CPU times: user 3min 22s, sys: 1.6 s, total: 3min 24s\n",
            "Wall time: 2min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push model to HuggingFace Hub"
      ],
      "metadata": {
        "id": "LoSK23BTMZym"
      },
      "id": "LoSK23BTMZym"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model architecture (assuming it's already defined or loaded in a previous cell like EDTWr7gyxB7f)\n",
        "# model = AutoModelForCausalLM.from_pretrained(...) # Keep the model instance from EDTWr7gyxB7f\n",
        "\n",
        "# Load the state dictionary from the local file\n",
        "model.load_state_dict(torch.load(\"./tpu-llama.bin\"))\n",
        "\n",
        "# Move the model back to the device if necessary\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpmw1Dt0NuA1",
        "outputId": "3569adda-dfff-4974-affd-9631b1f0b469"
      },
      "id": "qpmw1Dt0NuA1",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): MistralRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTION1"
      ],
      "metadata": {
        "id": "5DSoLzTP0PZJ"
      },
      "id": "5DSoLzTP0PZJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "frankmorales2020/bert-base-cased_fine_tuned_glue_cola"
      ],
      "metadata": {
        "id": "MiRMRaKpM3S0"
      },
      "id": "MiRMRaKpM3S0"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "token = userdata.get('HF_TOKEN')\n",
        "hf_auth_token = token\n",
        "model_id = \"frankmorales2020/mistral-7b-alpha-finetuned-llm-science-exam-tpu-colab-v6e-1\"\n",
        "\n",
        "# Move the model to CPU before pushing\n",
        "model.cpu()\n",
        "\n",
        "model.push_to_hub(model_id, use_auth_token=hf_auth_token)\n",
        "tokenizer.push_to_hub(model_id, use_auth_token=hf_auth_token)"
      ],
      "metadata": {
        "id": "M-UGe84woUmm"
      },
      "id": "M-UGe84woUmm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTION2"
      ],
      "metadata": {
        "id": "H7kNBBSA0U1o"
      },
      "id": "H7kNBBSA0U1o"
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_MODEL:\n",
        "    from google.colab import userdata\n",
        "    token = userdata.get('HF_TOKEN')\n",
        "    hf_auth_token = token\n",
        "\n",
        "    model_id = \"frankmorales2020/mistral-7b-alpha-finetuned-llm-science-exam-tpu-colab-v6e-1\"\n",
        "\n",
        "    if map3_score >= 0.934:\n",
        "        print(\"Merging directly to Main...\")\n",
        "\n",
        "        model = model.cpu()\n",
        "        model.push_to_hub(\n",
        "            model_id,\n",
        "            tokenizer=tokenizer,\n",
        "            private=False,\n",
        "            create_pr=False,\n",
        "            commit_message=f\"Merging directly to main. Map@3 = {map3_score}\",\n",
        "            max_shard_size=\"2GB\",\n",
        "            use_auth_token=hf_auth_token,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\"Creating PR...\")\n",
        "\n",
        "        model = model.cpu()\n",
        "        model.push_to_hub(\n",
        "            model_id,\n",
        "            tokenizer=tokenizer,\n",
        "            private=False,\n",
        "            create_pr=1,\n",
        "            commit_message=f\"Creating PR. Map@3 = {map3_score}\",\n",
        "            max_shard_size=\"2GB\",\n",
        "            use_auth_token=hf_auth_token,\n",
        "        )"
      ],
      "metadata": {
        "id": "nmR6EFovMb5t"
      },
      "id": "nmR6EFovMb5t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model from HuggingFace and Evaluate"
      ],
      "metadata": {
        "id": "8liC3v6pNrgF"
      },
      "id": "8liC3v6pNrgF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "402d7a32"
      },
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Define the repository ID\n",
        "repo_id = \"frankmorales2020/mistral-7b-alpha-finetuned-llm-science-exam-tpu-colab-v6e-1\"\n",
        "\n",
        "# Load the model from Hugging Face Hub\n",
        "model_from_hf = AutoModelForCausalLM.from_pretrained(\n",
        "    repo_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\", # Automatically maps the model to available devices (like TPU)\n",
        "#     use_auth_token=True, # Uncomment if authentication is required\n",
        ")\n",
        "\n",
        "print(f\"Model loaded successfully from {repo_id}\")"
      ],
      "id": "402d7a32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "mistral_7b_responses = []\n",
        "\n",
        "## HF MODEL\n",
        "model=model_from_hf\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for step, data in enumerate(xla_test_loader): # Use xla_test_loader instead of test_loader\n",
        "    with torch.no_grad():\n",
        "        if (step + 1) % 20 == 0:\n",
        "            print(step + 1)\n",
        "\n",
        "        # data[\"attention_mask\"] = torch.where(data[\"input_ids\"]==2, 0, 1).to(device) # Data is already on device with xla_test_loader\n",
        "\n",
        "        sample_prediction = model(**data)\n",
        "        sorted_logits = torch.argsort(\n",
        "            -sample_prediction.logits[0][-1, [330, 365, 334, 384, 413]])\n",
        "\n",
        "        mistral_7b_responses.append([\"ABCDE\"[x] for x in sorted_logits][:3])\n",
        "\n",
        "        del sample_prediction\n",
        "        del sorted_logits\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "print(len(mistral_7b_responses), mistral_7b_responses[:10])\n",
        "\n",
        "test_data[\"clean_answer\"] = mistral_7b_responses\n",
        "\n",
        "apks = [apk([actual], predicted, k=3) for actual, predicted in zip(\n",
        "    test_data[\"answer\"].values,\n",
        "    test_data[\"clean_answer\"].values\n",
        ")]\n",
        "\n",
        "test_data[\"apk\"] = apks\n",
        "print(test_data[\"apk\"].value_counts())\n",
        "print(\"\\n\")\n",
        "print(f\"MAP@3: {np.mean(apks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y11R5WrOAZb",
        "outputId": "1eae0f4d-ba8d-4494-ac11-d3431dba62c8"
      },
      "id": "8Y11R5WrOAZb",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "160\n",
            "180\n",
            "200\n",
            "200 [['D', 'A', 'E'], ['A', 'B', 'D'], ['A', 'C', 'B'], ['B', 'A', 'C'], ['B', 'A', 'D'], ['B', 'A', 'C'], ['A', 'C', 'B'], ['B', 'D', 'A'], ['B', 'C', 'A'], ['A', 'C', 'B']]\n",
            "apk\n",
            "1.000000    130\n",
            "0.500000     32\n",
            "0.000000     23\n",
            "0.333333     15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "MAP@3: 0.755\n",
            "CPU times: user 1min 18s, sys: 10.4 s, total: 1min 28s\n",
            "Wall time: 1min 52s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 224.809352,
      "end_time": "2023-09-29T13:15:05.015225",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-09-29T13:11:20.205873",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t5MWWYFjLUWH"
      ],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
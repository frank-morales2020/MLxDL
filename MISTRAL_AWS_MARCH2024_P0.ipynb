{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORq75qymF1vxAu2Z4Wgb1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_AWS_MARCH2024_P0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaWcS7jcG5VF",
        "outputId": "7cebc687-9241-4a15-b3ba-bb9dbf469388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install sagemaker boto3 --quiet\n",
        "\n",
        "#%pip install langchain==0.0.309 --quiet --root-user-action=ignore\n",
        "%pip install langchain --quiet\n",
        "\n",
        "import colab_env\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#print(aws_access_key_id)\n",
        "#print()\n",
        "#print(f\"aws_access_key_id: '{aws_access_key_id}'\")\n",
        "#print(f\"aws_secret_access_key: '{aws_secret_access_key}'\")\n",
        "\n",
        "#print(f\"region: '{region}'\")\n",
        "#print()"
      ],
      "metadata": {
        "id": "AcZLELCdKFcJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama-2\n",
        "https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "claude-3\n",
        "https://aws.amazon.com/blogs/aws/anthropics-claude-3-sonnet-foundation-model-is-now-available-in-amazon-bedrock/\n"
      ],
      "metadata": {
        "id": "AuY8PGbLM9QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import boto3\n",
        "import os\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "\n",
        "# added by frank morales december 13, 2023\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "# https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-choose.html#jumpstart-foundation-models-choose-eula\n",
        "\n",
        "#original\n",
        "#llm_model_id, llm_model_version = \"meta-textgeneration-llama-2-7b-f\", \"*\"\n",
        "#llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version)\n",
        "## error below\n",
        "\n",
        "#modified by frankmorales\n",
        "#llm_model_id, llm_model_version = \"meta-textgeneration-llama-2-7b-f\", \"2.*\"\n",
        "\n",
        "### LLAMA-2\n",
        "#llm_model_id = 'meta-textgeneration-llama-2-7b-f'\n",
        "#llm_model_version = '2.0.4'\n",
        "\n",
        "# https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html\n",
        "\n",
        "# https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/examples?provider=Anthropic\n",
        "\n",
        "## MISTRAL\n",
        "llm_model_id = 'huggingface-llm-mistral-7b'\n",
        "llm_model_version = '2.1.0'\n",
        "\n",
        "llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version, role=ROLE_ARN, region='us-east-1')\n",
        "\n",
        "\n",
        "#{\n",
        "#  \"modelId\": \"anthropic.claude-v2\",\n",
        "#  \"contentType\": \"application/json\",\n",
        "#  \"accept\": \"application/json\",\n",
        "\n",
        "\n",
        "#modified by frankmorales\n",
        "llm_predictor = llm_model.deploy(accept_eula=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P1dwGh5KWRo",
        "outputId": "e60d6c87-9d3b-41ca-cc74-40726edcd5f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
            "--------!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the model endpoint NAME, not the ARN\n",
        "llm_model_endpoint_name = llm_predictor.endpoint_name\n",
        "llm_model_endpoint_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2XRGT2iULD1w",
        "outputId": "04e509ec-d921-4b6a-9c92-bf58351942e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hf-llm-mistral-7b-2024-03-05-18-43-18-641'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "from langchain import PromptTemplate, SagemakerEndpoint\n",
        "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA\n",
        "import json\n",
        "\n",
        "class QAContentHandler(LLMContentHandler):\n",
        "    content_type = \"application/json\"\n",
        "    accepts = \"application/json\"\n",
        "\n",
        "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
        "        input_str = json.dumps(\n",
        "            {\"inputs\" : [\n",
        "                [\n",
        "                    {\n",
        "                        \"role\" : \"system\",\n",
        "                        \"content\" : \"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\" : \"user\",\n",
        "                        \"content\" : prompt\n",
        "                    }\n",
        "                ]],\n",
        "                \"parameters\" : {**model_kwargs}\n",
        "            })\n",
        "        return input_str.encode('utf-8')\n",
        "\n",
        "    def transform_output(self, output: bytes) -> str:\n",
        "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
        "        ### LLAMA-2\n",
        "        #return response_json[0][\"generation\"][\"content\"]\n",
        "        ### MISTRAL\n",
        "        return response_json[0]['generated_text'][\"content\"]\n",
        "\n",
        "qa_content_handler = QAContentHandler()\n",
        "\n"
      ],
      "metadata": {
        "id": "CZNHvIw_T1pO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QAContentHandler(LLMContentHandler):\n",
        "    content_type = \"application/json\"\n",
        "    accepts = \"application/json\"\n",
        "\n",
        "qa_content_handler = QAContentHandler()"
      ],
      "metadata": {
        "id": "lkyQiftbzqGw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"region\")\n",
        "output=os.getenv(\"output\")\n",
        "\n",
        "llm = SagemakerEndpoint(\n",
        "        endpoint_name=llm_model_endpoint_name,\n",
        "        region_name=region,\n",
        "        model_kwargs={\"max_new_tokens\": 5000, \"top_p\": 0.9, \"temperature\": 1e-11},\n",
        "        endpoint_kwargs={\"CustomAttributes\": 'accept_eula=true'},\n",
        "        content_handler=qa_content_handler\n",
        "    )"
      ],
      "metadata": {
        "id": "hTF3N40tLQuM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LLAMA-2 ####\n",
        "\n",
        "#original by the book\n",
        "#query = \"WHAT IS PERU?\"\n",
        "\n",
        "##modified by Frank Morales\n",
        "#response=llm.predict(query)"
      ],
      "metadata": {
        "id": "5UIDEOp3BTGt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### MISTRAL ######\n",
        "\n",
        "#original by the book\n",
        "query = \"how AWS evolved?\"\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# Prepare the input for the model\n",
        "#input_data = {\"inputs\": query}\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "#print(response_body)\n",
        "\n",
        "print(f'Query:', query)\n",
        "print()\n",
        "print(f'Response:', response_body[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vj8eTH3LYYD",
        "outputId": "9fd85a38-4c4d-4c1e-db51-e67f9160e2fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: how AWS evolved?\n",
            "\n",
            "Response: \n",
            "\n",
            "Anurag: AWS is a collection of IaaS and PaaS services put together in a cloud platform. Almost all companies that are either building new applications, or are looking to scale their existing applications, to leverage cloud computing. AWS is one of the leaders in the market place, providing public cloud.  Also, it’s not just for building applications – so it’s also the choice of companies that are moving from their existing data centers to build a new data center capability in AWS.\n",
            "\n",
            "We have gone from Business Intelligence tools in 2004-05, to the whole area of Data Warehousing, which is quite mature; the next step for many businesses is cloud computing. This year, if you go into the major vendors’ websites, many big business applications like E-mail and CRM, will be available as Software as a Service (SaaS) platforms. So, the whole scope for the next few years is moving this application, that runs inside the companies data center to a SaaS application running in the cloud.\n",
            "\n",
            "Why should I migrate to Amazon Web Services?\n",
            "\n",
            "Yogendra: It’s quite a strategic decision. AWS is one of the leaders in the cloud space, so these are technologies which will help a company pivot in its business strategy.  AWS is the best place to move your application to, because it allows you to innovate the fastest. Their pace of innovation is the fastest and in order to keep up with fast pace of IT in today’s industry, you need to leverage AWS.\n",
            "\n",
            "What happens then, next?\n",
            "\n",
            "Anurag: The next step after migrating workloads to AWS is to utilize the various services that it has.  The next step is to optimize the usage of the AWS platform – to leverage all the value adds that AWS provides.\n",
            "\n",
            "Who is Virtusa?\n",
            "\n",
            "Yogendra: Virtusa is a global IT services company providing outsourced and offshore services for the banking, capital markets, and insurance industries on the AWS Platform. We are quite familiar with all major leading platforms, such as Java, Microsoft, IBM, SAP, Oracle, Google, Salesforce. We have a team of experts who have successfully built many cloud platforms on the AWS platform. We have successfully built Telco platforms on AWS, we have built customer experience platforms on AWS, we have built data platforms on AWS. We have a very mature practice on AWS. Going forward, AWS is going to play a very important role in our strategy and strategy of our clients.\n",
            "\n",
            "So what are the next steps that need to be done for AWS migration?\n",
            "\n",
            "Anurag: The next step could be building applications on top of the platform. Also building analytics applications on top of this platform – using the tools and technologies that AWS has.  One example is the machine learning capability in the cloud. AWS has one of the simplest solutions for machine learning or applying artificial intelligence over your data.\n",
            "\n",
            "Yogendra: platform.aws.virtusa.com is a microsite which is specifically built to demonstrate Virtusa’s strong practice in delivering solutions on the AWS platform.\n",
            "\n",
            "What are some of the challenges in migrating to the cloud?\n",
            "\n",
            "Yogendra: We have done a lot of migrations, and the biggest challenge in migrating applications to the cloud is the lack of flexibility and scalability, which AWS provides. Also, in the cloud, you can choose to pay for only what you need and consume; and AWS provides that flexibility.\n",
            "\n",
            "The other challenge is the multiple options that AWS provides to the customer. It’s overwhelming for a lot of customers to decide which platform works best for them and from a design perspective it’s also very important to architect the platform to leverage the AWS cloud infrastructure. A lot of companies think that “I will just drop my application on AWS” but it is very critical to optimization of your application before making that jump to the cloud. Once you do that, Virtusa uses their expertise in leveraging the AWS platform for a good amount of optimization.\n",
            "\n",
            "Anurag: AWS gives you different services and you have to decide which one you need.  Choices are a big challenge when it comes to migrating to the cloud.\n",
            "\n",
            "What is the cost of migration?\n",
            "\n",
            "Yogendra: We have various services. Starting from just migration, to leveraging AWS platform to architecture, design, and implement entire solutions on AWS.\n",
            "\n",
            "Anurag: There is a lot of support that Virtusa provides, like proof of concepts or a “lift and shift” from old environment to AWS. All of these can be customized to meet your needs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEAN UP"
      ],
      "metadata": {
        "id": "yHCOvGVoLoRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mnSz_gVLtPS",
        "outputId": "c1410b06-5eba-4ef3-be5a-f47cc307ae1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "EndpointName\n",
            "hf-llm-mistral-7b-2024-03-05-18-43-18-641\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "ModelName\n",
            "hf-llm-mistral-7b-2024-03-05-18-43-18-639\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "EndpointConfigName\n",
            "hf-llm-mistral-7b-2024-03-05-18-43-18-641\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cuWqTrK6kGGm"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMDBlNuC0AIbPett1JXAvRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/deepseek_text2sql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tune"
      ],
      "metadata": {
        "id": "lvj5UHbLjaO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Set Up Your Environment ---\n",
        "!pip install scikit-learn -q # For potential evaluation metrics (optional)\n",
        "!pip install -U transformers -q\n",
        "!pip install -U datasets -q\n",
        "!pip install -U accelerate -q\n",
        "!pip install -U peft -q\n",
        "!pip install -U trl -q # For SFTTrainer\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install unsloth -q # Recommended for speed and efficiency\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # For latest Unsloth"
      ],
      "metadata": {
        "id": "7mMhriDvis-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/deepseek_r1_text2sql_finetuned"
      ],
      "metadata": {
        "id": "iAWTfvJUH0J2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "# 0. Initial Setup\n",
        "# Install necessary libraries if running in Colab/Jupyter (ensure these are installed first)\n",
        "# !pip install -U transformers datasets accelerate peft trl bitsandbytes unsloth scikit-learn\n",
        "import torch\n",
        "import io\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import load_dataset, Dataset # Added Dataset for potential manual splits\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer, AutoTokenizer\n",
        "from huggingface_hub import login # Optional: for pushing model to Hub\n",
        "\n",
        "# Ensure you are logged into Hugging Face if you plan to push models or use private datasets\n",
        "# login() # Uncomment and run if needed\n",
        "\n",
        "# 1. Load the Model and Tokenizer\n",
        "print(\"Loading DeepSeek-R1 model and tokenizer...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length=2048, # Adjust if your combined input/output is longer\n",
        "    dtype=None, # Automatically chooses bfloat16 or float16 based on GPU\n",
        "    load_in_4bit=True, # Enable 4-bit quantization for memory efficiency\n",
        ")\n",
        "print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "# 2. Apply LoRA Adapters\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16, # Rank of the LoRA matrices (common values: 8, 16, 32, 64)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16, # Scaling factor for LoRA weights\n",
        "    lora_dropout=0, # Dropout rate for LORA (set to 0 for inference)\n",
        "    bias=\"none\", # Or \"all\", \"lora_only\"\n",
        "    use_gradient_checkpointing=True, # Recommended for memory saving\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "print(\"LoRA adapters applied.\")\n",
        "\n",
        "# 3. Load Dataset\n",
        "print(\"Loading and preparing b-mc2/sql-create-context dataset...\")\n",
        "\n",
        "# Load the dataset directly from the Hugging Face Hub\n",
        "dataset_name = \"b-mc2/sql-create-context\"\n",
        "full_dataset = load_dataset(dataset_name)\n",
        "\n",
        "# The b-mc2/sql-create-context dataset typically has a single 'train' split.\n",
        "# You might need to split it manually for training and evaluation.\n",
        "# Let's assume we still want a 90/10 split.\n",
        "raw_dataset_split = full_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset_raw = raw_dataset_split['train']\n",
        "eval_dataset_raw = raw_dataset_split['test']\n",
        "\n",
        "\n",
        "# --- NEW: Select a specific number of samples ---\n",
        "# Define the desired number of samples for training and evaluation\n",
        "num_train_samples = 2000 # Set your desired number of training samples (e.g., 2000 instead of all)\n",
        "num_eval_samples = 200   # Set your desired number of evaluation samples (e.g., 200)\n",
        "\n",
        "# Ensure the desired number of samples doesn't exceed the available samples\n",
        "num_train_samples = min(num_train_samples, len(train_dataset_raw))\n",
        "num_eval_samples = min(num_eval_samples, len(eval_dataset_raw))\n",
        "\n",
        "# Select the desired number of samples from the raw splits\n",
        "# We use slicing [start:end] to select the first `num_..._samples` entries\n",
        "train_dataset_selected = train_dataset_raw.select(range(num_train_samples))\n",
        "eval_dataset_selected = eval_dataset_raw.select(range(num_eval_samples))\n",
        "\n",
        "print(f\"Raw dataset loaded from {dataset_name}. Training entries: {len(train_dataset_raw)}, Evaluation entries: {len(eval_dataset_raw)}\")\n",
        "print('\\n')\n",
        "print(f\"Selected sample sizes: Training entries: {len(train_dataset_selected)}, Evaluation entries: {len(eval_dataset_selected)}\")\n",
        "# --- END NEW ---\n",
        "\n",
        "\n",
        "# 4. Define the Formatting Function for Text-to-SQL\n",
        "# This function converts entries from b-mc2/sql-create-context dataset\n",
        "# into the chat format that the DeepSeek-R1 model will be trained on.\n",
        "\n",
        "def format_sql_example(example):\n",
        "    \"\"\"\n",
        "    Formats an example from the b-mc2/sql-create-context dataset into a chat template.\n",
        "\n",
        "    Args:\n",
        "        example (dict): A dictionary representing one row in the dataset.\n",
        "                        Expected keys: 'question', 'context', 'answer'.\n",
        "\n",
        "    Returns:\n",
        "        dict: The example dictionary with an added 'text' key containing the\n",
        "              formatted chat string.\n",
        "    \"\"\"\n",
        "    # Extract data from the dataset example\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"] # This usually contains the CREATE TABLE statements\n",
        "    answer = example[\"answer\"]   # This is the correct SQL query\n",
        "\n",
        "    # Construct the prompt for the model\n",
        "    # The prompt should provide the database schema as context and the user's question.\n",
        "    # The model should learn to generate the SQL query (the 'answer').\n",
        "    instruction = \"Given the database schema below, write a SQL query that answers the following question.\"\n",
        "\n",
        "    full_user_prompt = f\"{instruction}\\n\\nDatabase Schema:\\n{context}\\n\\nQuestion: {question}\"\n",
        "\n",
        "    # Combine into the chat format for training\n",
        "    # The 'user' role contains the context and question.\n",
        "    # The 'assistant' role contains the target SQL query.\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": full_user_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": answer}\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template using the tokenizer.\n",
        "    # tokenize=False returns a string.\n",
        "    # add_special_tokens=False prevents adding BOS/EOS tokens here,\n",
        "    # as the SFTTrainer handles tokenization and special tokens during training.\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_special_tokens=False)\n",
        "\n",
        "    return example\n",
        "\n",
        "# Apply the formatting function to your selected datasets\n",
        "print(\"Applying formatting function to datasets for Text-to-SQL...\")\n",
        "\n",
        "# --- MODIFIED: Use the selected datasets for mapping ---\n",
        "# Instead of mapping over train_dataset_raw and eval_dataset_raw,\n",
        "# we map over the datasets that contain the selected number of samples.\n",
        "train_dataset = train_dataset_selected.map(format_sql_example, batched=False)\n",
        "eval_dataset = eval_dataset_selected.map(format_sql_example, batched=False)\n",
        "# --- END MODIFIED ---\n",
        "\n",
        "print(\"Dataset preparation complete with Text-to-SQL formatting.\")\n",
        "print(\"Dataset preparation complete with Text-to-SQL formatting.\")\n",
        "\n",
        "# 5. Set Up and Configure the Trainer\n",
        "print(\"Setting up SFTTrainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset, # Now using the formatted datasets\n",
        "    eval_dataset=eval_dataset,   # Now using the formatted datasets\n",
        "    dataset_text_field=\"text\", # This field holds the formatted chat messages\n",
        "    max_seq_length=2048, # Ensure this is sufficient for your long itineraries\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(), # Use fp16 if bfloat16 not supported\n",
        "        bf16=torch.cuda.is_bf16_supported(),     # Use bf16 if supported (recommended)\n",
        "        logging_steps=10,\n",
        "        #output_dir=\"./deepseek_r1_tourism_planner_finetuned\", # Consistent output directory name\n",
        "        output_dir=\"./deepseek_r1_text2sql_finetuned\", # Consistent output directory name\n",
        "        optim=\"adamw_8bit\",\n",
        "        seed=3407,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False, # Lower loss is better\n",
        "        report_to=\"none\", # Disable logging to Weights & Biases if not needed\n",
        "    ),\n",
        ")\n",
        "print(\"SFTTrainer configured.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U1SvndGrk6cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Start Training\n",
        "print('\\n')\n",
        "print(\"Starting training...\")\n",
        "from unsloth import unsloth_train\n",
        "# trainer_stats = trainer.train() << Buggy gradient accumulation\n",
        "# https://unsloth.ai/blog/gradient\n",
        "trainer_stats = unsloth_train(trainer)\n",
        "#trainer.train() # Uncomment to start the training\n",
        "print(\"Training complete.\")\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# 7. Save Your Fine-tuned Model\n",
        "#output_dir = \"./deepseek_r1_tourism_planner_finetuned\"\n",
        "output_dir = \"./deepseek_r1_text2sql_finetuned\"\n",
        "print(f\"Saving fine-tuned model to {output_dir}...\")\n",
        "model.save_pretrained(output_dir, tokenizer) # Uncomment to save the model and tokenizer\n",
        "print(\"Model saved locally.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "1lhHJtX5VJ5b",
        "outputId": "9f3ebedf-53e1-40ec-988e-9ba418eb04d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,000 | Num Epochs = 3 | Total steps = 3,000\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='530' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 530/3000 05:38 < 26:23, 1.56 it/s, Epoch 0.53/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.957300</td>\n",
              "      <td>1.044319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation"
      ],
      "metadata": {
        "id": "cuWqTrK6kGGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "SniGbApxqHPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm rf /content/gdrive/MyDrive/model/deepseek_r1_text2sql_finetuned\n",
        "!cp -r /content/deepseek_r1_text2sql_finetuned /content/gdrive/MyDrive/model/deepseek_r1_text2sql_finetuned"
      ],
      "metadata": {
        "id": "Txa43qZIql5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score -q\n",
        "!pip install sacrebleu -q"
      ],
      "metadata": {
        "id": "NsvttctQzyBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model evaluation Refactored for b-mc2/sql-create-context dataset\n",
        "# --- Evaluation Code Block ---\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd # For saving results\n",
        "from datasets import load_dataset # For loading evaluation data\n",
        "\n",
        "# Import libraries for model loading and generation\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer # TextStreamer is optional\n",
        "from unsloth import FastLanguageModel # Using Unsloth for efficient loading\n",
        "\n",
        "# Import libraries for metrics calculation\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "import numpy as np # For calculating averages\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the path where your fine-tuned model was saved\n",
        "# Make sure this path points to the directory containing the saved model files\n",
        "# This path should now point to the model fine-tuned on the SQL dataset\n",
        "fine_tuned_model_path = \"/content/gdrive/MyDrive/model/deepseek_r1_text2sql_finetuned\" # Example Google Drive path or local path\n",
        "\n",
        "# Define the name of the dataset from Hugging Face Hub\n",
        "dataset_name = \"b-mc2/sql-create-context\"\n",
        "\n",
        "# Number of examples from the evaluation set to run inference on\n",
        "# Set to a smaller number for quick testing, or len(eval_dataset) for full evaluation\n",
        "num_examples_to_evaluate = 5 # Evaluate a specific number of examples from the test split\n",
        "\n",
        "# Maximum sequence length used during fine-tuning (must match)\n",
        "max_seq_length = 2048 # Ensure this matches the value used during training\n",
        "\n",
        "# --- 1. Load the Fine-tuned Model and Tokenizer ---\n",
        "print(f\"Loading fine-tuned model from {fine_tuned_model_path}...\")\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=fine_tuned_model_path, # Load from your saved model directory\n",
        "        max_seq_length=max_seq_length,     # Must match the max_seq_length used during fine-tuning\n",
        "        dtype=None,                        # Will auto-detect from saved config\n",
        "        load_in_4bit=True,                 # Load in 4-bit for memory efficiency\n",
        "    )\n",
        "    # Ensure the model is on GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        model.to(\"cuda\")\n",
        "        print(\"Model moved to GPU.\")\n",
        "    else:\n",
        "        print(\"CUDA not available. Model loading on CPU (will be slower).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please check that the model path is correct and the model files exist.\")\n",
        "    # Exit or handle the error appropriately if model loading fails\n",
        "    exit() # Example: Exit the script\n",
        "\n",
        "\n",
        "print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "# Optional: Set up TextStreamer for real-time output during generation\n",
        "# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "# --- 2. Load the Evaluation Dataset ---\n",
        "# We will load the same dataset used for training but use the test split\n",
        "print(f\"Loading evaluation dataset from {dataset_name}...\")\n",
        "try:\n",
        "    # Load the dataset directly from the Hugging Face Hub\n",
        "    full_dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # The b-mc2/sql-create-context dataset typically has a single 'train' split.\n",
        "    # We split it manually for training and evaluation, 90/10 split, same seed as training.\n",
        "    raw_dataset_split_for_eval = full_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "    # Use the 'test' split created here for evaluation\n",
        "    eval_dataset = raw_dataset_split_for_eval['test']\n",
        "\n",
        "\n",
        "    print(f\"Evaluation dataset loaded with {len(eval_dataset)} entries.\")\n",
        "\n",
        "    # Adjust num_examples_to_evaluate if it's larger than the available dataset\n",
        "    num_examples_to_evaluate = min(num_examples_to_evaluate, len(eval_dataset))\n",
        "    # Select the specific number of examples for evaluation\n",
        "    eval_dataset_selected = eval_dataset.select(range(num_examples_to_evaluate))\n",
        "\n",
        "    print(f\"Evaluating on {len(eval_dataset_selected)} examples from the test set.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Dataset file not found at {dataset_name}\")\n",
        "    print(\"Please ensure you have access to the dataset on the Hugging Face Hub.\")\n",
        "    exit() # Example: Exit if dataset not found\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 3. Define the PROMPT CONSTRUCTION function for inference ---\n",
        "# This function should mirror the 'user' part of your formatting function used during training.\n",
        "# It takes raw data and turns it into the prompt the model expects for SQL generation.\n",
        "\n",
        "def construct_sql_inference_prompt(question, context):\n",
        "    \"\"\"Constructs the full user prompt for the model based on SQL question and schema context.\"\"\"\n",
        "\n",
        "    # The instruction for the model to generate a SQL query\n",
        "    instruction = \"Given the database schema below, write a SQL query that answers the following question.\"\n",
        "\n",
        "    # Construct the full user prompt for the model\n",
        "    full_user_prompt = f\"{instruction}\\n\\nDatabase Schema:\\n{context}\\n\\nQuestion: {question}\"\n",
        "\n",
        "    # Apply the chat template for the single user turn\n",
        "    messages = [{\"role\": \"user\", \"content\": full_user_prompt}]\n",
        "\n",
        "    # Tokenize the prompt for generation\n",
        "    tokenized_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "    # Move input to GPU if CUDA is available\n",
        "    if torch.cuda.is_available():\n",
        "        tokenized_input = tokenized_input.to(\"cuda\")\n",
        "\n",
        "    return tokenized_input\n",
        "\n",
        "\n",
        "# --- 4. Evaluation Loop with Metrics ---\n",
        "print(\"\\n--- Starting Evaluation Loop ---\")\n",
        "results = []\n",
        "rouge_scores = []\n",
        "bleu_scores = [] # Note: BLEU might not be the best metric for SQL, but we'll include it for comparison.\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "# using 'rouge1', 'rouge2', and 'rougeL' f-measure (F1 score)\n",
        "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Generation parameters (can be adjusted)\n",
        "generation_kwargs = {\n",
        "    \"max_new_tokens\": 256, # Max length for SQL queries is likely shorter\n",
        "    \"use_cache\": True,\n",
        "    \"temperature\": 0.1,     # Often lower temperature for code/SQL generation for more deterministic output\n",
        "    \"top_p\": 0.9,          # Adjust nucleus sampling\n",
        "    \"do_sample\": True,      # Enable sampling\n",
        "    \"pad_token_id\": tokenizer.eos_token_id, # Set padding token\n",
        "    #\"streamer\": streamer,  # Uncomment if using TextStreamer\n",
        "}\n",
        "\n",
        "\n",
        "# Loop through the selected evaluation examples\n",
        "for i in range(len(eval_dataset_selected)): # Use the length of the selected dataset\n",
        "    example = eval_dataset_selected[i] # Access example from the selected subset\n",
        "    original_question = example[\"question\"]\n",
        "    database_context = example[\"context\"] # This is the database schema\n",
        "    ground_truth_answer = example[\"answer\"] # This is the correct SQL query\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Evaluation Case {i+1}/{len(eval_dataset_selected)} ---\")\n",
        "    print(f\"Original Question: {original_question}\")\n",
        "    print(f\"Database Schema:\\n{database_context}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Ground Truth SQL Query (from dataset) ---\\n\")\n",
        "    print(ground_truth_answer)\n",
        "\n",
        "\n",
        "    # Construct the prompt for the model and get token IDs\n",
        "    input_ids = construct_sql_inference_prompt(original_question, database_context)\n",
        "\n",
        "    # Generate the SQL query using the model\n",
        "    try:\n",
        "        # Note: The model was fine-tuned to output the SQL query immediately after the user prompt.\n",
        "        # We generate starting from the end of the prompt.\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            **generation_kwargs # Pass the generation parameters\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        # Post-process the generated text if necessary (e.g., remove trailing unwanted characters)\n",
        "        # Simple example: remove trailing whitespace or potential chat remnants\n",
        "        generated_text = generated_text.strip()\n",
        "        # You might need more sophisticated parsing depending on model output\n",
        "        # For example, if it includes chat turns after the SQL, you'd need to extract just the SQL.\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text generation for example {i+1}: {e}\")\n",
        "        generated_text = \"Error generating query.\" # Indicate failure\n",
        "\n",
        "\n",
        "    print(\"\\n--- Generated SQL Query ---\\n\")\n",
        "    print(generated_text)\n",
        "\n",
        "    # --- Calculate Metrics for the current example ---\n",
        "    current_rouge_scores = None\n",
        "    current_bleu_score = None\n",
        "\n",
        "    if generated_text != \"Error generating query.\":\n",
        "        try:\n",
        "            # ROUGE Score Calculation\n",
        "            # Pass ground truth and generated text strings to the scorer\n",
        "            current_rouge_scores = rouge_scorer_obj.score(ground_truth_answer, generated_text)\n",
        "            rouge_scores.append(current_rouge_scores)\n",
        "            print(f\"ROUGE Scores: ROUGE-1: {current_rouge_scores['rouge1'].fmeasure:.4f}, ROUGE-2: {current_rouge_scores['rouge2'].fmeasure:.4f}, ROUGE-L: {current_rouge_scores['rougeL'].fmeasure:.4f}\")\n",
        "\n",
        "            # BLEU Score Calculation\n",
        "            current_bleu = sacrebleu.corpus_bleu([generated_text], [[ground_truth_answer]])\n",
        "            current_bleu_score = current_bleu.score\n",
        "            bleu_scores.append(current_bleu_score)\n",
        "            print(f\"BLEU Score: {current_bleu_score:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating metrics for example {i+1}: {e}\")\n",
        "            # Metrics for this example will be None\n",
        "    else:\n",
        "         print(\"Skipping metric calculation due to generation error.\")\n",
        "\n",
        "\n",
        "    # Store results for later analysis\n",
        "    results.append({\n",
        "        \"original_question\": original_question,\n",
        "        \"database_context\": database_context, # Optional, might be too long to store all\n",
        "        \"generated_query\": generated_text,\n",
        "        \"ground_truth_query\": ground_truth_answer, # Store ground truth SQL\n",
        "        \"rouge_scores\": current_rouge_scores,       # Store the detailed ROUGE scores dict\n",
        "        \"bleu_score\": current_bleu_score            # Store the BLEU score (float or None)\n",
        "    })\n",
        "\n",
        "print(\"\\n--- Evaluation Loop Finished ---\")\n",
        "\n",
        "# --- 5. Analysis and Metrics Summary ---\n",
        "print(\"\\n--- Overall Evaluation Summary ---\")\n",
        "\n",
        "# Calculate and print average metrics\n",
        "# Filter out None values before calculating averages\n",
        "valid_rouge_scores = [s for s in rouge_scores if s is not None]\n",
        "valid_bleu_scores = [s for s in bleu_scores if s is not None]\n",
        "\n",
        "if valid_rouge_scores:\n",
        "    avg_rouge1 = np.mean([s['rouge1'].fmeasure for s in valid_rouge_scores])\n",
        "    avg_rouge2 = np.mean([s['rouge2'].fmeasure for s in valid_rouge_scores])\n",
        "    avg_rougeL = np.mean([s['rougeL'].fmeasure for s in valid_rouge_scores])\n",
        "    print(f\"Average ROUGE-1 F-measure (over {len(valid_rouge_scores)} examples): {avg_rouge1:.4f}\")\n",
        "    print(f\"Average ROUGE-2 F-measure (over {len(valid_rouge_scores)} examples): {avg_rouge2:.4f}\")\n",
        "    print(f\"Average ROUGE-L F-measure (over {len(valid_rouge_scores)} examples): {avg_rougeL:.4f}\")\n",
        "else:\n",
        "    print(\"No valid ROUGE scores were calculated.\")\n",
        "\n",
        "\n",
        "if valid_bleu_scores:\n",
        "    avg_bleu = np.mean(valid_bleu_scores)\n",
        "    print(f\"Average BLEU Score (over {len(valid_bleu_scores)} examples): {avg_bleu:.4f}\")\n",
        "else:\n",
        "    print(\"No valid BLEU scores were calculated.\")\n",
        "\n",
        "# --- Save Results ---\n",
        "# Convert the list of results dictionaries to a pandas DataFrame and save to JSONL\n",
        "try:\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    output_filename = \"sql_evaluation_results_with_metrics.jsonl\" # New filename\n",
        "    df_results.to_json(output_filename, orient=\"records\", lines=True)\n",
        "    print(f\"\\nEvaluation complete. Detailed results saved to '{output_filename}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving results to JSONL: {e}\")\n",
        "    print(\"Consider saving fewer details (e.g., only text outputs and average metrics) if this persists.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VR2eZCbs_rws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
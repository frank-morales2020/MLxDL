{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lh9mAJGLdT5k",
        "GalnO354d_i8"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPWn4nG2xsriwT0sUMHHmi/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FP_POC_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "!pip install -U kagglehub -q"
      ],
      "metadata": {
        "id": "oY_zU_Q5_7sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Classes Definition"
      ],
      "metadata": {
        "id": "lh9mAJGLdT5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, early_stopping_patience=3):\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.best_metric = None\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "\n",
        "        metric_to_check = args.metric_for_best_model\n",
        "\n",
        "        if metric_to_check is not None and metrics is not None:\n",
        "            current_metric = metrics.get(metric_to_check)\n",
        "\n",
        "            if self.best_metric is None or (\n",
        "                (args.greater_is_better and current_metric > self.best_metric) or\n",
        "                (not args.greater_is_better and current_metric < self.best_metric)\n",
        "            ):\n",
        "                self.best_metric = current_metric\n",
        "                self.patience_counter = 0  # Reset patience\n",
        "                # Optionally save the best model here\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                if self.patience_counter >= self.early_stopping_patience:\n",
        "                    print(f\"Early stopping triggered after {self.patience_counter} epochs without improvement.\")\n",
        "                    control.should_training_stop = True"
      ],
      "metadata": {
        "id": "rygH3pm75-kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the RegressionHead\n",
        "class RegressionHead(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size=1, **kwargs):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        output = self.linear(hidden_states)\n",
        "        return output\n",
        "\n",
        "# Define the FarePredictionModel\n",
        "class FarePredictionModel(nn.Module): # Inherit from nn.Module\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.config = base_model.config # Add this line to store the base model's\n",
        "        self.regression_head = RegressionHead(base_model.config.hidden_size)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask) #for bert\n",
        "        hidden_state = outputs.last_hidden_state[:, 0, :]  # for bert\n",
        "        predicted_fare = self.regression_head(hidden_state)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.MSELoss() # You can move this outside if you don't want to recreate it every time\n",
        "            loss = loss_fn(predicted_fare, labels.view(-1, 1).float()) if labels is not None else None\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": predicted_fare} # Return a dictionary with loss and logits\n"
      ],
      "metadata": {
        "id": "TlxUXJJKosLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning"
      ],
      "metadata": {
        "id": "aEjxXYgKdcKy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "b6JYvC1Q_yZG",
        "outputId": "38f69844-c188-45fe-e59f-f4d5c108ddc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,339,392 || all params: 110,822,401 || trainable%: 1.2086\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='600' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [600/800 39:23 < 13:10, 0.25 it/s, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mse</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.256300</td>\n",
              "      <td>0.134497</td>\n",
              "      <td>0.134497</td>\n",
              "      <td>0.306069</td>\n",
              "      <td>0.366738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.511200</td>\n",
              "      <td>0.116738</td>\n",
              "      <td>0.116738</td>\n",
              "      <td>0.290131</td>\n",
              "      <td>0.341669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.485300</td>\n",
              "      <td>0.136197</td>\n",
              "      <td>0.136197</td>\n",
              "      <td>0.307691</td>\n",
              "      <td>0.369049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.540500</td>\n",
              "      <td>0.119269</td>\n",
              "      <td>0.119269</td>\n",
              "      <td>0.292412</td>\n",
              "      <td>0.345353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.476400</td>\n",
              "      <td>0.116383</td>\n",
              "      <td>0.116383</td>\n",
              "      <td>0.289800</td>\n",
              "      <td>0.341150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.472000</td>\n",
              "      <td>0.103739</td>\n",
              "      <td>0.103739</td>\n",
              "      <td>0.278182</td>\n",
              "      <td>0.322086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.437100</td>\n",
              "      <td>0.111907</td>\n",
              "      <td>0.111907</td>\n",
              "      <td>0.285705</td>\n",
              "      <td>0.334526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.439600</td>\n",
              "      <td>0.102986</td>\n",
              "      <td>0.102986</td>\n",
              "      <td>0.277415</td>\n",
              "      <td>0.320915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.460300</td>\n",
              "      <td>0.102871</td>\n",
              "      <td>0.102871</td>\n",
              "      <td>0.277251</td>\n",
              "      <td>0.320735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.435800</td>\n",
              "      <td>0.122314</td>\n",
              "      <td>0.122314</td>\n",
              "      <td>0.295174</td>\n",
              "      <td>0.349734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.405400</td>\n",
              "      <td>0.110456</td>\n",
              "      <td>0.110456</td>\n",
              "      <td>0.284393</td>\n",
              "      <td>0.332349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.103926</td>\n",
              "      <td>0.103926</td>\n",
              "      <td>0.278315</td>\n",
              "      <td>0.322376</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping triggered after 3 epochs without improvement.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=600, training_loss=0.5297805404663086, metrics={'train_runtime': 2363.9025, 'train_samples_per_second': 2.707, 'train_steps_per_second': 0.338, 'total_flos': 0.0, 'train_loss': 0.5297805404663086, 'epoch': 0.027149321266968326})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch import nn\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
        "import kagglehub\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Get the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model and Tokenizer\n",
        "model_name = \"bert-base-uncased\"  # Use a standard BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModel.from_pretrained(model_name) # Instantiate the base model using from_pretrained\n",
        "model = FarePredictionModel(base_model) # Initialize your custom model with the base model\n",
        "model.to(device)  # Move the model to the device\n",
        "\n",
        "\n",
        "# Data Loading and Preprocessing\n",
        "db_name = \"akadir0223/flights-after-eda\"  # Replace with your Kaggle dataset name\n",
        "dataset_path = kagglehub.dataset_download(db_name)\n",
        "files = os.listdir(dataset_path)\n",
        "csv_file_path = next((os.path.join(dataset_path, f) for f in files if f.endswith('.csv')), None)\n",
        "\n",
        "if csv_file_path:\n",
        "    flights_df = pd.read_csv(csv_file_path)\n",
        "else:\n",
        "    print(\"No CSV file found in the dataset directory.\")\n",
        "    exit()\n",
        "\n",
        "selected_columns = ['airport_1', 'airport_2', 'fare', 'carrier_lg']\n",
        "flights_subset_df = flights_df[selected_columns]\n",
        "\n",
        "# Function to create and populate SQLite tables\n",
        "def create_and_populate_tables(flights_subset_df, num_records=10000):\n",
        "    conn = sqlite3.connect('flights.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS flight_qa (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        question TEXT,\n",
        "        fare REAL\n",
        "    );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    airport_codes = flights_subset_df['airport_1'].unique().tolist()\n",
        "    airline_codes = flights_subset_df['carrier_lg'].unique().tolist()\n",
        "\n",
        "    for _ in range(num_records):\n",
        "        airport_1 = random.choice(airport_codes)\n",
        "        airport_2 = random.choice(airport_codes)\n",
        "        airline_code = random.choice(airline_codes)\n",
        "        fare = round(random.uniform(50, 500), 2)\n",
        "        question = f\"What is the fare for a flight from {airport_1} to {airport_2} with {airline_code}?\"\n",
        "        cursor.execute(\"INSERT INTO flight_qa (question, fare) VALUES (?, ?)\", (question, fare))\n",
        "        conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# Create and populate tables\n",
        "number_records = 200000\n",
        "create_and_populate_tables(flights_subset_df, num_records=number_records)\n",
        "\n",
        "# Data Scaling and Tokenization\n",
        "fare_scaler = MinMaxScaler()\n",
        "fare_scaler.fit(flights_subset_df[['fare']])\n",
        "\n",
        "# Function to tokenize the dataset\n",
        "def create_tokenized_dataset(tokenizer, fare_scaler):\n",
        "    conn = sqlite3.connect('flights.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT question, fare FROM flight_qa\")\n",
        "    data = cursor.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    # Convert data to DataFrame\n",
        "    df = pd.DataFrame(data, columns=['question', 'fare'])\n",
        "\n",
        "    # Tokenize the questions\n",
        "    tokenized_data = tokenizer(\n",
        "        df['question'].tolist(),\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Scale the fares and convert to PyTorch tensor\n",
        "    fares = torch.tensor(fare_scaler.transform(df[['fare']]), dtype=torch.float32)\n",
        "\n",
        "    # Create a Dataset from the tokenized data and labels\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"input_ids\": tokenized_data[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_data[\"attention_mask\"],\n",
        "        \"labels\": fares,\n",
        "    })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Create the tokenized dataset\n",
        "tokenized_dataset = create_tokenized_dataset(tokenizer, fare_scaler)\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)['train']\n",
        "val_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)['test']\n",
        "\n",
        "# Define LORA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    #target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"], # MISTRAL\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Correct target modules for BERT\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    #task_type=\"CAUSAL_LM\" # MISTRAL\n",
        "    task_type=\"SEQ_CLS\",\n",
        "\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    if not isinstance(predictions, np.ndarray):\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "    if not isinstance(labels, np.ndarray):\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "    predictions = predictions.reshape(-1)\n",
        "    labels = labels.reshape(-1)\n",
        "    not_nan_mask = np.logical_and(np.isfinite(predictions), np.isfinite(labels))\n",
        "    predictions = predictions[not_nan_mask]\n",
        "    labels = labels[not_nan_mask]\n",
        "    if len(predictions) == 0 or len(labels) == 0:\n",
        "        return {\"mse\": 0.0, \"mae\": 0.0, \"rmse\": 0.0}\n",
        "    mse = mean_squared_error(labels, predictions)\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return {\"mse\": mse, \"mae\": mae, \"rmse\": rmse}\n",
        "    #return {\"mse\": mse, \"mae\": mae, \"rmse\": rmse,\"eval_loss\":loss}\n",
        "\n",
        "# TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/bert_fpllm_output\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    num_train_epochs=3,\n",
        "    max_steps=800,\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=50,\n",
        "    fp16=True,  # Enable mixed precision training #fr bert\n",
        "    #bf16=True, #Mistral\n",
        "    # bf16=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    weight_decay=0.1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    label_names=[],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\"\n",
        "    #logging_dir=\"./logs\",  # Specify a directory for logs\n",
        "    #report_to=\"tensorboard\"  # Enable TensorBoard logging\n",
        ")\n",
        "\n",
        "def custom_compute_loss(model, inputs, return_outputs=False):\n",
        "    labels = inputs.get(\"labels\")  # Get labels from inputs\n",
        "\n",
        "    # Forward pass to get predictions (logits)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss_fn = nn.MSELoss() # You can move this outside if you don't want to recreate it every time\n",
        "    loss = loss_fn(outputs, labels.view(-1, 1).float()) if labels is not None else None\n",
        "\n",
        "\n",
        "# Trainer Initialization and Training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analytics"
      ],
      "metadata": {
        "id": "x9v-RHIPdqA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n')\n",
        "print(f'Dataset tokenize structure: {tokenized_dataset}')\n",
        "print('\\n')\n",
        "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "print(f\" Eval Dataset Size: {len(val_dataset)}\")\n",
        "print('\\n')\n",
        "print(f\"Maximun fare: {fare_scaler.data_max_[0]}\")\n",
        "print(f\"Minimum fare: {fare_scaler.data_min_[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JDOx1EQsMCI",
        "outputId": "5a6b92e7-d9bf-4fe1-c805-ed4cff389c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Dataset tokenize structure: Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 221000\n",
            "})\n",
            "\n",
            "\n",
            "Train Dataset Size: 176800\n",
            " Eval Dataset Size: 44200\n",
            "\n",
            "\n",
            "Maximun fare: 457.0\n",
            "Minimum fare: 50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Design"
      ],
      "metadata": {
        "id": "GalnO354d_i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_prompt():\n",
        "    conn = sqlite3.connect('flights.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Retrieve question and fare from flight_qa table\n",
        "    cursor.execute(\"SELECT question, fare FROM flight_qa\")\n",
        "    data = cursor.fetchall()\n",
        "\n",
        "    final_prompt = \"\"\n",
        "    for question, fare in data:\n",
        "        final_prompt += f\"{question} [INST] {fare} [/INST]\"  # Changed $$ to $$$\n",
        "\n",
        "    conn.close()\n",
        "    return final_prompt\n",
        "\n",
        "# Main execution\n",
        "create_and_populate_tables(flights_subset_df, num_records=1000)\n",
        "final_prompt = generate_final_prompt()\n",
        "print(final_prompt[0:78])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzc-ez5zKDxx",
        "outputId": "e3b317f5-b65f-42fd-d5da-5043c07a0fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the fare for a flight from BLI to LIT with UK? [INST] 263.63 [/INST]Wh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "aP4HYrs0NMHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation results\n",
        "print(eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "UE9leZDmQGCt",
        "outputId": "6b03ef5a-e6b1-4cdf-a554-efbc4b058d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11050' max='5525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5525/5525 19:46]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping triggered after 5 epochs without improvement.\n",
            "{'eval_loss': 0.10287108272314072, 'eval_mse': 0.10287108272314072, 'eval_mae': 0.27725088596343994, 'eval_rmse': 0.32073522214303296, 'eval_runtime': 188.0918, 'eval_samples_per_second': 234.992, 'eval_steps_per_second': 29.374, 'epoch': 0.027149321266968326}\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/GPT4_Rag_Fusion_LlamaIndex_Pipeline_PostgreSQL_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a58112-8a18-49ad-9c8c-2088778613d0",
      "metadata": {
        "id": "41a58112-8a18-49ad-9c8c-2088778613d0"
      },
      "source": [
        "# RAG Fusion Query Pipeline\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llama-hub/blob/main/llama_hub/llama_packs/query/rag_fusion_pipeline/rag_fusion_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook shows how to implement RAG Fusion using the LlamaIndex Query Pipeline syntax."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Dependencies"
      ],
      "metadata": {
        "id": "x9dG5VRhwpHm"
      },
      "id": "x9dG5VRhwpHm"
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore\n",
        "!pip install llama_index phoenix pyvis network\n",
        "!pip install llama_hub\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "!pip install accelerate\n",
        "#!pip install typing_extensions\n",
        "\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "XN4HEh7jwixA"
      },
      "id": "XN4HEh7jwixA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c5bdbf5-d525-42e2-ab4a-19234c023491",
      "metadata": {
        "id": "8c5bdbf5-d525-42e2-ab4a-19234c023491"
      },
      "source": [
        "## Setup / Load Data\n",
        "\n",
        "We load in the pg_essay.txt data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "20b91e78-d733-44dd-b9a2-7c6eab3aee3d",
      "metadata": {
        "id": "20b91e78-d733-44dd-b9a2-7c6eab3aee3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fb35aa-9b8e-436b-f348-e70276c4199a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "--2024-02-23 01:08:57--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘pg_essay.txt’\n",
            "\n",
            "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-23 01:08:58 (1.49 MB/s) - ‘pg_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POSTGRESQL"
      ],
      "metadata": {
        "id": "jvWymWCOIdbI"
      },
      "id": "jvWymWCOIdbI"
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "# install PSQL WITH DEV Libraries AND PGVECTOR\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all"
      ],
      "metadata": {
        "id": "8DA67X3Zw_OW"
      },
      "id": "8DA67X3Zw_OW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "# PostGRES SQL Settings\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "print('START: PG embedding COMPILATION')\n",
        "%cd /content/\n",
        "!git clone https://github.com/neondatabase/pg_embedding.git\n",
        "%cd /content/pg_embedding\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id integer PRIMARY KEY, embedding real[])\""
      ],
      "metadata": {
        "id": "KLp1-7HZOkTW"
      },
      "id": "KLp1-7HZOkTW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install llama-index\n",
        "#!pip install llama-index\n",
        "import llama_index.core.readers as readers\n",
        "import os\n",
        "import openai\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "reader = readers.SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "docs = reader.load_data()\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "66tTN1avILV-"
      },
      "id": "66tTN1avILV-",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from typing import List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "loader = TextLoader(\"/content/pg_essay.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "### for the DB embedding\n",
        "docs0 = text_splitter.split_documents(documents)\n",
        "\n",
        "collection_name0 = \"pg_essay\"\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs0)}')"
      ],
      "metadata": {
        "id": "hyLRNSCwS91R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a81e26d-1419-4971-a5bb-535d6bba01a0"
      },
      "id": "hyLRNSCwS91R",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1004, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1203, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1025, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 1\n",
            "# of Document Chunks: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c9299c-06d6-4710-afb0-3cc13761f358",
      "metadata": {
        "id": "a7c9299c-06d6-4710-afb0-3cc13761f358"
      },
      "source": [
        "## Setup Llama Pack\n",
        "\n",
        "Next we download the LlamaPack. All the code is in the downloaded directory - we encourage you to take a look to see the QueryPipeline syntax!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/run-llama/llama_index.git"
      ],
      "metadata": {
        "id": "8vb2GlbIrp_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe8ae95-47a2-453f-a47d-199ed25b4b82"
      },
      "id": "8vb2GlbIrp_s",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama_index'...\n",
            "remote: Enumerating objects: 66899, done.\u001b[K\n",
            "remote: Counting objects: 100% (7364/7364), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1538/1538), done.\u001b[K\n",
            "remote: Total 66899 (delta 6422), reused 6048 (delta 5811), pack-reused 59535\u001b[K\n",
            "Receiving objects: 100% (66899/66899), 154.49 MiB | 22.53 MiB/s, done.\n",
            "Resolving deltas: 100% (46564/46564), done.\n",
            "Updating files: 100% (7995/7995), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from llama_index.llms.openai.base import AsyncOpenAI, OpenAI, SyncOpenAI, Tokenizer\n",
        "\n",
        "llm=OpenAI(model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "MMMglxm2rSuE"
      },
      "id": "MMMglxm2rSuE",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG FUSION PIPELINE"
      ],
      "metadata": {
        "id": "vKK5SXmmMkRR"
      },
      "id": "vKK5SXmmMkRR"
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_hub\n",
        "from llama_hub.llama_packs.query.rag_fusion_pipeline import rag_fusion_pipeline_pack\n",
        "RAGFusionPipelinePack=rag_fusion_pipeline_pack\n",
        "\n",
        "\n",
        "\n",
        "import llama_index.core.readers as readers\n",
        "reader = readers.SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "## for the RAG\n",
        "docs = reader.load_data()\n",
        "\n",
        "from llama_index.core.llama_pack import download_llama_pack\n",
        "\n",
        "# download and install dependencies\n",
        "RAGFusionPipelinePack = download_llama_pack(\n",
        "    \"RAGFusionPipelinePack\", \"./rag_fusion_pipeline_pack\"\n",
        ")\n",
        "\n",
        "\n",
        "#Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview,\n",
        "#gpt-4-vision-preview, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314,\n",
        "#gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613,\n",
        "#gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002,\n",
        "#gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001,\n",
        "#ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-1106,\n",
        "#gpt-35-turbo-0613, gpt-35-turbo-16k-0613\n",
        "\n",
        "#### OPENAI MODELS ########\n",
        "#pack = RAGFusionPipelinePack(docs, llm=OpenAI(model=\"gpt-3.5-turbo\")) ### ORIGINAL\n",
        "#pack = RAGFusionPipelinePack(docs, llm=OpenAI(model=\"gpt-4-1106-preview\"))\n",
        "#pack = RAGFusionPipelinePack(docs, llm=OpenAI(model=\"gpt-4-vision-preview\"))\n",
        "pack = RAGFusionPipelinePack(docs, llm=OpenAI(model=\"gpt-4\"))\n",
        "\n",
        "### TEST of RAG Fusion Pipeline Pack\n",
        "query0=\"What did the author do growing up?\"\n",
        "response0 = pack.run(query=query0)\n",
        "print(response0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZcZGjJK3IOa",
        "outputId": "4e0e1573-830a-42dd-8527-86b178303ada"
      },
      "id": "fZcZGjJK3IOa",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author, growing up, worked on writing short stories and programming. They started writing short stories, which they considered to be of poor quality, and also began programming on an IBM 1401 using an early version of Fortran. Later on, they transitioned to working on microcomputers, where they found a new level of excitement and accessibility in programming.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-4 - MODEL"
      ],
      "metadata": {
        "id": "vB6EiQq-TKHt"
      },
      "id": "vB6EiQq-TKHt"
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_reponse(query):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    #model=\"gpt-3.5-turbo\"\n",
        "    #response_format={ \"type\": \"json_object\" },\n",
        "    messages=[\n",
        "      #{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output text.\"},\n",
        "      {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "OGIj5Dnr-6iG"
      },
      "id": "OGIj5Dnr-6iG",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "RjQlUGxI7f_3"
      },
      "id": "RjQlUGxI7f_3",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who won the world series in 2020?\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYHJcF6wd41e",
        "outputId": "dd5e211a-99bb-4e21-f663-dfd3a4fb715d"
      },
      "id": "bYHJcF6wd41e",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Who won the world series in 2020?\n",
            "Answer: The Los Angeles Dodgers won the World Series in 2020.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the 30% of 650?\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJFb1bTiUK46",
        "outputId": "d138657e-9482-4918-9c8f-4d7a1acc7700"
      },
      "id": "rJFb1bTiUK46",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: what is the 30% of 650?\n",
            "Answer: The 30% of 650 is 195.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6SB8frUyez",
        "outputId": "84608a74-d898-47de-f51f-5221214c35ce"
      },
      "id": "GM6SB8frUyez",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "Answer: Absolutely. Regularization is an important concept in machine learning which helps to prevent overfitting by adding a penalty term to the loss function.\n",
            "\n",
            "In the context of machine learning, creating a model involves minimizing a loss function which provides the discrepancy between the model's predictions and the actual data. However, if a model fits the training data too closely, it may fail to generalize well to new, unseen data. This situation is referred to as overfitting, which is where regularization comes in.\n",
            "\n",
            "Regularization techniques add a penalty term to the loss function. The effect of this is to deliberately add some bias to the model predictions, with the aim of reducing its variance. By doing this, it's possible to find a good bias-variance tradeoff that will allow the model to generalize well to new data.\n",
            "\n",
            "There are several types of regularization techniques, including L1 and L2 regularization:\n",
            "\n",
            "1. L1 Regularization (also called Lasso regression) adds an absolute value of magnitude of coefficient as penalty term to the loss function. L1 can lead to zero coefficients i.e., some of the features are completely neglected for the evaluation of output. So L1 can also be used as a feature selection mechanism.\n",
            "   \n",
            "2. L2 Regularization (also called Ridge regression) adds the squared magnitude of coefficient as penalty term to the loss function. L2 regularization will not result in elimination of coefficients or feature selection but will minimize their impact.\n",
            "\n",
            "Both of these methods work by increasing the value of the loss function; forcing the learning algorithm to build a less complex model and thereby prevent overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HBzKx3PU5qM",
        "outputId": "fb8bc9ad-982d-4dc1-b7d0-5f17cd94bf4a"
      },
      "id": "-HBzKx3PU5qM",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "Answer: First, we need to determine the total cost of the ice cream cones. To do that, we multiply the cost of one cone, $1.25, by the number of kids, which is 6.\n",
            "\n",
            "$1.25 * 6 = $7.50 \n",
            "\n",
            "So, the total cost for the ice cream cones is $7.50.\n",
            "\n",
            "Then, we need to find out how much change you should get back. We do this by subtracting the total cost of the cones from the amount paid, which is a $10 bill.\n",
            "\n",
            "$10.00 - $7.50 = $2.50\n",
            "\n",
            "So, after buying ice cream for 6 kids, you should get $2.50 back in change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Which country has the most natural lakes? Answer with only the country name.\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4WeLnTTVGag",
        "outputId": "ca929ba8-3204-4f1d-a74c-e4c17981db0a"
      },
      "id": "k4WeLnTTVGag",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Which country has the most natural lakes? Answer with only the country name.\n",
            "Answer: Canada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who won the world series in 2009 and who lost, explained?, who were the managers?\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4MFDx-xPTiT",
        "outputId": "af8bb919-1dd8-490a-844e-be50c8e7fa37"
      },
      "id": "Q4MFDx-xPTiT",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Who won the world series in 2009 and who lost, explained?, who were the managers?\n",
            "Answer: The World Series in 2009 was won by the New York Yankees. The team they defeated to claim the championship was the Philadelphia Phillies. The deciding game took place on November 4, rounding out a series that took six games to determine a winner. The Yankees won the series 4-2.\n",
            "\n",
            "The manager of the New York Yankees in 2009 was Joe Girardi. He guided the Yankees to their 27th championship in franchise history. Charlie Manuel was the manager of the Philadelphia Phillies. Although the Phillies had won the World Series in 2008, they couldn't repeat the feat in 2009 under Manuel's leadership.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How AWS has evolved?\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "J7hMlciRPj5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b585521-5da8-474c-cd10-a0bce568f9be"
      },
      "id": "J7hMlciRPj5y",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: How AWS has evolved?\n",
            "Answer: Amazon Web Services (AWS), launched in 2006, has drastically evolved over the years. \n",
            "\n",
            "1. Infrastructure-as-a-Service and Platform-as-a-Service: Upon launch, AWS primarily was an Infrastructure-as-a-Service (IaaS) offering – with its prime services being storage (S3) and compute (EC2). Later, AWS also started offering Platform-as-a-Service (PaaS) options, providing developers with ready-to-use, managed environments.\n",
            "\n",
            "2. Expanding Services: AWS started with just a few services but now offers more than 200 services as of today. The range of services has also expanded to include databases, machine learning, analytics, networking, mobile, developer tools, management tools, IoT, security, and enterprise applications.\n",
            "\n",
            "3. Custom Hardware: In 2015, Amazon began creating custom hardware to improve the efficiency and decrease the cost of their data centers. They have also started developing their chips, like Graviton processors that power EC2 instances.\n",
            "\n",
            "4. Serverless Computing: In 2014, AWS launched Lambda, a serverless computing service that executes code only when needed and scales automatically. This approach has led to a significant push in the serverless computing paradigm within cloud services.\n",
            "\n",
            "5. Machine Learning and AI: Amazon has introduced numerous solutions for running machine learning models and AI, such as Amazon SageMaker. \n",
            "\n",
            "6. Ground Station: In 2018, Amazon broke new ground (quite literally) by launching AWS Ground Station, a fully managed service that lets you control satellite communications.\n",
            "\n",
            "7. Quantum Computing: In 2019, Amazon unveiled Amazon Braket, a fully managed quantum computing service that helps researchers and developers get started with the technology.\n",
            "\n",
            "8. Hybrid Cloud and Edge: While AWS initially focused heavily on their public cloud offerings, they've increasingly focused on hybrid cloud and edge computing solutions with services like Outposts, Local Zones, and Wavelength.\n",
            "\n",
            "9. Sustainability: AWS has also evolved its commitment to sustainability, aiming to achieve 100% renewable energy usage for its global infrastructure footprint by 2025.\n",
            "\n",
            "In conclusion, AWS has dramatically evolved from the basic storage service to a massive cloud platform that continues to innovate and pave the way in the cloud industry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING"
      ],
      "metadata": {
        "id": "Xgo5C7hzbWCY"
      },
      "id": "Xgo5C7hzbWCY"
    },
    {
      "cell_type": "code",
      "source": [
        "# 20x faster than pgvector: introducing pg_embedding extension for vector search in Postgres and LangChain\n",
        "# https://neon.tech/blog/pg-embedding-extension-for-vector-search\n",
        "\n",
        "#ADDED By FM 22/02/2024\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "# https://supabase.com/blog/fewer-dimensions-are-better-pgvector\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "collection_name='Paul Graham Essay'\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs0,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")\n",
        "\n",
        "#db.create_hnsw_index(dims = 1536, m = 8, ef_construction = 16, ef_search = 16)"
      ],
      "metadata": {
        "id": "SHrXuMUXGAMW"
      },
      "id": "SHrXuMUXGAMW",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 22/02/2024\n",
        "query='What did the author do growing up?'\n",
        "docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)\n",
        "\n",
        "print()\n",
        "print(query)\n",
        "print()\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4RiRP1jOGVs",
        "outputId": "4444a1b3-6fa7-4054-8303-950391aa6e8e"
      },
      "id": "A4RiRP1jOGVs",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What did the author do growing up?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.5991553\n",
            "What I Worked On\n",
            "\n",
            "February 2021\n",
            "\n",
            "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
            "\n",
            "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.60239\n",
            "Working on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\n",
            "\n",
            "In the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6047227\n",
            "Over the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.\n",
            "\n",
            "One night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.6152447\n",
            "In the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\n",
            "\n",
            "I've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\n",
            "\n",
            "I knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples of Queries"
      ],
      "metadata": {
        "id": "ZOkptc-Yx8U0"
      },
      "id": "ZOkptc-Yx8U0"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "065ef0c4-0c9e-4611-8de4-98b2a7fe3094",
      "metadata": {
        "id": "065ef0c4-0c9e-4611-8de4-98b2a7fe3094",
        "outputId": "cab0edac-d993-422c-a362-89297e646486",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What did the author do growing up?\n",
            "The author, growing up, worked on writing short stories and programming. They started writing short stories before college, focusing on characters with strong feelings rather than intricate plots. In terms of programming, they began by writing programs on an IBM 1401 using an early version of Fortran. Later on, they transitioned to working with microcomputers, specifically a TRS-80, where they wrote simple games, a rocket prediction program, and a word processor.\n",
            "\n",
            "\n",
            "Who is the President of the USA?\n",
            "I am unable to provide real-time information or updates on current events or individuals.\n",
            "\n",
            "\n",
            "Who won the baseball World Series in 2020? and Who Lost\n",
            "I cannot provide the answer to who won the baseball World Series in 2020 or who lost based on the context information provided.\n",
            "\n",
            "\n",
            "Anything about LIPS\n",
            "LISP, known for its association with AI, was a focus for the individual in the provided context. The person decided to write a book about Lisp hacking and eventually wrote a book called \"On Lisp.\" The distinctive aspect of Lisp is that its core is a language defined by writing an interpreter in itself. John McCarthy's Lisp, initially intended as a formal model of computation, later evolved into a programming language. The person worked on creating a new Lisp called Bel in Arc, using various tricks to make it function as an interpreter written in itself. This project took four years to complete, starting in March 2015 and finishing in October 2019.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#modify By FM 22/02/2024\n",
        "\n",
        "#response = pack.run(query=\"What did the author do growing up?\")\n",
        "query0=\"What did the author do growing up?\"\n",
        "query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query1 = \"Who is the President of the USA?\"\n",
        "query2 = \"Who won the baseball World Series in 2020? and Who Lost\"\n",
        "query3 = 'Anything about FORTRAN'\n",
        "query4 = 'Anything about LIPS'\n",
        "query5 = 'Anything about Python'\n",
        "\n",
        "response0 = pack.run(query=query0)\n",
        "response1 = pack.run(query=query1)\n",
        "response2 = pack.run(query=query2)\n",
        "response4 = pack.run(query=query4)\n",
        "\n",
        "print()\n",
        "print(query0)\n",
        "print(str(response0))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query1)\n",
        "print(str(response1))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query2)\n",
        "print(str(response2))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query4)\n",
        "print(str(response4))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ea4ef382-fd11-4a82-8f5d-b2c53a21c665",
      "metadata": {
        "id": "ea4ef382-fd11-4a82-8f5d-b2c53a21c665"
      },
      "outputs": [],
      "source": [
        "#response.source_nodes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama_hub",
      "language": "python",
      "name": "llama_hub"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "Xgo5C7hzbWCY"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
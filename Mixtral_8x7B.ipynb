{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Mixtral_8x7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "p-HJVuyATGCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysI7Fhp6dZcb"
      },
      "outputs": [],
      "source": [
        "#https://platform.openai.com/docs/guides/text-generation\n",
        "\n",
        "#!pip install gradio --quiet\n",
        "#!pip install xformer --quiet\n",
        "#!pip install chromadb --quiet\n",
        "\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "\n",
        "\n",
        "#!pip install unstructured --quiet\n",
        "#!pip install sentence-transformers --quiet\n",
        "\n",
        "\n",
        "%pip install openai==0.28  --root-user-action=ignore\n",
        "%pip install tiktoken\n",
        "\n",
        "!pip install -U transformers\n",
        "\n",
        "# https://huggingface.co/docs/transformers/model_doc/mixtral\n",
        "# https://github.com/Dao-AILab/flash-attention\n",
        "\n",
        "#!pip install -U flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and run a model using Flash Attention 2"
      ],
      "metadata": {
        "id": "BwfGwFjsR7g9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77HXC7IGdvZR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#from textwrap import fill\n",
        "#from IPython.display import Markdown, display\n",
        "\n",
        "#from langchain.prompts.chat import (\n",
        "#    ChatPromptTemplate,\n",
        "#    HumanMessagePromptTemplate,\n",
        "#    SystemMessagePromptTemplate,\n",
        "#    )\n",
        "\n",
        "#from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "#from langchain.vectorstores import Chroma\n",
        "#from langchain.schema import AIMessage, HumanMessage\n",
        "#from langchain.memory import ConversationBufferMemory\n",
        "#from langchain.embeddings import HuggingFaceEmbeddings\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "#from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "### T4 withOUT FlashAttention BUT need A100 to run with FlashAttention\n",
        "#RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
        "# https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "### A100\n",
        "# https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n",
        "#MODEL_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "# https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
        "#Quantization\n",
        "#Quantization techniques reduces memory and computational costs by representing weights and activations\n",
        "#with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldnâ€™t\n",
        "#be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization\n",
        "#algorithms and it supports 8-bit and 4-bit quantization with bitsandbytes.\n",
        "\n",
        "\n",
        "#( load_in_8bit = Falseload_in_4bit = Falsellm_int8_threshold = 6.0llm_int8_skip_modules = Nonellm_int8_enable_fp32_cpu_offload = Falsellm_int8_has_fp16_weight = False\n",
        "#bnb_4bit_compute_dtype = None\n",
        "# bnb_4bit_quant_type = 'fp4'bnb_4bit_use_double_quant = False**kwargs )\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    load_in_8bit_fp32_cpu_offload=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        "    #FlashAttention only supports Ampere GPUs or newer.\n",
        "    #attn_implementation=\"flash_attention_2\" NEED A100 IN GOOGLE COLAB\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.8\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VDhUCRg-eBTn"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipeline,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Completion - Use Cases"
      ],
      "metadata": {
        "id": "wxrAYpMYSFMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "782wTX6DlS2o",
        "outputId": "790ef9b5-dc41-4e4d-ac13-4ffe6230940c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  When answering, I will use the following steps:\n",
            "\n",
            "1. Calculate the total cost of the ice creams.\n",
            "2. Calculate how much money I paid and subtract that from the total cost to find out how much money I got back.\n",
            "\n",
            "Step 1: The total cost of the ice creams is found by multiplying the price of each cone by the number of kids, which gives us $1.25 x 6 = $<<1.25*6=7.50>>7.50.\n",
            "\n",
            "Step 2: I paid with a $10 bill, so I subtract the cost of the ice creams from $10, which gives us $10 - $7.50 = $<<10-7.5=2.50>>2.50.\n",
            "\n",
            "Therefore, I got $2.50 back.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: who is the President of the USA?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  The current President of the United States of America is Joe Biden. Biden took office on January 20, 2021 after being elected in the 2020 presidential election.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Who won the baseball World Series in 2020? and Who Lost\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  The Los Angeles Dodgers won the 2020 Major League Baseball World Series. They defeated the Tampa Bay Rays in 6 games, with the final game played on November 18, 2020, at Globe Life Field in Arlington, Texas.\n",
            "\n",
            "The New York Yankees lost the 2020 Major League Baseball World Series. They were defeated by the Los Angeles Dodgers in 6 games.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: what is the 20.5% of 40?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  The 20.5% of 40 is 8.2.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Yes, I'd be happy to explain the concept of regularization in machine learning. Regularization is a technique used in many machine learning algorithms to prevent overfitting. Overfitting occurs when a model is trained too well on the training data, fitting all the noise and erroneous data points, and in doing so, it starts to fail on new and unseen data. Overfitting can significantly impact the generalization error of a model, leading to poor predictions and inaccurate results.\n",
            "\n",
            "Regularization adds a penalty term to the loss function during the training of a model. This penalty term discourages large weights in the model, thereby preventing the model from fitting the training data too closely. By doing so, the model avoids overfitting and generalizes better to new data. There are two main types of regularization: L1 and L2.\n",
            "\n",
            "L1 regularization adds a penalty term that is proportional to the absolute value of the model weights. This discourages large weights more than small weights, leading the model to learn more sparse models.\n",
            "\n",
            "L2 regularization, also known as weight decay, adds a penalty term that is proportional to the square of the model weights. This encourages all weights to be small, leading the model to learn a more balanced model.\n",
            "\n",
            "Regularization can significantly improve the performance of a model and help prevent overfitting. It is a crucial technique for any data scientist to be familiar with when working with machine learning algorithms.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: Which country has the most natural lakes? Answer with only the country name.\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  Canada\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Prompt: How AWS has evolved?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer:  AWS (Amazon Web Services) has evolved significantly since its launch in 2006. Here are a few key ways in which AWS has evolved:\n",
            "\n",
            "1. Growing Range of Services: AWS initially offered a limited set of web services, including EC2, S3, and ELB. However, over the years, the range of services has expanded significantly, with offerings in compute, storage, databases, analytics, machine learning, security, and more.\n",
            "2. Wider Geographic Reach: AWS has expanded its data centers and regions, allowing it to serve customers across the world. This has enabled customers to deploy applications closer to their end-users, reducing latency and improving performance.\n",
            "3. Scalability and Flexibility: AWS has evolved to become a highly scalable and flexible platform, allowing customers to quickly and easily add or remove resources as needed. This has enabled businesses to adapt quickly to changing market conditions and new opportunities.\n",
            "4. Enhanced Security: AWS has made significant investments in security, including robust encryption, access controls, and compliance certifications. This has helped businesses meet regulatory requirements and protect their data.\n",
            "5. Integration with Other AWS Services: AWS services are designed to work together seamlessly, allowing businesses to build complex, end-to-end solutions quickly and easily. This has enabled businesses to streamline their operations, reduce costs, and drive innovation.\n",
            "6. Emergence of New Business Models: AWS has enabled new business models, such as serverless computing and pay-as-you-go pricing, which have become increasingly popular in the cloud computing industry.\n",
            "7. Integration with On-Premises Infrastructure: AWS has evolved to integrate seamlessly with on-premises infrastructure, allowing businesses to extend their applications and data center environments into the cloud. This has enabled businesses to take advantage of the benefits of cloud computing, while still maintaining control over certain workloads.\n",
            "\n",
            "Overall, AWS has evolved from a simple web services provider to a comprehensive cloud computing platform that offers a wide range of services, advanced security features, and seamless integration with on-premises infrastructure. This has enabled businesses to take advantage of the benefits of cloud computing and drive innovation in their operations.\n"
          ]
        }
      ],
      "source": [
        "query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query1 = \"who is the President of the USA?\"\n",
        "query2 = \"Who won the baseball World Series in 2020? and Who Lost\"\n",
        "\n",
        "device=\"cuda\"\n",
        "def prompt_completion(query):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"%s\"%query}\n",
        "    ]\n",
        "\n",
        "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = encodeds.to(device)\n",
        "\n",
        "    #https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id\n",
        "    #RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=512, do_sample=True, negative_prompt_attention_mask='attention_mask',\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(generated_ids)\n",
        "    print()\n",
        "    print()\n",
        "    result=decoded[0].replace('<s> [INST] %s [/INST]'%query,\"\")\n",
        "    result=result.replace('</s>',\"\")\n",
        "    print('Prompt: %s'%query)\n",
        "    print('-'*80)\n",
        "    print('Answer: %s'%result)\n",
        "\n",
        "prompt_completion(query)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query1)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query2)\n",
        "\n",
        "query3 = \"what is the 20.5% of 40?\"\n",
        "query4 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "query5 ='Which country has the most natural lakes? Answer with only the country name.'\n",
        "\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query3)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query4)\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query5)\n",
        "\n",
        "\n",
        "query6 = \"How AWS has evolved?\"\n",
        "print()\n",
        "print('='*80)\n",
        "prompt_completion(query6)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3gRdeBQdpreHWh852qo48",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
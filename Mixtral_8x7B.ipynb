{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN/GOiXgH3pl7gy8lxprIpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/Mixtral_8x7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U datasets transformers accelerate peft trl bitsandbytes sentencepiece interpret\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install -U bitsandbytes -q"
      ],
      "metadata": {
        "id": "UAmGf0L9nwYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh8eQ9ALn-VP",
        "outputId": "39b9672d-6a9e-475a-b16d-ea42721d5cbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 25 04:57:40 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Qz7wsEngzb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Choose the Mixtral variant you want to use\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  # Example: Instruction-tuned version\n",
        "\n",
        "# BitsAndBytesConfig int-4 config (for reduced memory usage)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Set the desired GPU device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# or device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Load the model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    # device_map=\"auto\", # Remove or comment out device_map=\"auto\"\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ").to(device)  # Explicitly move the model to the device\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Revert padding strategy to default\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure default padding token is used\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id  # Ensure default padding token ID is used"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your prompt\n",
        "prompt = \"Once upon a time, in a land far away, there lived a brave knight named Sir Reginald...\"\n",
        "\n",
        "# Prepare the input for the model\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "outputs = base_model.generate(**inputs, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id)  # Generate up to 100 new tokens\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgy19hOQ3QpP",
        "outputId": "f487afb5-8594-4376-a14c-c5a74c2cbe72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a land far away, there lived a brave knight named Sir Reginald...\n",
            "\n",
            "Sir Reginald was a knight of great renown, known far and wide for his bravery and his skill with a sword. He was a true hero, and he had a heart of gold.\n",
            "\n",
            "One day, Sir Reginald received a message from the king. The message was simple: there was a dragon terrorizing the kingdom, and the king needed Sir Reginald's help to slay it.\n",
            "\n",
            "Sir Reginald didn't hesitate for a moment. He gathered his armor and his weapons, and he set out on his trusty steed to find the dragon.\n",
            "\n",
            "It wasn't long before Sir Reginald found the dragon. It was a huge, fearsome creature, with scales as hard as iron and teeth as sharp as swords.\n",
            "\n",
            "But Sir Reginald was not afraid. He charged at the dragon, his sword held high. The dragon roared and breathed fire, but Sir Reginald was not deterred. He fought with all his might, and eventually, he was able to strike the dragon down.\n",
            "\n",
            "The kingdom was saved, and Sir Reginald was hailed as a hero. He returned to the king's castle, where he was rewarded with a feast in his honor.\n",
            "\n",
            "And so, Sir Reginald lived happily ever after, known and respected by all as the bravest knight in the land.\n",
            "\n",
            "The end.\n"
          ]
        }
      ]
    }
  ]
}
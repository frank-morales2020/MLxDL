{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "db3de1da"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPa2/os7zsgoecLBFZ4LMbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/HRM_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/sapientinc/HRM\n",
        "\n",
        "\n",
        "https://github.com/Dao-AILab/flash-attention\n",
        "\n",
        "\n",
        "https://github.com/sapientinc/HRM/issues/12"
      ],
      "metadata": {
        "id": "BphziTlllzhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HRM"
      ],
      "metadata": {
        "id": "BkcjTkjSQ5JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sapientinc/HRM.git"
      ],
      "metadata": {
        "id": "RlDXI74QVxkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9298e7b1"
      },
      "source": [
        "!pip install torch adam-atan2 einops tqdm coolname pydantic argdantic wandb omegaconf hydra-core huggingface_hub -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e46fda10",
        "outputId": "7656e48e-4731-4c34-a992-e1774338e308"
      },
      "source": [
        "!python /content/HRM/dataset/build_sudoku_dataset.py --output-dir /content/data/sudoku-extreme-100-aug-100 --subsample-size 100 --num-aug 100"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv: 100% 719M/719M [00:01<00:00, 481MB/s]\n",
            "100% 100/100 [00:01<00:00, 79.63it/s]\n",
            "test.csv: 100% 79.4M/79.4M [00:00<00:00, 230MB/s]\n",
            "100% 422786/422786 [00:00<00:00, 1800661.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc3f8132",
        "outputId": "081337e7-6439-447a-8338-b39112b8f829"
      },
      "source": [
        "!ls -l /content/data/sudoku-extreme-100-aug-100"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "-rw-r--r-- 1 root root   11 Aug  4 04:13 identifiers.json\n",
            "drwxr-xr-x 2 root root 4096 Aug  4 04:13 test\n",
            "drwxr-xr-x 2 root root 4096 Aug  4 04:13 train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0516b30"
      },
      "source": [
        "!git clone https://github.com/Dao-AILab/flash-attention.git\n",
        "%cd flash-attention\n",
        "!python setup.py install --user\n",
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e79411ae",
        "outputId": "0f364e8a-e233-46bf-aabd-da38007059c3"
      },
      "source": [
        "!OMP_NUM_THREADS=1 torchrun --nproc-per-node 1 --standalone /content/HRM/pretrain.py data_path=/content/data/sudoku-extreme-100-aug-100 epochs=200 eval_interval=1 global_batch_size=4 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/5000 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mf2023morales\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250804_092234-tk4fmdvm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mHierarchicalReasoningModel_ACTV1 speedy-hawk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/f2023morales/Sudoku-extreme-100-aug-100%20ACT-torch\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/f2023morales/Sudoku-extreme-100-aug-100%20ACT-torch/runs/tk4fmdvm\u001b[0m\n",
            "[Rank 0, World Size 1]: Epoch 0\n",
            "  0% 25/5000 [00:40<23:59,  3.46it/s]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3de1da"
      },
      "source": [
        "## Verify flash-attn installation\n",
        "\n",
        "### Subtask:\n",
        "Verify that `flash-attn` is correctly installed and accessible in the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadeb6ff"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to verify that `flash-attn` is installed correctly. I will import the library and print its version.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cf0f301",
        "outputId": "f2f1cc08-9c43-4c33-b5a2-fbe4ce03d620"
      },
      "source": [
        "import flash_attn\n",
        "print(flash_attn.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada4aa4b"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ImportError` indicates a problem with the `flash-attn` installation, specifically an undefined symbol in the compiled CUDA extension. This often happens due to a mismatch between the PyTorch and `flash-attn` versions or their CUDA dependencies. I will try to reinstall `flash-attn` with the `--no-cache-dir` option to ensure a clean build.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzCNf_nU9bMe"
      },
      "source": [
        "!pip uninstall -y flash-attn\n",
        "!pip install --no-cache-dir flash-attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1094831"
      },
      "source": [
        "**Reasoning**:\n",
        "The reinstallation did not solve the issue. The error message \"undefined symbol\" strongly suggests a C++ ABI compatibility issue between PyTorch and the compiled `flash-attn` extension. This can be caused by using different compiler versions or C++ standards. Since reinstalling `flash-attn` didn't work, I'll try reinstalling both `torch` and `flash-attn` together, which might ensure they are built with compatible compilers and libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f67e7b59"
      },
      "source": [
        "!pip uninstall -y torch torchvision torchaudio flash-attn\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install flash-attn --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcc1thxx-WNS",
        "outputId": "4af83df6-0e4f-4d91-f34a-656a5355792d"
      },
      "source": [
        "import flash_attn\n",
        "print(flash_attn.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "645e7b60"
      },
      "source": [
        "!pip uninstall -y torch torchvision torchaudio adam-atan2\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install adam-atan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9df72af0",
        "outputId": "aa837347-1618-471b-e4fc-7899a9fcdf15"
      },
      "source": [
        "!pip uninstall -y adam-atan2\n",
        "!pip install adam-atan2 --no-cache-dir"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: adam_atan2 0.0.3\n",
            "Uninstalling adam_atan2-0.0.3:\n",
            "  Successfully uninstalled adam_atan2-0.0.3\n",
            "Collecting adam-atan2\n",
            "  Downloading adam_atan2-0.0.3.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: adam-atan2\n",
            "  Building wheel for adam-atan2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adam-atan2: filename=adam_atan2-0.0.3-cp311-cp311-linux_x86_64.whl size=196164 sha256=9ccd0e62a75141db73cac0981492f727922a75149993dfe1da6965bf0cb5310b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x_d5jsjy/wheels/43/58/2a/9b3bd25c65b754ff4182332021a5ffff4fe68382acae55520f\n",
            "Successfully built adam-atan2\n",
            "Installing collected packages: adam-atan2\n",
            "Successfully installed adam-atan2-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandb"
      ],
      "metadata": {
        "id": "Uenqt_QlckNi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5fd73f1"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/wandb/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrghWi6jWnxy",
        "outputId": "d3fd8520-1e42-4cc6-d470-d240ba513ba9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 1 root root 4.0K Aug  4 09:07 ..\n",
            "drwxr-xr-x 5 root root 4.0K Aug  4 09:07 run-20250804_090712-4vy4qugl\n",
            "drwxr-xr-x 3 root root 4.0K Aug  4 09:07 .\n",
            "lrwxrwxrwx 1 root root   52 Aug  4 09:07 debug-internal.log -> run-20250804_090712-4vy4qugl/logs/debug-internal.log\n",
            "lrwxrwxrwx 1 root root   43 Aug  4 09:07 debug.log -> run-20250804_090712-4vy4qugl/logs/debug.log\n",
            "lrwxrwxrwx 1 root root   28 Aug  4 09:07 latest-run -> run-20250804_090712-4vy4qugl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15d735f1",
        "outputId": "c72c3f18-efbf-40b3-8450-dcc236285871"
      },
      "source": [
        "!ls -ltha /content/wandb/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 1 root root 4.0K Aug  4 09:07 ..\n",
            "drwxr-xr-x 5 root root 4.0K Aug  4 09:07 run-20250804_090712-4vy4qugl\n",
            "drwxr-xr-x 3 root root 4.0K Aug  4 09:07 .\n",
            "lrwxrwxrwx 1 root root   52 Aug  4 09:07 debug-internal.log -> run-20250804_090712-4vy4qugl/logs/debug-internal.log\n",
            "lrwxrwxrwx 1 root root   43 Aug  4 09:07 debug.log -> run-20250804_090712-4vy4qugl/logs/debug.log\n",
            "lrwxrwxrwx 1 root root   28 Aug  4 09:07 latest-run -> run-20250804_090712-4vy4qugl\n"
          ]
        }
      ]
    }
  ]
}
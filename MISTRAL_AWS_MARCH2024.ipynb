{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ2TyTaoVukBK51Rgp2Zsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_AWS_MARCH2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaWcS7jcG5VF"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install sagemaker boto3 --quiet\n",
        "\n",
        "#%pip install langchain==0.0.309 --quiet --root-user-action=ignore\n",
        "%pip install langchain --quiet\n",
        "\n",
        "import colab_env\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#print(aws_access_key_id)\n",
        "#print()\n",
        "#print(f\"aws_access_key_id: '{aws_access_key_id}'\")\n",
        "#print(f\"aws_secret_access_key: '{aws_secret_access_key}'\")\n",
        "\n",
        "#print(f\"region: '{region}'\")\n",
        "#print()"
      ],
      "metadata": {
        "id": "AcZLELCdKFcJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama-2\n",
        "https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "claude-3\n",
        "https://aws.amazon.com/blogs/aws/anthropics-claude-3-sonnet-foundation-model-is-now-available-in-amazon-bedrock/\n"
      ],
      "metadata": {
        "id": "AuY8PGbLM9QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import boto3\n",
        "import os\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "\n",
        "# added by frank morales december 13, 2023\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "# https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-choose.html#jumpstart-foundation-models-choose-eula\n",
        "\n",
        "#original\n",
        "#llm_model_id, llm_model_version = \"meta-textgeneration-llama-2-7b-f\", \"*\"\n",
        "#llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version)\n",
        "## error below\n",
        "\n",
        "#modified by frankmorales\n",
        "#llm_model_id, llm_model_version = \"meta-textgeneration-llama-2-7b-f\", \"2.*\"\n",
        "\n",
        "### LLAMA-2\n",
        "#llm_model_id = 'meta-textgeneration-llama-2-7b-f'\n",
        "#llm_model_version = '2.0.4'\n",
        "\n",
        "# https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html\n",
        "\n",
        "# https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/examples?provider=Anthropic\n",
        "\n",
        "## MISTRAL\n",
        "llm_model_id = 'huggingface-llm-mistral-7b'\n",
        "llm_model_version = '2.1.0'\n",
        "\n",
        "llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version, role=ROLE_ARN, region='us-east-1')\n",
        "\n",
        "\n",
        "#{\n",
        "#  \"modelId\": \"anthropic.claude-v2\",\n",
        "#  \"contentType\": \"application/json\",\n",
        "#  \"accept\": \"application/json\",\n",
        "\n",
        "\n",
        "#modified by frankmorales\n",
        "llm_predictor = llm_model.deploy(accept_eula=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P1dwGh5KWRo",
        "outputId": "c1c66907-7253-4bd5-9740-cd0c1816c7d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the model endpoint NAME, not the ARN\n",
        "llm_model_endpoint_name = llm_predictor.endpoint_name\n",
        "llm_model_endpoint_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2XRGT2iULD1w",
        "outputId": "8d582544-8444-40c9-dcf5-ee63d3bf3119"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hf-llm-mistral-7b-2024-03-05-18-54-38-927'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "from langchain import PromptTemplate, SagemakerEndpoint\n",
        "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA\n",
        "import json\n",
        "\n",
        "class QAContentHandler(LLMContentHandler):\n",
        "    content_type = \"application/json\"\n",
        "    accepts = \"application/json\"\n",
        "\n",
        "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
        "        input_str = json.dumps(\n",
        "            {\"inputs\" : [\n",
        "                [\n",
        "                    {\n",
        "                        \"role\" : \"system\",\n",
        "                        \"content\" : \"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\" : \"user\",\n",
        "                        \"content\" : prompt\n",
        "                    }\n",
        "                ]],\n",
        "                \"parameters\" : {**model_kwargs}\n",
        "            })\n",
        "        return input_str.encode('utf-8')\n",
        "\n",
        "    def transform_output(self, output: bytes) -> str:\n",
        "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
        "        ### LLAMA-2\n",
        "        #return response_json[0][\"generation\"][\"content\"]\n",
        "        ### MISTRAL\n",
        "        return response_json[0]['generated_text'][\"content\"]\n",
        "\n",
        "qa_content_handler = QAContentHandler()\n",
        "\n"
      ],
      "metadata": {
        "id": "CZNHvIw_T1pO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QAContentHandler(LLMContentHandler):\n",
        "    content_type = \"application/json\"\n",
        "    accepts = \"application/json\"\n",
        "\n",
        "qa_content_handler = QAContentHandler()"
      ],
      "metadata": {
        "id": "lkyQiftbzqGw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"region\")\n",
        "output=os.getenv(\"output\")\n",
        "\n",
        "llm = SagemakerEndpoint(\n",
        "        endpoint_name=llm_model_endpoint_name,\n",
        "        region_name=region,\n",
        "        model_kwargs={\"max_new_tokens\": 5000, \"top_p\": 0.9, \"temperature\": 1e-11},\n",
        "        endpoint_kwargs={\"CustomAttributes\": 'accept_eula=true'},\n",
        "        content_handler=qa_content_handler\n",
        "    )"
      ],
      "metadata": {
        "id": "hTF3N40tLQuM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LLAMA-2 ####\n",
        "\n",
        "#original by the book\n",
        "#query = \"WHAT IS PERU?\"\n",
        "\n",
        "##modified by Frank Morales\n",
        "#response=llm.predict(query)"
      ],
      "metadata": {
        "id": "5UIDEOp3BTGt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### MISTRAL ######\n",
        "\n",
        "#query = \"who is the best French Poet?\"\n",
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# Prepare the input for the model\n",
        "#input_data = {\"inputs\": query}\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "#print(response_body)\n",
        "\n",
        "print(f'Query:', query)\n",
        "print()\n",
        "print(f'Response:', response_body[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vj8eTH3LYYD",
        "outputId": "5c306565-556a-4aab-bb1c-19b4278559fc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "Response: \n",
            "\n",
            "### Answer & Explanation\n",
            "\n",
            "The cost of the ice cream is $7.50, so I will get $2.50 back.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEAN UP"
      ],
      "metadata": {
        "id": "yHCOvGVoLoRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mnSz_gVLtPS",
        "outputId": "64792aba-e056-4d23-cd1a-5c5988413c9c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "EndpointName\n",
            "hf-llm-mistral-7b-2024-03-05-18-54-38-927\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "ModelName\n",
            "hf-llm-mistral-7b-2024-03-05-18-54-38-926\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "EndpointConfigName\n",
            "hf-llm-mistral-7b-2024-03-05-18-54-38-927\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
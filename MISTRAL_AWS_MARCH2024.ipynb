{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLWJqkCcx4OljiLuXqz7LC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_AWS_MARCH2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaWcS7jcG5VF"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install sagemaker boto3 --quiet\n",
        "\n",
        "#%pip install langchain==0.0.309 --quiet --root-user-action=ignore\n",
        "%pip install langchain --quiet\n",
        "\n",
        "import colab_env\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#print(aws_access_key_id)\n",
        "#print()\n",
        "#print(f\"aws_access_key_id: '{aws_access_key_id}'\")\n",
        "#print(f\"aws_secret_access_key: '{aws_secret_access_key}'\")\n",
        "\n",
        "#print(f\"region: '{region}'\")\n",
        "#print()"
      ],
      "metadata": {
        "id": "AcZLELCdKFcJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama-2\n",
        "https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "claude-3\n",
        "https://aws.amazon.com/blogs/aws/anthropics-claude-3-sonnet-foundation-model-is-now-available-in-amazon-bedrock/\n",
        "\n",
        "mistral 8x7b https://aws.amazon.com/blogs/machine-learning/mixtral-8x7b-is-now-available-in-amazon-sagemaker-jumpstart/\n"
      ],
      "metadata": {
        "id": "AuY8PGbLM9QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import boto3\n",
        "import os\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "\n",
        "# added by frank morales december 13, 2023\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "# https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-choose.html#jumpstart-foundation-models-choose-eula\n",
        "\n",
        "#original\n",
        "#llm_model_id, llm_model_version = \"meta-textgeneration-llama-2-7b-f\", \"*\"\n",
        "#llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version)\n",
        "## error below\n",
        "\n",
        "#modified by frankmorales\n",
        "#llm_model_id, llm_model_version = \"meta-textgeneration-llama-2-7b-f\", \"2.*\"\n",
        "\n",
        "### LLAMA-2\n",
        "#llm_model_id = 'meta-textgeneration-llama-2-7b-f'\n",
        "#llm_model_version = '2.0.4'\n",
        "\n",
        "# https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html\n",
        "\n",
        "# https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/examples?provider=Anthropic\n",
        "\n",
        "## MISTRAL 7B ### MARCH 5, 2024\n",
        "#llm_model_id = 'huggingface-llm-mistral-7b'\n",
        "#llm_model_version = '2.1.0'\n",
        "\n",
        "## MISTRAL 8x7B ### MARCH 5, 2024\n",
        "llm_model_id = 'huggingface-llm-mixtral-8x7b'\n",
        "llm_model_version = '*'\n",
        "\n",
        "\n",
        "#model = JumpStartModel(model_id=\"huggingface-llm-mixtral-8x7b\",role=ROLE_ARN, region='us-east-1')\n",
        "#predictor = model.deploy()\n",
        "#payload = {\"inputs\": \"Hello!\"}\n",
        "#predictor.predict(payload)\n",
        "\n",
        "\n",
        "llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version, role=ROLE_ARN, region='us-east-1')\n",
        "\n",
        "\n",
        "#{\n",
        "#  \"modelId\": \"anthropic.claude-v2\",\n",
        "#  \"contentType\": \"application/json\",\n",
        "#  \"accept\": \"application/json\",\n",
        "\n",
        "\n",
        "#modified by frankmorales\n",
        "llm_predictor = llm_model.deploy(accept_eula=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P1dwGh5KWRo",
        "outputId": "59658261-e1d1-4fed-cf99-b53947d8a980"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the model endpoint NAME, not the ARN\n",
        "llm_model_endpoint_name = llm_predictor.endpoint_name\n",
        "llm_model_endpoint_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2XRGT2iULD1w",
        "outputId": "33a70670-1a1d-4abe-d5f1-6d1bd9e50483"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hf-llm-mixtral-8x7b-2024-03-05-20-14-34-400'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### MISTRAL 8x7b ###### CASE#1\n",
        "\n",
        "#query = \"who is the best French Poet?\"\n",
        "query = \"Write a program to compute factorial in python:\"\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# Prepare the input for the model\n",
        "#input_data = {\"inputs\": query}\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "#print(response_body)\n",
        "\n",
        "print(f'Query:', query)\n",
        "print()\n",
        "print(f'Response:', response_body[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNb1GPq0TN96",
        "outputId": "dab51af8-86e0-4ea8-a18e-aedfbb447b1c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Write a program to compute factorial in python:\n",
            "\n",
            "Response: \n",
            "\n",
            "The factorial of any positive number ‘n’ is given by n!.\n",
            "\n",
            "n! = n * (n-1) * (n-2) * (n-3) * … * 1\n",
            "\n",
            "It is calculated as n! = n * (n – 1)! or n! = n * (n – 1) * (n – 2)! or n! = n * (n – 1) * (n – 2) * (n – 3)!,\n",
            "\n",
            "For example, 5! = 5 * 4 * 3 * 2 * 1,\n",
            "\n",
            "and 6! = 6 * 5 * 4 * 3 * 2 * 1.\n",
            "\n",
            "A factorial is calculated by multiplying a number by each number less than itself, it is denoted with an exclamation mark (!):\n",
            "\n",
            "For example, 5! = 5 * 4 * 3 * 2 * 1 = 120\n",
            "\n",
            "A factorial of a number can be calculated by the following general formula:\n",
            "\n",
            "```\n",
            "n! = n * (n - 1)!, if n is greater than or equal to 1\n",
            "```\n",
            "\n",
            "Now we can compute a factorial for any integer number in python multiplying itself by each number less than it self until it reaches number 1. The below program takes a number as input and outputs its factorial.\n",
            "\n",
            "```\n",
            "# factorial.py\n",
            "\n",
            "def fac(n):\n",
            "    result = 1\n",
            "    for i in range(1, n + 1):\n",
            "        result *= i\n",
            "    return result\n",
            "\n",
            "# Driver Code\n",
            "n = int(input(\"Enter a number to calculate its factorial:\\n\"))\n",
            "print(f\"The factorial of {n} is {fac(n)}\")\n",
            "```\n",
            "\n",
            "Output:\n",
            "\n",
            "```\n",
            "Enter a number to calculate its factorial:\n",
            "5\n",
            "The factorial of 5 is 120\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### MISTRAL 8x7b ###### CASE#2\n",
        "\n",
        "#query = \"who is the best French Poet?\"\n",
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "\n",
        "\n",
        "# Create a boto3 client for SageMaker runtime\n",
        "sm_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# Prepare the input for the model\n",
        "#input_data = {\"inputs\": query}\n",
        "\n",
        "### WITH PARAMETRS\n",
        "n=5\n",
        "MNT=512*n\n",
        "model_kwargs={\"max_new_tokens\": MNT, \"temperature\": 0.9}\n",
        "input_data = ({\"inputs\": query, \"parameters\" : {**model_kwargs}})\n",
        "\n",
        "response = sm_client.invoke_endpoint(EndpointName=llm_model_endpoint_name, Body=json.dumps(input_data), ContentType=\"application/json\")\n",
        "\n",
        "# Decode the response from the model\n",
        "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
        "#print(response_body)\n",
        "\n",
        "print(f'Query:', query)\n",
        "print()\n",
        "print(f'Response:', response_body[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vj8eTH3LYYD",
        "outputId": "9bfbd9da-ecf3-4915-ecde-c2689c5679d9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "Response: \n",
            "\n",
            "### Answer & Explanation\n",
            "\n",
            "You paid with a $10 bill.\n",
            "Each cone costs $1.25.\n",
            "$10/$1.25=8x 0.25$\n",
            "You get $2.50 back.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEAN UP"
      ],
      "metadata": {
        "id": "yHCOvGVoLoRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mnSz_gVLtPS",
        "outputId": "4c8034a7-6c99-45e9-a9a3-81752dd96348"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "EndpointName\n",
            "hf-llm-mixtral-8x7b-2024-03-05-20-14-34-400\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "ModelName\n",
            "hf-llm-mixtral-8x7b-2024-03-05-20-14-34-398\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "EndpointConfigName\n",
            "hf-llm-mixtral-8x7b-2024-03-05-20-14-34-400\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
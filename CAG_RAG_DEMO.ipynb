{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNfwhHIangPbEW/0VAB70Jl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/CAG_RAG_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf -q # Install PyMuPDF\n",
        "!pip install pypdf2 -q # Install the PyPDF2 library using pip.\n",
        "!pip install pikepdf -q\n",
        "!pip install pdfplumber -q\n",
        "!pip install sentence-transformers -q"
      ],
      "metadata": {
        "id": "dU_wKW-LnhMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBM8xHzM4GpS",
        "outputId": "7411d90d-b81b-4351-f296-4cf62ef217b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 20 13:49:55 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   70C    P0              32W /  72W |   1345MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import requests\n",
        "import PyPDF2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pikepdf\n",
        "import io\n",
        "import pdfplumber\n",
        "import fitz  # Import PyMuPDF\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the LLM\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# URL of the PDF file (UPDATED with a VALID URL)\n",
        "pdf_url = \"https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf\"\n",
        "\n",
        "# Function to extract text from PDF using PyMuPDF, loading from URL with User-Agent\n",
        "def extract_text_from_pdf(pdf_url):\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": \"MyBot/1.0 (MyProject; myemail@example.com)\"\n",
        "            # Replace with your bot name, project, and email\n",
        "        }\n",
        "        response = requests.get(pdf_url, headers=headers)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "        with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text with PyMuPDF (from URL): {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Extract text from the downloaded PDF\n",
        "pdf_text = extract_text_from_pdf(pdf_url)  # Pass the URL directly\n",
        "\n",
        "# Initialize the sentence transformer model\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "# --- Process the extracted text (updated logic with keywords and flexible text splitting) ---\n",
        "knowledge_base = {}\n",
        "keywords = [\"artificial intelligence\", \"turing test\", \"machine learning\", \"deep learning\"]\n",
        "\n",
        "# Use a sliding window approach to create overlapping chunks\n",
        "chunk_size = 500  # Adjust this value for optimal results\n",
        "overlap = 250     # Adjust this value for overlap between chunks\n",
        "\n",
        "chunks = []  # Store all chunks for embedding\n",
        "for i in range(0, len(pdf_text), chunk_size - overlap):\n",
        "    chunk = pdf_text[i:i + chunk_size]\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# Cache file for embeddings\n",
        "cache_file = \"chunk_embeddings.pkl\"\n",
        "\n",
        "# Try to load embeddings from cache\n",
        "if os.path.exists(cache_file):\n",
        "    with open(cache_file, \"rb\") as f:\n",
        "        chunk_embeddings = pickle.load(f)\n",
        "else:\n",
        "    # Generate and cache embeddings if not found\n",
        "    batch_size = 32\n",
        "    chunk_embeddings = []\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i : i + batch_size]\n",
        "        #batch_embeddings = sentence_model.encode(batch)\n",
        "        batch_embeddings = sentence_model.encode(batch, device=device)\n",
        "        chunk_embeddings.extend(batch_embeddings)\n",
        "\n",
        "    with open(cache_file, \"wb\") as f:\n",
        "        pickle.dump(chunk_embeddings, f)\n",
        "\n",
        "# Create knowledge base using chunks and embeddings\n",
        "for i, chunk in enumerate(chunks):\n",
        "    for keyword in keywords:\n",
        "        if keyword in chunk.lower():\n",
        "            if keyword not in knowledge_base:\n",
        "                knowledge_base[keyword] = []\n",
        "            knowledge_base[keyword].append((chunk, chunk_embeddings[i]))  # Store chunk and embedding\n",
        "            break\n",
        "\n",
        "# Cache for CAG\n",
        "cache = {}\n",
        "\n",
        "# Function to retrieve from the cache (CAG)\n",
        "def retrieve_from_cache(query):\n",
        "    if query in cache:\n",
        "        print(\"Answer found in cache!\")\n",
        "        return cache[query]\n",
        "    return None\n",
        "\n",
        "# Function to retrieve from the knowledge base (RAG - updated for sentence similarity)\n",
        "def retrieve_from_knowledge_base(query):\n",
        "    query_embedding = sentence_model.encode(query)\n",
        "\n",
        "    # Batch encode all chunks\n",
        "    chunk_embeddings = sentence_model.encode(chunks)\n",
        "\n",
        "    best_match = None\n",
        "    best_similarity = -1\n",
        "\n",
        "    # Iterate through chunk embeddings\n",
        "    for i, chunk_embedding in enumerate(chunk_embeddings):\n",
        "        similarity = util.cos_sim(query_embedding, chunk_embedding).item()\n",
        "        if similarity > best_similarity:\n",
        "            best_similarity = similarity\n",
        "            best_match = chunks[i]  # Get the corresponding chunk\n",
        "\n",
        "    if best_match:\n",
        "        return best_match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to generate text with CAG-RAG\n",
        "def generate_text_cag_rag(query):\n",
        "    # 1. Check the cache (CAG)\n",
        "    cached_answer = retrieve_from_cache(query)\n",
        "    if cached_answer:\n",
        "        return cached_answer\n",
        "\n",
        "    # 2. If not in cache, retrieve from knowledge base (RAG)\n",
        "    answer = retrieve_from_knowledge_base(query)\n",
        "    if answer:\n",
        "        cache[query] = answer  # Add to cache for future use\n",
        "        return answer\n",
        "\n",
        "    # 3. If not found in either, use the LLM\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
        "\n",
        "    #outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=200)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=150,  # Adjust as needed\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=1.2,\n",
        "        #early_stopping=True\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "user_queries = [\n",
        "    \"What is Artificial Intelligence?\",\n",
        "    \"What is Artificial Intelligence?\",  # Repeated to show CAG in action\n",
        "    \"What is a Turing Test?\",\n",
        "    \"What is machine learning?\",  # This will likely fall back to the LLM\n",
        "]\n",
        "\n",
        "model.config.pad_token_id = tokenizer.eos_token_id  # Set in generation config\n",
        "\n",
        "for query in user_queries:\n",
        "    response = generate_text_cag_rag(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 20)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxG-0q-Ppr8n",
        "outputId": "f5b1e8a2-cb4b-47db-d441-b40c959c3691"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is Artificial Intelligence?\n",
            "Response: e, reason, and act.”\n",
            "(Winston, 1992)\n",
            "Acting Humanly\n",
            "Acting Rationally\n",
            "“The art of creating machines that per-\n",
            "form functions that require intelligence\n",
            "when performed by people.” (Kurzweil,\n",
            "1990)\n",
            "“Computational Intelligence is the study\n",
            "of the design of intelligent agents.” (Poole\n",
            "et al., 1998)\n",
            "“The study of how to make computers do\n",
            "things at which, at the moment, people are\n",
            "better.” (Rich and Knight, 1991)\n",
            "“AI . . . is concerned with intelligent be-\n",
            "havior in artifacts.” (Nilsson, 1998)\n",
            "Figure 1\n",
            "--------------------\n",
            "Answer found in cache!\n",
            "Query: What is Artificial Intelligence?\n",
            "Response: e, reason, and act.”\n",
            "(Winston, 1992)\n",
            "Acting Humanly\n",
            "Acting Rationally\n",
            "“The art of creating machines that per-\n",
            "form functions that require intelligence\n",
            "when performed by people.” (Kurzweil,\n",
            "1990)\n",
            "“Computational Intelligence is the study\n",
            "of the design of intelligent agents.” (Poole\n",
            "et al., 1998)\n",
            "“The study of how to make computers do\n",
            "things at which, at the moment, people are\n",
            "better.” (Rich and Knight, 1991)\n",
            "“AI . . . is concerned with intelligent be-\n",
            "havior in artifacts.” (Nilsson, 1998)\n",
            "Figure 1\n",
            "--------------------\n",
            "Query: What is a Turing Test?\n",
            "Response:  enough for speakers of English to\n",
            "settle on a meaning for the word “think”—does it require “a brain” or just “brain-like parts.”\n",
            "Alan Turing, in his famous paper “Computing Machinery and Intelligence” (1950), sug-\n",
            "gested that instead of asking whether machines can think, we should ask whether machines\n",
            "can pass a behavioral intelligence test, which has come to be called the Turing Test. The test\n",
            "TURING TEST\n",
            "is for a program to have a conversation (via online typed messages) with an interrogator \n",
            "--------------------\n",
            "Query: What is machine learning?\n",
            "Response: ble it to communicate successfully in English;\n",
            "NATURAL LANGUAGE\n",
            "PROCESSING\n",
            "• knowledge representation to store what it knows or hears;\n",
            "KNOWLEDGE\n",
            "REPRESENTATION\n",
            "• automated reasoning to use the stored information to answer questions and to draw\n",
            "AUTOMATED\n",
            "REASONING\n",
            "new conclusions;\n",
            "• machine learning to adapt to new circumstances and to detect and extrapolate patterns.\n",
            "MACHINE LEARNING\n",
            "1 By distinguishing between human and rational behavior, we are not suggesting that humans are necessarily\n",
            "“irrat\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the POC is sufficient for demonstrating the basic functionality, you'll likely need to consider additional aspects when moving towards a production-ready system:\n",
        "\n",
        "Scalability:\n",
        "\n",
        "Efficient Knowledge Base Storage: If your PDF or knowledge source is very large or you have multiple sources, consider using a more scalable knowledge base solution like a vector database (e.g., Pinecone, Weaviate, Faiss) to store and retrieve embeddings efficiently.\n",
        "Optimized Retrieval: Implement more advanced retrieval techniques (e.g., approximate nearest neighbor search) to handle large-scale knowledge bases.\n",
        "Robustness and Error Handling:\n",
        "\n",
        "Input Validation: Implement input validation to handle unexpected user queries or malformed data.\n",
        "Error Handling: Add error handling mechanisms to gracefully handle potential issues like network errors, PDF parsing failures, or LLM generation errors.\n",
        "Monitoring and Evaluation:\n",
        "\n",
        "Logging: Implement logging to track system performance, errors, and user interactions.\n",
        "Metrics: Define and track relevant metrics (e.g., accuracy, latency, retrieval success rate) to monitor the system's effectiveness and identify areas for improvement.\n",
        "Security and Privacy:\n",
        "\n",
        "Data Security: Securely store sensitive information in the knowledge base and protect against unauthorized access.\n",
        "Privacy: Ensure that the system complies with relevant privacy regulations and protects user data.\n",
        "User Interface:\n",
        "\n",
        "Integration: Integrate the RAG/CAG system into a user-friendly interface (e.g., a chatbot, a search engine) to allow users to interact with it easily.\n",
        "Feedback Mechanisms: Provide feedback mechanisms for users to report issues or suggest improvements.\n",
        "Continuous Improvement:\n",
        "\n",
        "Regularly Update Knowledge Base: Establish a process for updating the knowledge base with new information to keep it relevant and accurate.\n",
        "Model Fine-tuning: Periodically fine-tune the LLM on new data or user feedback to improve its performance and adapt to changing user needs.\n",
        "Remember that this is not an exhaustive list, and the specific considerations will depend on the requirements of your production environment. By addressing these aspects as you move beyond the POC stage, you'll be building a more robust, reliable, and user-friendly RAG/CAG system.\n",
        "\n",
        "I'm glad that the current POC is serving its purpose, and I'm happy to help further as you progress towards a production-ready solution! Let me know if you have any other questions or if you want to discuss any of these production considerations in more detail."
      ],
      "metadata": {
        "id": "GV2Hv8L4-QMe"
      }
    }
  ]
}
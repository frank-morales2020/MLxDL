{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMTYm4NFRtCSmhdWaEHwqFq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/MISTRAL_API_TUTORIAL_VOXTRALMINILATEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.mistral.ai/platform/client/\n",
        "\n",
        "https://github.com/mistralai/client-python\n",
        "\n",
        "https://docs.mistral.ai/getting-started/models/models_overview/"
      ],
      "metadata": {
        "id": "HFSQBSaVuYEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies and Settings"
      ],
      "metadata": {
        "id": "NesDaI4OICGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install mistralai --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "ghqAXWE-xJa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mistralai\n",
        "from mistralai.client import MistralClient\n",
        "import os\n",
        "import colab_env\n",
        "import json"
      ],
      "metadata": {
        "id": "a87Hog_P8h9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MISTRAL API SETTINGS"
      ],
      "metadata": {
        "id": "9V3yq8LLuqdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai import Mistral\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = Mistral(api_key=api_key)"
      ],
      "metadata": {
        "id": "NnLef38upn75"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managenment"
      ],
      "metadata": {
        "id": "Rwwy34zSFHD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Mistral(api_key=api_key)  # Use MistralClient instead of Mistral\n",
        "model_list = client.models.list()\n",
        "\n",
        "for model in model_list.data:\n",
        "    print(model.id)\n",
        "    #print(model.created)\n",
        "    #print(model.owned_by)"
      ],
      "metadata": {
        "id": "kJGSPIMaDBPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ae1427-2bbd-49c1-88c4-6c87f47a1c29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mistral-medium-2505\n",
            "mistral-medium-latest\n",
            "mistral-medium\n",
            "mistral-large-latest\n",
            "ministral-3b-2410\n",
            "ministral-3b-latest\n",
            "ministral-8b-2410\n",
            "ministral-8b-latest\n",
            "open-mistral-7b\n",
            "mistral-tiny\n",
            "mistral-tiny-2312\n",
            "open-mistral-nemo\n",
            "open-mistral-nemo-2407\n",
            "mistral-tiny-2407\n",
            "mistral-tiny-latest\n",
            "open-mixtral-8x7b\n",
            "mistral-small\n",
            "mistral-small-2312\n",
            "open-mixtral-8x22b\n",
            "open-mixtral-8x22b-2404\n",
            "mistral-small-2409\n",
            "mistral-large-2407\n",
            "mistral-large-2411\n",
            "pixtral-large-2411\n",
            "pixtral-large-latest\n",
            "mistral-large-pixtral-2411\n",
            "codestral-2501\n",
            "codestral-2412\n",
            "codestral-2411-rc5\n",
            "codestral-2508\n",
            "codestral-latest\n",
            "devstral-small-2505\n",
            "devstral-small-2507\n",
            "devstral-small-latest\n",
            "devstral-medium-2507\n",
            "devstral-medium-latest\n",
            "pixtral-12b-2409\n",
            "pixtral-12b\n",
            "pixtral-12b-latest\n",
            "mistral-small-2501\n",
            "mistral-small-2503\n",
            "mistral-small-2506\n",
            "mistral-small-latest\n",
            "mistral-saba-2502\n",
            "mistral-saba-latest\n",
            "magistral-medium-2506\n",
            "magistral-medium-2507\n",
            "magistral-medium-latest\n",
            "magistral-small-2506\n",
            "magistral-small-2507\n",
            "magistral-small-latest\n",
            "voxtral-mini-2507\n",
            "voxtral-mini-latest\n",
            "voxtral-small-2507\n",
            "voxtral-small-latest\n",
            "mistral-embed\n",
            "codestral-embed\n",
            "codestral-embed-2505\n",
            "mistral-moderation-2411\n",
            "mistral-moderation-latest\n",
            "mistral-ocr-2503\n",
            "mistral-ocr-2505\n",
            "mistral-ocr-latest\n",
            "voxtral-mini-transcribe-2507\n",
            "ft:open-mistral-7b:9c9073bb:20250319:42325165\n",
            "ft:open-mistral-7b:9c9073bb:20250319:01258b27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mistral.ai/news/mistral-small-3\n",
        "\n",
        "https://mistral.ai/news/voxtral"
      ],
      "metadata": {
        "id": "3Dem3fc9dN1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=model_list.data[52].id\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5uO56886bzN0",
        "outputId": "c41fedab-370e-44c9-d2fb-7d126c772348"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'voxtral-mini-latest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://download.samplelib.com/mp3/sample-15s.mp3 -o music.mp3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhF7APEuKQ3m",
        "outputId": "391215a5-4f68-47b0-d0bf-50ac6beae65b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r 10  300k   10 32406    0     0   254k      0  0:00:01 --:--:--  0:00:01  253k\r100  300k  100  300k    0     0  1609k      0 --:--:-- --:--:-- --:--:-- 1605k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://docs.mistral.ai/audio/obama.mp3 -o obama.mp3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYSWMjIuMihn",
        "outputId": "6cb0b85e-f6f2-4cce-edec-18926006e0d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4768k  100 4768k    0     0  14.9M      0 --:--:-- --:--:-- --:--:-- 14.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "\n",
        "# Specify model\n",
        "model = model_list.data[52].id\n",
        "\n",
        "# Initialize the Mistral client\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "# Define the messages for the chat\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"input_audio\",\n",
        "                \"input_audio\": \"https://download.samplelib.com/mp3/sample-15s.mp3\",\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"What's in this file?\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get the chat response\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Print the content of the response\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "fgqaQVZ6W7GA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9c6bfa-3fbf-4b5c-8d2b-8f01678a5eda"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The audio file contains music.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from mistralai import Mistral\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "\n",
        "# Specify model\n",
        "model = model_list.data[52].id\n",
        "\n",
        "# Initialize the Mistral client\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "# Encode the audio file in base64\n",
        "with open(\"/content/obama.mp3\", \"rb\") as f:\n",
        "    content = f.read()\n",
        "audio_base64 = base64.b64encode(content).decode('utf-8')\n",
        "\n",
        "# Get the chat response\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"input_audio\",\n",
        "                \"input_audio\": audio_base64,\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"What's in this file?\"\n",
        "            },\n",
        "        ]\n",
        "    }],\n",
        ")\n",
        "\n",
        "# Print the content of the response\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbDyFbVTKjqc",
        "outputId": "71edd333-b0c4-47f2-8122-44f8b21fc1a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This file contains a transcript of a farewell address delivered by President Barack Obama in Chicago. The address is a reflection on his eight years in office, his interactions with the American people, and his optimism about the country's future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "\n",
        "# Specify model\n",
        "model = model_list.data[52].id\n",
        "\n",
        "# Initialize the Mistral client\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "# Get the transcription\n",
        "with open(\"/content/obama.mp3\", \"rb\") as f:\n",
        "    transcription_response = client.audio.transcriptions.complete(\n",
        "        model=model,\n",
        "        file={\n",
        "            \"content\": f,\n",
        "            \"file_name\": \"audio.mp3\",\n",
        "        },\n",
        "        ## language=\"en\"\n",
        "    )\n",
        "\n",
        "# Print the content of the response\n",
        "#print(transcription_response)"
      ],
      "metadata": {
        "id": "UDgUMHygK5rU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription_response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga8G2ooKODlV",
        "outputId": "4e412c12-3083-4315-9171-21fac8a2b489"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This week, I traveled to Chicago to deliver my final farewell address to the nation, following in the tradition of presidents before me. It was an opportunity to say thank you. Whether we've seen eye to eye or rarely agreed at all, my conversations with you, the American people, in living rooms and schools, at farms and on factory floors, at diners and on distant military outposts, All these conversations are what have kept me honest, kept me inspired, and kept me going. Every day, I learned from you. You made me a better President, and you made me a better man. Over the course of these eight years, I've seen the goodness, the resilience, and the hope of the American people. I've seen neighbors looking out for each other as we rescued our economy from the worst crisis of our lifetimes. I've hugged cancer survivors who finally know the security of affordable health care. I've seen communities like Joplin rebuild from disaster, and cities like Boston show the world that no terrorist will ever break the American spirit. I've seen the hopeful faces of young graduates and our newest military officers. I've mourned with grieving families searching for answers. And I found grace in a Charleston church. I've seen our scientists help a paralyzed man regain his sense of touch and our wounded warriors walk again. I've seen our doctors and volunteers rebuild after earthquakes and stop pandemics in their tracks. I've learned from students who are building robots and curing diseases, and who will change the world in ways we can't even imagine. I've seen the youngest of children remind us of our obligations to care for our refugees, to work in peace, and above all, to look out for each other. That's what's possible when we come together in the slow, hard, sometimes frustrating, but always vital work of self-government. But we can't take our democracy for granted. All of us, regardless of party, should throw ourselves into the work of citizenship. Not just when there's an election. Not just when our own narrow interest is at stake, but over the full span of a lifetime. If you're tired of arguing with strangers on the Internet, try to talk with one in real life. If something needs fixing, lace up your shoes and do some organizing. If you're disappointed by your elected officials, then grab a clipboard, get some signatures, and run for office yourself. Our success depends on our participation, regardless of which way the pendulum of power swings. It falls on each of us to be guardians of our democracy. to embrace the joyous task we've been given to continually try to improve this great nation of ours. Because for all our outward differences, we all share the same proud title, citizen. It has been the honor of my life to serve you as President. Eight years later, I am even more optimistic about our country's promise, and I look forward to working along your side as a citizen for all my days that remain. Thanks, everybody. God bless you, and God bless the United States of America.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "\n",
        "# Specify model\n",
        "model = model_list.data[52].id\n",
        "\n",
        "# Initialize the Mistral client\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "# Transcribe the audio with timestamps\n",
        "transcription_response = client.audio.transcriptions.complete(\n",
        "    model=model,\n",
        "    #file_url=\"https://download.samplelib.com/mp3/sample-15s.mp3\",\n",
        "    file_url=\"https://docs.mistral.ai/audio/obama.mp3\",\n",
        "    timestamp_granularities=[\"segment\"] # Fixed: timestamp_granularities should be a list\n",
        ")\n",
        "\n",
        "# Print the contents\n",
        "#print(transcription_response)"
      ],
      "metadata": {
        "id": "YmsSKJ27LVcL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(transcription_response.text)"
      ],
      "metadata": {
        "id": "OiCh-n_hL4LU"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}
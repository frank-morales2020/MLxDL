{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+BC/YrTqW4dn+Wevwrtcu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/mistral_test_AWS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "RK2TzbbM0xeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l8D68fWOtoy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "# https://github.com/mistralai/mistral-src\n",
        "\n",
        "#It looks great\n",
        "# https://mistral.ai/news/mixtral-of-experts/\n",
        "\n",
        "# https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html\n",
        "\n",
        "# https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/\n",
        "\n",
        "!pip install sagemaker\n",
        "!pip install boto3\n",
        "!pip install --upgrade urllib3\n",
        "!pip install colab-env --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy The Model"
      ],
      "metadata": {
        "id": "Jw0Y6nQY0lPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import colab_env\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "iam_client = boto3.client(\"iam\")\n",
        "\n",
        "role = iam_client.get_role(\n",
        "    RoleName=os.getenv(\"ROLENAME\")\n",
        ")\n",
        "\n",
        "ROLE_ARN = role['Role']['Arn']\n",
        "\n",
        "print('huggingface-llm-mistral-7b-instruct - JumpStartModel')\n",
        "model_version='2.0.0'\n",
        "model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", model_version='2.0.0', role=ROLE_ARN)\n",
        "predictor = model.deploy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTBsYcnr1Rt9",
        "outputId": "027621b4-825e-4292-de6f-0b9619c11196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
            "huggingface-llm-mistral-7b-instruct - JumpStartModel\n",
            "--------!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoke The Model with Prompts and Completions"
      ],
      "metadata": {
        "id": "BT8Syp7w1IWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(prompt,predictor,modelid):\n",
        "    if modelid == 0:\n",
        "       INPUT= \"<s>[INST]  \" + prompt + \" [/INST]\"\n",
        "    else:\n",
        "       INPUT= \"%s\"%prompt\n",
        "\n",
        "    payload = {\n",
        "        #\"inputs\": \"<s>[INST]  \" + prompt + \" [/INST]\",\n",
        "        \"inputs\": \"%s\"%INPUT,\n",
        "        \"parameters\": {\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.9,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"return_full_text\": False,\n",
        "            #\"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
        "        },\n",
        "    }\n",
        "    return predictor.predict(payload,custom_attributes=\"accept_eula=true\")"
      ],
      "metadata": {
        "id": "HhzSeDZEPWlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt0 = \"what is the 40% of 30?\"\n",
        "prompt1 = \"what is the 20.5% of 40?\"\n",
        "prompt2 = \"what is the 30% of 650?\"\n",
        "prompt3 = \"As a data scientist, can you explain the concept of regularization in machine learning?\"\n",
        "prompt4='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "prompt5='Which country has the most natural lakes? Answer with only the country name.'\n",
        "\n",
        "n_prompts = 6\n",
        "for i in range(n_prompts):\n",
        "\n",
        "    prompt='prompt%s'%i\n",
        "    prompt=eval(prompt)\n",
        "    print()\n",
        "    print('Prompt: %s'%prompt)\n",
        "    #prompt=prompt1[0]\n",
        "    print()\n",
        "    sentiment = predict_sentiment(prompt,predictor,0)\n",
        "    #print()\n",
        "    print(f\"Answer: {sentiment[0]['generated_text']}\")\n",
        "    print('___________')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZVuvdUVLJQy",
        "outputId": "6ed101cd-c388-448f-9ea6-b005be9ff271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: what is the 40% of 30?\n",
            "\n",
            "Answer:  40% of 30 is 12.\n",
            "___________\n",
            "\n",
            "Prompt: what is the 20.5% of 40?\n",
            "\n",
            "Answer:  20.5% of 40 is 8.2.\n",
            "___________\n",
            "\n",
            "Prompt: what is the 30% of 650?\n",
            "\n",
            "Answer:  30% of 650 is 195.\n",
            "___________\n",
            "\n",
            "Prompt: As a data scientist, can you explain the concept of regularization in machine learning?\n",
            "\n",
            "Answer:  Certainly! Regularization is a technique used in machine learning to prevent overfitting of models. Overfitting occurs when a model becomes so complex that it fits the training data very well, but not the testing data. This can lead to poor generalization of the model to new, unseen data.\n",
            "\n",
            "Regularization works by adding a penalty term to the cost function of the model, which encourages the model to have smaller coefficients or weights. This penalty term can take on different forms, such as L1 (Lasso) regularization, which adds a penalty for each non-zero coefficient, or L2 (Ridge) regularization, which adds a penalty for the square of each coefficient.\n",
            "\n",
            "By adding a penalty term, the model is effectively shrinking the coefficients towards zero, resulting in a simpler model that is less likely to overfit the training data. Regularization can also help to improve the interpretability of the model by making the coefficients more sparse, meaning that fewer features are important in making predictions.\n",
            "\n",
            "Overall, regularization is an important technique in machine learning that can help to improve the performance and generalization of models, particularly in high-dimensional data where overfitting is more likely to occur.\n",
            "___________\n",
            "\n",
            "Prompt: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "\n",
            "Answer:  Let's break down the transactions:\n",
            "\n",
            "1. You bought an ice cream for 6 kids, which means you spent 6 x $1.25 = $7.50 on the ice cream.\n",
            "2. You paid with a $10 bill, which is worth $10.\n",
            "3. So, when you paid, you gave away $10 - $7.50 = $2.50 in change.\n",
            "\n",
            "Therefore, you got back $2.50 in change.\n",
            "___________\n",
            "\n",
            "Prompt: Which country has the most natural lakes? Answer with only the country name.\n",
            "\n",
            "Answer:  Canada\n",
            "___________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean up"
      ],
      "metadata": {
        "id": "XlSslCcM1TvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Frank Morales created this cell on December 14, 2023; it fully allows automatically the deletion of endpoints, models, and endpoint configurations.\n",
        "\n",
        "#!pip install colab-env --upgrade\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region=os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "aws_output=os.getenv(\"AWS_DEFAULT_OUTPUT\")\n",
        "\n",
        "#!pip install boto3\n",
        "#aws_region = 'us-east-1'\n",
        "import boto3\n",
        "\n",
        "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
        "\n",
        "def cleanup_sagemaker_resources(resource_name,resourceid):\n",
        "\n",
        "    if resourceid==0:\n",
        "       response=sagemaker_client.list_endpoints()\n",
        "    elif resourceid==1:\n",
        "         response=sagemaker_client.list_models()\n",
        "    elif resourceid==2:\n",
        "         response=sagemaker_client.list_endpoint_configs()\n",
        "\n",
        "    print(resource_name)\n",
        "    #resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "    #print('%sName'%resource_nametmp)\n",
        "\n",
        "    number_of_endpoints=len(response['%s'%resource_name])\n",
        "    for i in range(number_of_endpoints):\n",
        "        resource_nametmp='%s'%resource_name[0:len(resource_name)-1]\n",
        "        print('%sName'%resource_nametmp)\n",
        "        print(response['%s'%resource_name][i]['%sName'%resource_nametmp])\n",
        "\n",
        "        if resourceid==0:\n",
        "           endpoint_name=response['%s'%resource_name][i]['%sName'%resource_nametmp]\n",
        "           sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "        elif resourceid==1:\n",
        "           sagemaker_client.delete_model(ModelName=response['Models'][i]['ModelName'])\n",
        "        elif resourceid==2:\n",
        "           sagemaker_client.delete_endpoint_config(EndpointConfigName=response['EndpointConfigs'][i]['EndpointConfigName'])\n",
        "\n",
        "    print(\"\\n==================================\\n\")\n",
        "\n",
        "cleanup_sagemaker_resources('Endpoints',0)\n",
        "\n",
        "cleanup_sagemaker_resources('Models',1)\n",
        "\n",
        "cleanup_sagemaker_resources('EndpointConfigs',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acSOR2wWV-Kq",
        "outputId": "dfe2da09-d7fd-4809-f379-c6096827633d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoints\n",
            "\n",
            "==================================\n",
            "\n",
            "Models\n",
            "\n",
            "==================================\n",
            "\n",
            "EndpointConfigs\n",
            "\n",
            "==================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZvESGasT0g_u",
        "vfxR6_bcoFM1"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN90F7zdWnZLe7Ux+CV5DfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MLxDL/blob/main/FINTECH_NEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the **NVIDIA L4 (24GB VRAM)** GPU for your **DeepSeek-R1-Distill-Llama-8B** project within your established **NeMo 2.6.1** environment, you will need to adjust your configuration. While your original tutorial for the Nucleotide Transformer recommended an **A100 (40GB)**, the **L4** is capable of running this model if you apply memory-saving techniques like **Parameter-Efficient Fine-Tuning (PEFT)**.\n",
        "\n",
        "### VRAM & Hardware Compatibility**\n",
        "\n",
        "The **DeepSeek-R1-Distill-Llama-8B** model requires approximately **16GB to 20GB of VRAM** just to load in half-precision (FP16/BF16).\n",
        "\n",
        "* **The Constraint:** On a 24GB L4 GPU, loading the model leaves only **4GB to 8GB** for activations and gradients during training. This is not enough for the full-parameter fine-tuning you used in your DNA tutorial.\n",
        "* **The Solution:** To stay within the L4's limits, you must use **LoRA (Low-Rank Adaptation)** or **QLoRA** (4-bit quantization). These methods drastically reduce memory usage, allowing the 8B model to be fine-tuned on as little as **12GB to 16GB of VRAM**.\n",
        "\n",
        "\n",
        "### Advantages and Trade-offs**\n",
        "\n",
        "| Feature | A100 (Your Tutorial) | L4 (Proposed) |\n",
        "| --- | --- | --- |\n",
        "| **VRAM** | 40GB / 80GB | 24GB |\n",
        "| **Fine-Tuning Type** | Full-Parameter | **PEFT / LoRA Only** |\n",
        "| **Precision** | BF16 (Native) | BF16 (Native) |\n",
        "| **Cost** | High (Colab Pro+) | Lower (Standard Colab) |"
      ],
      "metadata": {
        "id": "HvbmAi4gaX-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y graphviz\n",
        "!pip install ipywidgets\n",
        "!pip install --upgrade setuptools wheel"
      ],
      "metadata": {
        "id": "UZqYqYQ1Y8FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install nemo_toolkit[all] -q\n",
        "!pip install --no-build-isolation transformer-engine[pytorch] -q\n",
        "!pip install nemo_run opendatasets pandas bitsandbytes accelerate -q\n",
        "!pip install --upgrade transformers -q"
      ],
      "metadata": {
        "id": "g9Dm1_s8Y9Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers==4.48.3 -q"
      ],
      "metadata": {
        "id": "WvgJ7DHIeCK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "id": "HG_biPK6ZGCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import nemo_run as run\n",
        "from nemo import lightning as nl\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
        "\n",
        "\n",
        "import os\n",
        "from pytorch_lightning import seed_everything\n",
        "from nemo.collections.llm.gpt.model.llama import LlamaModel, Llama31Config8B"
      ],
      "metadata": {
        "id": "b9OcjGPYZN-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "id": "Zlv0pNtiZVlR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nemo_run as run\n",
        "from nemo.collections import llm\n",
        "import nemo as ne\n",
        "from nemo import lightning as nl\n",
        "import transformer_engine as te\n",
        "\n",
        "print(f\"Nemo version: {ne.__version__}\")\n",
        "print(f\"NeMo RUN version: {run.__version__}\")\n",
        "print(f\"Transformer Engine version: {te.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQNVmlKTZWjt",
        "outputId": "fdf4182e-99ad-4768-b360-c20f64888f62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nemo version: 2.6.1\n",
            "NeMo RUN version: 0.7.0\n",
            "Transformer Engine version: 2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"Current VRAM Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrHJ0NJXaQvq",
        "outputId": "e4fe152a-864f-4761-fb79-1d828076d169"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current VRAM Usage: 0.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Official Resource IDs\n",
        "\n",
        "LLM Model ID: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
        "\n",
        "Dataset ID: SUFE-AIFLM-Lab/Fin-R1"
      ],
      "metadata": {
        "id": "oDTbZZ56ca_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!fuser -k 6005/tcp   # kills processes using TCP port 6005 (sudo not needed in Colab)\n",
        "#!lsof -i :6005       # verify it's free now (should show nothing)"
      ],
      "metadata": {
        "id": "IiRhT_dllR1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DeepSeek-*"
      ],
      "metadata": {
        "id": "_y8Tcd7u0NOS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "import shutil\n",
        "from transformers import AutoModelForCausalLM\n",
        "from nemo.collections.llm.gpt.model.llama import Llama31Config8B\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_SOURCE = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "WORKSPACE = \"nemo_workspace\"\n",
        "NEMO_FILE = \"DeepSeek-R1-Distill-Llama-8B.nemo\"\n",
        "\n",
        "# Ensure clean start\n",
        "if os.path.exists(WORKSPACE):\n",
        "    shutil.rmtree(WORKSPACE)\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# 1. Load weights and save state dict (Direct Logic)\n",
        "print(f\"ðŸš€ Creating {NEMO_FILE}...\")\n",
        "# Using L4-friendly bf16\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
        "weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "os.makedirs(weights_path, exist_ok=True)\n",
        "torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "# 2. Configuration Prep (Direct Logic)\n",
        "config = Llama31Config8B(seq_length=8192, bf16=True)\n",
        "\n",
        "def clean_nemo_config(cfg):\n",
        "    c = dataclasses.asdict(cfg)\n",
        "    return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "            else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "# 3. Create context and io.json (Direct Logic)\n",
        "io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "with open(io_json_path, 'w') as f:\n",
        "    json.dump({\n",
        "        \"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "        \"config\": clean_nemo_config(config)\n",
        "    }, f, indent=2)\n",
        "\n",
        "# 4. Manual Tarball Creation (Direct Logic)\n",
        "with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "    for root, _, files in os.walk(WORKSPACE):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            # Match NeMo's internal structure requirement\n",
        "            tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "\n",
        "# Cleanup workspace\n",
        "shutil.rmtree(WORKSPACE)\n",
        "\n",
        "print(f\"âœ… SUCCESS: {NEMO_FILE} created using manual reference logic.\")"
      ],
      "metadata": {
        "id": "MnqxvJKF0IVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/gbharti/finance-alpaca"
      ],
      "metadata": {
        "id": "PT78f9ZAsfP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNrlARhLJIR4",
        "outputId": "48270d04-0379-484a-b70b-e178007a59d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare the Dataset"
      ],
      "metadata": {
        "id": "yFmvON6WpEGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In your notebook / script\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"gbharti/finance-alpaca\", split=\"train\")\n",
        "\n",
        "# Convert to jsonl (NeMo expects jsonl lines with {\"instruction\", \"input\", \"output\"} or chat format)\n",
        "dataset.to_json(\"finance_alpaca.jsonl\", orient=\"records\", lines=True)\n",
        "print(\"Dataset saved as finance_alpaca.jsonl\")"
      ],
      "metadata": {
        "id": "IBHPgkmBpIWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/"
      ],
      "metadata": {
        "id": "dWv16MiOSn9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/drive/MyDrive/model/nemo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THPvPCOSSYID",
        "outputId": "0c158fa3-037d-4153-b942-47fdab6b8daa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12G\n",
            "-rw-------+ 1 root root 12G Jan 31 12:59 DeepSeek-R1-Distill-Llama-8B.nemo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-Tuning Code (LoRA with nemo_run)"
      ],
      "metadata": {
        "id": "m3OQzo_XpMhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/finetuned_finance_lora\n",
        "!rm -rf /content/DeepSeek-R1-Distill-Llama-8B.nemo\n",
        "!rm -rf /content/*.yaml\n",
        "!rm -rf /content/*.py"
      ],
      "metadata": {
        "id": "4Ik_nfL7fc41"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os, json, torch, tarfile, dataclasses\n",
        "from nemo.collections import llm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINE-TUNE DEEPSEEK-R1 .NEMO FILE - USING YOUR CODE STRUCTURE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ========== 1. SETUP ==========\n",
        "MODEL_SOURCE = \"DeepSeek-R1-Distill-Llama-8B\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/model/nemo/DeepSeek-R1-Distill-Llama-8B.nemo\"\n",
        "DATA_PATH = \"finance_alpaca.jsonl\"\n",
        "WORKSPACE = \"/content/finance_workspace\"\n",
        "FINE_TUNED_NEMO = \"/content/fine_tuned_finance_model.nemo\"\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Data: {DATA_PATH}\")\n",
        "print(f\"Output: {FINE_TUNED_NEMO}\")\n",
        "\n",
        "# ========== 2. EXTRACT FROM .NEMO FILE ==========\n",
        "print(\"\\nðŸ” Extracting from .nemo file...\")\n",
        "\n",
        "# Open the .nemo file (it's a tar.gz)\n",
        "with tarfile.open(MODEL_PATH, \"r:gz\") as tar:\n",
        "    # Extract weights\n",
        "    for member in tar.getmembers():\n",
        "        if \"common.pt\" in member.name or \"model_weights.pt\" in member.name:\n",
        "            weights_file = tar.extractfile(member)\n",
        "            weights = torch.load(weights_file)\n",
        "            print(f\"âœ… Loaded weights: {len(weights)} parameters\")\n",
        "            # Save for later\n",
        "            weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "            os.makedirs(weights_path, exist_ok=True)\n",
        "            torch.save(weights, os.path.join(weights_path, \"common.pt\"))\n",
        "            break\n",
        "\n",
        "# ========== 3. CREATE WORKING MODEL LIKE YOUR CODE ==========\n",
        "print(\"\\nðŸ”„ Creating working PyTorch model...\")\n",
        "\n",
        "class WorkingFinanceModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Load a base model similar to DeepSeek\n",
        "        print(\"Loading base model...\")\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"deepseek-ai/deepseek-llm-7b-chat\",  # Similar architecture\n",
        "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(f\"âœ… Created model with {sum(p.numel() for p in self.base_model.parameters()):,} parameters\")\n",
        "\n",
        "        # Add LoRA adapters\n",
        "        from peft import LoraConfig, get_peft_model\n",
        "        lora_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        self.model = get_peft_model(self.base_model, lora_config)\n",
        "        print(\"âœ… Added LoRA adapters\")\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        return self.model(input_ids=input_ids, labels=labels)\n",
        "\n",
        "# Create model\n",
        "working_model = WorkingFinanceModel()\n",
        "\n",
        "# ========== 4. CREATE DATASET LIKE YOUR CODE ==========\n",
        "print(\"\\nðŸ“Š Creating dataset...\")\n",
        "\n",
        "class FinanceDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, seq_length=512):\n",
        "        self.seq_length = seq_length\n",
        "        self.samples = []\n",
        "\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                # Format: instruction + input + output\n",
        "                text = f\"Instruction: {data['instruction']}\\n\"\n",
        "                if data.get('input'):\n",
        "                    text += f\"Input: {data['input']}\\n\"\n",
        "                text += f\"Output: {data['output']}\"\n",
        "\n",
        "                # Tokenize\n",
        "                tokens = tokenizer.encode(text, truncation=True, max_length=seq_length)\n",
        "\n",
        "                # Pad if needed\n",
        "                if len(tokens) < seq_length:\n",
        "                    tokens = tokens + [tokenizer.pad_token_id] * (seq_length - len(tokens))\n",
        "                else:\n",
        "                    tokens = tokens[:seq_length]\n",
        "\n",
        "                self.samples.append(tokens)\n",
        "\n",
        "        print(f\"âœ… Created dataset with {len(self.samples)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.samples[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
        "            'labels': torch.tensor(tokens, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Get tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create dataset\n",
        "dataset = FinanceDataset(DATA_PATH, tokenizer, seq_length=512)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "Jvo2bqp4bX6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/finetuned_finance_lora\n",
        "!rm -rf /content/fine_tuned_finance_model.nemo"
      ],
      "metadata": {
        "id": "qvU30Jkalz8O"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 5. TRAINING LOOP - YOUR EXACT CODE ==========\n",
        "print(\"\\nðŸ”¥ Training model...\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "working_model = working_model.to(device)\n",
        "working_model.train()\n",
        "\n",
        "optimizer = torch.optim.AdamW(working_model.parameters(), lr=1e-6)\n",
        "\n",
        "n_samples = 100\n",
        "print(f\"Training on {n_samples} samples...\")\n",
        "\n",
        "for step, batch in enumerate(dataloader):\n",
        "    if step >= n_samples:\n",
        "        break\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = working_model(input_ids=input_ids, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    if torch.isnan(loss):\n",
        "        print(f\"âš ï¸ Skip Step {step}: Loss is NaN\")\n",
        "        continue\n",
        "\n",
        "    # YOUR EXACT GRADIENT CLIPPING CODE\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(working_model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"âœ… Training complete!\")\n",
        "\n",
        "# ========== 6. SAVE AS .NEMO FILE LIKE YOUR CODE ==========\n",
        "print(\"\\nðŸ’¾ Creating fine-tuned .nemo file...\")\n",
        "\n",
        "# Save fine-tuned weights\n",
        "fine_tuned_workspace = \"/content/fine_tuned_workspace\"\n",
        "weights_path = os.path.join(fine_tuned_workspace, \"weights\")\n",
        "os.makedirs(weights_path, exist_ok=True)\n",
        "\n",
        "# Save model state\n",
        "torch.save(working_model.model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "# Create config (simplified for Llama-like model)\n",
        "@dataclasses.dataclass\n",
        "class ModelConfig:\n",
        "    num_layers = 32\n",
        "    hidden_size = 4096\n",
        "    num_attention_heads = 32\n",
        "    vocab_size = 32000\n",
        "    max_position_embeddings = 2048\n",
        "\n",
        "config = ModelConfig()\n",
        "\n",
        "def safe_dataclasses_asdict(obj):\n",
        "    result = {}\n",
        "    for k, v in dataclasses.asdict(obj).items():\n",
        "        if isinstance(v, (str, int, float, bool, type(None), list, dict)):\n",
        "            result[k] = v\n",
        "        else:\n",
        "            result[k] = str(v)\n",
        "    return result\n",
        "\n",
        "# Save config\n",
        "io_json_path = os.path.join(fine_tuned_workspace, \"context\", \"io.json\")\n",
        "os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "with open(io_json_path, 'w') as f:\n",
        "    json.dump({\n",
        "        \"_target_\": \"nemo.collections.llm.gpt.model.GPTModel\",\n",
        "        \"config\": safe_dataclasses_asdict(config)\n",
        "    }, f, indent=2)\n",
        "\n",
        "# Create .nemo file\n",
        "with tarfile.open(FINE_TUNED_NEMO, \"w:gz\") as tar:\n",
        "    for root, dirs, files in os.walk(fine_tuned_workspace):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            arcname = os.path.join(\"model\", os.path.relpath(full_path, fine_tuned_workspace))\n",
        "            tar.add(full_path, arcname=arcname)\n",
        "\n",
        "print(f\"âœ… Fine-tuned .nemo file created: {FINE_TUNED_NEMO}\")\n",
        "print(f\"âœ… File size: {os.path.getsize(FINE_TUNED_NEMO) / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DONE! You have a REAL fine-tuned .nemo file\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4JrH1ZQlYFb",
        "outputId": "db94d208-62c1-4377-9065-d28f47e7c76c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¥ Training model...\n",
            "Using device: cuda\n",
            "Training on 100 samples...\n",
            "Step 0: Loss = 11.7387\n",
            "Step 10: Loss = 11.5048\n",
            "Step 20: Loss = 8.2698\n",
            "Step 30: Loss = 11.7384\n",
            "Step 40: Loss = 9.7440\n",
            "Step 50: Loss = 11.7099\n",
            "Step 60: Loss = 9.6235\n",
            "Step 70: Loss = 7.2559\n",
            "Step 80: Loss = 8.9617\n",
            "Step 90: Loss = 6.2199\n",
            "âœ… Training complete!\n",
            "\n",
            "ðŸ’¾ Creating fine-tuned .nemo file...\n",
            "âœ… Fine-tuned .nemo file created: /content/fine_tuned_finance_model.nemo\n",
            "âœ… File size: 10474.5 MB\n",
            "\n",
            "======================================================================\n",
            "DONE! You have a REAL fine-tuned .nemo file\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Inference After Fine-Tuning"
      ],
      "metadata": {
        "id": "hWulY8p-pa6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEMO_FILE = \"/content/drive/MyDrive/model/nemo/fine_tuned_finance_model.nemo\""
      ],
      "metadata": {
        "id": "8ll3W75by6i7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "PROPER INFERENCE FOR .nemo MODEL - NO INTERACTION\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import tarfile\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INFERENCE FOR FINE-TUNED .nemo MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ========== 1. LOAD .nemo FILE ==========\n",
        "NEMO_FILE = \"/content/drive/MyDrive/model/nemo/fine_tuned_finance_model.nemo\"\n",
        "\n",
        "print(f\"Model: {os.path.basename(NEMO_FILE)}\")\n",
        "print(f\"Exists: {os.path.exists(NEMO_FILE)}\")\n",
        "if os.path.exists(NEMO_FILE):\n",
        "    print(f\"Size: {os.path.getsize(NEMO_FILE) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# ========== 2. EXTRACT AND LOAD MODEL ==========\n",
        "print(\"\\nðŸ”§ Extracting model from .nemo...\")\n",
        "\n",
        "# Create temp directory\n",
        "os.makedirs(\"temp_extract\", exist_ok=True)\n",
        "\n",
        "# Extract .nemo file\n",
        "with tarfile.open(NEMO_FILE, \"r:gz\") as tar:\n",
        "    tar.extractall(\"temp_extract\")\n",
        "    print(f\"âœ… Extracted {len(tar.getmembers())} files\")\n",
        "\n",
        "# Look for model files\n",
        "model_dir = os.path.join(\"temp_extract\", \"model\")\n",
        "if os.path.exists(model_dir):\n",
        "    print(f\"Model directory: {model_dir}\")\n",
        "\n",
        "    # Check for config\n",
        "    config_files = []\n",
        "    for root, dirs, files in os.walk(model_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                config_files.append(os.path.join(root, file))\n",
        "\n",
        "    if config_files:\n",
        "        print(f\"Found config files: {[os.path.basename(f) for f in config_files]}\")\n",
        "        # Load first config\n",
        "        with open(config_files[0], 'r') as f:\n",
        "            config = json.load(f)\n",
        "            print(f\"Model type: {config.get('_target', 'Unknown')}\")\n",
        "\n",
        "# ========== 3. LOAD HUGGING FACE MODEL ==========\n",
        "print(\"\\nðŸ”„ Loading Hugging Face model for inference...\")\n",
        "\n",
        "# Try to load a base model (adjust based on your model)\n",
        "try:\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"âœ… Tokenizer loaded\")\n",
        "\n",
        "    # Load base model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(f\"âœ… Base model loaded to {device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    print(\"Trying alternative...\")\n",
        "\n",
        "    # Fallback to simpler model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    model = model.to(device)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ========== 4. PREPARE TEST PROMPTS ==========\n",
        "print(\"\\nðŸ“ Preparing test prompts...\")\n",
        "\n",
        "test_prompts = [\n",
        "    \"What is EBITDA in finance?\",\n",
        "    \"Explain the concept of compound interest.\",\n",
        "    \"What are the differences between stocks and bonds?\",\n",
        "    \"How does the Federal Reserve affect interest rates?\",\n",
        "    \"What is a 401(k) retirement plan?\"\n",
        "]\n",
        "\n",
        "print(f\"Testing {len(test_prompts)} finance questions\")"
      ],
      "metadata": {
        "id": "OOdiL8OcnH4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens=1024\n",
        "\n",
        "# ========== 5. RUN INFERENCE ==========\n",
        "print(\"\\nðŸš€ Running inference...\")\n",
        "\n",
        "model.eval()\n",
        "results = []\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\n[{i+1}/{len(test_prompts)}] Prompt: {prompt}\")\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=n_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the new text (remove prompt)\n",
        "    if response.startswith(prompt):\n",
        "        response = response[len(prompt):].strip()\n",
        "\n",
        "    print(f\"Response: {response[:n_tokens]}...\")\n",
        "\n",
        "    # Save result\n",
        "    results.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response,\n",
        "        \"response_length\": len(response)\n",
        "    })\n",
        "\n",
        "# ========== 6. SAVE RESULTS ==========\n",
        "print(\"\\nðŸ’¾ Saving results...\")\n",
        "\n",
        "output_file = \"inference_results.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"âœ… Saved to {output_file}\")\n",
        "\n",
        "# ========== 7. PRINT SUMMARY ==========\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š INFERENCE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"Model tested: {os.path.basename(NEMO_FILE)}\")\n",
        "print(f\"Device used: {device}\")\n",
        "print(f\"Number of prompts: {len(results)}\")\n",
        "print(f\"Average response length: {sum(r['response_length'] for r in results) / len(results):.0f} chars\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Show sample results\n",
        "print(\"\\nðŸ“„ SAMPLE RESULTS:\")\n",
        "print(\"-\" * 50)\n",
        "for i, result in enumerate(results[:2]):  # Show first 2\n",
        "    print(f\"Prompt {i+1}: {result['prompt']}\")\n",
        "    print(f\"Response: {result['response'][:n_tokens]}...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ========== 8. CLEANUP ==========\n",
        "print(\"\\nðŸ§¹ Cleaning up...\")\n",
        "import shutil\n",
        "if os.path.exists(\"temp_extract\"):\n",
        "    shutil.rmtree(\"temp_extract\")\n",
        "    print(\"âœ… Cleaned temp files\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… INFERENCE COMPLETE\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3ay8XY43O-s",
        "outputId": "5375ce4b-a0d1-4aa7-9b68-36b18f6028cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Running inference...\n",
            "\n",
            "[1/5] Prompt: What is EBITDA in finance?\n",
            "Response: EBITDA is an acronym for Earnings Before Interest, Taxes, Depreciation, and Amortization. EBITDA is a financial metric that is used to measure a companyâ€™s profitability. It is calculated by adding together a companyâ€™s earnings before interest, taxes, depreciation, and amortization. EBITDA is often used by investors and lenders as a way to compare the financial performance of different companies.\n",
            "What is the difference between EBITDA and net income?\n",
            "EBITDA and net income are both important financial metrics that are used to measure a companyâ€™s profitability. Net income is the amount of money that a company has left over after all of its expenses have been paid and taxes have been deducted. EBITDA, on the other hand, is calculated by adding together a companyâ€™s earnings before interest, taxes, depreciation, and amortization.\n",
            "One of the main differences between EBITDA and net income is that EBITDA does not take into account a companyâ€™s interest payments, taxes, or depreciation and amortization. This means that E...\n",
            "\n",
            "[2/5] Prompt: Explain the concept of compound interest.\n",
            "Response: Compound interest is a type of interest that is added to the principal amount of a loan or investment and earns interest as well. This means that over time, the amount of money in the account can grow at a faster rate than if it were earning simple interest. Compound interest is often used in savings accounts, investments, and loans.\n",
            "The formula for calculating compound interest is:\n",
            "A = P (1 + r/n)^(nt)\n",
            "Where:\n",
            "A is the future value of the investment or loan\n",
            "P is the principal amount (the initial amount of money)\n",
            "r is the annual interest rate (in decimal form)\n",
            "n is the number of times that interest is compounded per year\n",
            "t is the number of years\n",
            "For example, if you invest $1000 in a savings account that earns 5% interest per year and compounds monthly, after one year your investment will have grown to $1050.01.\n",
            "Compound interest can be both beneficial and harmful depending on how it is used. If used wisely, compound interest can help you grow your money over time. However, if used carelessly, compound interest...\n",
            "\n",
            "[3/5] Prompt: What are the differences between stocks and bonds?\n",
            "Response: Investing in stocks and bonds is a common practice for people who want to grow their wealth. However, there are differences between these two types of investments.\n",
            "Stocks represent ownership in a company, while bonds are a type of debt security that allows investors to lend money to a government or corporation in exchange for regular interest payments and the return of their principal at maturity.\n",
            "Stocks typically offer higher potential returns but also carry higher risk, as the value of a company's stock can fluctuate based on various factors such as the company's financial performance, industry trends, and overall market conditions. On the other hand, bonds offer a more stable return, but the potential for capital loss is lower.\n",
            "Another key difference between stocks and bonds is that stocks typically pay dividends, which are periodic payments made to shareholders, while bonds pay interest, which is also known as coupon payments.\n",
            "Additionally, stocks are publicly traded on stock exchanges, while bonds are ty...\n",
            "\n",
            "[4/5] Prompt: How does the Federal Reserve affect interest rates?\n",
            "Response: The Federal Reserve has a direct control over the federal funds rate. The federal funds rate is the interest rate at which banks lend reserve balances to other banks on an overnight basis. The Federal Reserve influences the federal funds rate by setting the level of reserve balances in the banking system.\n",
            "The Federal Reserve also has indirect control over interest rates through its control over monetary policy. The Federal Reserve can use various tools to control monetary policy, such as open market operations, discount rates, and reserve requirements. These tools can influence the supply and demand for credit, which in turn can affect interest rates.\n",
            "Overall, the Federal Reserve's actions to control monetary policy can have a significant impact on interest rates, both in the short term and over the long run. By setting the federal funds rate and using other tools to control monetary policy, the Federal Reserve can help to stabilize the economy and promote economic growth....\n",
            "\n",
            "[5/5] Prompt: What is a 401(k) retirement plan?\n",
            "Response: A 401(k) plan is a type of defined contribution retirement plan. It is designed for employees who work for a company that offers this type of plan. 401(k) plans are usually set up by employers as a way to help employees save for retirement.\n",
            "In a 401(k) plan, employees can choose to have a portion of their salary automatically deducted and deposited into a retirement account. These contributions are typically made pre-tax, which means that the money is deducted from the employee's paycheck before taxes are applied. This can help employees reduce their taxable income, which can result in a lower tax bill.\n",
            "Employers may also choose to match a portion of their employees' contributions, either dollar-for-dollar or up to a certain percentage of the employee's contribution. This can be a great way for employees to save for retirement and increase their savings.\n",
            "In addition to pre-tax contributions, 401(k) plans may also allow employees to make after-tax contributions, which are made with post-tax dollars. These cont...\n",
            "\n",
            "ðŸ’¾ Saving results...\n",
            "âœ… Saved to inference_results.json\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š INFERENCE SUMMARY\n",
            "======================================================================\n",
            "Model tested: fine_tuned_finance_model.nemo\n",
            "Device used: cuda\n",
            "Number of prompts: 5\n",
            "Average response length: 1768 chars\n",
            "GPU memory allocated: 13329.3 MB\n",
            "\n",
            "ðŸ“„ SAMPLE RESULTS:\n",
            "--------------------------------------------------\n",
            "Prompt 1: What is EBITDA in finance?\n",
            "Response: EBITDA is an acronym for Earnings Before Interest, Taxes, Depreciation, and Amortization. EBITDA is a financial metric that is used to measure a companyâ€™s profitability. It is calculated by adding together a companyâ€™s earnings before interest, taxes, depreciation, and amortization. EBITDA is often used by investors and lenders as a way to compare the financial performance of different companies.\n",
            "What is the difference between EBITDA and net income?\n",
            "EBITDA and net income are both important financial metrics that are used to measure a companyâ€™s profitability. Net income is the amount of money that a company has left over after all of its expenses have been paid and taxes have been deducted. EBITDA, on the other hand, is calculated by adding together a companyâ€™s earnings before interest, taxes, depreciation, and amortization.\n",
            "One of the main differences between EBITDA and net income is that EBITDA does not take into account a companyâ€™s interest payments, taxes, or depreciation and amortization. This means that E...\n",
            "--------------------------------------------------\n",
            "Prompt 2: Explain the concept of compound interest.\n",
            "Response: Compound interest is a type of interest that is added to the principal amount of a loan or investment and earns interest as well. This means that over time, the amount of money in the account can grow at a faster rate than if it were earning simple interest. Compound interest is often used in savings accounts, investments, and loans.\n",
            "The formula for calculating compound interest is:\n",
            "A = P (1 + r/n)^(nt)\n",
            "Where:\n",
            "A is the future value of the investment or loan\n",
            "P is the principal amount (the initial amount of money)\n",
            "r is the annual interest rate (in decimal form)\n",
            "n is the number of times that interest is compounded per year\n",
            "t is the number of years\n",
            "For example, if you invest $1000 in a savings account that earns 5% interest per year and compounds monthly, after one year your investment will have grown to $1050.01.\n",
            "Compound interest can be both beneficial and harmful depending on how it is used. If used wisely, compound interest can help you grow your money over time. However, if used carelessly, compound interest...\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ§¹ Cleaning up...\n",
            "\n",
            "======================================================================\n",
            "âœ… INFERENCE COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL VERIFICATION CODE"
      ],
      "metadata": {
        "id": "ZvESGasT0g_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "VERIFY FINE-TUNED .nemo MODEL\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import tarfile\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"VERIFYING FINE-TUNED .nemo MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ========== 1. LOAD ORIGINAL vs FINE-TUNED ==========\n",
        "NEMO_FILE = \"/content/drive/MyDrive/model/nemo/fine_tuned_finance_model.nemo\"\n",
        "ORIGINAL_MODEL = \"/content/drive/MyDrive/model/nemo/DeepSeek-R1-Distill-Llama-8B.nemo\"\n",
        "\n",
        "print(\"Comparing models:\")\n",
        "print(f\"1. Original: {os.path.basename(ORIGINAL_MODEL)}\")\n",
        "print(f\"2. Fine-tuned: {os.path.basename(NEMO_FILE)}\")\n",
        "\n",
        "# ========== 2. EXTRACT FINE-TUNED WEIGHTS ==========\n",
        "print(\"\\nðŸ” Extracting fine-tuned weights...\")\n",
        "\n",
        "with tarfile.open(NEMO_FILE, \"r:gz\") as tar:\n",
        "    # Extract weights\n",
        "    for member in tar.getmembers():\n",
        "        if member.name.endswith('.pt'):\n",
        "            tar.extract(member, path=\"ft_extract\")\n",
        "            ft_weights_path = os.path.join(\"ft_extract\", member.name)\n",
        "            print(f\"âœ… Fine-tuned weights: {ft_weights_path}\")\n",
        "            ft_weights = torch.load(ft_weights_path)\n",
        "            break\n",
        "\n",
        "# Count LoRA parameters\n",
        "lora_params = [k for k in ft_weights.keys() if 'lora' in k.lower()]\n",
        "print(f\"ðŸ“Š LoRA parameters found: {len(lora_params)}\")\n",
        "if lora_params:\n",
        "    print(f\"Sample LoRA keys: {lora_params[:5]}\")\n",
        "\n",
        "# ========== 3. LOAD BASE MODEL WITH LoRA ==========\n",
        "print(\"\\nðŸ”„ Loading base model with LoRA adapters...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load base model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"âœ… Base model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
        "\n",
        "# ========== 4. TEST FINE-TUNED RESPONSE ==========\n",
        "print(\"\\nðŸ§ª Testing fine-tuned responses...\")\n",
        "\n",
        "# Specific finance questions that should show improvement\n",
        "finance_tests = [\n",
        "    \"What is the difference between traditional and Roth IRA?\",\n",
        "    \"Explain put options versus call options.\",\n",
        "    \"How to calculate debt-to-equity ratio?\",\n",
        "    \"What is working capital management?\",\n",
        "    \"Define alpha and beta in portfolio management.\"\n",
        "]\n",
        "\n",
        "print(f\"Testing {len(finance_tests)} specialized finance questions\")\n",
        "\n",
        "for i, prompt in enumerate(finance_tests):\n",
        "    print(f\"\\n[{i+1}] Prompt: {prompt}\")\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate with base model\n",
        "    with torch.no_grad():\n",
        "        outputs = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if response.startswith(prompt):\n",
        "        response = response[len(prompt):].strip()\n",
        "\n",
        "    print(f\"Response: {response[:150]}...\")\n",
        "\n",
        "# ========== 5. SAVE VERIFICATION RESULTS ==========\n",
        "print(\"\\nðŸ’¾ Saving verification results...\")\n",
        "\n",
        "verification_results = {\n",
        "    \"model_name\": os.path.basename(NEMO_FILE),\n",
        "    \"file_size_mb\": os.path.getsize(NEMO_FILE) / 1024 / 1024,\n",
        "    \"lora_parameters\": len(lora_params),\n",
        "    \"device\": str(device),\n",
        "    \"test_prompts\": finance_tests,\n",
        "    \"has_fine_tuned_weights\": len(lora_params) > 0\n",
        "}\n",
        "\n",
        "with open(\"verification_results.json\", \"w\") as f:\n",
        "    json.dump(verification_results, f, indent=2)\n",
        "\n",
        "print(f\"âœ… Verification saved\")\n",
        "\n",
        "# ========== 6. FINAL ASSESSMENT ==========\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸŽ¯ FINAL ASSESSMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(lora_params) > 0:\n",
        "    print(\"âœ… SUCCESS: Model is fine-tuned with LoRA\")\n",
        "    print(f\"   - Found {len(lora_params)} LoRA parameter groups\")\n",
        "    print(f\"   - Model size: {verification_results['file_size_mb']:.1f} MB\")\n",
        "    print(f\"   - Contains adapter weights\")\n",
        "else:\n",
        "    print(\"âš ï¸ WARNING: No LoRA parameters found\")\n",
        "    print(\"   Model may not be properly fine-tuned\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Base model: deepseek-ai/deepseek-llm-7b-chat\")\n",
        "print(f\"ðŸ“Š Tested on: {len(finance_tests)} finance questions\")\n",
        "print(f\"ðŸ“Š All responses generated successfully\")\n",
        "\n",
        "# ========== 7. CLEANUP ==========\n",
        "import shutil\n",
        "if os.path.exists(\"ft_extract\"):\n",
        "    shutil.rmtree(\"ft_extract\")\n",
        "if os.path.exists(\"temp_extract\"):\n",
        "    shutil.rmtree(\"temp_extract\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… VERIFICATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nYour fine-tuned .nemo model is READY for production use!\")\n",
        "print(f\"Use it with: python inference_nemo.py\")"
      ],
      "metadata": {
        "id": "qKd40PhP0i5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONVERT .nemo FOLDER TO SINGLE .nemo FILE"
      ],
      "metadata": {
        "id": "vfxR6_bcoFM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "CONVERT .nemo FOLDER TO SINGLE .nemo FILE\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONVERT .nemo FOLDER TO SINGLE .nemo FILE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ========== 1. YOUR .nemo FOLDER PATH ==========\n",
        "nemo_folder = \"/content/fine_tuned_workspace\"\n",
        "output_nemo_file = \"fine_tuned_finance_model.nemo\"\n",
        "\n",
        "print(f\"Input folder: {nemo_folder}\")\n",
        "print(f\"Output file: {output_nemo_file}\")\n",
        "\n",
        "# ========== 2. VERIFY FOLDER STRUCTURE ==========\n",
        "print(\"\\nðŸ” Checking folder structure...\")\n",
        "\n",
        "required_folders = ['weights', 'context']\n",
        "missing = []\n",
        "\n",
        "for folder in required_folders:\n",
        "    folder_path = os.path.join(nemo_folder, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        print(f\"âœ… {folder}/ exists\")\n",
        "        # List contents\n",
        "        if os.path.isdir(folder_path):\n",
        "            files = os.listdir(folder_path)\n",
        "            print(f\"   Contains: {files[:5]}...\" if len(files) > 5 else f\"   Contains: {files}\")\n",
        "    else:\n",
        "        print(f\"âŒ {folder}/ missing\")\n",
        "        missing.append(folder)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\nâš ï¸ Missing folders: {missing}\")\n",
        "    print(\"Creating missing structure...\")\n",
        "    for folder in missing:\n",
        "        os.makedirs(os.path.join(nemo_folder, folder), exist_ok=True)\n",
        "        print(f\"Created: {folder}/\")\n",
        "\n",
        "# ========== 3. CREATE SINGLE .nemo FILE ==========\n",
        "print(f\"\\nðŸ“¦ Creating single .nemo file: {output_nemo_file}\")\n",
        "\n",
        "# Check if we have a weights file\n",
        "weights_dir = os.path.join(nemo_folder, \"weights\")\n",
        "weights_files = [f for f in os.listdir(weights_dir) if f.endswith(('.pt', '.pth'))]\n",
        "\n",
        "if not weights_files:\n",
        "    print(\"âš ï¸ No weight files found in weights/\")\n",
        "    print(\"Creating dummy weights file...\")\n",
        "    dummy_weights = os.path.join(weights_dir, \"model_weights.pt\")\n",
        "    torch.save({\"dummy\": \"weights\"}, dummy_weights)\n",
        "    weights_files = [\"model_weights.pt\"]\n",
        "\n",
        "print(f\"Using weights file: {weights_files[0]}\")\n",
        "\n",
        "# Check context config\n",
        "context_dir = os.path.join(nemo_folder, \"context\")\n",
        "config_files = [f for f in os.listdir(context_dir) if f.endswith('.json')]\n",
        "\n",
        "if not config_files:\n",
        "    print(\"âš ï¸ No config file found in context/\")\n",
        "    print(\"Creating default config...\")\n",
        "    default_config = {\n",
        "        \"_target_\": \"nemo.collections.llm.gpt.model.GPTModel\",\n",
        "        \"config\": {\n",
        "            \"num_layers\": 32,\n",
        "            \"hidden_size\": 4096,\n",
        "            \"num_attention_heads\": 32,\n",
        "            \"vocab_size\": 32000\n",
        "        }\n",
        "    }\n",
        "    config_path = os.path.join(context_dir, \"io.json\")\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(default_config, f, indent=2)\n",
        "    config_files = [\"io.json\"]\n",
        "\n",
        "print(f\"Using config file: {config_files[0]}\")\n",
        "\n",
        "# ========== 4. CREATE TAR.GZ (.nemo FILE) ==========\n",
        "print(f\"\\nðŸŽ¯ Creating {output_nemo_file}...\")\n",
        "\n",
        "with tarfile.open(output_nemo_file, \"w:gz\") as tar:\n",
        "    # Add all files from the folder structure\n",
        "    for root, dirs, files in os.walk(nemo_folder):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            # Calculate archive path\n",
        "            arcname = os.path.join(\"model\", os.path.relpath(file_path, nemo_folder))\n",
        "\n",
        "            print(f\"Adding: {arcname}\")\n",
        "            tar.add(file_path, arcname=arcname)\n",
        "\n",
        "print(f\"âœ… Created {output_nemo_file}\")\n",
        "print(f\"âœ… File size: {os.path.getsize(output_nemo_file) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# ========== 5. VERIFY THE .nemo FILE ==========\n",
        "print(\"\\nðŸ” Verifying .nemo file...\")\n",
        "\n",
        "try:\n",
        "    with tarfile.open(output_nemo_file, \"r:gz\") as tar:\n",
        "        members = tar.getmembers()\n",
        "        print(f\"âœ… File contains {len(members)} items\")\n",
        "\n",
        "        print(\"\\nðŸ“ Contents:\")\n",
        "        for member in members[:10]:  # Show first 10\n",
        "            print(f\"  â€¢ {member.name} ({member.size} bytes)\")\n",
        "\n",
        "        if len(members) > 10:\n",
        "            print(f\"  ... and {len(members) - 10} more\")\n",
        "\n",
        "        # Check for essential files\n",
        "        essential_files = [\n",
        "            \"model/weights/\",\n",
        "            \"model/context/\",\n",
        "        ]\n",
        "\n",
        "        has_weights = any(\"weights\" in m.name and m.name.endswith('.pt') for m in members)\n",
        "        has_config = any(\"context\" in m.name and m.name.endswith('.json') for m in members)\n",
        "\n",
        "        print(f\"\\nâœ… Has weights file: {has_weights}\")\n",
        "        print(f\"âœ… Has config file: {has_config}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error verifying file: {e}\")\n",
        "\n",
        "# ========== 6. CLEANUP OPTION ==========\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Input folder: {os.path.abspath(nemo_folder)}\")\n",
        "print(f\"Output file: {os.path.abspath(output_nemo_file)}\")\n",
        "print(f\"File size: {os.path.getsize(output_nemo_file) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "cleanup = input(\"\\nDelete original folder? (y/n): \")\n",
        "if cleanup.lower() == 'y':\n",
        "    shutil.rmtree(nemo_folder)\n",
        "    print(f\"âœ… Deleted {nemo_folder}\")\n",
        "else:\n",
        "    print(f\"âœ… Kept original folder\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DONE! You now have a SINGLE .nemo file\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "YoI6Alg8mI2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}